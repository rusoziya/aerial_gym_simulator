Importing module 'gym_38' (/home/ziyar/aerialgym/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_38.so)
Setting GYM_USD_PLUG_INFO_PATH to /home/ziyar/aerialgym/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json
[36m[2025-07-01 18:03:36,492][110781] Queried available GPUs: 0
[37m[1m[2025-07-01 18:03:36,492][110781] Environment var CUDA_VISIBLE_DEVICES is 0
PyTorch version 1.13.1
Device count 1
/home/ziyar/aerialgym/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/_bindings/src/gymtorch
ninja: no work to do.
Warp 1.0.0-beta.5 initialized:
   CUDA Toolkit: 11.5, Driver: 12.4
   Devices:
     "cpu"    | x86_64
     "cuda:0" | NVIDIA GeForce RTX 4080 Laptop GPU (sm_89)
   Kernel cache: /home/ziyar/.cache/warp/1.0.0-beta.5
[SUBPROCESS] DCE task action_space_dim: 3
[SUBPROCESS] Target Sample Factory action space: 6D
[SUBPROCESS] Setting num_envs to 6 based on env_agents=6
[SUBPROCESS] Set SF_ENV_AGENTS=6 environment variable
[SUBPROCESS] DCE config batch_size: 1536
[SUBPROCESS] Using MODIFIED ARCHITECTURE (6 environments - inference compatible)
Registered quad_with_obstacles and dce_navigation_task in subprocess
[isaacgym:gymutil.py] Unknown args:  ['--env=quad_with_obstacles', '--train_for_env_steps=100000000', '--experiment=MEDIUM_CONFIG_6_ENVS_TEST_1536batch_FIXEDINPUT_V5', '--async_rl=True', '--use_env_info_cache=False', '--normalize_input=True']
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:0
GPU Pipeline: enabled
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/torch/utils/cpp_extension.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
  from pkg_resources import packaging  # type: ignore[attr-defined]
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
Using /home/ziyar/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Emitting ninja build file /home/ziyar/.cache/torch_extensions/py38_cu117/gymtorch/build.ninja...
Building extension module gymtorch...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module gymtorch...
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/networkx/classes/graph.py:23: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  from collections import Mapping
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/networkx/classes/reportviews.py:95: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  from collections import Mapping, Set, Iterable
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/networkx/readwrite/graphml.py:346: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.int, "int"), (np.int8, "int"),
/home/ziyar/aerialgym/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/torch_utils.py:135: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  def get_axis_params(value, axis_idx, x_value=0., dtype=np.float, n_dims=3):
[37m[1746 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task] - INFO : Found SF_ENV_AGENTS environment variable: 6 (dce_navigation_task.py:23)
[37m[1746 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task] - INFO : Detected env_agents=6 from environment - setting environment count. (dce_navigation_task.py:29)
[37m[1746 ms][base_task] - INFO : Setting seed: 2485806068 (base_task.py:38)
[37m[1746 ms][navigation_task] - INFO : Building environment for navigation task. (navigation_task.py:44)
[37m[1746 ms][navigation_task] - INFO : Sim Name: base_sim, Env Name: env_with_obstacles, Robot Name: lmf2, Controller Name: lmf2_velocity_control (navigation_task.py:45)
[37m[1746 ms][env_manager] - INFO : Populating environments. (env_manager.py:73)
[37m[1746 ms][env_manager] - INFO : Creating simulation instance. (env_manager.py:87)
[37m[1746 ms][env_manager] - INFO : Instantiating IGE object. (env_manager.py:88)
[37m[1746 ms][IsaacGymEnvManager] - INFO : Creating Isaac Gym Environment (IGE_env_manager.py:41)
[37m[1746 ms][IsaacGymEnvManager] - INFO : Acquiring gym object (IGE_env_manager.py:73)
[37m[1746 ms][IsaacGymEnvManager] - INFO : Acquired gym object (IGE_env_manager.py:75)
[37m[1747 ms][IsaacGymEnvManager] - INFO : Fixing devices (IGE_env_manager.py:89)
[37m[1747 ms][IsaacGymEnvManager] - INFO : Using GPU pipeline for simulation. (IGE_env_manager.py:102)
[37m[1747 ms][IsaacGymEnvManager] - INFO : Sim Device type: cuda, Sim Device ID: 0 (IGE_env_manager.py:105)
[31m[1747 ms][IsaacGymEnvManager] - CRITICAL : 
[31m Setting graphics device to -1.
[31m This is done because the simulation is run in headless mode and no Isaac Gym cameras are used.
[31m No need to worry. The simulation and warp rendering will work as expected. (IGE_env_manager.py:112)
[37m[1747 ms][IsaacGymEnvManager] - INFO : Graphics Device ID: -1 (IGE_env_manager.py:119)
[37m[1747 ms][IsaacGymEnvManager] - INFO : Creating Isaac Gym Simulation Object (IGE_env_manager.py:120)
[33m[1747 ms][IsaacGymEnvManager] - WARNING : If you have set the CUDA_VISIBLE_DEVICES environment variable, please ensure that you set it
[33mto a particular one that works for your system to use the viewer or Isaac Gym cameras.
[33mIf you want to run parallel simulations on multiple GPUs with camera sensors,
[33mplease disable Isaac Gym and use warp (by setting use_warp=True), set the viewer to headless. (IGE_env_manager.py:127)
[33m[1747 ms][IsaacGymEnvManager] - WARNING : If you see a segfault in the next lines, it is because of the discrepancy between the CUDA device and the graphics device.
[33mPlease ensure that the CUDA device and the graphics device are the same. (IGE_env_manager.py:132)
[37m[2542 ms][IsaacGymEnvManager] - INFO : Created Isaac Gym Simulation Object (IGE_env_manager.py:136)
[37m[2542 ms][IsaacGymEnvManager] - INFO : Created Isaac Gym Environment (IGE_env_manager.py:43)
*** Can't create empty tensor
WARNING: allocation matrix is not full rank. Rank: 4
creating render graph
Module warp.utils load on device 'cuda:0' took 1.35 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_camera_kernels load on device 'cuda:0' took 7.69 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_stereo_camera_kernels load on device 'cuda:0' took 11.92 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_lidar_kernels load on device 'cuda:0' took 5.56 ms
finishing capture of render graph
Encoder network initialized.
Defined encoder.
[ImgDecoder] Starting create_model
[ImgDecoder] Done with create_model
Defined decoder.
Loading weights from file:  /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/utils/vae/weights/ICRA_test_set_more_sim_data_kld_beta_3_LD_64_epoch_49.pth
[37m[2745 ms][env_manager] - INFO : IGE object instantiated. (env_manager.py:109)
[37m[2745 ms][env_manager] - INFO : Creating warp environment. (env_manager.py:112)
[37m[2745 ms][env_manager] - INFO : Warp environment created. (env_manager.py:114)
[37m[2745 ms][env_manager] - INFO : Creating robot manager. (env_manager.py:118)
[37m[2745 ms][BaseRobot] - INFO : [DONE] Initializing controller (base_robot.py:26)
[37m[2745 ms][BaseRobot] - INFO : Initializing controller lmf2_velocity_control (base_robot.py:29)
[33m[2745 ms][base_multirotor] - WARNING : Creating 6 multirotors. (base_multirotor.py:32)
[37m[2746 ms][env_manager] - INFO : [DONE] Creating robot manager. (env_manager.py:123)
[37m[2746 ms][env_manager] - INFO : [DONE] Creating simulation instance. (env_manager.py:125)
[37m[2746 ms][asset_loader] - INFO : Loading asset: model.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2746 ms][asset_loader] - INFO : Loading asset: panel.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2748 ms][asset_loader] - INFO : Loading asset: cuboidal_rod.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2749 ms][asset_loader] - INFO : Loading asset: 1_x_1_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2750 ms][asset_loader] - INFO : Loading asset: 0_5_x_0_5_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2750 ms][asset_loader] - INFO : Loading asset: gate.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2753 ms][asset_loader] - INFO : Loading asset: left_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2754 ms][asset_loader] - INFO : Loading asset: right_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2755 ms][asset_loader] - INFO : Loading asset: back_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2756 ms][asset_loader] - INFO : Loading asset: front_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2757 ms][asset_loader] - INFO : Loading asset: bottom_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2757 ms][asset_loader] - INFO : Loading asset: top_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2758 ms][asset_loader] - INFO : Loading asset: small_cube.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2760 ms][env_manager] - INFO : Populating environment 0 (env_manager.py:179)
[33m[3126 ms][robot_manager] - WARNING : 
[33mRobot mass: 1.2400000467896461,
[33mInertia: tensor([[0.0134, 0.0000, 0.0000],
[33m        [0.0000, 0.0144, 0.0000],
[33m        [0.0000, 0.0000, 0.0138]], device='cuda:0'),
[33mRobot COM: tensor([[0., 0., 0., 1.]], device='cuda:0') (robot_manager.py:437)
[33m[3126 ms][robot_manager] - WARNING : Calculated robot mass and inertia for this robot. This code assumes that your robot is the same across environments. (robot_manager.py:440)
[31m[3126 ms][robot_manager] - CRITICAL : If your robot differs across environments you need to perform this computation for each different robot here. (robot_manager.py:443)
[37m[3135 ms][env_manager] - INFO : [DONE] Populating environments. (env_manager.py:75)
[33m[3142 ms][IsaacGymEnvManager] - WARNING : Headless: True (IGE_env_manager.py:424)
[37m[3143 ms][IsaacGymEnvManager] - INFO : Headless mode. Viewer not created. (IGE_env_manager.py:434)
[33m[3187 ms][asset_manager] - WARNING : Number of obstacles to be kept in the environment: 9 (asset_manager.py:32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/control/motor_model.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(self.min_thrust, device=self.device, dtype=torch.float32).expand(
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/control/motor_model.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(self.max_thrust, device=self.device, dtype=torch.float32).expand(
[33m[3380 ms][control_allocation] - WARNING : Control allocation does not account for actuator limits. This leads to suboptimal allocation (control_allocation.py:48)
[37m[3380 ms][WarpSensor] - INFO : Camera sensor initialized (warp_sensor.py:50)
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.
  logger.warn(
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.
  logger.warn(
[36m[2025-07-01 18:03:40,635][110891] Env info: EnvInfo(obs_space=Dict('obs': Box(-inf, inf, (81,), float32)), action_space=Box(-1.0, 1.0, (6,), float32), num_agents=6, gpu_actions=True, gpu_observations=True, action_splits=None, all_discrete=None, frameskip=1, reward_shaping_scheme=None, env_info_protocol_version=1)
[33m[2025-07-01 18:03:41,308][110781] In serial mode all components run on the same process. Only use async_rl and serial mode together for debugging.
[36m[2025-07-01 18:03:41,308][110781] Starting experiment with the following configuration:
[36mhelp=False
[36malgo=APPO
[36menv=quad_with_obstacles
[36mexperiment=MEDIUM_CONFIG_6_ENVS_TEST_1536batch_FIXEDINPUT_V5
[36mtrain_dir=/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir
[36mrestart_behavior=resume
[36mdevice=gpu
[36mseed=None
[36mnum_policies=1
[36masync_rl=True
[36mserial_mode=True
[36mbatched_sampling=True
[36mnum_batches_to_accumulate=2
[36mworker_num_splits=1
[36mpolicy_workers_per_policy=1
[36mmax_policy_lag=1000
[36mnum_workers=1
[36mnum_envs_per_worker=1
[36mbatch_size=1536
[36mnum_batches_per_epoch=4
[36mnum_epochs=4
[36mrollout=32
[36mrecurrence=32
[36mshuffle_minibatches=False
[36mgamma=0.98
[36mreward_scale=0.1
[36mreward_clip=1000.0
[36mvalue_bootstrap=True
[36mnormalize_returns=True
[36mexploration_loss_coeff=0.001
[36mvalue_loss_coeff=2.0
[36mkl_loss_coeff=0.1
[36mexploration_loss=entropy
[36mgae_lambda=0.95
[36mppo_clip_ratio=0.2
[36mppo_clip_value=1.0
[36mwith_vtrace=False
[36mvtrace_rho=1.0
[36mvtrace_c=1.0
[36moptimizer=adam
[36madam_eps=1e-06
[36madam_beta1=0.9
[36madam_beta2=0.999
[36mmax_grad_norm=1.0
[36mlearning_rate=0.0003
[36mlr_schedule=kl_adaptive_epoch
[36mlr_schedule_kl_threshold=0.016
[36mlr_adaptive_min=1e-06
[36mlr_adaptive_max=0.01
[36mobs_subtract_mean=0.0
[36mobs_scale=1.0
[36mnormalize_input=True
[36mnormalize_input_keys=None
[36mdecorrelate_experience_max_seconds=0
[36mdecorrelate_envs_on_one_worker=True
[36mactor_worker_gpus=[0]
[36mset_workers_cpu_affinity=True
[36mforce_envs_single_thread=False
[36mdefault_niceness=0
[36mlog_to_file=True
[36mexperiment_summaries_interval=10
[36mflush_summaries_interval=30
[36mstats_avg=100
[36msummaries_use_frameskip=True
[36mheartbeat_interval=20
[36mheartbeat_reporting_interval=180
[36mtrain_for_env_steps=100000000
[36mtrain_for_seconds=10000000000
[36msave_every_sec=120
[36mkeep_checkpoints=2
[36mload_checkpoint_kind=latest
[36msave_milestones_sec=-1
[36msave_best_every_sec=5
[36msave_best_metric=reward
[36msave_best_after=5000000
[36mbenchmark=False
[36mencoder_mlp_layers=[512, 256, 64]
[36mencoder_conv_architecture=convnet_simple
[36mencoder_conv_mlp_layers=[]
[36muse_rnn=True
[36mrnn_size=64
[36mrnn_type=gru
[36mrnn_num_layers=1
[36mdecoder_mlp_layers=[]
[36mnonlinearity=elu
[36mpolicy_initialization=torch_default
[36mpolicy_init_gain=1.0
[36mactor_critic_share_weights=True
[36madaptive_stddev=True
[36mcontinuous_tanh_scale=0.0
[36minitial_stddev=1.0
[36muse_env_info_cache=False
[36menv_gpu_actions=True
[36menv_gpu_observations=True
[36menv_frameskip=1
[36menv_framestack=1
[36mpixel_format=CHW
[36muse_record_episode_statistics=False
[36mwith_wandb=True
[36mwandb_user=ziya-ruso-ucl
[36mwandb_project=vae_rl_navigation
[36mwandb_group=dce_navigation_training
[36mwandb_job_type=SF
[36mwandb_tags=['aerial_gym', 'dce', 'navigation', 'sample_factory']
[36mwith_pbt=False
[36mpbt_mix_policies_in_one_env=True
[36mpbt_period_env_steps=5000000
[36mpbt_start_mutation=20000000
[36mpbt_replace_fraction=0.3
[36mpbt_mutation_rate=0.15
[36mpbt_replace_reward_gap=0.1
[36mpbt_replace_reward_gap_absolute=1e-06
[36mpbt_optimize_gamma=False
[36mpbt_target_objective=true_objective
[36mpbt_perturb_min=1.1
[36mpbt_perturb_max=1.5
[36menv_agents=6
[36mobs_key=obs
[36msubtask=None
[36mige_api_version=preview4
[36meval_stats=False
[36maction_space_dim=6
[36mcommand_line=--env=quad_with_obstacles --train_for_env_steps=100000000 --experiment=MEDIUM_CONFIG_6_ENVS_TEST_1536batch_FIXEDINPUT_V5 --async_rl=True --use_env_info_cache=False --normalize_input=True
[36mcli_args={'env': 'quad_with_obstacles', 'experiment': 'MEDIUM_CONFIG_6_ENVS_TEST_1536batch_FIXEDINPUT_V5', 'async_rl': True, 'normalize_input': True, 'train_for_env_steps': 100000000, 'use_env_info_cache': False}
[36mgit_hash=7f35eed17f2afcde33e3a7aec669b48e9e8e34cd
[36mgit_repo_name=https://github.com/ntnu-arl/aerial_gym_simulator.git
[36mwandb_unique_id=MEDIUM_CONFIG_6_ENVS_TEST_1536batch_FIXEDINPUT_V5_20250701_180333_386689
[36m[2025-07-01 18:03:41,309][110781] Saving configuration to /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/MEDIUM_CONFIG_6_ENVS_TEST_1536batch_FIXEDINPUT_V5/config.json...
[isaacgym:gymutil.py] Unknown args:  ['--env=quad_with_obstacles', '--train_for_env_steps=100000000', '--experiment=MEDIUM_CONFIG_6_ENVS_TEST_1536batch_FIXEDINPUT_V5', '--async_rl=True', '--use_env_info_cache=False', '--normalize_input=True']
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:0
GPU Pipeline: enabled
*** Can't create empty tensor
WARNING: allocation matrix is not full rank. Rank: 4
[36m[2025-07-01 18:03:41,507][110781] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[36m[2025-07-01 18:03:41,508][110781] Rollout worker 0 uses device cuda:0
[36m[2025-07-01 18:03:41,521][110781] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[37m[1m[2025-07-01 18:03:41,522][110781] InferenceWorker_p0-w0: min num requests: 1
[36m[2025-07-01 18:03:41,522][110781] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[37m[1m[2025-07-01 18:03:41,523][110781] Starting seed is not provided
[36m[2025-07-01 18:03:41,524][110781] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[36m[2025-07-01 18:03:41,524][110781] Initializing actor-critic model on device cuda:0
[36m[2025-07-01 18:03:41,524][110781] RunningMeanStd input shape: (81,)
[36m[2025-07-01 18:03:41,525][110781] RunningMeanStd input shape: (1,)
[36m[2025-07-01 18:03:41,551][110781] Created Actor Critic model with architecture:
[36m[2025-07-01 18:03:41,551][110781] ActorCriticSharedWeights(
[36m  (obs_normalizer): ObservationNormalizer(
[36m    (running_mean_std): RunningMeanStdDictInPlace(
[36m      (running_mean_std): ModuleDict(
[36m        (obs): RunningMeanStdInPlace()
[36m      )
[36m    )
[36m  )
[36m  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)
[36m  (encoder): MultiInputEncoder(
[36m    (encoders): ModuleDict(
[36m      (obs): MlpEncoder(
[36m        (mlp_head): RecursiveScriptModule(
[36m          original_name=Sequential
[36m          (0): RecursiveScriptModule(original_name=Linear)
[36m          (1): RecursiveScriptModule(original_name=ELU)
[36m          (2): RecursiveScriptModule(original_name=Linear)
[36m          (3): RecursiveScriptModule(original_name=ELU)
[36m          (4): RecursiveScriptModule(original_name=Linear)
[36m          (5): RecursiveScriptModule(original_name=ELU)
[36m        )
[36m      )
[36m    )
[36m  )
[36m  (core): ModelCoreRNN(
[36m    (core): GRU(64, 64)
[36m  )
[36m  (decoder): MlpDecoder(
[36m    (mlp): Identity()
[36m  )
[36m  (critic_linear): Linear(in_features=64, out_features=1, bias=True)
[36m  (action_parameterization): ActionParameterizationDefault(
[36m    (distribution_linear): Linear(in_features=64, out_features=12, bias=True)
[36m  )
[36m)
[36m[2025-07-01 18:03:41,955][110781] Using optimizer <class 'torch.optim.adam.Adam'>
[33m[2025-07-01 18:03:41,956][110781] No checkpoints found
[36m[2025-07-01 18:03:41,956][110781] Did not load from checkpoint, starting from scratch!
[36m[2025-07-01 18:03:41,956][110781] Initialized policy 0 weights for model version 0
[36m[2025-07-01 18:03:41,956][110781] LearnerWorker_p0 finished initialization!
[36m[2025-07-01 18:03:41,957][110781] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[37m[1m[2025-07-01 18:03:41,961][110781] Inference worker 0-0 is ready!
[37m[1m[2025-07-01 18:03:41,961][110781] All inference workers are ready! Signal rollout workers to start!
[36m[2025-07-01 18:03:41,961][110781] EnvRunner 0-0 uses policy 0
[37m[10531 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task] - INFO : Found SF_ENV_AGENTS environment variable: 6 (dce_navigation_task.py:23)
[37m[10531 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task] - INFO : Detected env_agents=6 from environment - setting environment count. (dce_navigation_task.py:29)
[37m[10531 ms][base_task] - INFO : Setting seed: 1735264644 (base_task.py:38)
[37m[10531 ms][navigation_task] - INFO : Building environment for navigation task. (navigation_task.py:44)
[37m[10531 ms][navigation_task] - INFO : Sim Name: base_sim, Env Name: env_with_obstacles, Robot Name: lmf2, Controller Name: lmf2_velocity_control (navigation_task.py:45)
[37m[10531 ms][env_manager] - INFO : Populating environments. (env_manager.py:73)
[37m[10531 ms][env_manager] - INFO : Creating simulation instance. (env_manager.py:87)
[37m[10531 ms][env_manager] - INFO : Instantiating IGE object. (env_manager.py:88)
[37m[10531 ms][IsaacGymEnvManager] - INFO : Creating Isaac Gym Environment (IGE_env_manager.py:41)
[37m[10531 ms][IsaacGymEnvManager] - INFO : Acquiring gym object (IGE_env_manager.py:73)
[37m[10531 ms][IsaacGymEnvManager] - INFO : Acquired gym object (IGE_env_manager.py:75)
[37m[10532 ms][IsaacGymEnvManager] - INFO : Fixing devices (IGE_env_manager.py:89)
[37m[10532 ms][IsaacGymEnvManager] - INFO : Using GPU pipeline for simulation. (IGE_env_manager.py:102)
[37m[10532 ms][IsaacGymEnvManager] - INFO : Sim Device type: cuda, Sim Device ID: 0 (IGE_env_manager.py:105)
[31m[10532 ms][IsaacGymEnvManager] - CRITICAL : 
[31m Setting graphics device to -1.
[31m This is done because the simulation is run in headless mode and no Isaac Gym cameras are used.
[31m No need to worry. The simulation and warp rendering will work as expected. (IGE_env_manager.py:112)
[37m[10532 ms][IsaacGymEnvManager] - INFO : Graphics Device ID: -1 (IGE_env_manager.py:119)
[37m[10532 ms][IsaacGymEnvManager] - INFO : Creating Isaac Gym Simulation Object (IGE_env_manager.py:120)
[33m[10532 ms][IsaacGymEnvManager] - WARNING : If you have set the CUDA_VISIBLE_DEVICES environment variable, please ensure that you set it
[33mto a particular one that works for your system to use the viewer or Isaac Gym cameras.
[33mIf you want to run parallel simulations on multiple GPUs with camera sensors,
[33mplease disable Isaac Gym and use warp (by setting use_warp=True), set the viewer to headless. (IGE_env_manager.py:127)
[33m[10532 ms][IsaacGymEnvManager] - WARNING : If you see a segfault in the next lines, it is because of the discrepancy between the CUDA device and the graphics device.
[33mPlease ensure that the CUDA device and the graphics device are the same. (IGE_env_manager.py:132)
[37m[11335 ms][IsaacGymEnvManager] - INFO : Created Isaac Gym Simulation Object (IGE_env_manager.py:136)
[37m[11335 ms][IsaacGymEnvManager] - INFO : Created Isaac Gym Environment (IGE_env_manager.py:43)
[37m[11537 ms][env_manager] - INFO : IGE object instantiated. (env_manager.py:109)
[37m[11537 ms][env_manager] - INFO : Creating warp environment. (env_manager.py:112)
[37m[11537 ms][env_manager] - INFO : Warp environment created. (env_manager.py:114)
[37m[11537 ms][env_manager] - INFO : Creating robot manager. (env_manager.py:118)
[37m[11537 ms][BaseRobot] - INFO : [DONE] Initializing controller (base_robot.py:26)
[37m[11537 ms][BaseRobot] - INFO : Initializing controller lmf2_velocity_control (base_robot.py:29)
[33m[11537 ms][base_multirotor] - WARNING : Creating 6 multirotors. (base_multirotor.py:32)
[37m[11537 ms][env_manager] - INFO : [DONE] Creating robot manager. (env_manager.py:123)
[37m[11537 ms][env_manager] - INFO : [DONE] Creating simulation instance. (env_manager.py:125)
[37m[11537 ms][asset_loader] - INFO : Loading asset: model.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[11538 ms][asset_loader] - INFO : Loading asset: panel.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[11541 ms][asset_loader] - INFO : Loading asset: cuboidal_rod.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[11542 ms][asset_loader] - INFO : Loading asset: 1_x_1_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[11543 ms][asset_loader] - INFO : Loading asset: small_cube.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[11544 ms][asset_loader] - INFO : Loading asset: 0_5_x_0_5_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[11545 ms][asset_loader] - INFO : Loading asset: left_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[11546 ms][asset_loader] - INFO : Loading asset: right_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[11547 ms][asset_loader] - INFO : Loading asset: back_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[11548 ms][asset_loader] - INFO : Loading asset: front_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[11548 ms][asset_loader] - INFO : Loading asset: bottom_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[11549 ms][asset_loader] - INFO : Loading asset: top_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[11550 ms][asset_loader] - INFO : Loading asset: gate.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[11553 ms][env_manager] - INFO : Populating environment 0 (env_manager.py:179)
[33m[11569 ms][robot_manager] - WARNING : 
[33mRobot mass: 1.2400000467896461,
[33mInertia: tensor([[0.0134, 0.0000, 0.0000],
[33m        [0.0000, 0.0144, 0.0000],
[33m        [0.0000, 0.0000, 0.0138]], device='cuda:0'),
[33mRobot COM: tensor([[0., 0., 0., 1.]], device='cuda:0') (robot_manager.py:437)
[33m[11570 ms][robot_manager] - WARNING : Calculated robot mass and inertia for this robot. This code assumes that your robot is the same across environments. (robot_manager.py:440)
[31m[11570 ms][robot_manager] - CRITICAL : If your robot differs across environments you need to perform this computation for each different robot here. (robot_manager.py:443)
[37m[11579 ms][env_manager] - INFO : [DONE] Populating environments. (env_manager.py:75)
[33m[11585 ms][IsaacGymEnvManager] - WARNING : Headless: True (IGE_env_manager.py:424)
[37m[11585 ms][IsaacGymEnvManager] - INFO : Headless mode. Viewer not created. (IGE_env_manager.py:434)
[33m[11627 ms][asset_manager] - WARNING : Number of obstacles to be kept in the environment: 9 (asset_manager.py:32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/control/motor_model.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(self.min_thrust, device=self.device, dtype=torch.float32).expand(
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/control/motor_model.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(self.max_thrust, device=self.device, dtype=torch.float32).expand(
[33m[11816 ms][control_allocation] - WARNING : Control allocation does not account for actuator limits. This leads to suboptimal allocation (control_allocation.py:48)
[37m[11817 ms][WarpSensor] - INFO : Camera sensor initialized (warp_sensor.py:50)
creating render graph
Module warp.utils load on device 'cuda:0' took 1.64 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_camera_kernels load on device 'cuda:0' took 8.40 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_stereo_camera_kernels load on device 'cuda:0' took 11.33 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_lidar_kernels load on device 'cuda:0' took 5.58 ms
finishing capture of render graph
Encoder network initialized.
Defined encoder.
[ImgDecoder] Starting create_model
[ImgDecoder] Done with create_model
Defined decoder.
Loading weights from file:  /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/utils/vae/weights/ICRA_test_set_more_sim_data_kld_beta_3_LD_64_epoch_49.pth
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.
  logger.warn(
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.
  logger.warn(
[31m[14300 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14300 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0, 1, 2, 3, 4, 5], device='cuda:0') (navigation_task.py:196)
[31m[14300 ms][navigation_task] - CRITICAL : Time at crash: tensor([1, 1, 1, 1, 1, 1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 18:03:46,658][110781] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 0. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 18:03:46,659][110781] Avg episode reward: [(0, '-100.000')]
[31m[15633 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15634 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[15634 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[17511 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[17511 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[17511 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 18:03:50,857][110781] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 21.4. Samples: 90. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 18:03:50,858][110781] Avg episode reward: [(0, '-91.187')]
[36m[2025-07-01 18:03:55,888][110781] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 103.4. Samples: 954. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 18:03:55,889][110781] Avg episode reward: [(0, '-95.151')]
[36m[2025-07-01 18:04:00,863][110781] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 128.0. Samples: 1818. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 18:04:00,864][110781] Avg episode reward: [(0, '-96.225')]
[37m[1m[2025-07-01 18:04:01,587][110781] Heartbeat connected on Batcher_0
[37m[1m[2025-07-01 18:04:01,587][110781] Heartbeat connected on LearnerWorker_p0
[37m[1m[2025-07-01 18:04:01,587][110781] Heartbeat connected on InferenceWorker_p0-w0
[37m[1m[2025-07-01 18:04:01,587][110781] Heartbeat connected on RolloutWorker_w0
[36m[2025-07-01 18:04:05,894][110781] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 117.3. Samples: 2256. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 18:04:05,895][110781] Avg episode reward: [(0, '-97.587')]
[31m[37404 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[37404 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[37405 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 18:04:10,876][110781] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 128.6. Samples: 3114. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 18:04:10,877][110781] Avg episode reward: [(0, '-98.369')]
[36m[2025-07-01 18:04:15,882][110781] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 136.3. Samples: 3984. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 18:04:15,883][110781] Avg episode reward: [(0, '-97.828')]
[36m[2025-07-01 18:04:20,849][110781] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 129.5. Samples: 4428. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 18:04:20,850][110781] Avg episode reward: [(0, '-98.566')]
[31m[51104 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[51105 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[51105 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 18:04:25,871][110781] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 136.5. Samples: 5352. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 18:04:25,871][110781] Avg episode reward: [(0, '-97.385')]
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/torch/nn/modules/module.py:1194: UserWarning: operator() profile_node %104 : int[] = prim::profile_ivalue(%102)
 does not have profile information (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
[36m[2025-07-01 18:04:30,884][110781] Fps is (10 sec: 612.3, 60 sec: 138.9, 300 sec: 138.9). Total num frames: 6144. Throughput: 0: 140.3. Samples: 6204. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:04:30,885][110781] Avg episode reward: [(0, '-95.875')]
[36m[2025-07-01 18:04:35,883][110781] Fps is (10 sec: 613.7, 60 sec: 124.8, 300 sec: 124.8). Total num frames: 6144. Throughput: 0: 146.2. Samples: 6672. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:04:35,883][110781] Avg episode reward: [(0, '-97.107')]
[31m[68937 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[68937 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[68937 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 18:04:40,877][110781] Fps is (10 sec: 0.0, 60 sec: 113.3, 300 sec: 113.3). Total num frames: 6144. Throughput: 0: 146.8. Samples: 7560. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:04:40,877][110781] Avg episode reward: [(0, '-98.838')]
[36m[2025-07-01 18:04:45,868][110781] Fps is (10 sec: 0.0, 60 sec: 103.8, 300 sec: 103.8). Total num frames: 6144. Throughput: 0: 146.0. Samples: 8388. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:04:45,868][110781] Avg episode reward: [(0, '-98.712')]
[36m[2025-07-01 18:04:50,869][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 95.7). Total num frames: 6144. Throughput: 0: 146.5. Samples: 8844. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:04:50,869][110781] Avg episode reward: [(0, '-100.433')]
[31m[83032 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[83032 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[83032 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 18:04:55,883][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 88.8). Total num frames: 6144. Throughput: 0: 147.0. Samples: 9732. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:04:55,883][110781] Avg episode reward: [(0, '-99.726')]
[36m[2025-07-01 18:05:00,860][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 82.8). Total num frames: 6144. Throughput: 0: 146.1. Samples: 10554. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:05:00,860][110781] Avg episode reward: [(0, '-98.156')]
[36m[2025-07-01 18:05:05,890][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 77.5). Total num frames: 6144. Throughput: 0: 146.4. Samples: 11022. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:05:05,890][110781] Avg episode reward: [(0, '-98.417')]
[36m[2025-07-01 18:05:10,849][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 73.0). Total num frames: 6144. Throughput: 0: 145.4. Samples: 11892. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:05:10,850][110781] Avg episode reward: [(0, '-98.345')]
[36m[2025-07-01 18:05:15,856][110781] Fps is (10 sec: 616.5, 60 sec: 204.9, 300 sec: 137.8). Total num frames: 12288. Throughput: 0: 144.6. Samples: 12708. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:05:15,856][110781] Avg episode reward: [(0, '-97.982')]
[36m[2025-07-01 18:05:20,888][110781] Fps is (10 sec: 612.0, 60 sec: 204.7, 300 sec: 130.4). Total num frames: 12288. Throughput: 0: 143.3. Samples: 13122. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:05:20,889][110781] Avg episode reward: [(0, '-97.253')]
[36m[2025-07-01 18:05:25,854][110781] Fps is (10 sec: 0.0, 60 sec: 204.9, 300 sec: 123.9). Total num frames: 12288. Throughput: 0: 143.7. Samples: 14022. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:05:25,854][110781] Avg episode reward: [(0, '-97.979')]
[36m[2025-07-01 18:05:30,865][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 117.9). Total num frames: 12288. Throughput: 0: 145.2. Samples: 14922. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:05:30,865][110781] Avg episode reward: [(0, '-99.452')]
[36m[2025-07-01 18:05:35,894][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 112.5). Total num frames: 12288. Throughput: 0: 144.3. Samples: 15342. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:05:35,895][110781] Avg episode reward: [(0, '-98.875')]
[37m[1m[2025-07-01 18:05:35,939][110781] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/MEDIUM_CONFIG_6_ENVS_TEST_1536batch_FIXEDINPUT_V5/checkpoint_p0/checkpoint_000000032_12288.pth...
[36m[2025-07-01 18:05:40,885][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 107.6). Total num frames: 12288. Throughput: 0: 143.3. Samples: 16182. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:05:40,886][110781] Avg episode reward: [(0, '-97.401')]
[36m[2025-07-01 18:05:45,858][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 103.1). Total num frames: 12288. Throughput: 0: 143.7. Samples: 17022. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:05:45,858][110781] Avg episode reward: [(0, '-98.272')]
[36m[2025-07-01 18:05:50,855][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 98.9). Total num frames: 12288. Throughput: 0: 143.4. Samples: 17472. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:05:50,855][110781] Avg episode reward: [(0, '-98.739')]
[36m[2025-07-01 18:05:55,874][110781] Fps is (10 sec: 613.4, 60 sec: 204.8, 300 sec: 142.6). Total num frames: 18432. Throughput: 0: 143.8. Samples: 18366. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-07-01 18:05:55,875][110781] Avg episode reward: [(0, '-98.367')]
[36m[2025-07-01 18:06:00,870][110781] Fps is (10 sec: 613.5, 60 sec: 204.8, 300 sec: 137.3). Total num frames: 18432. Throughput: 0: 145.2. Samples: 19242. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-07-01 18:06:00,870][110781] Avg episode reward: [(0, '-99.830')]
[36m[2025-07-01 18:06:05,855][110781] Fps is (10 sec: 0.0, 60 sec: 204.9, 300 sec: 132.4). Total num frames: 18432. Throughput: 0: 146.4. Samples: 19704. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-07-01 18:06:05,856][110781] Avg episode reward: [(0, '-98.245')]
[36m[2025-07-01 18:06:10,884][110781] Fps is (10 sec: 0.0, 60 sec: 204.7, 300 sec: 127.8). Total num frames: 18432. Throughput: 0: 143.5. Samples: 20484. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-07-01 18:06:10,884][110781] Avg episode reward: [(0, '-97.366')]
[36m[2025-07-01 18:06:15,854][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 123.5). Total num frames: 18432. Throughput: 0: 142.7. Samples: 21342. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-07-01 18:06:15,854][110781] Avg episode reward: [(0, '-98.060')]
[36m[2025-07-01 18:06:20,863][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 119.5). Total num frames: 18432. Throughput: 0: 143.4. Samples: 21792. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-07-01 18:06:20,863][110781] Avg episode reward: [(0, '-97.113')]
[36m[2025-07-01 18:06:25,853][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 115.8). Total num frames: 18432. Throughput: 0: 144.9. Samples: 22698. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-07-01 18:06:25,853][110781] Avg episode reward: [(0, '-98.389')]
[36m[2025-07-01 18:06:30,866][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 112.2). Total num frames: 18432. Throughput: 0: 145.0. Samples: 23550. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-07-01 18:06:30,866][110781] Avg episode reward: [(0, '-98.376')]
[36m[2025-07-01 18:06:35,864][110781] Fps is (10 sec: 0.0, 60 sec: 102.5, 300 sec: 108.9). Total num frames: 18432. Throughput: 0: 144.9. Samples: 23994. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-07-01 18:06:35,864][110781] Avg episode reward: [(0, '-97.088')]
[36m[2025-07-01 18:06:40,887][110781] Fps is (10 sec: 613.1, 60 sec: 204.8, 300 sec: 141.1). Total num frames: 24576. Throughput: 0: 144.1. Samples: 24852. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 18:06:40,887][110781] Avg episode reward: [(0, '-96.736')]
[36m[2025-07-01 18:06:45,864][110781] Fps is (10 sec: 614.4, 60 sec: 204.8, 300 sec: 137.1). Total num frames: 24576. Throughput: 0: 143.9. Samples: 25716. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 18:06:45,865][110781] Avg episode reward: [(0, '-96.057')]
[36m[2025-07-01 18:06:50,887][110781] Fps is (10 sec: 0.0, 60 sec: 204.7, 300 sec: 133.4). Total num frames: 24576. Throughput: 0: 143.1. Samples: 26148. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 18:06:50,888][110781] Avg episode reward: [(0, '-98.422')]
[36m[2025-07-01 18:06:55,850][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 129.9). Total num frames: 24576. Throughput: 0: 145.2. Samples: 27012. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 18:06:55,850][110781] Avg episode reward: [(0, '-96.598')]
[36m[2025-07-01 18:07:00,872][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 126.5). Total num frames: 24576. Throughput: 0: 146.2. Samples: 27924. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 18:07:00,873][110781] Avg episode reward: [(0, '-97.993')]
[36m[2025-07-01 18:07:05,859][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 123.4). Total num frames: 24576. Throughput: 0: 146.1. Samples: 28368. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 18:07:05,860][110781] Avg episode reward: [(0, '-98.109')]
[36m[2025-07-01 18:07:10,878][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 120.3). Total num frames: 24576. Throughput: 0: 145.8. Samples: 29262. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 18:07:10,878][110781] Avg episode reward: [(0, '-98.118')]
[36m[2025-07-01 18:07:15,881][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 117.5). Total num frames: 24576. Throughput: 0: 146.6. Samples: 30150. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 18:07:15,881][110781] Avg episode reward: [(0, '-97.299')]
[36m[2025-07-01 18:07:20,860][110781] Fps is (10 sec: 615.5, 60 sec: 204.8, 300 sec: 143.4). Total num frames: 30720. Throughput: 0: 146.0. Samples: 30564. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:07:20,861][110781] Avg episode reward: [(0, '-95.609')]
[36m[2025-07-01 18:07:25,873][110781] Fps is (10 sec: 614.9, 60 sec: 204.7, 300 sec: 140.1). Total num frames: 30720. Throughput: 0: 146.6. Samples: 31446. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:07:25,874][110781] Avg episode reward: [(0, '-95.372')]
[36m[2025-07-01 18:07:30,879][110781] Fps is (10 sec: 0.0, 60 sec: 204.8, 300 sec: 137.0). Total num frames: 30720. Throughput: 0: 146.8. Samples: 32322. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:07:30,880][110781] Avg episode reward: [(0, '-98.352')]
[36m[2025-07-01 18:07:35,858][110781] Fps is (10 sec: 0.0, 60 sec: 204.8, 300 sec: 134.0). Total num frames: 30720. Throughput: 0: 147.7. Samples: 32790. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:07:35,859][110781] Avg episode reward: [(0, '-96.768')]
[37m[1m[2025-07-01 18:07:35,900][110781] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/MEDIUM_CONFIG_6_ENVS_TEST_1536batch_FIXEDINPUT_V5/checkpoint_p0/checkpoint_000000080_30720.pth...
[36m[2025-07-01 18:07:40,873][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 131.2). Total num frames: 30720. Throughput: 0: 146.3. Samples: 33600. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:07:40,873][110781] Avg episode reward: [(0, '-96.589')]
[36m[2025-07-01 18:07:45,847][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 128.4). Total num frames: 30720. Throughput: 0: 145.1. Samples: 34452. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:07:45,847][110781] Avg episode reward: [(0, '-98.327')]
[36m[2025-07-01 18:07:50,872][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 125.8). Total num frames: 30720. Throughput: 0: 145.0. Samples: 34896. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:07:50,872][110781] Avg episode reward: [(0, '-98.496')]
[36m[2025-07-01 18:07:55,872][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 123.3). Total num frames: 30720. Throughput: 0: 146.0. Samples: 35832. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:07:55,872][110781] Avg episode reward: [(0, '-97.566')]
[36m[2025-07-01 18:08:00,860][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 120.8). Total num frames: 30720. Throughput: 0: 146.5. Samples: 36738. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:08:00,860][110781] Avg episode reward: [(0, '-98.186')]
[36m[2025-07-01 18:08:05,871][110781] Fps is (10 sec: 614.4, 60 sec: 204.8, 300 sec: 142.2). Total num frames: 36864. Throughput: 0: 146.4. Samples: 37152. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 18:08:05,871][110781] Avg episode reward: [(0, '-98.340')]
[36m[2025-07-01 18:08:10,876][110781] Fps is (10 sec: 613.4, 60 sec: 204.8, 300 sec: 139.5). Total num frames: 36864. Throughput: 0: 146.4. Samples: 38034. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 18:08:10,876][110781] Avg episode reward: [(0, '-95.268')]
[36m[2025-07-01 18:08:15,874][110781] Fps is (10 sec: 0.0, 60 sec: 204.8, 300 sec: 136.9). Total num frames: 36864. Throughput: 0: 147.4. Samples: 38952. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 18:08:15,874][110781] Avg episode reward: [(0, '-97.052')]
[36m[2025-07-01 18:08:20,848][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 134.4). Total num frames: 36864. Throughput: 0: 147.0. Samples: 39402. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 18:08:20,848][110781] Avg episode reward: [(0, '-93.923')]
[36m[2025-07-01 18:08:25,858][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 132.0). Total num frames: 36864. Throughput: 0: 149.5. Samples: 40326. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 18:08:25,858][110781] Avg episode reward: [(0, '-98.104')]
[36m[2025-07-01 18:08:30,861][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 129.7). Total num frames: 36864. Throughput: 0: 150.9. Samples: 41244. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 18:08:30,862][110781] Avg episode reward: [(0, '-96.162')]
[36m[2025-07-01 18:08:35,869][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 127.5). Total num frames: 36864. Throughput: 0: 150.0. Samples: 41646. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 18:08:35,869][110781] Avg episode reward: [(0, '-97.247')]
[36m[2025-07-01 18:08:40,862][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 125.3). Total num frames: 36864. Throughput: 0: 148.3. Samples: 42504. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 18:08:40,862][110781] Avg episode reward: [(0, '-95.933')]
[36m[2025-07-01 18:08:45,865][110781] Fps is (10 sec: 614.6, 60 sec: 204.7, 300 sec: 145.8). Total num frames: 43008. Throughput: 0: 146.8. Samples: 43344. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:08:45,865][110781] Avg episode reward: [(0, '-96.434')]
[36m[2025-07-01 18:08:50,847][110781] Fps is (10 sec: 615.3, 60 sec: 204.9, 300 sec: 145.8). Total num frames: 43008. Throughput: 0: 147.7. Samples: 43794. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:08:50,847][110781] Avg episode reward: [(0, '-93.369')]
[36m[2025-07-01 18:08:55,856][110781] Fps is (10 sec: 0.0, 60 sec: 204.9, 300 sec: 145.8). Total num frames: 43008. Throughput: 0: 148.2. Samples: 44700. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:08:55,856][110781] Avg episode reward: [(0, '-93.289')]
[36m[2025-07-01 18:09:00,870][110781] Fps is (10 sec: 0.0, 60 sec: 204.8, 300 sec: 145.8). Total num frames: 43008. Throughput: 0: 147.2. Samples: 45576. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:09:00,870][110781] Avg episode reward: [(0, '-94.021')]
[36m[2025-07-01 18:09:05,869][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 43008. Throughput: 0: 147.0. Samples: 46020. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:09:05,869][110781] Avg episode reward: [(0, '-94.540')]
[36m[2025-07-01 18:09:10,860][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 43008. Throughput: 0: 146.7. Samples: 46926. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:09:10,860][110781] Avg episode reward: [(0, '-95.181')]
[36m[2025-07-01 18:09:15,863][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 43008. Throughput: 0: 145.7. Samples: 47802. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:09:15,863][110781] Avg episode reward: [(0, '-93.784')]
[36m[2025-07-01 18:09:20,847][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 43008. Throughput: 0: 146.2. Samples: 48222. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:09:20,848][110781] Avg episode reward: [(0, '-95.747')]
[36m[2025-07-01 18:09:25,866][110781] Fps is (10 sec: 614.2, 60 sec: 204.8, 300 sec: 145.8). Total num frames: 49152. Throughput: 0: 146.9. Samples: 49116. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 18:09:25,866][110781] Avg episode reward: [(0, '-95.773')]
[36m[2025-07-01 18:09:30,848][110781] Fps is (10 sec: 614.4, 60 sec: 204.8, 300 sec: 145.8). Total num frames: 49152. Throughput: 0: 148.5. Samples: 50022. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 18:09:30,848][110781] Avg episode reward: [(0, '-94.220')]
[36m[2025-07-01 18:09:35,862][110781] Fps is (10 sec: 0.0, 60 sec: 204.8, 300 sec: 145.8). Total num frames: 49152. Throughput: 0: 148.5. Samples: 50478. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 18:09:35,862][110781] Avg episode reward: [(0, '-97.753')]
[37m[1m[2025-07-01 18:09:35,906][110781] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/MEDIUM_CONFIG_6_ENVS_TEST_1536batch_FIXEDINPUT_V5/checkpoint_p0/checkpoint_000000128_49152.pth...
[36m[2025-07-01 18:09:35,910][110781] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/MEDIUM_CONFIG_6_ENVS_TEST_1536batch_FIXEDINPUT_V5/checkpoint_p0/checkpoint_000000032_12288.pth
[36m[2025-07-01 18:09:40,856][110781] Fps is (10 sec: 0.0, 60 sec: 204.8, 300 sec: 145.8). Total num frames: 49152. Throughput: 0: 148.4. Samples: 51378. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 18:09:40,856][110781] Avg episode reward: [(0, '-96.447')]
[36m[2025-07-01 18:09:45,847][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 49152. Throughput: 0: 149.3. Samples: 52290. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 18:09:45,847][110781] Avg episode reward: [(0, '-91.625')]
[36m[2025-07-01 18:09:50,879][110781] Fps is (10 sec: 0.0, 60 sec: 102.3, 300 sec: 145.8). Total num frames: 49152. Throughput: 0: 148.6. Samples: 52710. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 18:09:50,879][110781] Avg episode reward: [(0, '-91.952')]
[36m[2025-07-01 18:09:55,874][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 49152. Throughput: 0: 148.2. Samples: 53598. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 18:09:55,874][110781] Avg episode reward: [(0, '-88.972')]
[36m[2025-07-01 18:10:00,855][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 49152. Throughput: 0: 148.8. Samples: 54498. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 18:10:00,855][110781] Avg episode reward: [(0, '-90.562')]
[36m[2025-07-01 18:10:05,864][110781] Fps is (10 sec: 615.0, 60 sec: 204.8, 300 sec: 166.6). Total num frames: 55296. Throughput: 0: 149.4. Samples: 54948. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:10:05,864][110781] Avg episode reward: [(0, '-89.701')]
[36m[2025-07-01 18:10:10,886][110781] Fps is (10 sec: 612.5, 60 sec: 204.7, 300 sec: 145.8). Total num frames: 55296. Throughput: 0: 148.5. Samples: 55800. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:10:10,886][110781] Avg episode reward: [(0, '-90.140')]
[36m[2025-07-01 18:10:15,867][110781] Fps is (10 sec: 0.0, 60 sec: 204.8, 300 sec: 145.8). Total num frames: 55296. Throughput: 0: 146.9. Samples: 56634. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:10:15,868][110781] Avg episode reward: [(0, '-95.418')]
[36m[2025-07-01 18:10:20,847][110781] Fps is (10 sec: 0.0, 60 sec: 204.8, 300 sec: 145.8). Total num frames: 55296. Throughput: 0: 146.8. Samples: 57084. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:10:20,847][110781] Avg episode reward: [(0, '-91.907')]
[36m[2025-07-01 18:10:25,873][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 55296. Throughput: 0: 147.5. Samples: 58020. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:10:25,874][110781] Avg episode reward: [(0, '-90.846')]
[36m[2025-07-01 18:10:30,866][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 55296. Throughput: 0: 147.8. Samples: 58944. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:10:30,866][110781] Avg episode reward: [(0, '-90.890')]
[36m[2025-07-01 18:10:35,866][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 55296. Throughput: 0: 148.8. Samples: 59406. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:10:35,866][110781] Avg episode reward: [(0, '-89.856')]
[36m[2025-07-01 18:10:40,877][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 55296. Throughput: 0: 149.2. Samples: 60312. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:10:40,877][110781] Avg episode reward: [(0, '-86.457')]
[36m[2025-07-01 18:10:45,854][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 55296. Throughput: 0: 149.1. Samples: 61206. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:10:45,854][110781] Avg episode reward: [(0, '-83.757')]
[36m[2025-07-01 18:10:50,891][110781] Fps is (10 sec: 613.5, 60 sec: 204.8, 300 sec: 145.8). Total num frames: 61440. Throughput: 0: 148.2. Samples: 61620. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:10:50,892][110781] Avg episode reward: [(0, '-80.576')]
[36m[2025-07-01 18:10:55,882][110781] Fps is (10 sec: 612.7, 60 sec: 204.8, 300 sec: 145.8). Total num frames: 61440. Throughput: 0: 149.5. Samples: 62526. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:10:55,883][110781] Avg episode reward: [(0, '-79.658')]
[36m[2025-07-01 18:11:00,887][110781] Fps is (10 sec: 0.0, 60 sec: 204.7, 300 sec: 145.8). Total num frames: 61440. Throughput: 0: 149.3. Samples: 63354. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:11:00,887][110781] Avg episode reward: [(0, '-76.097')]
[36m[2025-07-01 18:11:05,865][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 61440. Throughput: 0: 148.1. Samples: 63750. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:11:05,865][110781] Avg episode reward: [(0, '-76.449')]
[36m[2025-07-01 18:11:10,872][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 61440. Throughput: 0: 146.3. Samples: 64602. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:11:10,872][110781] Avg episode reward: [(0, '-74.835')]
[36m[2025-07-01 18:11:15,874][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 61440. Throughput: 0: 146.0. Samples: 65514. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:11:15,874][110781] Avg episode reward: [(0, '-72.491')]
[36m[2025-07-01 18:11:20,857][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 61440. Throughput: 0: 145.6. Samples: 65958. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:11:20,857][110781] Avg episode reward: [(0, '-74.299')]
[36m[2025-07-01 18:11:25,873][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 61440. Throughput: 0: 144.3. Samples: 66804. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:11:25,873][110781] Avg episode reward: [(0, '-72.299')]
[36m[2025-07-01 18:11:30,865][110781] Fps is (10 sec: 613.9, 60 sec: 204.8, 300 sec: 166.6). Total num frames: 67584. Throughput: 0: 144.1. Samples: 67692. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:11:30,866][110781] Avg episode reward: [(0, '-73.031')]
[36m[2025-07-01 18:11:35,866][110781] Fps is (10 sec: 614.9, 60 sec: 204.8, 300 sec: 145.8). Total num frames: 67584. Throughput: 0: 144.6. Samples: 68124. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:11:35,866][110781] Avg episode reward: [(0, '-71.193')]
[37m[1m[2025-07-01 18:11:35,905][110781] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/MEDIUM_CONFIG_6_ENVS_TEST_1536batch_FIXEDINPUT_V5/checkpoint_p0/checkpoint_000000176_67584.pth...
[36m[2025-07-01 18:11:35,909][110781] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/MEDIUM_CONFIG_6_ENVS_TEST_1536batch_FIXEDINPUT_V5/checkpoint_p0/checkpoint_000000080_30720.pth
[31m[487497 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[487498 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[487498 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 18:11:40,867][110781] Fps is (10 sec: 0.0, 60 sec: 204.8, 300 sec: 145.8). Total num frames: 67584. Throughput: 0: 145.3. Samples: 69060. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:11:40,867][110781] Avg episode reward: [(0, '-72.283')]
[36m[2025-07-01 18:11:45,853][110781] Fps is (10 sec: 0.0, 60 sec: 204.8, 300 sec: 145.8). Total num frames: 67584. Throughput: 0: 146.9. Samples: 69960. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:11:45,854][110781] Avg episode reward: [(0, '-63.893')]
[36m[2025-07-01 18:11:50,862][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 67584. Throughput: 0: 148.1. Samples: 70416. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:11:50,863][110781] Avg episode reward: [(0, '-67.167')]
[36m[2025-07-01 18:11:55,879][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 67584. Throughput: 0: 148.8. Samples: 71298. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:11:55,880][110781] Avg episode reward: [(0, '-67.580')]
[36m[2025-07-01 18:12:00,873][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 67584. Throughput: 0: 147.7. Samples: 72162. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:12:00,873][110781] Avg episode reward: [(0, '-66.350')]
[36m[2025-07-01 18:12:05,849][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 67584. Throughput: 0: 148.2. Samples: 72624. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:12:05,849][110781] Avg episode reward: [(0, '-65.204')]
[36m[2025-07-01 18:12:10,870][110781] Fps is (10 sec: 614.6, 60 sec: 204.8, 300 sec: 166.6). Total num frames: 73728. Throughput: 0: 148.7. Samples: 73494. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 18:12:10,870][110781] Avg episode reward: [(0, '-64.695')]
[36m[2025-07-01 18:12:15,850][110781] Fps is (10 sec: 614.3, 60 sec: 204.9, 300 sec: 145.8). Total num frames: 73728. Throughput: 0: 149.6. Samples: 74424. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 18:12:15,851][110781] Avg episode reward: [(0, '-63.987')]
[36m[2025-07-01 18:12:20,871][110781] Fps is (10 sec: 0.0, 60 sec: 204.7, 300 sec: 145.8). Total num frames: 73728. Throughput: 0: 150.4. Samples: 74892. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 18:12:20,872][110781] Avg episode reward: [(0, '-68.286')]
[36m[2025-07-01 18:12:25,858][110781] Fps is (10 sec: 0.0, 60 sec: 204.9, 300 sec: 145.8). Total num frames: 73728. Throughput: 0: 150.3. Samples: 75822. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 18:12:25,859][110781] Avg episode reward: [(0, '-66.318')]
[36m[2025-07-01 18:12:30,865][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 73728. Throughput: 0: 150.2. Samples: 76722. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 18:12:30,865][110781] Avg episode reward: [(0, '-61.589')]
[36m[2025-07-01 18:12:35,891][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 73728. Throughput: 0: 149.9. Samples: 77166. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 18:12:35,891][110781] Avg episode reward: [(0, '-58.409')]
[36m[2025-07-01 18:12:40,857][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 73728. Throughput: 0: 150.6. Samples: 78072. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 18:12:40,857][110781] Avg episode reward: [(0, '-51.567')]
[36m[2025-07-01 18:12:45,861][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 73728. Throughput: 0: 150.4. Samples: 78930. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 18:12:45,861][110781] Avg episode reward: [(0, '-47.288')]
[36m[2025-07-01 18:12:50,849][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 73728. Throughput: 0: 150.3. Samples: 79386. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 18:12:50,849][110781] Avg episode reward: [(0, '-44.251')]
[36m[2025-07-01 18:12:55,877][110781] Fps is (10 sec: 613.4, 60 sec: 204.8, 300 sec: 166.6). Total num frames: 79872. Throughput: 0: 151.0. Samples: 80292. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 18:12:55,877][110781] Avg episode reward: [(0, '-42.482')]
[36m[2025-07-01 18:13:00,864][110781] Fps is (10 sec: 613.5, 60 sec: 204.8, 300 sec: 145.8). Total num frames: 79872. Throughput: 0: 150.6. Samples: 81204. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 18:13:00,864][110781] Avg episode reward: [(0, '-40.184')]
[36m[2025-07-01 18:13:05,875][110781] Fps is (10 sec: 0.0, 60 sec: 204.7, 300 sec: 145.8). Total num frames: 79872. Throughput: 0: 149.9. Samples: 81636. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 18:13:05,875][110781] Avg episode reward: [(0, '-31.080')]
[36m[2025-07-01 18:13:10,873][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 79872. Throughput: 0: 148.2. Samples: 82494. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 18:13:10,874][110781] Avg episode reward: [(0, '-30.510')]
[36m[2025-07-01 18:13:15,901][110781] Fps is (10 sec: 0.0, 60 sec: 102.3, 300 sec: 145.8). Total num frames: 79872. Throughput: 0: 148.3. Samples: 83400. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 18:13:15,901][110781] Avg episode reward: [(0, '-34.288')]
[36m[2025-07-01 18:13:20,856][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 79872. Throughput: 0: 148.4. Samples: 83838. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 18:13:20,857][110781] Avg episode reward: [(0, '-31.101')]
[36m[2025-07-01 18:13:25,881][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 79872. Throughput: 0: 147.9. Samples: 84732. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 18:13:25,882][110781] Avg episode reward: [(0, '-27.948')]
[36m[2025-07-01 18:13:30,873][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 79872. Throughput: 0: 148.8. Samples: 85626. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 18:13:30,874][110781] Avg episode reward: [(0, '-31.631')]
[33m[600605 ms][navigation_task] - WARNING : Curriculum Level: 36, Curriculum progress fraction: 0.0 (navigation_task.py:262)
[33m[600605 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.00146484375
[33mCrash Rate: 0.9306640625
[33mTimeout Rate: 0.06787109375 (navigation_task.py:265)
[33m[600605 ms][navigation_task] - WARNING : 
[33mSuccesses: 3
[33mCrashes : 1906
[33mTimeouts: 139 (navigation_task.py:268)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/task/navigation_task/navigation_task.py:275: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.infos["curriculum/success_rate"] = torch.tensor(success_rate, dtype=torch.float32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/task/navigation_task/navigation_task.py:276: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.infos["curriculum/crash_rate"] = torch.tensor(crash_rate, dtype=torch.float32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/task/navigation_task/navigation_task.py:277: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.infos["curriculum/timeout_rate"] = torch.tensor(timeout_rate, dtype=torch.float32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/task/navigation_task/navigation_task.py:278: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.infos["curriculum/total_successes"] = torch.tensor(self.success_aggregate, dtype=torch.float32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/task/navigation_task/navigation_task.py:279: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.infos["curriculum/total_crashes"] = torch.tensor(self.crashes_aggregate, dtype=torch.float32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/task/navigation_task/navigation_task.py:280: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.infos["curriculum/total_timeouts"] = torch.tensor(self.timeouts_aggregate, dtype=torch.float32)
[36m[2025-07-01 18:13:35,855][110781] Fps is (10 sec: 616.0, 60 sec: 204.9, 300 sec: 166.6). Total num frames: 86016. Throughput: 0: 148.6. Samples: 86076. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 18:13:35,855][110781] Avg episode reward: [(0, '-35.466')]
[37m[1m[2025-07-01 18:13:35,897][110781] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/MEDIUM_CONFIG_6_ENVS_TEST_1536batch_FIXEDINPUT_V5/checkpoint_p0/checkpoint_000000224_86016.pth...
[36m[2025-07-01 18:13:35,901][110781] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/MEDIUM_CONFIG_6_ENVS_TEST_1536batch_FIXEDINPUT_V5/checkpoint_p0/checkpoint_000000128_49152.pth
[36m[2025-07-01 18:13:40,876][110781] Fps is (10 sec: 614.2, 60 sec: 204.7, 300 sec: 145.8). Total num frames: 86016. Throughput: 0: 148.5. Samples: 86976. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 18:13:40,876][110781] Avg episode reward: [(0, '-38.639')]
[36m[2025-07-01 18:13:45,881][110781] Fps is (10 sec: 0.0, 60 sec: 204.7, 300 sec: 145.8). Total num frames: 86016. Throughput: 0: 148.1. Samples: 87870. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 18:13:45,881][110781] Avg episode reward: [(0, '-36.081')]
[36m[2025-07-01 18:13:50,847][110781] Fps is (10 sec: 0.0, 60 sec: 204.8, 300 sec: 145.8). Total num frames: 86016. Throughput: 0: 149.0. Samples: 88338. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 18:13:50,847][110781] Avg episode reward: [(0, '-32.305')]
[36m[2025-07-01 18:13:55,852][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 86016. Throughput: 0: 148.6. Samples: 89178. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 18:13:55,853][110781] Avg episode reward: [(0, '-29.876')]
[36m[2025-07-01 18:14:00,853][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 86016. Throughput: 0: 147.9. Samples: 90048. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 18:14:00,853][110781] Avg episode reward: [(0, '-37.415')]
[36m[2025-07-01 18:14:05,882][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 86016. Throughput: 0: 147.8. Samples: 90492. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 18:14:05,882][110781] Avg episode reward: [(0, '-38.790')]
[36m[2025-07-01 18:14:10,856][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 86016. Throughput: 0: 147.6. Samples: 91368. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 18:14:10,856][110781] Avg episode reward: [(0, '-31.741')]
[36m[2025-07-01 18:14:15,872][110781] Fps is (10 sec: 615.0, 60 sec: 204.9, 300 sec: 166.6). Total num frames: 92160. Throughput: 0: 147.7. Samples: 92274. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 18:14:15,872][110781] Avg episode reward: [(0, '-34.264')]
[36m[2025-07-01 18:14:20,872][110781] Fps is (10 sec: 613.4, 60 sec: 204.7, 300 sec: 145.8). Total num frames: 92160. Throughput: 0: 147.5. Samples: 92718. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 18:14:20,872][110781] Avg episode reward: [(0, '-31.781')]
[36m[2025-07-01 18:14:25,855][110781] Fps is (10 sec: 0.0, 60 sec: 204.9, 300 sec: 145.8). Total num frames: 92160. Throughput: 0: 146.7. Samples: 93576. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 18:14:25,855][110781] Avg episode reward: [(0, '-28.635')]
[36m[2025-07-01 18:14:30,873][110781] Fps is (10 sec: 0.0, 60 sec: 204.8, 300 sec: 145.8). Total num frames: 92160. Throughput: 0: 147.9. Samples: 94524. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 18:14:30,873][110781] Avg episode reward: [(0, '-24.521')]
[36m[2025-07-01 18:14:35,879][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 92160. Throughput: 0: 148.0. Samples: 95004. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 18:14:35,879][110781] Avg episode reward: [(0, '-20.019')]
[36m[2025-07-01 18:14:40,874][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 92160. Throughput: 0: 149.4. Samples: 95904. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 18:14:40,875][110781] Avg episode reward: [(0, '-18.943')]
[36m[2025-07-01 18:14:45,871][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 92160. Throughput: 0: 150.1. Samples: 96804. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 18:14:45,872][110781] Avg episode reward: [(0, '-21.362')]
[36m[2025-07-01 18:14:50,911][110781] Fps is (10 sec: 0.0, 60 sec: 102.3, 300 sec: 145.8). Total num frames: 92160. Throughput: 0: 149.2. Samples: 97212. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 18:14:50,911][110781] Avg episode reward: [(0, '-21.732')]
[36m[2025-07-01 18:14:55,855][110781] Fps is (10 sec: 615.4, 60 sec: 204.8, 300 sec: 166.6). Total num frames: 98304. Throughput: 0: 149.2. Samples: 98082. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 18:14:55,855][110781] Avg episode reward: [(0, '-13.827')]
[36m[2025-07-01 18:15:00,877][110781] Fps is (10 sec: 616.5, 60 sec: 204.7, 300 sec: 145.8). Total num frames: 98304. Throughput: 0: 148.5. Samples: 98958. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 18:15:00,877][110781] Avg episode reward: [(0, '-16.854')]
[36m[2025-07-01 18:15:05,884][110781] Fps is (10 sec: 0.0, 60 sec: 204.8, 300 sec: 145.8). Total num frames: 98304. Throughput: 0: 148.1. Samples: 99384. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 18:15:05,884][110781] Avg episode reward: [(0, '-12.086')]
[36m[2025-07-01 18:15:10,863][110781] Fps is (10 sec: 0.0, 60 sec: 204.8, 300 sec: 145.8). Total num frames: 98304. Throughput: 0: 148.9. Samples: 100278. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 18:15:10,864][110781] Avg episode reward: [(0, '-14.107')]
[36m[2025-07-01 18:15:15,889][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 98304. Throughput: 0: 147.1. Samples: 101148. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 18:15:15,890][110781] Avg episode reward: [(0, '-18.017')]
[36m[2025-07-01 18:15:20,865][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 98304. Throughput: 0: 146.0. Samples: 101574. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 18:15:20,865][110781] Avg episode reward: [(0, '-16.937')]
[36m[2025-07-01 18:15:25,851][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 98304. Throughput: 0: 145.0. Samples: 102426. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 18:15:25,851][110781] Avg episode reward: [(0, '-11.152')]
[36m[2025-07-01 18:15:30,856][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 98304. Throughput: 0: 143.9. Samples: 103278. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 18:15:30,856][110781] Avg episode reward: [(0, '-12.311')]
[31m[722146 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[722147 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[722147 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 18:15:35,847][110781] Fps is (10 sec: 0.0, 60 sec: 102.5, 300 sec: 145.8). Total num frames: 98304. Throughput: 0: 144.7. Samples: 103716. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 18:15:35,847][110781] Avg episode reward: [(0, '-15.800')]
[37m[1m[2025-07-01 18:15:35,887][110781] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/MEDIUM_CONFIG_6_ENVS_TEST_1536batch_FIXEDINPUT_V5/checkpoint_p0/checkpoint_000000256_98304.pth...
[36m[2025-07-01 18:15:35,890][110781] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/MEDIUM_CONFIG_6_ENVS_TEST_1536batch_FIXEDINPUT_V5/checkpoint_p0/checkpoint_000000176_67584.pth
[36m[2025-07-01 18:15:40,854][110781] Fps is (10 sec: 614.6, 60 sec: 204.9, 300 sec: 166.6). Total num frames: 104448. Throughput: 0: 145.3. Samples: 104622. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:15:40,854][110781] Avg episode reward: [(0, '-15.193')]
[36m[2025-07-01 18:15:45,863][110781] Fps is (10 sec: 613.4, 60 sec: 204.8, 300 sec: 145.8). Total num frames: 104448. Throughput: 0: 145.0. Samples: 105480. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:15:45,863][110781] Avg episode reward: [(0, '-12.933')]
[36m[2025-07-01 18:15:50,881][110781] Fps is (10 sec: 0.0, 60 sec: 204.9, 300 sec: 145.8). Total num frames: 104448. Throughput: 0: 145.2. Samples: 105918. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:15:50,881][110781] Avg episode reward: [(0, '-11.237')]
[36m[2025-07-01 18:15:55,882][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 104448. Throughput: 0: 144.2. Samples: 106770. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:15:55,882][110781] Avg episode reward: [(0, '-13.086')]
[36m[2025-07-01 18:16:00,882][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 104448. Throughput: 0: 145.0. Samples: 107670. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:16:00,882][110781] Avg episode reward: [(0, '-11.484')]
[36m[2025-07-01 18:16:05,859][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 104448. Throughput: 0: 144.4. Samples: 108072. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:16:05,860][110781] Avg episode reward: [(0, '-15.405')]
[36m[2025-07-01 18:16:10,871][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 104448. Throughput: 0: 144.1. Samples: 108912. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:16:10,871][110781] Avg episode reward: [(0, '-10.280')]
[36m[2025-07-01 18:16:15,850][110781] Fps is (10 sec: 0.0, 60 sec: 102.5, 300 sec: 145.8). Total num frames: 104448. Throughput: 0: 146.0. Samples: 109848. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:16:15,850][110781] Avg episode reward: [(0, '-8.790')]
[36m[2025-07-01 18:16:20,861][110781] Fps is (10 sec: 615.0, 60 sec: 204.8, 300 sec: 166.6). Total num frames: 110592. Throughput: 0: 146.9. Samples: 110328. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-07-01 18:16:20,861][110781] Avg episode reward: [(0, '-14.614')]
[36m[2025-07-01 18:16:25,872][110781] Fps is (10 sec: 613.0, 60 sec: 204.7, 300 sec: 145.8). Total num frames: 110592. Throughput: 0: 147.0. Samples: 111240. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-07-01 18:16:25,872][110781] Avg episode reward: [(0, '-12.492')]
[36m[2025-07-01 18:16:30,879][110781] Fps is (10 sec: 0.0, 60 sec: 204.7, 300 sec: 145.8). Total num frames: 110592. Throughput: 0: 146.9. Samples: 112092. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-07-01 18:16:30,879][110781] Avg episode reward: [(0, '-7.954')]
[36m[2025-07-01 18:16:35,877][110781] Fps is (10 sec: 0.0, 60 sec: 204.7, 300 sec: 145.8). Total num frames: 110592. Throughput: 0: 147.3. Samples: 112548. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-07-01 18:16:35,877][110781] Avg episode reward: [(0, '-12.939')]
[36m[2025-07-01 18:16:40,878][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 110592. Throughput: 0: 148.0. Samples: 113430. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-07-01 18:16:40,879][110781] Avg episode reward: [(0, '-2.192')]
[36m[2025-07-01 18:16:45,858][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 110592. Throughput: 0: 147.8. Samples: 114318. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-07-01 18:16:45,858][110781] Avg episode reward: [(0, '-8.000')]
[36m[2025-07-01 18:16:50,869][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 110592. Throughput: 0: 148.4. Samples: 114750. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-07-01 18:16:50,869][110781] Avg episode reward: [(0, '-3.087')]
[36m[2025-07-01 18:16:55,865][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 110592. Throughput: 0: 148.8. Samples: 115608. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-07-01 18:16:55,866][110781] Avg episode reward: [(0, '1.038')]
[36m[2025-07-01 18:17:00,858][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 110592. Throughput: 0: 146.4. Samples: 116436. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-07-01 18:17:00,858][110781] Avg episode reward: [(0, '-0.270')]
[36m[2025-07-01 18:17:05,872][110781] Fps is (10 sec: 614.0, 60 sec: 204.8, 300 sec: 145.8). Total num frames: 116736. Throughput: 0: 145.3. Samples: 116868. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 18:17:05,873][110781] Avg episode reward: [(0, '0.795')]
[36m[2025-07-01 18:17:10,883][110781] Fps is (10 sec: 612.9, 60 sec: 204.8, 300 sec: 145.8). Total num frames: 116736. Throughput: 0: 144.6. Samples: 117750. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 18:17:10,883][110781] Avg episode reward: [(0, '3.004')]
[36m[2025-07-01 18:17:15,852][110781] Fps is (10 sec: 0.0, 60 sec: 204.8, 300 sec: 145.8). Total num frames: 116736. Throughput: 0: 146.4. Samples: 118674. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 18:17:15,853][110781] Avg episode reward: [(0, '4.804')]
[36m[2025-07-01 18:17:20,847][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 116736. Throughput: 0: 147.2. Samples: 119166. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 18:17:20,847][110781] Avg episode reward: [(0, '9.026')]
[36m[2025-07-01 18:17:25,874][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 116736. Throughput: 0: 146.8. Samples: 120036. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 18:17:25,874][110781] Avg episode reward: [(0, '4.537')]
[36m[2025-07-01 18:17:30,850][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 116736. Throughput: 0: 147.1. Samples: 120936. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 18:17:30,850][110781] Avg episode reward: [(0, '3.551')]
[36m[2025-07-01 18:17:35,873][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 116736. Throughput: 0: 147.9. Samples: 121404. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 18:17:35,873][110781] Avg episode reward: [(0, '9.244')]
[37m[1m[2025-07-01 18:17:35,914][110781] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/MEDIUM_CONFIG_6_ENVS_TEST_1536batch_FIXEDINPUT_V5/checkpoint_p0/checkpoint_000000304_116736.pth...
[36m[2025-07-01 18:17:35,917][110781] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/MEDIUM_CONFIG_6_ENVS_TEST_1536batch_FIXEDINPUT_V5/checkpoint_p0/checkpoint_000000224_86016.pth
[36m[2025-07-01 18:17:40,868][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 116736. Throughput: 0: 149.5. Samples: 122334. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 18:17:40,868][110781] Avg episode reward: [(0, '10.287')]
[36m[2025-07-01 18:17:45,868][110781] Fps is (10 sec: 614.7, 60 sec: 204.8, 300 sec: 166.6). Total num frames: 122880. Throughput: 0: 151.3. Samples: 123246. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:17:45,868][110781] Avg episode reward: [(0, '6.639')]
[36m[2025-07-01 18:17:50,855][110781] Fps is (10 sec: 615.2, 60 sec: 204.8, 300 sec: 145.8). Total num frames: 122880. Throughput: 0: 151.7. Samples: 123690. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:17:50,855][110781] Avg episode reward: [(0, '4.987')]
[36m[2025-07-01 18:17:55,874][110781] Fps is (10 sec: 0.0, 60 sec: 204.8, 300 sec: 145.8). Total num frames: 122880. Throughput: 0: 150.8. Samples: 124536. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:17:55,874][110781] Avg episode reward: [(0, '-0.320')]
[36m[2025-07-01 18:18:00,872][110781] Fps is (10 sec: 0.0, 60 sec: 204.8, 300 sec: 145.8). Total num frames: 122880. Throughput: 0: 149.5. Samples: 125406. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:18:00,873][110781] Avg episode reward: [(0, '3.483')]
[36m[2025-07-01 18:18:05,900][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 122880. Throughput: 0: 149.2. Samples: 125886. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:18:05,900][110781] Avg episode reward: [(0, '-0.754')]
[36m[2025-07-01 18:18:10,895][110781] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 145.8). Total num frames: 122880. Throughput: 0: 148.2. Samples: 126708. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 18:18:10,895][110781] Avg episode reward: [(0, '-0.550')]
[37m[1m[2025-07-01 18:18:14,554][110781] Keyboard interrupt detected in the event loop EvtLoop [Runner_EvtLoop, process=main process 110781], exiting...
[37m[1m[2025-07-01 18:18:14,554][110781] Runner profile tree view:
[37m[1mmain_loop: 873.0312
[37m[1m[2025-07-01 18:18:14,555][110781] Collected {0: 122880}, FPS: 140.8