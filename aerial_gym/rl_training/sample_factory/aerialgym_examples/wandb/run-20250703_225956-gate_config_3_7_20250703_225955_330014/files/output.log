Importing module 'gym_38' (/home/ziyar/aerialgym/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_38.so)
Setting GYM_USD_PLUG_INFO_PATH to /home/ziyar/aerialgym/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json
[36m[2025-07-03 22:59:59,195][139965] Queried available GPUs: 0
[37m[1m[2025-07-03 22:59:59,195][139965] Environment var CUDA_VISIBLE_DEVICES is 0
PyTorch version 1.13.1
Device count 1
/home/ziyar/aerialgym/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/_bindings/src/gymtorch
ninja: no work to do.
Warp 1.0.0-beta.5 initialized:
   CUDA Toolkit: 11.5, Driver: 12.4
   Devices:
     "cpu"    | x86_64
     "cuda:0" | NVIDIA GeForce RTX 4080 Laptop GPU (sm_89)
   Kernel cache: /home/ziyar/.cache/warp/1.0.0-beta.5
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/torch/utils/cpp_extension.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
  from pkg_resources import packaging  # type: ignore[attr-defined]
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
Using /home/ziyar/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Emitting ninja build file /home/ziyar/.cache/torch_extensions/py38_cu117/gymtorch/build.ninja...
Building extension module gymtorch...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module gymtorch...
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/networkx/classes/graph.py:23: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  from collections import Mapping
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/networkx/classes/reportviews.py:95: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  from collections import Mapping, Set, Iterable
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/networkx/readwrite/graphml.py:346: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.int, "int"), (np.int8, "int"),
/home/ziyar/aerialgym/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/torch_utils.py:135: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  def get_axis_params(value, axis_idx, x_value=0., dtype=np.float, n_dims=3):
[37m[2292 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task_gate] - INFO : DCE Gate Navigation Task - Using SF_HEADLESS environment variable: False (dce_navigation_task_gate.py:22)
[37m[2292 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task_gate] - INFO : DCE Gate Navigation Task - Final headless mode: False (dce_navigation_task_gate.py:29)
[37m[2292 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task_gate] - INFO : Found SF_ENV_AGENTS environment variable: 16 (dce_navigation_task_gate.py:39)
[37m[2292 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task_gate] - INFO : Detected env_agents=16 from environment - setting environment count. (dce_navigation_task_gate.py:45)
[37m[2292 ms][base_task] - INFO : Setting seed: 950070467 (base_task.py:38)
[37m[2292 ms][navigation_task_gate] - INFO : Building environment for gate navigation task. (navigation_task_gate.py:48)
[37m[2293 ms][navigation_task_gate] - INFO : Sim Name: base_sim, Env Name: gate_env, Robot Name: lmf2, Controller Name: lmf2_position_control (navigation_task_gate.py:49)
[37m[2293 ms][env_manager] - INFO : Populating environments. (env_manager.py:73)
[37m[2293 ms][env_manager] - INFO : Creating simulation instance. (env_manager.py:87)
[37m[2293 ms][env_manager] - INFO : Instantiating IGE object. (env_manager.py:88)
[37m[2293 ms][IsaacGymEnvManager] - INFO : Creating Isaac Gym Environment (IGE_env_manager.py:41)
[37m[2293 ms][IsaacGymEnvManager] - INFO : Acquiring gym object (IGE_env_manager.py:73)
[37m[2293 ms][IsaacGymEnvManager] - INFO : Acquired gym object (IGE_env_manager.py:75)
[37m[2294 ms][IsaacGymEnvManager] - INFO : Fixing devices (IGE_env_manager.py:89)
[37m[2294 ms][IsaacGymEnvManager] - INFO : Using GPU pipeline for simulation. (IGE_env_manager.py:102)
[37m[2294 ms][IsaacGymEnvManager] - INFO : Sim Device type: cuda, Sim Device ID: 0 (IGE_env_manager.py:105)
[37m[2294 ms][IsaacGymEnvManager] - INFO : Graphics Device ID: 0 (IGE_env_manager.py:119)
[37m[2294 ms][IsaacGymEnvManager] - INFO : Creating Isaac Gym Simulation Object (IGE_env_manager.py:120)
[33m[2294 ms][IsaacGymEnvManager] - WARNING : If you have set the CUDA_VISIBLE_DEVICES environment variable, please ensure that you set it
[33mto a particular one that works for your system to use the viewer or Isaac Gym cameras.
[33mIf you want to run parallel simulations on multiple GPUs with camera sensors,
[33mplease disable Isaac Gym and use warp (by setting use_warp=True), set the viewer to headless. (IGE_env_manager.py:127)
[33m[2294 ms][IsaacGymEnvManager] - WARNING : If you see a segfault in the next lines, it is because of the discrepancy between the CUDA device and the graphics device.
[33mPlease ensure that the CUDA device and the graphics device are the same. (IGE_env_manager.py:132)
[37m[3393 ms][IsaacGymEnvManager] - INFO : Created Isaac Gym Simulation Object (IGE_env_manager.py:136)
[37m[3393 ms][IsaacGymEnvManager] - INFO : Created Isaac Gym Environment (IGE_env_manager.py:43)
[37m[3619 ms][env_manager] - INFO : IGE object instantiated. (env_manager.py:109)
[37m[3619 ms][env_manager] - INFO : Creating warp environment. (env_manager.py:112)
[37m[3619 ms][env_manager] - INFO : Warp environment created. (env_manager.py:114)
[37m[3619 ms][env_manager] - INFO : Creating robot manager. (env_manager.py:118)
[37m[3619 ms][BaseRobot] - INFO : [DONE] Initializing controller (base_robot.py:26)
[37m[3619 ms][BaseRobot] - INFO : Initializing controller lmf2_position_control (base_robot.py:29)
[33m[3619 ms][base_multirotor] - WARNING : Creating 16 multirotors. (base_multirotor.py:32)
[37m[3619 ms][env_manager] - INFO : [DONE] Creating robot manager. (env_manager.py:123)
[37m[3619 ms][env_manager] - INFO : [DONE] Creating simulation instance. (env_manager.py:125)
[37m[3619 ms][asset_loader] - INFO : Loading asset: model.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[3620 ms][asset_loader] - INFO : Loading asset: gate.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[3624 ms][asset_loader] - INFO : Loading asset: cuboidal_rod.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[3625 ms][asset_loader] - INFO : Loading asset: 1_x_1_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[3626 ms][asset_loader] - INFO : Loading asset: left_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[3628 ms][asset_loader] - INFO : Loading asset: right_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[3629 ms][asset_loader] - INFO : Loading asset: front_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[3630 ms][asset_loader] - INFO : Loading asset: back_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[3632 ms][asset_loader] - INFO : Loading asset: bottom_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[3633 ms][asset_loader] - INFO : Loading asset: top_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[3634 ms][asset_loader] - INFO : Loading asset: 0_5_x_0_5_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[3635 ms][asset_loader] - INFO : Loading asset: small_cube.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[3637 ms][env_manager] - INFO : Populating environment 0 (env_manager.py:179)
[33m[4051 ms][robot_manager] - WARNING : 
[33mRobot mass: 1.2400000467896461,
[33mInertia: tensor([[0.0134, 0.0000, 0.0000],
[33m        [0.0000, 0.0144, 0.0000],
[33m        [0.0000, 0.0000, 0.0138]], device='cuda:0'),
[33mRobot COM: tensor([[0., 0., 0., 1.]], device='cuda:0') (robot_manager.py:437)
[33m[4051 ms][robot_manager] - WARNING : Calculated robot mass and inertia for this robot. This code assumes that your robot is the same across environments. (robot_manager.py:440)
[31m[4051 ms][robot_manager] - CRITICAL : If your robot differs across environments you need to perform this computation for each different robot here. (robot_manager.py:443)
[37m[4073 ms][env_manager] - INFO : [DONE] Populating environments. (env_manager.py:75)
[33m[4082 ms][IsaacGymEnvManager] - WARNING : Headless: False (IGE_env_manager.py:424)
[37m[4082 ms][IsaacGymEnvManager] - INFO : Creating viewer (IGE_env_manager.py:426)
[33m[4180 ms][IGE_viewer_control] - WARNING : Instructions for using the viewer with the keyboard:
[33mESC: Quit
[33mV: Toggle Viewer Sync
[33mS: Sync Frame Time
[33mF: Toggle Camera Follow
[33mP: Toggle Camera Follow Type
[33mR: Reset All Environments
[33mUP: Switch Target Environment Up
[33mDOWN: Switch Target Environment Down
[33mSPACE: Pause Simulation
[33m (IGE_viewer_control.py:153)
[37m[4180 ms][IsaacGymEnvManager] - INFO : Created viewer (IGE_env_manager.py:432)
[33m[4218 ms][asset_manager] - WARNING : Number of obstacles to be kept in the environment: 10 (asset_manager.py:32)
[SUBPROCESS] FORCED headless mode for all Sample Factory training: headless=True
[SUBPROCESS] This prevents Isaac Gym viewer conflicts across all processes
[SUBPROCESS] Task action_space_dim: 4
[SUBPROCESS] Target Sample Factory action space: 4D
[SUBPROCESS] Setting num_envs to 16 based on env_agents=16
[SUBPROCESS] Set SF_ENV_AGENTS=16 environment variable
[SUBPROCESS] Config batch_size: 2048
[SUBPROCESS] Using STANDARD CONFIG (16 environments)
Registered quad_with_obstacles_gate and dce_navigation_task_gate in subprocess
[isaacgym:gymutil.py] Unknown args:  ['--env=quad_with_obstacles_gate', '--experiment=gate_config_3_7', '--train_dir=./train_dir', '--num_workers=1', '--num_envs_per_worker=1', '--env_agents=16', '--obs_key=observations', '--batch_size=2048', '--num_batches_to_accumulate=2', '--num_batches_per_epoch=8', '--num_epochs=4', '--rollout=32', '--learning_rate=0.0003', '--use_rnn=true', '--rnn_size=64', '--rnn_num_layers=1', '--encoder_mlp_layers', '512', '256', '64', '--gamma=0.98', '--reward_scale=0.1', '--max_grad_norm=1.0', '--async_rl=true', '--normalize_input=true', '--use_env_info_cache=false', '--with_wandb=true', '--wandb_project=gate_navigation_dual_camera', '--wandb_user=ziya-ruso-ucl', '--wandb_group=gate_navigation_training', '--wandb_tags', 'aerial_gym', 'gate_navigation', 'dual_camera', 'x500', 'sample_factory', 'memory_optimized', '--save_every_sec=120', '--save_best_every_sec=5', '--train_for_env_steps=100000000', '--save_gifs=true']
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:0
GPU Pipeline: enabled
*** Can't create empty tensor
WARNING: allocation matrix is not full rank. Rank: 4
creating render graph
Module warp.utils load on device 'cuda:0' took 1.68 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_camera_kernels load on device 'cuda:0' took 8.83 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_stereo_camera_kernels load on device 'cuda:0' took 12.51 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_lidar_kernels load on device 'cuda:0' took 6.45 ms
finishing capture of render graph
Encoder network initialized.
Defined encoder.
[ImgDecoder] Starting create_model
[ImgDecoder] Done with create_model
Defined decoder.
Loading weights from file:  /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/utils/vae/weights/ICRA_test_set_more_sim_data_kld_beta_3_LD_64_epoch_49.pth
[AerialGymVecEnv] GIF saving ENABLED for dual cameras (drone + static)
[AerialGymVecEnv] Forced action space shape: (4,)
[AerialGymVecEnv] is_multiagent: True, num_agents: 16
[AerialGymVecEnv] Detected observation space: 145D
[AerialGymVecEnv] Using GATE NAVIGATION configuration (145D = 17D basic + 64D drone VAE + 64D static camera VAE)
[make_aerialgym_env] Final action space shape: (4,)
[make_aerialgym_env] Expected 4D action space: Box(-1.0, 1.0, (4,), float32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/control/motor_model.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(self.min_thrust, device=self.device, dtype=torch.float32).expand(
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/control/motor_model.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(self.max_thrust, device=self.device, dtype=torch.float32).expand(
[33m[4445 ms][control_allocation] - WARNING : Control allocation does not account for actuator limits. This leads to suboptimal allocation (control_allocation.py:48)
[37m[4447 ms][WarpSensor] - INFO : Camera sensor initialized (warp_sensor.py:50)
[37m[5122 ms][navigation_task_gate] - INFO : Setting up static camera for gate navigation... (navigation_task_gate.py:522)
[37m[5122 ms][navigation_task_gate] - INFO : Static camera properties: 480x270, FOV: 87.0° (navigation_task_gate.py:541)
[37m[5193 ms][navigation_task_gate] - INFO : ✓ Static camera setup complete (navigation_task_gate.py:558)
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.
  logger.warn(
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.
  logger.warn(
[36m[2025-07-03 23:00:04,663][140075] Env info: EnvInfo(obs_space=Dict('obs': Box(-inf, inf, (145,), float32)), action_space=Box(-1.0, 1.0, (4,), float32), num_agents=16, gpu_actions=True, gpu_observations=True, action_splits=None, all_discrete=None, frameskip=1, reward_shaping_scheme=None, env_info_protocol_version=1)
[33m[2025-07-03 23:00:05,698][139965] In serial mode all components run on the same process. Only use async_rl and serial mode together for debugging.
[36m[2025-07-03 23:00:05,698][139965] Starting experiment with the following configuration:
[36mhelp=False
[36malgo=APPO
[36menv=quad_with_obstacles_gate
[36mexperiment=gate_config_3_7
[36mtrain_dir=./train_dir
[36mrestart_behavior=resume
[36mdevice=gpu
[36mseed=None
[36mnum_policies=1
[36masync_rl=True
[36mserial_mode=True
[36mbatched_sampling=True
[36mnum_batches_to_accumulate=2
[36mworker_num_splits=1
[36mpolicy_workers_per_policy=1
[36mmax_policy_lag=1000
[36mnum_workers=1
[36mnum_envs_per_worker=1
[36mbatch_size=2048
[36mnum_batches_per_epoch=8
[36mnum_epochs=4
[36mrollout=32
[36mrecurrence=32
[36mshuffle_minibatches=False
[36mgamma=0.98
[36mreward_scale=0.1
[36mreward_clip=1000.0
[36mvalue_bootstrap=True
[36mnormalize_returns=True
[36mexploration_loss_coeff=0.001
[36mvalue_loss_coeff=2.0
[36mkl_loss_coeff=0.1
[36mexploration_loss=entropy
[36mgae_lambda=0.95
[36mppo_clip_ratio=0.2
[36mppo_clip_value=1.0
[36mwith_vtrace=False
[36mvtrace_rho=1.0
[36mvtrace_c=1.0
[36moptimizer=adam
[36madam_eps=1e-06
[36madam_beta1=0.9
[36madam_beta2=0.999
[36mmax_grad_norm=1.0
[36mlearning_rate=0.0003
[36mlr_schedule=kl_adaptive_epoch
[36mlr_schedule_kl_threshold=0.016
[36mlr_adaptive_min=1e-06
[36mlr_adaptive_max=0.01
[36mobs_subtract_mean=0.0
[36mobs_scale=1.0
[36mnormalize_input=True
[36mnormalize_input_keys=None
[36mdecorrelate_experience_max_seconds=0
[36mdecorrelate_envs_on_one_worker=True
[36mactor_worker_gpus=[0]
[36mset_workers_cpu_affinity=True
[36mforce_envs_single_thread=False
[36mdefault_niceness=0
[36mlog_to_file=True
[36mexperiment_summaries_interval=10
[36mflush_summaries_interval=30
[36mstats_avg=100
[36msummaries_use_frameskip=True
[36mheartbeat_interval=20
[36mheartbeat_reporting_interval=180
[36mtrain_for_env_steps=100000000
[36mtrain_for_seconds=10000000000
[36msave_every_sec=120
[36mkeep_checkpoints=5
[36mload_checkpoint_kind=latest
[36msave_milestones_sec=-1
[36msave_best_every_sec=5
[36msave_best_metric=reward
[36msave_best_after=100000
[36mbenchmark=False
[36mencoder_mlp_layers=[512, 256, 64]
[36mencoder_conv_architecture=convnet_simple
[36mencoder_conv_mlp_layers=[]
[36muse_rnn=True
[36mrnn_size=64
[36mrnn_type=gru
[36mrnn_num_layers=1
[36mdecoder_mlp_layers=[]
[36mnonlinearity=elu
[36mpolicy_initialization=torch_default
[36mpolicy_init_gain=1.0
[36mactor_critic_share_weights=True
[36madaptive_stddev=True
[36mcontinuous_tanh_scale=0.0
[36minitial_stddev=1.0
[36muse_env_info_cache=False
[36menv_gpu_actions=True
[36menv_gpu_observations=True
[36menv_frameskip=1
[36menv_framestack=1
[36mpixel_format=CHW
[36muse_record_episode_statistics=False
[36mwith_wandb=True
[36mwandb_user=ziya-ruso-ucl
[36mwandb_project=gate_navigation_dual_camera
[36mwandb_group=gate_navigation_training
[36mwandb_job_type=SF
[36mwandb_tags=['aerial_gym', 'gate_navigation', 'dual_camera', 'x500', 'sample_factory', 'memory_optimized']
[36mwith_pbt=False
[36mpbt_mix_policies_in_one_env=True
[36mpbt_period_env_steps=5000000
[36mpbt_start_mutation=20000000
[36mpbt_replace_fraction=0.3
[36mpbt_mutation_rate=0.15
[36mpbt_replace_reward_gap=0.1
[36mpbt_replace_reward_gap_absolute=1e-06
[36mpbt_optimize_gamma=False
[36mpbt_target_objective=true_objective
[36mpbt_perturb_min=1.1
[36mpbt_perturb_max=1.5
[36menv_agents=16
[36mheadless=False
[36msave_gifs=True
[36mobs_key=observations
[36msubtask=None
[36mige_api_version=preview4
[36meval_stats=False
[36maction_space_dim=3
[36mcommand_line=--env=quad_with_obstacles_gate --experiment=gate_config_3_7 --train_dir=./train_dir --num_workers=1 --num_envs_per_worker=1 --env_agents=16 --obs_key=observations --batch_size=2048 --num_batches_to_accumulate=2 --num_batches_per_epoch=8 --num_epochs=4 --rollout=32 --learning_rate=0.0003 --use_rnn=true --rnn_size=64 --rnn_num_layers=1 --encoder_mlp_layers 512 256 64 --gamma=0.98 --reward_scale=0.1 --max_grad_norm=1.0 --async_rl=true --normalize_input=true --use_env_info_cache=false --with_wandb=true --wandb_project=gate_navigation_dual_camera --wandb_user=ziya-ruso-ucl --wandb_group=gate_navigation_training --wandb_tags aerial_gym gate_navigation dual_camera x500 sample_factory memory_optimized --save_every_sec=120 --save_best_every_sec=5 --train_for_env_steps=100000000 --headless=false --save_gifs=true
[36mcli_args={'env': 'quad_with_obstacles_gate', 'experiment': 'gate_config_3_7', 'train_dir': './train_dir', 'async_rl': True, 'num_batches_to_accumulate': 2, 'num_workers': 1, 'num_envs_per_worker': 1, 'batch_size': 2048, 'num_batches_per_epoch': 8, 'num_epochs': 4, 'rollout': 32, 'gamma': 0.98, 'reward_scale': 0.1, 'max_grad_norm': 1.0, 'learning_rate': 0.0003, 'normalize_input': True, 'train_for_env_steps': 100000000, 'save_every_sec': 120, 'save_best_every_sec': 5, 'encoder_mlp_layers': [512, 256, 64], 'use_rnn': True, 'rnn_size': 64, 'rnn_num_layers': 1, 'use_env_info_cache': False, 'with_wandb': True, 'wandb_user': 'ziya-ruso-ucl', 'wandb_project': 'gate_navigation_dual_camera', 'wandb_group': 'gate_navigation_training', 'wandb_tags': ['aerial_gym', 'gate_navigation', 'dual_camera', 'x500', 'sample_factory', 'memory_optimized'], 'env_agents': 16, 'headless': False, 'save_gifs': True, 'obs_key': 'observations'}
[36mgit_hash=0cfb8043f86f71fecc5b96bf9393310f6c956cd6
[36mgit_repo_name=git@github.com:rusoziya/aerial_gym_simulator.git
[36mwandb_unique_id=gate_config_3_7_20250703_225955_330014
[36m[2025-07-03 23:00:05,699][139965] Saving configuration to ./train_dir/gate_config_3_7/config.json...
[isaacgym:gymutil.py] Unknown args:  ['--env=quad_with_obstacles_gate', '--experiment=gate_config_3_7', '--train_dir=./train_dir', '--num_workers=1', '--num_envs_per_worker=1', '--env_agents=16', '--obs_key=observations', '--batch_size=2048', '--num_batches_to_accumulate=2', '--num_batches_per_epoch=8', '--num_epochs=4', '--rollout=32', '--learning_rate=0.0003', '--use_rnn=true', '--rnn_size=64', '--rnn_num_layers=1', '--encoder_mlp_layers', '512', '256', '64', '--gamma=0.98', '--reward_scale=0.1', '--max_grad_norm=1.0', '--async_rl=true', '--normalize_input=true', '--use_env_info_cache=false', '--with_wandb=true', '--wandb_project=gate_navigation_dual_camera', '--wandb_user=ziya-ruso-ucl', '--wandb_group=gate_navigation_training', '--wandb_tags', 'aerial_gym', 'gate_navigation', 'dual_camera', 'x500', 'sample_factory', 'memory_optimized', '--save_every_sec=120', '--save_best_every_sec=5', '--train_for_env_steps=100000000', '--save_gifs=true']
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:0
GPU Pipeline: enabled
*** Can't create empty tensor
[36m[2025-07-03 23:00:05,869][139965] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[36m[2025-07-03 23:00:05,869][139965] Rollout worker 0 uses device cuda:0
[36m[2025-07-03 23:00:05,883][139965] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[37m[1m[2025-07-03 23:00:05,883][139965] InferenceWorker_p0-w0: min num requests: 1
[36m[2025-07-03 23:00:05,883][139965] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[37m[1m[2025-07-03 23:00:05,884][139965] Starting seed is not provided
[36m[2025-07-03 23:00:05,884][139965] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[36m[2025-07-03 23:00:05,884][139965] Initializing actor-critic model on device cuda:0
[36m[2025-07-03 23:00:05,885][139965] RunningMeanStd input shape: (145,)
[36m[2025-07-03 23:00:05,885][139965] RunningMeanStd input shape: (1,)
[36m[2025-07-03 23:00:05,916][139965] Created Actor Critic model with architecture:
[36m[2025-07-03 23:00:05,916][139965] ActorCriticSharedWeights(
[36m  (obs_normalizer): ObservationNormalizer(
[36m    (running_mean_std): RunningMeanStdDictInPlace(
[36m      (running_mean_std): ModuleDict(
[36m        (obs): RunningMeanStdInPlace()
[36m      )
[36m    )
[36m  )
[36m  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)
[36m  (encoder): MultiInputEncoder(
[36m    (encoders): ModuleDict(
[36m      (obs): MlpEncoder(
[36m        (mlp_head): RecursiveScriptModule(
[36m          original_name=Sequential
[36m          (0): RecursiveScriptModule(original_name=Linear)
[36m          (1): RecursiveScriptModule(original_name=ELU)
[36m          (2): RecursiveScriptModule(original_name=Linear)
[36m          (3): RecursiveScriptModule(original_name=ELU)
[36m          (4): RecursiveScriptModule(original_name=Linear)
[36m          (5): RecursiveScriptModule(original_name=ELU)
[36m        )
[36m      )
[36m    )
[36m  )
[36m  (core): ModelCoreRNN(
[36m    (core): GRU(64, 64)
[36m  )
[36m  (decoder): MlpDecoder(
[36m    (mlp): Identity()
[36m  )
[36m  (critic_linear): Linear(in_features=64, out_features=1, bias=True)
[36m  (action_parameterization): ActionParameterizationDefault(
[36m    (distribution_linear): Linear(in_features=64, out_features=8, bias=True)
[36m  )
[36m)
[36m[2025-07-03 23:00:06,403][139965] Using optimizer <class 'torch.optim.adam.Adam'>
[33m[2025-07-03 23:00:06,404][139965] No checkpoints found
[36m[2025-07-03 23:00:06,404][139965] Did not load from checkpoint, starting from scratch!
[36m[2025-07-03 23:00:06,404][139965] Initialized policy 0 weights for model version 0
[36m[2025-07-03 23:00:06,404][139965] LearnerWorker_p0 finished initialization!
[36m[2025-07-03 23:00:06,405][139965] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[37m[1m[2025-07-03 23:00:06,415][139965] Inference worker 0-0 is ready!
[37m[1m[2025-07-03 23:00:06,416][139965] All inference workers are ready! Signal rollout workers to start!
[36m[2025-07-03 23:00:06,416][139965] EnvRunner 0-0 uses policy 0
[37m[13671 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task_gate] - INFO : DCE Gate Navigation Task - Using SF_HEADLESS environment variable: False (dce_navigation_task_gate.py:22)
[37m[13671 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task_gate] - INFO : DCE Gate Navigation Task - Final headless mode: False (dce_navigation_task_gate.py:29)
[37m[13671 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task_gate] - INFO : Found SF_ENV_AGENTS environment variable: 16 (dce_navigation_task_gate.py:39)
[37m[13671 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task_gate] - INFO : Detected env_agents=16 from environment - setting environment count. (dce_navigation_task_gate.py:45)
[37m[13671 ms][base_task] - INFO : Setting seed: 1311058698 (base_task.py:38)
[37m[13672 ms][navigation_task_gate] - INFO : Building environment for gate navigation task. (navigation_task_gate.py:48)
[37m[13672 ms][navigation_task_gate] - INFO : Sim Name: base_sim, Env Name: gate_env, Robot Name: lmf2, Controller Name: lmf2_position_control (navigation_task_gate.py:49)
[37m[13672 ms][env_manager] - INFO : Populating environments. (env_manager.py:73)
[37m[13672 ms][env_manager] - INFO : Creating simulation instance. (env_manager.py:87)
[37m[13672 ms][env_manager] - INFO : Instantiating IGE object. (env_manager.py:88)
[37m[13672 ms][IsaacGymEnvManager] - INFO : Creating Isaac Gym Environment (IGE_env_manager.py:41)
[37m[13672 ms][IsaacGymEnvManager] - INFO : Acquiring gym object (IGE_env_manager.py:73)
[37m[13672 ms][IsaacGymEnvManager] - INFO : Acquired gym object (IGE_env_manager.py:75)
[37m[13674 ms][IsaacGymEnvManager] - INFO : Fixing devices (IGE_env_manager.py:89)
[37m[13674 ms][IsaacGymEnvManager] - INFO : Using GPU pipeline for simulation. (IGE_env_manager.py:102)
[37m[13674 ms][IsaacGymEnvManager] - INFO : Sim Device type: cuda, Sim Device ID: 0 (IGE_env_manager.py:105)
[37m[13674 ms][IsaacGymEnvManager] - INFO : Graphics Device ID: 0 (IGE_env_manager.py:119)
[37m[13674 ms][IsaacGymEnvManager] - INFO : Creating Isaac Gym Simulation Object (IGE_env_manager.py:120)
[33m[13674 ms][IsaacGymEnvManager] - WARNING : If you have set the CUDA_VISIBLE_DEVICES environment variable, please ensure that you set it
[33mto a particular one that works for your system to use the viewer or Isaac Gym cameras.
[33mIf you want to run parallel simulations on multiple GPUs with camera sensors,
[33mplease disable Isaac Gym and use warp (by setting use_warp=True), set the viewer to headless. (IGE_env_manager.py:127)
[33m[13674 ms][IsaacGymEnvManager] - WARNING : If you see a segfault in the next lines, it is because of the discrepancy between the CUDA device and the graphics device.
[33mPlease ensure that the CUDA device and the graphics device are the same. (IGE_env_manager.py:132)
[37m[14785 ms][IsaacGymEnvManager] - INFO : Created Isaac Gym Simulation Object (IGE_env_manager.py:136)
[37m[14785 ms][IsaacGymEnvManager] - INFO : Created Isaac Gym Environment (IGE_env_manager.py:43)
[37m[15020 ms][env_manager] - INFO : IGE object instantiated. (env_manager.py:109)
[37m[15020 ms][env_manager] - INFO : Creating warp environment. (env_manager.py:112)
[37m[15020 ms][env_manager] - INFO : Warp environment created. (env_manager.py:114)
[37m[15020 ms][env_manager] - INFO : Creating robot manager. (env_manager.py:118)
[37m[15020 ms][BaseRobot] - INFO : [DONE] Initializing controller (base_robot.py:26)
[37m[15020 ms][BaseRobot] - INFO : Initializing controller lmf2_position_control (base_robot.py:29)
[33m[15020 ms][base_multirotor] - WARNING : Creating 16 multirotors. (base_multirotor.py:32)
[37m[15020 ms][env_manager] - INFO : [DONE] Creating robot manager. (env_manager.py:123)
[37m[15020 ms][env_manager] - INFO : [DONE] Creating simulation instance. (env_manager.py:125)
[37m[15020 ms][asset_loader] - INFO : Loading asset: model.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[15021 ms][asset_loader] - INFO : Loading asset: gate.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[15026 ms][asset_loader] - INFO : Loading asset: cuboidal_rod.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[15028 ms][asset_loader] - INFO : Loading asset: left_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[15030 ms][asset_loader] - INFO : Loading asset: right_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[15031 ms][asset_loader] - INFO : Loading asset: front_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[15032 ms][asset_loader] - INFO : Loading asset: back_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[15033 ms][asset_loader] - INFO : Loading asset: bottom_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[15034 ms][asset_loader] - INFO : Loading asset: top_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[15035 ms][asset_loader] - INFO : Loading asset: 1_x_1_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[15037 ms][asset_loader] - INFO : Loading asset: 0_5_x_0_5_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[15038 ms][asset_loader] - INFO : Loading asset: small_cube.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[15040 ms][env_manager] - INFO : Populating environment 0 (env_manager.py:179)
[33m[15061 ms][robot_manager] - WARNING : 
[33mRobot mass: 1.2400000467896461,
[33mInertia: tensor([[0.0134, 0.0000, 0.0000],
[33m        [0.0000, 0.0144, 0.0000],
[33m        [0.0000, 0.0000, 0.0138]], device='cuda:0'),
[33mRobot COM: tensor([[0., 0., 0., 1.]], device='cuda:0') (robot_manager.py:437)
[33m[15061 ms][robot_manager] - WARNING : Calculated robot mass and inertia for this robot. This code assumes that your robot is the same across environments. (robot_manager.py:440)
[31m[15061 ms][robot_manager] - CRITICAL : If your robot differs across environments you need to perform this computation for each different robot here. (robot_manager.py:443)
[37m[15084 ms][env_manager] - INFO : [DONE] Populating environments. (env_manager.py:75)
[33m[15093 ms][IsaacGymEnvManager] - WARNING : Headless: False (IGE_env_manager.py:424)
[37m[15093 ms][IsaacGymEnvManager] - INFO : Creating viewer (IGE_env_manager.py:426)
[33m[15195 ms][IGE_viewer_control] - WARNING : Instructions for using the viewer with the keyboard:
[33mESC: Quit
[33mV: Toggle Viewer Sync
[33mS: Sync Frame Time
[33mF: Toggle Camera Follow
[33mP: Toggle Camera Follow Type
[33mR: Reset All Environments
[33mUP: Switch Target Environment Up
[33mDOWN: Switch Target Environment Down
[33mSPACE: Pause Simulation
[33m (IGE_viewer_control.py:153)
[37m[15195 ms][IsaacGymEnvManager] - INFO : Created viewer (IGE_env_manager.py:432)
[33m[15237 ms][asset_manager] - WARNING : Number of obstacles to be kept in the environment: 10 (asset_manager.py:32)
WARNING: allocation matrix is not full rank. Rank: 4
creating render graph
Module warp.utils load on device 'cuda:0' took 2.08 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_camera_kernels load on device 'cuda:0' took 9.32 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_stereo_camera_kernels load on device 'cuda:0' took 13.47 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_lidar_kernels load on device 'cuda:0' took 6.63 ms
finishing capture of render graph
Encoder network initialized.
Defined encoder.
[ImgDecoder] Starting create_model
[ImgDecoder] Done with create_model
Defined decoder.
Loading weights from file:  /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/utils/vae/weights/ICRA_test_set_more_sim_data_kld_beta_3_LD_64_epoch_49.pth
[AerialGymVecEnv] GIF saving ENABLED for dual cameras (drone + static)
[AerialGymVecEnv] Forced action space shape: (4,)
[AerialGymVecEnv] is_multiagent: True, num_agents: 16
[AerialGymVecEnv] Detected observation space: 145D
[AerialGymVecEnv] Using GATE NAVIGATION configuration (145D = 17D basic + 64D drone VAE + 64D static camera VAE)
[make_aerialgym_env] Final action space shape: (4,)
[make_aerialgym_env] Expected 4D action space: Box(-1.0, 1.0, (4,), float32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/control/motor_model.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(self.min_thrust, device=self.device, dtype=torch.float32).expand(
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/control/motor_model.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(self.max_thrust, device=self.device, dtype=torch.float32).expand(
[33m[15457 ms][control_allocation] - WARNING : Control allocation does not account for actuator limits. This leads to suboptimal allocation (control_allocation.py:48)
[37m[15459 ms][WarpSensor] - INFO : Camera sensor initialized (warp_sensor.py:50)
[37m[16138 ms][navigation_task_gate] - INFO : Setting up static camera for gate navigation... (navigation_task_gate.py:522)
[37m[16139 ms][navigation_task_gate] - INFO : Static camera properties: 480x270, FOV: 87.0° (navigation_task_gate.py:541)
[37m[16215 ms][navigation_task_gate] - INFO : ✓ Static camera setup complete (navigation_task_gate.py:558)
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.
  logger.warn(
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.
  logger.warn(
[36m[2025-07-03 23:00:09,124][139965] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 0. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[GIF] Episode 0 terminated - saving GIFs (every 100 episodes)
[GIF] Saved drone depth: ./gif_episodes/episode_0000_drone_depth.gif
[GIF] Saved drone segmentation: ./gif_episodes/episode_0000_drone_seg.gif
[GIF] Saved static depth: ./gif_episodes/episode_0000_static_depth.gif
[GIF] Saved static segmentation: ./gif_episodes/episode_0000_static_seg.gif
[GIF] Saved merged dual camera: ./gif_episodes/episode_0000_merged_dual_camera.gif
[36m[2025-07-03 23:00:13,325][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 7.6. Samples: 32. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 23:00:13,325][139965] Avg episode reward: [(0, '-100.000')]
[33m[20888 ms][IGE_viewer_control] - WARNING : Camera follow: True (IGE_viewer_control.py:217)
[33m[23436 ms][IGE_viewer_control] - WARNING : Camera follow: False (IGE_viewer_control.py:217)
[36m[2025-07-03 23:00:18,281][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 26.2. Samples: 240. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 23:00:18,281][139965] Avg episode reward: [(0, '-52.554')]
[36m[2025-07-03 23:00:23,313][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 73.3. Samples: 1040. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 23:00:23,314][139965] Avg episode reward: [(0, '-30.471')]
[37m[1m[2025-07-03 23:00:26,019][139965] Heartbeat connected on Batcher_0
[37m[1m[2025-07-03 23:00:26,019][139965] Heartbeat connected on LearnerWorker_p0
[37m[1m[2025-07-03 23:00:26,019][139965] Heartbeat connected on InferenceWorker_p0-w0
[37m[1m[2025-07-03 23:00:26,019][139965] Heartbeat connected on RolloutWorker_w0
[36m[2025-07-03 23:00:28,298][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 98.5. Samples: 1888. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 23:00:28,299][139965] Avg episode reward: [(0, '-14.456')]
[36m[2025-07-03 23:00:33,263][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 97.4. Samples: 2352. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 23:00:33,264][139965] Avg episode reward: [(0, '-13.626')]
[36m[2025-07-03 23:00:38,340][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 110.6. Samples: 3232. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 23:00:38,340][139965] Avg episode reward: [(0, '-15.242')]
[36m[2025-07-03 23:00:43,359][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 122.9. Samples: 4208. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 23:00:43,360][139965] Avg episode reward: [(0, '-17.134')]
[36m[2025-07-03 23:00:48,314][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 118.0. Samples: 4624. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 23:00:48,314][139965] Avg episode reward: [(0, '-12.089')]
[36m[2025-07-03 23:00:53,295][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 126.8. Samples: 5600. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 23:00:53,296][139965] Avg episode reward: [(0, '-12.506')]
[36m[2025-07-03 23:00:58,318][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 144.0. Samples: 6512. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 23:00:58,318][139965] Avg episode reward: [(0, '-14.871')]
[36m[2025-07-03 23:01:03,284][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 150.4. Samples: 7008. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 23:01:03,284][139965] Avg episode reward: [(0, '-14.201')]
[36m[2025-07-03 23:01:08,342][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 151.7. Samples: 7872. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 23:01:08,343][139965] Avg episode reward: [(0, '-14.278')]
[36m[2025-07-03 23:01:13,322][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 151.7. Samples: 8720. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 23:01:13,322][139965] Avg episode reward: [(0, '-12.741')]
[36m[2025-07-03 23:01:18,356][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 150.8. Samples: 9152. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 23:01:18,357][139965] Avg episode reward: [(0, '-14.057')]
[36m[2025-07-03 23:01:23,331][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 150.8. Samples: 10016. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 23:01:23,331][139965] Avg episode reward: [(0, '-13.632')]
[36m[2025-07-03 23:01:28,371][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 147.5. Samples: 10848. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 23:01:28,372][139965] Avg episode reward: [(0, '-14.418')]
[36m[2025-07-03 23:01:33,331][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 148.2. Samples: 11296. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 23:01:33,331][139965] Avg episode reward: [(0, '-14.449')]
[36m[2025-07-03 23:01:38,268][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 146.2. Samples: 12176. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 23:01:38,269][139965] Avg episode reward: [(0, '-15.271')]
[36m[2025-07-03 23:01:43,295][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 146.9. Samples: 13120. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 23:01:43,296][139965] Avg episode reward: [(0, '-16.016')]
[33m[111934 ms][navigation_task_gate] - WARNING : Gate Navigation Curriculum Level: 3, Progress: 0.0 (navigation_task_gate.py:469)
[33m[111934 ms][navigation_task_gate] - WARNING : 
[33mSuccess Rate: 0.0
[33mCrash Rate: 1.0
[33mTimeout Rate: 0.0 (navigation_task_gate.py:472)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/task/navigation_task_gate/navigation_task_gate.py:479: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.infos["curriculum/success_rate"] = torch.tensor(success_rate, dtype=torch.float32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/task/navigation_task_gate/navigation_task_gate.py:480: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.infos["curriculum/crash_rate"] = torch.tensor(crash_rate, dtype=torch.float32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/task/navigation_task_gate/navigation_task_gate.py:481: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.infos["curriculum/timeout_rate"] = torch.tensor(timeout_rate, dtype=torch.float32)
[36m[2025-07-03 23:01:48,275][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 146.2. Samples: 13584. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 23:01:48,276][139965] Avg episode reward: [(0, '-17.601')]
[36m[2025-07-03 23:01:53,263][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 145.0. Samples: 14384. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 23:01:53,264][139965] Avg episode reward: [(0, '-16.115')]
[36m[2025-07-03 23:01:58,362][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 145.6. Samples: 15280. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 23:01:58,363][139965] Avg episode reward: [(0, '-13.668')]
[37m[1m[2025-07-03 23:01:58,456][139965] Saving ./train_dir/gate_config_3_7/checkpoint_p0/checkpoint_000000000_0.pth...
[36m[2025-07-03 23:02:03,277][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 145.7. Samples: 15696. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 23:02:03,278][139965] Avg episode reward: [(0, '-15.639')]
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/torch/nn/modules/module.py:1194: UserWarning: operator() profile_node %104 : int[] = prim::profile_ivalue(%102)
 does not have profile information (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
[36m[2025-07-03 23:02:08,381][139965] Fps is (10 sec: 1635.4, 60 sec: 272.9, 300 sec: 137.4). Total num frames: 16384. Throughput: 0: 143.1. Samples: 16464. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-03 23:02:08,381][139965] Avg episode reward: [(0, '-16.521')]
[36m[2025-07-03 23:02:13,261][139965] Fps is (10 sec: 1641.1, 60 sec: 273.3, 300 sec: 132.0). Total num frames: 16384. Throughput: 0: 143.6. Samples: 17296. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-03 23:02:13,262][139965] Avg episode reward: [(0, '-20.515')]
[36m[2025-07-03 23:02:18,286][139965] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 126.8). Total num frames: 16384. Throughput: 0: 143.1. Samples: 17728. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-03 23:02:18,286][139965] Avg episode reward: [(0, '-19.305')]
[36m[2025-07-03 23:02:23,261][139965] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 122.1). Total num frames: 16384. Throughput: 0: 140.8. Samples: 18512. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-03 23:02:23,261][139965] Avg episode reward: [(0, '-19.795')]
[36m[2025-07-03 23:02:28,282][139965] Fps is (10 sec: 0.0, 60 sec: 273.5, 300 sec: 117.7). Total num frames: 16384. Throughput: 0: 140.5. Samples: 19440. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-03 23:02:28,282][139965] Avg episode reward: [(0, '-16.931')]
[36m[2025-07-03 23:02:33,270][139965] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 113.7). Total num frames: 16384. Throughput: 0: 141.2. Samples: 19936. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-03 23:02:33,271][139965] Avg episode reward: [(0, '-15.645')]
[GIF] Episode 100 terminated - saving GIFs (every 100 episodes)
[GIF] Saved drone depth: ./gif_episodes/episode_0001_drone_depth.gif
[GIF] Saved drone segmentation: ./gif_episodes/episode_0001_drone_seg.gif
[GIF] Saved static depth: ./gif_episodes/episode_0001_static_depth.gif
[GIF] Saved static segmentation: ./gif_episodes/episode_0001_static_seg.gif
[GIF] Saved merged dual camera: ./gif_episodes/episode_0001_merged_dual_camera.gif
[36m[2025-07-03 23:02:38,301][139965] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 109.8). Total num frames: 16384. Throughput: 0: 142.5. Samples: 20800. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-03 23:02:38,301][139965] Avg episode reward: [(0, '-17.487')]
[36m[2025-07-03 23:02:43,270][139965] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 106.3). Total num frames: 16384. Throughput: 0: 143.2. Samples: 21712. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-03 23:02:43,271][139965] Avg episode reward: [(0, '-18.474')]
[36m[2025-07-03 23:02:48,277][139965] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 102.9). Total num frames: 16384. Throughput: 0: 143.3. Samples: 22144. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-03 23:02:48,277][139965] Avg episode reward: [(0, '-18.943')]
[36m[2025-07-03 23:02:53,385][139965] Fps is (10 sec: 0.0, 60 sec: 272.5, 300 sec: 99.7). Total num frames: 16384. Throughput: 0: 145.4. Samples: 23008. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-03 23:02:53,385][139965] Avg episode reward: [(0, '-19.334')]
[36m[2025-07-03 23:02:58,325][139965] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 96.8). Total num frames: 16384. Throughput: 0: 147.0. Samples: 23920. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-03 23:02:58,325][139965] Avg episode reward: [(0, '-15.649')]
[36m[2025-07-03 23:03:03,278][139965] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 94.1). Total num frames: 16384. Throughput: 0: 147.6. Samples: 24368. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-03 23:03:03,278][139965] Avg episode reward: [(0, '-16.795')]
[36m[2025-07-03 23:03:08,372][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 91.4). Total num frames: 16384. Throughput: 0: 148.3. Samples: 25200. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-03 23:03:08,372][139965] Avg episode reward: [(0, '-18.744')]
[36m[2025-07-03 23:03:13,330][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 88.9). Total num frames: 16384. Throughput: 0: 146.3. Samples: 26032. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-03 23:03:13,331][139965] Avg episode reward: [(0, '-19.584')]
[33m[202805 ms][navigation_task_gate] - WARNING : Gate Navigation Curriculum Level: 3, Progress: 0.0 (navigation_task_gate.py:469)
[33m[202806 ms][navigation_task_gate] - WARNING : 
[33mSuccess Rate: 0.0
[33mCrash Rate: 1.0
[33mTimeout Rate: 0.0 (navigation_task_gate.py:472)
[36m[2025-07-03 23:03:18,324][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 86.6). Total num frames: 16384. Throughput: 0: 144.9. Samples: 26464. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-03 23:03:18,324][139965] Avg episode reward: [(0, '-20.505')]
[36m[2025-07-03 23:03:23,377][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 84.3). Total num frames: 16384. Throughput: 0: 144.8. Samples: 27328. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-03 23:03:23,377][139965] Avg episode reward: [(0, '-18.789')]
[36m[2025-07-03 23:03:28,293][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 82.3). Total num frames: 16384. Throughput: 0: 143.9. Samples: 28192. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-03 23:03:28,293][139965] Avg episode reward: [(0, '-18.272')]
[36m[2025-07-03 23:03:33,336][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 80.2). Total num frames: 16384. Throughput: 0: 143.1. Samples: 28592. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-03 23:03:33,336][139965] Avg episode reward: [(0, '-16.221')]
[36m[2025-07-03 23:03:38,356][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 78.3). Total num frames: 16384. Throughput: 0: 143.4. Samples: 29456. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-03 23:03:38,356][139965] Avg episode reward: [(0, '-18.836')]
[36m[2025-07-03 23:03:43,264][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 76.5). Total num frames: 16384. Throughput: 0: 142.4. Samples: 30320. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-03 23:03:43,264][139965] Avg episode reward: [(0, '-20.812')]
[36m[2025-07-03 23:03:48,265][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 74.8). Total num frames: 16384. Throughput: 0: 143.0. Samples: 30800. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-03 23:03:48,266][139965] Avg episode reward: [(0, '-20.908')]
[36m[2025-07-03 23:03:53,352][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 73.1). Total num frames: 16384. Throughput: 0: 142.3. Samples: 31600. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-03 23:03:53,352][139965] Avg episode reward: [(0, '-16.602')]
[36m[2025-07-03 23:03:58,298][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 71.5). Total num frames: 16384. Throughput: 0: 143.0. Samples: 32464. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-03 23:03:58,299][139965] Avg episode reward: [(0, '-18.417')]
[37m[1m[2025-07-03 23:03:58,390][139965] Saving ./train_dir/gate_config_3_7/checkpoint_p0/checkpoint_000000032_16384.pth...
[36m[2025-07-03 23:04:03,278][139965] Fps is (10 sec: 1650.6, 60 sec: 273.1, 300 sec: 139.9). Total num frames: 32768. Throughput: 0: 141.3. Samples: 32816. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-03 23:04:03,278][139965] Avg episode reward: [(0, '-18.449')]
[36m[2025-07-03 23:04:08,297][139965] Fps is (10 sec: 1638.6, 60 sec: 273.4, 300 sec: 137.0). Total num frames: 32768. Throughput: 0: 141.4. Samples: 33680. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-03 23:04:08,298][139965] Avg episode reward: [(0, '-12.546')]
[36m[2025-07-03 23:04:13,387][139965] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 134.2). Total num frames: 32768. Throughput: 0: 139.8. Samples: 34496. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-03 23:04:13,387][139965] Avg episode reward: [(0, '-12.668')]
[36m[2025-07-03 23:04:18,281][139965] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 131.5). Total num frames: 32768. Throughput: 0: 140.6. Samples: 34912. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-03 23:04:18,282][139965] Avg episode reward: [(0, '-12.784')]
[36m[2025-07-03 23:04:23,335][139965] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 128.9). Total num frames: 32768. Throughput: 0: 140.9. Samples: 35792. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-03 23:04:23,335][139965] Avg episode reward: [(0, '-12.172')]
[36m[2025-07-03 23:04:28,343][139965] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 126.4). Total num frames: 32768. Throughput: 0: 140.6. Samples: 36656. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-03 23:04:28,344][139965] Avg episode reward: [(0, '-15.153')]
[36m[2025-07-03 23:04:33,309][139965] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 124.0). Total num frames: 32768. Throughput: 0: 139.2. Samples: 37072. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-03 23:04:33,309][139965] Avg episode reward: [(0, '-14.821')]
[36m[2025-07-03 23:04:38,264][139965] Fps is (10 sec: 0.0, 60 sec: 273.5, 300 sec: 121.8). Total num frames: 32768. Throughput: 0: 140.7. Samples: 37920. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-03 23:04:38,264][139965] Avg episode reward: [(0, '-11.682')]
[36m[2025-07-03 23:04:43,324][139965] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 119.5). Total num frames: 32768. Throughput: 0: 140.7. Samples: 38800. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-03 23:04:43,325][139965] Avg episode reward: [(0, '-14.593')]
[36m[2025-07-03 23:04:48,353][139965] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 117.4). Total num frames: 32768. Throughput: 0: 143.0. Samples: 39264. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-03 23:04:48,353][139965] Avg episode reward: [(0, '-15.711')]
[33m[296933 ms][navigation_task_gate] - WARNING : Gate Navigation Curriculum Level: 3, Progress: 0.0 (navigation_task_gate.py:469)
[33m[296933 ms][navigation_task_gate] - WARNING : 
[33mSuccess Rate: 0.0
[33mCrash Rate: 1.0
[33mTimeout Rate: 0.0 (navigation_task_gate.py:472)
[36m[2025-07-03 23:04:53,269][139965] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 115.3). Total num frames: 32768. Throughput: 0: 142.3. Samples: 40080. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-03 23:04:53,269][139965] Avg episode reward: [(0, '-15.889')]
[GIF] Episode 200 terminated - saving GIFs (every 100 episodes)
[GIF] Saved drone depth: ./gif_episodes/episode_0002_drone_depth.gif
[GIF] Saved drone segmentation: ./gif_episodes/episode_0002_drone_seg.gif
[GIF] Saved static depth: ./gif_episodes/episode_0002_static_depth.gif
[GIF] Saved static segmentation: ./gif_episodes/episode_0002_static_seg.gif
[GIF] Saved merged dual camera: ./gif_episodes/episode_0002_merged_dual_camera.gif
[36m[2025-07-03 23:04:58,284][139965] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 113.3). Total num frames: 32768. Throughput: 0: 142.5. Samples: 40896. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-03 23:04:58,284][139965] Avg episode reward: [(0, '-14.976')]
[36m[2025-07-03 23:05:03,366][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.4). Total num frames: 32768. Throughput: 0: 141.2. Samples: 41280. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-03 23:05:03,367][139965] Avg episode reward: [(0, '-12.053')]
[36m[2025-07-03 23:05:08,395][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 32768. Throughput: 0: 141.7. Samples: 42176. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-03 23:05:08,396][139965] Avg episode reward: [(0, '-9.974')]
[36m[2025-07-03 23:05:13,297][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 32768. Throughput: 0: 140.2. Samples: 42960. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-03 23:05:13,297][139965] Avg episode reward: [(0, '-12.024')]
[36m[2025-07-03 23:05:18,283][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 32768. Throughput: 0: 140.9. Samples: 43408. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-03 23:05:18,284][139965] Avg episode reward: [(0, '-14.715')]
[36m[2025-07-03 23:05:23,264][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 32768. Throughput: 0: 140.1. Samples: 44224. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-03 23:05:23,265][139965] Avg episode reward: [(0, '-12.987')]
[36m[2025-07-03 23:05:28,380][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.0). Total num frames: 32768. Throughput: 0: 141.0. Samples: 45152. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-03 23:05:28,381][139965] Avg episode reward: [(0, '-10.665')]
[36m[2025-07-03 23:05:33,376][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 32768. Throughput: 0: 139.7. Samples: 45552. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-03 23:05:33,377][139965] Avg episode reward: [(0, '-11.209')]
[36m[2025-07-03 23:05:38,309][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 32768. Throughput: 0: 139.6. Samples: 46368. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-03 23:05:38,310][139965] Avg episode reward: [(0, '-13.495')]
[36m[2025-07-03 23:05:43,303][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 32768. Throughput: 0: 140.4. Samples: 47216. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-03 23:05:43,304][139965] Avg episode reward: [(0, '-13.345')]
[36m[2025-07-03 23:05:48,330][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 32768. Throughput: 0: 141.3. Samples: 47632. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-03 23:05:48,330][139965] Avg episode reward: [(0, '-13.388')]
[36m[2025-07-03 23:05:53,308][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 32768. Throughput: 0: 141.1. Samples: 48512. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-03 23:05:53,308][139965] Avg episode reward: [(0, '-12.498')]
[36m[2025-07-03 23:05:58,287][139965] Fps is (10 sec: 1645.5, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 49152. Throughput: 0: 141.9. Samples: 49344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:05:58,287][139965] Avg episode reward: [(0, '-14.023')]
[37m[1m[2025-07-03 23:05:58,410][139965] Saving ./train_dir/gate_config_3_7/checkpoint_p0/checkpoint_000000096_49152.pth...
[36m[2025-07-03 23:06:03,273][139965] Fps is (10 sec: 1644.2, 60 sec: 273.5, 300 sec: 166.7). Total num frames: 49152. Throughput: 0: 142.3. Samples: 49808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:06:03,273][139965] Avg episode reward: [(0, '-8.294')]
[36m[2025-07-03 23:06:08,338][139965] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 166.6). Total num frames: 49152. Throughput: 0: 142.3. Samples: 50640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:06:08,338][139965] Avg episode reward: [(0, '-0.493')]
[36m[2025-07-03 23:06:13,260][139965] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 166.7). Total num frames: 49152. Throughput: 0: 141.9. Samples: 51520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:06:13,261][139965] Avg episode reward: [(0, '-4.592')]
[36m[2025-07-03 23:06:18,284][139965] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 49152. Throughput: 0: 142.9. Samples: 51968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:06:18,284][139965] Avg episode reward: [(0, '-8.210')]
[36m[2025-07-03 23:06:23,351][139965] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 166.6). Total num frames: 49152. Throughput: 0: 143.2. Samples: 52816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:06:23,352][139965] Avg episode reward: [(0, '-5.477')]
[36m[2025-07-03 23:06:28,318][139965] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 166.6). Total num frames: 49152. Throughput: 0: 144.3. Samples: 53712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:06:28,318][139965] Avg episode reward: [(0, '-0.781')]
[33m[396416 ms][navigation_task_gate] - WARNING : Gate Navigation Curriculum Level: 3, Progress: 0.0 (navigation_task_gate.py:469)
[33m[396416 ms][navigation_task_gate] - WARNING : 
[33mSuccess Rate: 0.0
[33mCrash Rate: 1.0
[33mTimeout Rate: 0.0 (navigation_task_gate.py:472)
[36m[2025-07-03 23:06:33,297][139965] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 166.6). Total num frames: 49152. Throughput: 0: 145.9. Samples: 54192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:06:33,297][139965] Avg episode reward: [(0, '-0.926')]
[36m[2025-07-03 23:06:38,367][139965] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 166.6). Total num frames: 49152. Throughput: 0: 144.5. Samples: 55024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:06:38,367][139965] Avg episode reward: [(0, '-2.091')]
[36m[2025-07-03 23:06:43,286][139965] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 49152. Throughput: 0: 145.4. Samples: 55888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:06:43,286][139965] Avg episode reward: [(0, '-1.817')]
[36m[2025-07-03 23:06:48,347][139965] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 49152. Throughput: 0: 145.5. Samples: 56368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:06:48,347][139965] Avg episode reward: [(0, '-2.591')]
[36m[2025-07-03 23:06:53,262][139965] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 166.7). Total num frames: 49152. Throughput: 0: 146.7. Samples: 57232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:06:53,262][139965] Avg episode reward: [(0, '0.719')]
[36m[2025-07-03 23:06:58,262][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 49152. Throughput: 0: 147.2. Samples: 58144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:06:58,263][139965] Avg episode reward: [(0, '-3.895')]
[36m[2025-07-03 23:07:03,338][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 49152. Throughput: 0: 146.3. Samples: 58560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:07:03,338][139965] Avg episode reward: [(0, '-6.553')]
[36m[2025-07-03 23:07:08,363][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.0). Total num frames: 49152. Throughput: 0: 147.5. Samples: 59456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:07:08,364][139965] Avg episode reward: [(0, '-4.003')]
[36m[2025-07-03 23:07:13,288][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 49152. Throughput: 0: 146.6. Samples: 60304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:07:13,289][139965] Avg episode reward: [(0, '-0.450')]
[36m[2025-07-03 23:07:18,374][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.0). Total num frames: 49152. Throughput: 0: 145.2. Samples: 60736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:07:18,374][139965] Avg episode reward: [(0, '0.197')]
[36m[2025-07-03 23:07:23,283][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 49152. Throughput: 0: 145.7. Samples: 61568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:07:23,284][139965] Avg episode reward: [(0, '1.621')]
[36m[2025-07-03 23:07:28,347][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.0). Total num frames: 49152. Throughput: 0: 145.6. Samples: 62448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:07:28,348][139965] Avg episode reward: [(0, '-4.217')]
[36m[2025-07-03 23:07:33,300][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 49152. Throughput: 0: 144.2. Samples: 62848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:07:33,300][139965] Avg episode reward: [(0, '-5.818')]
[GIF] Episode 300 terminated - saving GIFs (every 100 episodes)
[GIF] Saved drone depth: ./gif_episodes/episode_0003_drone_depth.gif
[GIF] Saved drone segmentation: ./gif_episodes/episode_0003_drone_seg.gif
[GIF] Saved static depth: ./gif_episodes/episode_0003_static_depth.gif
[GIF] Saved static segmentation: ./gif_episodes/episode_0003_static_seg.gif
[GIF] Saved merged dual camera: ./gif_episodes/episode_0003_merged_dual_camera.gif
[36m[2025-07-03 23:07:38,341][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 49152. Throughput: 0: 143.7. Samples: 63712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:07:38,341][139965] Avg episode reward: [(0, '-3.695')]
[36m[2025-07-03 23:07:43,335][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 49152. Throughput: 0: 145.2. Samples: 64688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:07:43,335][139965] Avg episode reward: [(0, '-0.723')]
[36m[2025-07-03 23:07:48,420][139965] Fps is (10 sec: 1625.5, 60 sec: 272.7, 300 sec: 166.6). Total num frames: 65536. Throughput: 0: 146.2. Samples: 65152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:07:48,421][139965] Avg episode reward: [(0, '-2.438')]
[36m[2025-07-03 23:07:53,269][139965] Fps is (10 sec: 1649.3, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 65536. Throughput: 0: 146.4. Samples: 66032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:07:53,269][139965] Avg episode reward: [(0, '5.041')]
[36m[2025-07-03 23:07:58,337][139965] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 166.6). Total num frames: 65536. Throughput: 0: 144.9. Samples: 66832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:07:58,338][139965] Avg episode reward: [(0, '13.347')]
[37m[1m[2025-07-03 23:07:58,498][139965] Saving ./train_dir/gate_config_3_7/checkpoint_p0/checkpoint_000000128_65536.pth...
[36m[2025-07-03 23:08:03,289][139965] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 166.7). Total num frames: 65536. Throughput: 0: 145.0. Samples: 67248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:08:03,289][139965] Avg episode reward: [(0, '13.847')]
[36m[2025-07-03 23:08:08,288][139965] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 166.6). Total num frames: 65536. Throughput: 0: 145.8. Samples: 68128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:08:08,289][139965] Avg episode reward: [(0, '10.124')]
[36m[2025-07-03 23:08:13,263][139965] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 166.7). Total num frames: 65536. Throughput: 0: 147.5. Samples: 69072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:08:13,264][139965] Avg episode reward: [(0, '11.071')]
[33m[504011 ms][navigation_task_gate] - WARNING : Gate Navigation Curriculum Level: 3, Progress: 0.0 (navigation_task_gate.py:469)
[33m[504011 ms][navigation_task_gate] - WARNING : 
[33mSuccess Rate: 0.0
[33mCrash Rate: 1.0
[33mTimeout Rate: 0.0 (navigation_task_gate.py:472)
[36m[2025-07-03 23:08:18,338][139965] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 166.6). Total num frames: 65536. Throughput: 0: 148.9. Samples: 69552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:08:18,339][139965] Avg episode reward: [(0, '7.728')]
[36m[2025-07-03 23:08:23,293][139965] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 65536. Throughput: 0: 149.8. Samples: 70448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:08:23,294][139965] Avg episode reward: [(0, '6.560')]
[36m[2025-07-03 23:08:28,347][139965] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 65536. Throughput: 0: 146.5. Samples: 71280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:08:28,347][139965] Avg episode reward: [(0, '11.223')]
[36m[2025-07-03 23:08:33,280][139965] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 166.7). Total num frames: 65536. Throughput: 0: 146.6. Samples: 71728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:08:33,281][139965] Avg episode reward: [(0, '9.278')]
[36m[2025-07-03 23:08:38,273][139965] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 166.6). Total num frames: 65536. Throughput: 0: 145.8. Samples: 72592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:08:38,273][139965] Avg episode reward: [(0, '12.793')]
[36m[2025-07-03 23:08:43,404][139965] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 166.5). Total num frames: 65536. Throughput: 0: 147.7. Samples: 73488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:08:43,404][139965] Avg episode reward: [(0, '13.060')]
[36m[2025-07-03 23:08:48,315][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 65536. Throughput: 0: 147.8. Samples: 73904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:08:48,315][139965] Avg episode reward: [(0, '9.276')]
[36m[2025-07-03 23:08:53,317][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 65536. Throughput: 0: 148.2. Samples: 74800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:08:53,318][139965] Avg episode reward: [(0, '4.934')]
[36m[2025-07-03 23:08:58,372][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.0). Total num frames: 65536. Throughput: 0: 145.4. Samples: 75632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:08:58,373][139965] Avg episode reward: [(0, '7.767')]
[36m[2025-07-03 23:09:03,304][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 65536. Throughput: 0: 144.1. Samples: 76032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:09:03,305][139965] Avg episode reward: [(0, '10.445')]
[36m[2025-07-03 23:09:08,288][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 65536. Throughput: 0: 142.6. Samples: 76864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:09:08,288][139965] Avg episode reward: [(0, '14.176')]
[36m[2025-07-03 23:09:13,305][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 65536. Throughput: 0: 144.5. Samples: 77776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:09:13,305][139965] Avg episode reward: [(0, '9.566')]
[36m[2025-07-03 23:09:18,333][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 65536. Throughput: 0: 145.6. Samples: 78288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:09:18,334][139965] Avg episode reward: [(0, '9.015')]
[36m[2025-07-03 23:09:23,323][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 65536. Throughput: 0: 148.1. Samples: 79264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:09:23,324][139965] Avg episode reward: [(0, '10.638')]
[36m[2025-07-03 23:09:28,297][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 65536. Throughput: 0: 149.7. Samples: 80208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:09:28,297][139965] Avg episode reward: [(0, '11.103')]
[36m[2025-07-03 23:09:33,276][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 65536. Throughput: 0: 151.2. Samples: 80704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:09:33,277][139965] Avg episode reward: [(0, '8.217')]
[36m[2025-07-03 23:09:38,289][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 65536. Throughput: 0: 150.8. Samples: 81584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:09:38,290][139965] Avg episode reward: [(0, '6.838')]
[36m[2025-07-03 23:09:43,343][139965] Fps is (10 sec: 1627.5, 60 sec: 273.3, 300 sec: 166.6). Total num frames: 81920. Throughput: 0: 153.3. Samples: 82528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:09:43,344][139965] Avg episode reward: [(0, '13.451')]
[36m[2025-07-03 23:09:48,372][139965] Fps is (10 sec: 1625.1, 60 sec: 272.8, 300 sec: 166.6). Total num frames: 81920. Throughput: 0: 153.7. Samples: 82960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:09:48,372][139965] Avg episode reward: [(0, '19.218')]
[36m[2025-07-03 23:09:53,308][139965] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 81920. Throughput: 0: 155.0. Samples: 83840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:09:53,308][139965] Avg episode reward: [(0, '32.037')]
[36m[2025-07-03 23:09:58,366][139965] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 81920. Throughput: 0: 153.7. Samples: 84704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:09:58,366][139965] Avg episode reward: [(0, '32.955')]
[37m[1m[2025-07-03 23:09:58,511][139965] Saving ./train_dir/gate_config_3_7/checkpoint_p0/checkpoint_000000160_81920.pth...
[36m[2025-07-03 23:10:03,294][139965] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.7). Total num frames: 81920. Throughput: 0: 151.6. Samples: 85104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:10:03,295][139965] Avg episode reward: [(0, '31.037')]
[36m[2025-07-03 23:10:08,282][139965] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 81920. Throughput: 0: 149.5. Samples: 85984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:10:08,283][139965] Avg episode reward: [(0, '24.229')]
[36m[2025-07-03 23:10:13,293][139965] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 81920. Throughput: 0: 147.6. Samples: 86848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:10:13,293][139965] Avg episode reward: [(0, '22.940')]
[33m[623434 ms][navigation_task_gate] - WARNING : Gate Navigation Curriculum Level: 3, Progress: 0.0 (navigation_task_gate.py:469)
[33m[623434 ms][navigation_task_gate] - WARNING : 
[33mSuccess Rate: 0.0
[33mCrash Rate: 1.0
[33mTimeout Rate: 0.0 (navigation_task_gate.py:472)
[36m[2025-07-03 23:10:18,322][139965] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 81920. Throughput: 0: 145.6. Samples: 87264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:10:18,323][139965] Avg episode reward: [(0, '25.196')]
[36m[2025-07-03 23:10:23,345][139965] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 81920. Throughput: 0: 145.2. Samples: 88128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:10:23,345][139965] Avg episode reward: [(0, '28.425')]
[36m[2025-07-03 23:10:28,282][139965] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.7). Total num frames: 81920. Throughput: 0: 144.6. Samples: 89024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:10:28,282][139965] Avg episode reward: [(0, '31.513')]
[36m[2025-07-03 23:10:33,264][139965] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 81920. Throughput: 0: 145.1. Samples: 89472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:10:33,264][139965] Avg episode reward: [(0, '31.035')]
[36m[2025-07-03 23:10:38,264][139965] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 166.6). Total num frames: 81920. Throughput: 0: 144.5. Samples: 90336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:10:38,264][139965] Avg episode reward: [(0, '27.836')]
[GIF] Episode 400 terminated - saving GIFs (every 100 episodes)
[GIF] Saved drone depth: ./gif_episodes/episode_0004_drone_depth.gif
[GIF] Saved drone segmentation: ./gif_episodes/episode_0004_drone_seg.gif
[GIF] Saved static depth: ./gif_episodes/episode_0004_static_depth.gif
[GIF] Saved static segmentation: ./gif_episodes/episode_0004_static_seg.gif
[GIF] Saved merged dual camera: ./gif_episodes/episode_0004_merged_dual_camera.gif
[36m[2025-07-03 23:10:43,273][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 81920. Throughput: 0: 145.7. Samples: 91248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:10:43,274][139965] Avg episode reward: [(0, '25.812')]
[36m[2025-07-03 23:10:48,280][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 81920. Throughput: 0: 145.8. Samples: 91664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:10:48,280][139965] Avg episode reward: [(0, '28.472')]
[36m[2025-07-03 23:10:53,299][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 81920. Throughput: 0: 144.7. Samples: 92496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:10:53,299][139965] Avg episode reward: [(0, '27.685')]
[36m[2025-07-03 23:10:58,281][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 81920. Throughput: 0: 145.5. Samples: 93392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:10:58,282][139965] Avg episode reward: [(0, '30.947')]
[36m[2025-07-03 23:11:03,274][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 81920. Throughput: 0: 146.6. Samples: 93856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:11:03,274][139965] Avg episode reward: [(0, '32.351')]
[36m[2025-07-03 23:11:08,316][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 81920. Throughput: 0: 146.9. Samples: 94736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:11:08,317][139965] Avg episode reward: [(0, '29.295')]
[36m[2025-07-03 23:11:13,310][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 81920. Throughput: 0: 146.4. Samples: 95616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:11:13,310][139965] Avg episode reward: [(0, '29.650')]
[36m[2025-07-03 23:11:18,333][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 81920. Throughput: 0: 145.2. Samples: 96016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:11:18,333][139965] Avg episode reward: [(0, '30.309')]
[36m[2025-07-03 23:11:23,367][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 81920. Throughput: 0: 144.4. Samples: 96848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:11:23,368][139965] Avg episode reward: [(0, '32.457')]
[36m[2025-07-03 23:11:28,266][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 81920. Throughput: 0: 144.7. Samples: 97760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:11:28,267][139965] Avg episode reward: [(0, '30.980')]
[36m[2025-07-03 23:11:33,311][139965] Fps is (10 sec: 1647.6, 60 sec: 272.9, 300 sec: 166.6). Total num frames: 98304. Throughput: 0: 145.3. Samples: 98208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:11:33,312][139965] Avg episode reward: [(0, '37.180')]
[36m[2025-07-03 23:11:38,372][139965] Fps is (10 sec: 1621.2, 60 sec: 272.6, 300 sec: 166.6). Total num frames: 98304. Throughput: 0: 145.2. Samples: 99040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:11:38,372][139965] Avg episode reward: [(0, '48.682')]
[36m[2025-07-03 23:11:43,313][139965] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 166.6). Total num frames: 98304. Throughput: 0: 145.3. Samples: 99936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:11:43,314][139965] Avg episode reward: [(0, '60.768')]
[36m[2025-07-03 23:11:48,338][139965] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 166.6). Total num frames: 98304. Throughput: 0: 145.6. Samples: 100416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:11:48,339][139965] Avg episode reward: [(0, '64.619')]
[36m[2025-07-03 23:11:53,261][139965] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 166.6). Total num frames: 98304. Throughput: 0: 144.5. Samples: 101232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:11:53,261][139965] Avg episode reward: [(0, '61.461')]
[36m[2025-07-03 23:11:58,266][139965] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.7). Total num frames: 98304. Throughput: 0: 144.9. Samples: 102128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:11:58,266][139965] Avg episode reward: [(0, '55.246')]
[37m[1m[2025-07-03 23:11:58,399][139965] Saving ./train_dir/gate_config_3_7/checkpoint_p0/checkpoint_000000192_98304.pth...
[36m[2025-07-03 23:11:58,405][139965] Removing ./train_dir/gate_config_3_7/checkpoint_p0/checkpoint_000000000_0.pth
[36m[2025-07-03 23:12:03,345][139965] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 166.6). Total num frames: 98304. Throughput: 0: 145.4. Samples: 102560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:12:03,345][139965] Avg episode reward: [(0, '52.919')]
[36m[2025-07-03 23:12:08,278][139965] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 166.6). Total num frames: 98304. Throughput: 0: 146.1. Samples: 103408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:12:08,278][139965] Avg episode reward: [(0, '57.146')]
[36m[2025-07-03 23:12:13,381][139965] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 166.6). Total num frames: 98304. Throughput: 0: 145.4. Samples: 104320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:12:13,382][139965] Avg episode reward: [(0, '65.435')]
[36m[2025-07-03 23:12:18,379][139965] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 166.6). Total num frames: 98304. Throughput: 0: 145.2. Samples: 104752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:12:18,379][139965] Avg episode reward: [(0, '77.445')]
[36m[2025-07-03 23:12:23,358][139965] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 98304. Throughput: 0: 145.5. Samples: 105584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:12:23,358][139965] Avg episode reward: [(0, '71.678')]
[36m[2025-07-03 23:12:28,270][139965] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 98304. Throughput: 0: 145.2. Samples: 106464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:12:28,270][139965] Avg episode reward: [(0, '61.699')]
[36m[2025-07-03 23:12:33,306][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 98304. Throughput: 0: 144.8. Samples: 106928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:12:33,306][139965] Avg episode reward: [(0, '56.623')]
[36m[2025-07-03 23:12:38,377][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 98304. Throughput: 0: 147.5. Samples: 107888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:12:38,378][139965] Avg episode reward: [(0, '58.293')]
[33m[769139 ms][navigation_task_gate] - WARNING : Gate Navigation Curriculum Level: 3, Progress: 0.0 (navigation_task_gate.py:469)
[33m[769139 ms][navigation_task_gate] - WARNING : 
[33mSuccess Rate: 0.0
[33mCrash Rate: 1.0
[33mTimeout Rate: 0.0 (navigation_task_gate.py:472)
[36m[2025-07-03 23:12:43,378][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 98304. Throughput: 0: 146.8. Samples: 108752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:12:43,379][139965] Avg episode reward: [(0, '59.086')]
[36m[2025-07-03 23:12:48,285][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 98304. Throughput: 0: 148.1. Samples: 109216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:12:48,285][139965] Avg episode reward: [(0, '57.206')]
[36m[2025-07-03 23:12:53,299][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 98304. Throughput: 0: 147.5. Samples: 110048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:12:53,300][139965] Avg episode reward: [(0, '57.415')]
[36m[2025-07-03 23:12:58,350][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 98304. Throughput: 0: 147.3. Samples: 110944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:12:58,350][139965] Avg episode reward: [(0, '53.970')]
[36m[2025-07-03 23:13:03,259][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 98304. Throughput: 0: 148.7. Samples: 111424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:13:03,259][139965] Avg episode reward: [(0, '58.201')]
[36m[2025-07-03 23:13:08,311][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 98304. Throughput: 0: 150.2. Samples: 112336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:13:08,311][139965] Avg episode reward: [(0, '60.772')]
[36m[2025-07-03 23:13:13,338][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 98304. Throughput: 0: 149.1. Samples: 113184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:13:13,338][139965] Avg episode reward: [(0, '64.509')]
[36m[2025-07-03 23:13:18,299][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 98304. Throughput: 0: 148.3. Samples: 113600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:13:18,300][139965] Avg episode reward: [(0, '66.648')]
[36m[2025-07-03 23:13:23,674][139965] Fps is (10 sec: 1585.0, 60 sec: 271.6, 300 sec: 166.4). Total num frames: 114688. Throughput: 0: 145.2. Samples: 114464. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:13:23,675][139965] Avg episode reward: [(0, '67.668')]
[37m[1m[2025-07-03 23:13:23,785][139965] Saving new best policy, reward=67.668!
[36m[2025-07-03 23:13:28,295][139965] Fps is (10 sec: 1639.1, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 114688. Throughput: 0: 144.6. Samples: 115248. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:13:28,295][139965] Avg episode reward: [(0, '73.319')]
[37m[1m[2025-07-03 23:13:28,398][139965] Saving new best policy, reward=73.319!
[36m[2025-07-03 23:13:33,351][139965] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 166.6). Total num frames: 114688. Throughput: 0: 144.5. Samples: 115728. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:13:33,351][139965] Avg episode reward: [(0, '78.037')]
[37m[1m[2025-07-03 23:13:33,459][139965] Saving new best policy, reward=78.037!
[36m[2025-07-03 23:13:38,361][139965] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 114688. Throughput: 0: 144.9. Samples: 116576. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:13:38,361][139965] Avg episode reward: [(0, '92.690')]
[37m[1m[2025-07-03 23:13:38,508][139965] Saving new best policy, reward=92.690!
[36m[2025-07-03 23:13:43,361][139965] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 114688. Throughput: 0: 144.0. Samples: 117424. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:13:43,361][139965] Avg episode reward: [(0, '106.317')]
[37m[1m[2025-07-03 23:13:43,482][139965] Saving new best policy, reward=106.317!
[36m[2025-07-03 23:13:48,340][139965] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 166.6). Total num frames: 114688. Throughput: 0: 143.0. Samples: 117872. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:13:48,340][139965] Avg episode reward: [(0, '122.841')]
[37m[1m[2025-07-03 23:13:48,434][139965] Saving new best policy, reward=122.841!
[36m[2025-07-03 23:13:53,314][139965] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 114688. Throughput: 0: 143.3. Samples: 118784. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:13:53,314][139965] Avg episode reward: [(0, '122.950')]
[37m[1m[2025-07-03 23:13:53,413][139965] Saving new best policy, reward=122.950!
[36m[2025-07-03 23:13:58,317][139965] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 166.6). Total num frames: 114688. Throughput: 0: 143.7. Samples: 119648. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:13:58,318][139965] Avg episode reward: [(0, '119.773')]
[37m[1m[2025-07-03 23:13:58,419][139965] Saving ./train_dir/gate_config_3_7/checkpoint_p0/checkpoint_000000224_114688.pth...
[36m[2025-07-03 23:13:58,428][139965] Removing ./train_dir/gate_config_3_7/checkpoint_p0/checkpoint_000000032_16384.pth
[36m[2025-07-03 23:14:03,262][139965] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 114688. Throughput: 0: 144.5. Samples: 120096. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:14:03,262][139965] Avg episode reward: [(0, '109.217')]
[36m[2025-07-03 23:14:08,296][139965] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 114688. Throughput: 0: 147.7. Samples: 121056. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:14:08,296][139965] Avg episode reward: [(0, '101.239')]
[36m[2025-07-03 23:14:13,316][139965] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 166.6). Total num frames: 114688. Throughput: 0: 149.3. Samples: 121968. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:14:13,316][139965] Avg episode reward: [(0, '104.266')]
[36m[2025-07-03 23:14:18,278][139965] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 166.6). Total num frames: 114688. Throughput: 0: 148.5. Samples: 122400. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:14:18,278][139965] Avg episode reward: [(0, '103.085')]
[36m[2025-07-03 23:14:23,315][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 114688. Throughput: 0: 150.9. Samples: 123360. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:14:23,316][139965] Avg episode reward: [(0, '109.002')]
[36m[2025-07-03 23:14:28,346][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 114688. Throughput: 0: 151.9. Samples: 124256. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:14:28,346][139965] Avg episode reward: [(0, '115.927')]
[36m[2025-07-03 23:14:33,275][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 114688. Throughput: 0: 151.3. Samples: 124672. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:14:33,275][139965] Avg episode reward: [(0, '117.631')]
[36m[2025-07-03 23:14:38,318][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 114688. Throughput: 0: 151.1. Samples: 125584. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:14:38,318][139965] Avg episode reward: [(0, '117.615')]
[36m[2025-07-03 23:14:43,324][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 114688. Throughput: 0: 150.4. Samples: 126416. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:14:43,325][139965] Avg episode reward: [(0, '118.464')]
[36m[2025-07-03 23:14:48,296][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 114688. Throughput: 0: 149.9. Samples: 126848. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:14:48,296][139965] Avg episode reward: [(0, '124.830')]
[37m[1m[2025-07-03 23:14:48,386][139965] Saving new best policy, reward=124.830!
[36m[2025-07-03 23:14:53,343][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 114688. Throughput: 0: 149.2. Samples: 127776. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:14:53,344][139965] Avg episode reward: [(0, '122.440')]
[GIF] Episode 500 terminated - saving GIFs (every 100 episodes)
[GIF] Saved drone depth: ./gif_episodes/episode_0005_drone_depth.gif
[GIF] Saved drone segmentation: ./gif_episodes/episode_0005_drone_seg.gif
[GIF] Saved static depth: ./gif_episodes/episode_0005_static_depth.gif
[GIF] Saved static segmentation: ./gif_episodes/episode_0005_static_seg.gif
[GIF] Saved merged dual camera: ./gif_episodes/episode_0005_merged_dual_camera.gif
[36m[2025-07-03 23:14:58,405][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.0). Total num frames: 114688. Throughput: 0: 147.3. Samples: 128608. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:14:58,405][139965] Avg episode reward: [(0, '125.027')]
[37m[1m[2025-07-03 23:14:58,505][139965] Saving new best policy, reward=125.027!
[36m[2025-07-03 23:15:03,331][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 114688. Throughput: 0: 148.1. Samples: 129072. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:15:03,332][139965] Avg episode reward: [(0, '118.039')]
[36m[2025-07-03 23:15:08,261][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 114688. Throughput: 0: 148.1. Samples: 130016. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:15:08,262][139965] Avg episode reward: [(0, '112.718')]
[36m[2025-07-03 23:15:13,364][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 114688. Throughput: 0: 146.4. Samples: 130848. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:15:13,364][139965] Avg episode reward: [(0, '126.282')]
[37m[1m[2025-07-03 23:15:13,487][139965] Saving new best policy, reward=126.282!
[36m[2025-07-03 23:15:18,288][139965] Fps is (10 sec: 1634.1, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 131072. Throughput: 0: 145.4. Samples: 131216. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 23:15:18,288][139965] Avg episode reward: [(0, '123.646')]
[36m[2025-07-03 23:15:23,299][139965] Fps is (10 sec: 1649.1, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 131072. Throughput: 0: 145.5. Samples: 132128. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 23:15:23,299][139965] Avg episode reward: [(0, '137.168')]
[37m[1m[2025-07-03 23:15:23,387][139965] Saving new best policy, reward=137.168!
[36m[2025-07-03 23:15:28,352][139965] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 131072. Throughput: 0: 146.8. Samples: 133024. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 23:15:28,353][139965] Avg episode reward: [(0, '149.643')]
[37m[1m[2025-07-03 23:15:28,476][139965] Saving new best policy, reward=149.643!
[36m[2025-07-03 23:15:33,306][139965] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 166.6). Total num frames: 131072. Throughput: 0: 146.8. Samples: 133456. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 23:15:33,307][139965] Avg episode reward: [(0, '157.231')]
[37m[1m[2025-07-03 23:15:33,434][139965] Saving new best policy, reward=157.231!
[36m[2025-07-03 23:15:38,316][139965] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 131072. Throughput: 0: 145.2. Samples: 134304. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 23:15:38,317][139965] Avg episode reward: [(0, '165.699')]
[37m[1m[2025-07-03 23:15:38,489][139965] Saving new best policy, reward=165.699!
[36m[2025-07-03 23:15:43,331][139965] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 131072. Throughput: 0: 147.8. Samples: 135248. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 23:15:43,332][139965] Avg episode reward: [(0, '180.151')]
[37m[1m[2025-07-03 23:15:43,452][139965] Saving new best policy, reward=180.151!
[36m[2025-07-03 23:15:48,295][139965] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 131072. Throughput: 0: 147.0. Samples: 135680. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 23:15:48,296][139965] Avg episode reward: [(0, '168.278')]
[36m[2025-07-03 23:15:53,380][139965] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 166.6). Total num frames: 131072. Throughput: 0: 143.6. Samples: 136496. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 23:15:53,380][139965] Avg episode reward: [(0, '174.672')]
[36m[2025-07-03 23:15:58,279][139965] Fps is (10 sec: 0.0, 60 sec: 273.6, 300 sec: 166.6). Total num frames: 131072. Throughput: 0: 145.0. Samples: 137360. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 23:15:58,279][139965] Avg episode reward: [(0, '183.983')]
[37m[1m[2025-07-03 23:15:58,376][139965] Saving ./train_dir/gate_config_3_7/checkpoint_p0/checkpoint_000000256_131072.pth...
[36m[2025-07-03 23:15:58,383][139965] Removing ./train_dir/gate_config_3_7/checkpoint_p0/checkpoint_000000096_49152.pth
[37m[1m[2025-07-03 23:15:58,383][139965] Saving new best policy, reward=183.983!
[33m[969926 ms][navigation_task_gate] - WARNING : Gate Navigation Curriculum Level: 3, Progress: 0.0 (navigation_task_gate.py:469)
[33m[969926 ms][navigation_task_gate] - WARNING : 
[33mSuccess Rate: 0.0
[33mCrash Rate: 1.0
[33mTimeout Rate: 0.0 (navigation_task_gate.py:472)
[36m[2025-07-03 23:16:03,286][139965] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 166.6). Total num frames: 131072. Throughput: 0: 146.5. Samples: 137808. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 23:16:03,286][139965] Avg episode reward: [(0, '177.656')]
[36m[2025-07-03 23:16:08,361][139965] Fps is (10 sec: 0.0, 60 sec: 272.6, 300 sec: 166.6). Total num frames: 131072. Throughput: 0: 146.3. Samples: 138720. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 23:16:08,361][139965] Avg episode reward: [(0, '180.195')]
[36m[2025-07-03 23:16:13,317][139965] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 166.6). Total num frames: 131072. Throughput: 0: 145.9. Samples: 139584. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 23:16:13,317][139965] Avg episode reward: [(0, '201.545')]
[37m[1m[2025-07-03 23:16:13,405][139965] Saving new best policy, reward=201.545!
[36m[2025-07-03 23:16:18,354][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 131072. Throughput: 0: 145.3. Samples: 140000. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 23:16:18,354][139965] Avg episode reward: [(0, '208.105')]
[37m[1m[2025-07-03 23:16:18,361][139965] Saving new best policy, reward=208.105!
[36m[2025-07-03 23:16:23,325][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 131072. Throughput: 0: 145.4. Samples: 140848. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 23:16:23,325][139965] Avg episode reward: [(0, '211.137')]
[37m[1m[2025-07-03 23:16:23,415][139965] Saving new best policy, reward=211.137!
[36m[2025-07-03 23:16:28,300][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 131072. Throughput: 0: 143.0. Samples: 141680. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 23:16:28,300][139965] Avg episode reward: [(0, '216.476')]
[37m[1m[2025-07-03 23:16:28,427][139965] Saving new best policy, reward=216.476!
[36m[2025-07-03 23:16:33,298][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 131072. Throughput: 0: 141.9. Samples: 142064. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 23:16:33,298][139965] Avg episode reward: [(0, '225.698')]
[37m[1m[2025-07-03 23:16:33,383][139965] Saving new best policy, reward=225.698!
[36m[2025-07-03 23:16:38,322][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 131072. Throughput: 0: 145.6. Samples: 143040. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 23:16:38,322][139965] Avg episode reward: [(0, '229.316')]
[37m[1m[2025-07-03 23:16:38,434][139965] Saving new best policy, reward=229.316!
[36m[2025-07-03 23:16:43,262][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 131072. Throughput: 0: 147.3. Samples: 143984. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 23:16:43,263][139965] Avg episode reward: [(0, '226.065')]
[36m[2025-07-03 23:16:48,316][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 131072. Throughput: 0: 147.1. Samples: 144432. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 23:16:48,317][139965] Avg episode reward: [(0, '205.165')]
[36m[2025-07-03 23:16:53,308][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 131072. Throughput: 0: 147.4. Samples: 145344. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 23:16:53,309][139965] Avg episode reward: [(0, '196.960')]
[36m[2025-07-03 23:16:58,321][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 131072. Throughput: 0: 147.9. Samples: 146240. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 23:16:58,321][139965] Avg episode reward: [(0, '197.531')]
[36m[2025-07-03 23:17:03,348][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 131072. Throughput: 0: 148.3. Samples: 146672. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 23:17:03,349][139965] Avg episode reward: [(0, '182.199')]
[36m[2025-07-03 23:17:08,305][139965] Fps is (10 sec: 1641.1, 60 sec: 273.3, 300 sec: 166.7). Total num frames: 147456. Throughput: 0: 146.6. Samples: 147440. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 23:17:08,305][139965] Avg episode reward: [(0, '177.519')]
[36m[2025-07-03 23:17:13,335][139965] Fps is (10 sec: 1640.6, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 147456. Throughput: 0: 147.8. Samples: 148336. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 23:17:13,335][139965] Avg episode reward: [(0, '187.042')]
[36m[2025-07-03 23:17:18,339][139965] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 147456. Throughput: 0: 147.8. Samples: 148720. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 23:17:18,339][139965] Avg episode reward: [(0, '246.584')]
[37m[1m[2025-07-03 23:17:18,426][139965] Saving new best policy, reward=246.584!
[36m[2025-07-03 23:17:23,348][139965] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 147456. Throughput: 0: 145.7. Samples: 149600. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 23:17:23,348][139965] Avg episode reward: [(0, '266.574')]
[37m[1m[2025-07-03 23:17:23,459][139965] Saving new best policy, reward=266.574!
[36m[2025-07-03 23:17:28,309][139965] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 147456. Throughput: 0: 143.8. Samples: 150464. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 23:17:28,310][139965] Avg episode reward: [(0, '301.674')]
[37m[1m[2025-07-03 23:17:28,398][139965] Saving new best policy, reward=301.674!
[36m[2025-07-03 23:17:33,269][139965] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 166.7). Total num frames: 147456. Throughput: 0: 144.5. Samples: 150928. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 23:17:33,270][139965] Avg episode reward: [(0, '352.175')]
[37m[1m[2025-07-03 23:17:33,354][139965] Saving new best policy, reward=352.175!
[36m[2025-07-03 23:17:38,341][139965] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 147456. Throughput: 0: 144.6. Samples: 151856. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 23:17:38,342][139965] Avg episode reward: [(0, '360.330')]
[37m[1m[2025-07-03 23:17:38,496][139965] Saving new best policy, reward=360.330!
[36m[2025-07-03 23:17:43,301][139965] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 166.6). Total num frames: 147456. Throughput: 0: 145.8. Samples: 152800. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 23:17:43,301][139965] Avg episode reward: [(0, '395.308')]
[37m[1m[2025-07-03 23:17:43,383][139965] Saving new best policy, reward=395.308!
[36m[2025-07-03 23:17:48,345][139965] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 166.6). Total num frames: 147456. Throughput: 0: 147.6. Samples: 153312. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 23:17:48,346][139965] Avg episode reward: [(0, '440.795')]
[37m[1m[2025-07-03 23:17:48,515][139965] Saving new best policy, reward=440.795!
[36m[2025-07-03 23:17:53,360][139965] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 166.6). Total num frames: 147456. Throughput: 0: 149.5. Samples: 154176. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 23:17:53,360][139965] Avg episode reward: [(0, '413.469')]
[36m[2025-07-03 23:17:58,269][139965] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 166.6). Total num frames: 147456. Throughput: 0: 148.5. Samples: 155008. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 23:17:58,269][139965] Avg episode reward: [(0, '399.347')]
[37m[1m[2025-07-03 23:17:58,386][139965] Saving ./train_dir/gate_config_3_7/checkpoint_p0/checkpoint_000000288_147456.pth...
[36m[2025-07-03 23:17:58,408][139965] Removing ./train_dir/gate_config_3_7/checkpoint_p0/checkpoint_000000128_65536.pth
[36m[2025-07-03 23:18:03,314][139965] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 166.6). Total num frames: 147456. Throughput: 0: 149.8. Samples: 155456. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 23:18:03,315][139965] Avg episode reward: [(0, '386.623')]
[36m[2025-07-03 23:18:08,383][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 147456. Throughput: 0: 150.3. Samples: 156368. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 23:18:08,384][139965] Avg episode reward: [(0, '389.153')]
[36m[2025-07-03 23:18:13,384][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 147456. Throughput: 0: 150.5. Samples: 157248. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 23:18:13,384][139965] Avg episode reward: [(0, '399.988')]
[36m[2025-07-03 23:18:18,298][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.2). Total num frames: 147456. Throughput: 0: 149.9. Samples: 157680. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 23:18:18,299][139965] Avg episode reward: [(0, '419.274')]
[36m[2025-07-03 23:18:23,364][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 147456. Throughput: 0: 149.3. Samples: 158576. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 23:18:23,364][139965] Avg episode reward: [(0, '413.906')]
[36m[2025-07-03 23:18:28,280][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 147456. Throughput: 0: 147.6. Samples: 159440. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 23:18:28,280][139965] Avg episode reward: [(0, '432.165')]
[36m[2025-07-03 23:18:33,341][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 147456. Throughput: 0: 146.5. Samples: 159904. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 23:18:33,342][139965] Avg episode reward: [(0, '435.826')]
[36m[2025-07-03 23:18:38,339][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 147456. Throughput: 0: 148.7. Samples: 160864. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 23:18:38,339][139965] Avg episode reward: [(0, '459.523')]
[37m[1m[2025-07-03 23:18:38,501][139965] Saving new best policy, reward=459.523!
[36m[2025-07-03 23:18:43,259][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 147456. Throughput: 0: 151.9. Samples: 161840. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 23:18:43,259][139965] Avg episode reward: [(0, '457.632')]
[36m[2025-07-03 23:18:48,281][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 147456. Throughput: 0: 151.9. Samples: 162288. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 23:18:48,281][139965] Avg episode reward: [(0, '451.318')]
[36m[2025-07-03 23:18:53,400][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.0). Total num frames: 147456. Throughput: 0: 153.2. Samples: 163264. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 23:18:53,400][139965] Avg episode reward: [(0, '442.323')]
[36m[2025-07-03 23:18:58,380][139965] Fps is (10 sec: 1622.3, 60 sec: 272.6, 300 sec: 166.6). Total num frames: 163840. Throughput: 0: 152.2. Samples: 164096. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-03 23:18:58,380][139965] Avg episode reward: [(0, '462.747')]
[37m[1m[2025-07-03 23:18:58,392][139965] Saving new best policy, reward=462.747!
[36m[2025-07-03 23:19:03,362][139965] Fps is (10 sec: 1644.6, 60 sec: 272.8, 300 sec: 166.6). Total num frames: 163840. Throughput: 0: 152.0. Samples: 164528. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-03 23:19:03,363][139965] Avg episode reward: [(0, '475.225')]
[37m[1m[2025-07-03 23:19:03,486][139965] Saving new best policy, reward=475.225!
[36m[2025-07-03 23:19:08,305][139965] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 166.6). Total num frames: 163840. Throughput: 0: 151.7. Samples: 165392. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-03 23:19:08,306][139965] Avg episode reward: [(0, '485.152')]
[37m[1m[2025-07-03 23:19:08,463][139965] Saving new best policy, reward=485.152!
[36m[2025-07-03 23:19:13,347][139965] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 166.6). Total num frames: 163840. Throughput: 0: 150.9. Samples: 166240. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-03 23:19:13,347][139965] Avg episode reward: [(0, '493.647')]
[37m[1m[2025-07-03 23:19:13,491][139965] Saving new best policy, reward=493.647!
[36m[2025-07-03 23:19:18,278][139965] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 166.6). Total num frames: 163840. Throughput: 0: 150.6. Samples: 166672. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-03 23:19:18,278][139965] Avg episode reward: [(0, '517.821')]
[37m[1m[2025-07-03 23:19:18,387][139965] Saving new best policy, reward=517.821!
[36m[2025-07-03 23:19:23,299][139965] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 166.6). Total num frames: 163840. Throughput: 0: 148.8. Samples: 167552. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-03 23:19:23,299][139965] Avg episode reward: [(0, '589.001')]
[37m[1m[2025-07-03 23:19:23,435][139965] Saving new best policy, reward=589.001!
[36m[2025-07-03 23:19:28,296][139965] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 163840. Throughput: 0: 146.7. Samples: 168448. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-03 23:19:28,297][139965] Avg episode reward: [(0, '608.653')]
[37m[1m[2025-07-03 23:19:28,409][139965] Saving new best policy, reward=608.653!
[36m[2025-07-03 23:19:33,316][139965] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 166.6). Total num frames: 163840. Throughput: 0: 146.7. Samples: 168896. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-03 23:19:33,317][139965] Avg episode reward: [(0, '657.277')]
[37m[1m[2025-07-03 23:19:33,417][139965] Saving new best policy, reward=657.277!
[36m[2025-07-03 23:19:38,284][139965] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 166.6). Total num frames: 163840. Throughput: 0: 145.8. Samples: 169808. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-03 23:19:38,285][139965] Avg episode reward: [(0, '685.517')]
[37m[1m[2025-07-03 23:19:38,416][139965] Saving new best policy, reward=685.517!
[36m[2025-07-03 23:19:43,268][139965] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 163840. Throughput: 0: 147.6. Samples: 170720. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-03 23:19:43,269][139965] Avg episode reward: [(0, '728.111')]
[37m[1m[2025-07-03 23:19:43,370][139965] Saving new best policy, reward=728.111!
[36m[2025-07-03 23:19:48,328][139965] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 166.6). Total num frames: 163840. Throughput: 0: 148.0. Samples: 171184. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-03 23:19:48,328][139965] Avg episode reward: [(0, '761.402')]
[37m[1m[2025-07-03 23:19:48,470][139965] Saving new best policy, reward=761.402!
[36m[2025-07-03 23:19:53,372][139965] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 166.6). Total num frames: 163840. Throughput: 0: 149.1. Samples: 172112. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-03 23:19:53,372][139965] Avg episode reward: [(0, '811.553')]
[37m[1m[2025-07-03 23:19:53,491][139965] Saving new best policy, reward=811.553!
[36m[2025-07-03 23:19:58,390][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 163840. Throughput: 0: 149.2. Samples: 172960. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-03 23:19:58,391][139965] Avg episode reward: [(0, '815.048')]
[37m[1m[2025-07-03 23:19:58,542][139965] Saving ./train_dir/gate_config_3_7/checkpoint_p0/checkpoint_000000320_163840.pth...
[36m[2025-07-03 23:19:58,549][139965] Removing ./train_dir/gate_config_3_7/checkpoint_p0/checkpoint_000000160_81920.pth
[37m[1m[2025-07-03 23:19:58,550][139965] Saving new best policy, reward=815.048!
[36m[2025-07-03 23:20:03,299][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 163840. Throughput: 0: 148.2. Samples: 173344. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-03 23:20:03,300][139965] Avg episode reward: [(0, '829.768')]
[37m[1m[2025-07-03 23:20:03,399][139965] Saving new best policy, reward=829.768!
[36m[2025-07-03 23:20:08,301][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.7). Total num frames: 163840. Throughput: 0: 148.6. Samples: 174240. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-03 23:20:08,302][139965] Avg episode reward: [(0, '873.204')]
[37m[1m[2025-07-03 23:20:08,453][139965] Saving new best policy, reward=873.204!
[36m[2025-07-03 23:20:13,336][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 163840. Throughput: 0: 147.8. Samples: 175104. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-03 23:20:13,336][139965] Avg episode reward: [(0, '881.887')]
[37m[1m[2025-07-03 23:20:13,431][139965] Saving new best policy, reward=881.887!
[36m[2025-07-03 23:20:18,263][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 163840. Throughput: 0: 148.1. Samples: 175552. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-03 23:20:18,264][139965] Avg episode reward: [(0, '871.631')]
[36m[2025-07-03 23:20:23,282][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 163840. Throughput: 0: 150.8. Samples: 176592. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-03 23:20:23,283][139965] Avg episode reward: [(0, '868.844')]
[36m[2025-07-03 23:20:28,354][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 163840. Throughput: 0: 151.2. Samples: 177536. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-03 23:20:28,355][139965] Avg episode reward: [(0, '871.414')]
[36m[2025-07-03 23:20:33,384][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 163840. Throughput: 0: 150.2. Samples: 177952. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-03 23:20:33,385][139965] Avg episode reward: [(0, '887.761')]
[37m[1m[2025-07-03 23:20:33,554][139965] Saving new best policy, reward=887.761!
[36m[2025-07-03 23:20:38,325][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 163840. Throughput: 0: 149.5. Samples: 178832. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-03 23:20:38,326][139965] Avg episode reward: [(0, '873.245')]
[36m[2025-07-03 23:20:43,353][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 163840. Throughput: 0: 148.7. Samples: 179648. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-03 23:20:43,353][139965] Avg episode reward: [(0, '852.864')]
[36m[2025-07-03 23:20:48,266][139965] Fps is (10 sec: 1648.2, 60 sec: 273.3, 300 sec: 166.7). Total num frames: 180224. Throughput: 0: 149.8. Samples: 180080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:20:48,267][139965] Avg episode reward: [(0, '825.576')]
[36m[2025-07-03 23:20:53,276][139965] Fps is (10 sec: 1651.1, 60 sec: 273.5, 300 sec: 166.6). Total num frames: 180224. Throughput: 0: 148.7. Samples: 180928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:20:53,276][139965] Avg episode reward: [(0, '838.609')]
[36m[2025-07-03 23:20:58,295][139965] Fps is (10 sec: 0.0, 60 sec: 273.5, 300 sec: 166.6). Total num frames: 180224. Throughput: 0: 149.5. Samples: 181824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:20:58,296][139965] Avg episode reward: [(0, '881.885')]
[36m[2025-07-03 23:21:03,260][139965] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 166.7). Total num frames: 180224. Throughput: 0: 149.7. Samples: 182288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:21:03,260][139965] Avg episode reward: [(0, '898.258')]
[37m[1m[2025-07-03 23:21:03,360][139965] Saving new best policy, reward=898.258!
[36m[2025-07-03 23:21:08,342][139965] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 166.6). Total num frames: 180224. Throughput: 0: 146.3. Samples: 183184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:21:08,343][139965] Avg episode reward: [(0, '918.523')]
[37m[1m[2025-07-03 23:21:08,485][139965] Saving new best policy, reward=918.523!
[36m[2025-07-03 23:21:13,344][139965] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 180224. Throughput: 0: 145.8. Samples: 184096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:21:13,345][139965] Avg episode reward: [(0, '944.732')]
[37m[1m[2025-07-03 23:21:13,445][139965] Saving new best policy, reward=944.732!
[36m[2025-07-03 23:21:18,340][139965] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 166.6). Total num frames: 180224. Throughput: 0: 146.6. Samples: 184544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:21:18,340][139965] Avg episode reward: [(0, '991.531')]
[37m[1m[2025-07-03 23:21:18,428][139965] Saving new best policy, reward=991.531!
[36m[2025-07-03 23:21:23,287][139965] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 180224. Throughput: 0: 148.7. Samples: 185520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:21:23,287][139965] Avg episode reward: [(0, '1028.899')]
[37m[1m[2025-07-03 23:21:23,386][139965] Saving new best policy, reward=1028.899!
[36m[2025-07-03 23:21:28,269][139965] Fps is (10 sec: 0.0, 60 sec: 273.5, 300 sec: 166.6). Total num frames: 180224. Throughput: 0: 151.4. Samples: 186448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:21:28,269][139965] Avg episode reward: [(0, '1095.010')]
[37m[1m[2025-07-03 23:21:28,358][139965] Saving new best policy, reward=1095.010!
[36m[2025-07-03 23:21:33,323][139965] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 166.6). Total num frames: 180224. Throughput: 0: 150.2. Samples: 186848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:21:33,323][139965] Avg episode reward: [(0, '1094.684')]
[36m[2025-07-03 23:21:38,337][139965] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 180224. Throughput: 0: 152.0. Samples: 187776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:21:38,338][139965] Avg episode reward: [(0, '1173.554')]
[37m[1m[2025-07-03 23:21:38,426][139965] Saving new best policy, reward=1173.554!
[36m[2025-07-03 23:21:43,298][139965] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 166.6). Total num frames: 180224. Throughput: 0: 152.2. Samples: 188672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:21:43,298][139965] Avg episode reward: [(0, '1241.781')]
[37m[1m[2025-07-03 23:21:43,399][139965] Saving new best policy, reward=1241.781!
[36m[2025-07-03 23:21:48,274][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 180224. Throughput: 0: 152.1. Samples: 189136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:21:48,275][139965] Avg episode reward: [(0, '1349.147')]
[37m[1m[2025-07-03 23:21:48,365][139965] Saving new best policy, reward=1349.147!
[36m[2025-07-03 23:21:53,278][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 180224. Throughput: 0: 153.5. Samples: 190080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:21:53,278][139965] Avg episode reward: [(0, '1384.535')]
[37m[1m[2025-07-03 23:21:53,360][139965] Saving new best policy, reward=1384.535!
[36m[2025-07-03 23:21:58,276][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.7). Total num frames: 180224. Throughput: 0: 154.2. Samples: 191024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:21:58,276][139965] Avg episode reward: [(0, '1416.417')]
[37m[1m[2025-07-03 23:21:58,417][139965] Saving ./train_dir/gate_config_3_7/checkpoint_p0/checkpoint_000000352_180224.pth...
[36m[2025-07-03 23:21:58,425][139965] Removing ./train_dir/gate_config_3_7/checkpoint_p0/checkpoint_000000192_98304.pth
[37m[1m[2025-07-03 23:21:58,426][139965] Saving new best policy, reward=1416.417!
[36m[2025-07-03 23:22:03,289][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 180224. Throughput: 0: 153.4. Samples: 191440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:22:03,290][139965] Avg episode reward: [(0, '1440.654')]
[37m[1m[2025-07-03 23:22:03,439][139965] Saving new best policy, reward=1440.654!
[36m[2025-07-03 23:22:08,375][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 180224. Throughput: 0: 150.5. Samples: 192304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:22:08,375][139965] Avg episode reward: [(0, '1436.336')]
[36m[2025-07-03 23:22:13,268][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 180224. Throughput: 0: 150.0. Samples: 193200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:22:13,268][139965] Avg episode reward: [(0, '1500.243')]
[37m[1m[2025-07-03 23:22:13,367][139965] Saving new best policy, reward=1500.243!
[36m[2025-07-03 23:22:18,336][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 180224. Throughput: 0: 152.1. Samples: 193696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:22:18,336][139965] Avg episode reward: [(0, '1493.872')]
[36m[2025-07-03 23:22:23,287][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 180224. Throughput: 0: 152.0. Samples: 194608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:22:23,287][139965] Avg episode reward: [(0, '1513.983')]
[37m[1m[2025-07-03 23:22:23,399][139965] Saving new best policy, reward=1513.983!
[36m[2025-07-03 23:22:28,290][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 180224. Throughput: 0: 152.6. Samples: 195536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:22:28,291][139965] Avg episode reward: [(0, '1501.398')]
[36m[2025-07-03 23:22:33,320][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 180224. Throughput: 0: 151.3. Samples: 195952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:22:33,321][139965] Avg episode reward: [(0, '1510.790')]
[36m[2025-07-03 23:22:38,287][139965] Fps is (10 sec: 1638.9, 60 sec: 273.3, 300 sec: 166.6). Total num frames: 196608. Throughput: 0: 151.1. Samples: 196880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:22:38,288][139965] Avg episode reward: [(0, '1535.866')]
[37m[1m[2025-07-03 23:22:38,413][139965] Saving new best policy, reward=1535.866!
[36m[2025-07-03 23:22:43,294][139965] Fps is (10 sec: 1642.7, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 196608. Throughput: 0: 150.7. Samples: 197808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:22:43,295][139965] Avg episode reward: [(0, '1590.870')]
[37m[1m[2025-07-03 23:22:43,380][139965] Saving new best policy, reward=1590.870!
[36m[2025-07-03 23:22:48,350][139965] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 166.6). Total num frames: 196608. Throughput: 0: 150.2. Samples: 198208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:22:48,350][139965] Avg episode reward: [(0, '1668.253')]
[37m[1m[2025-07-03 23:22:48,455][139965] Saving new best policy, reward=1668.253!
[36m[2025-07-03 23:22:53,337][139965] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 166.6). Total num frames: 196608. Throughput: 0: 151.6. Samples: 199120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:22:53,337][139965] Avg episode reward: [(0, '1729.556')]
[37m[1m[2025-07-03 23:22:53,439][139965] Saving new best policy, reward=1729.556!
[36m[2025-07-03 23:22:58,277][139965] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 196608. Throughput: 0: 151.8. Samples: 200032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:22:58,277][139965] Avg episode reward: [(0, '1801.648')]
[37m[1m[2025-07-03 23:22:58,379][139965] Saving new best policy, reward=1801.648!
[36m[2025-07-03 23:23:03,317][139965] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 166.7). Total num frames: 196608. Throughput: 0: 150.5. Samples: 200464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:23:03,318][139965] Avg episode reward: [(0, '1890.275')]
[37m[1m[2025-07-03 23:23:03,415][139965] Saving new best policy, reward=1890.275!
[36m[2025-07-03 23:23:08,297][139965] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 166.7). Total num frames: 196608. Throughput: 0: 149.7. Samples: 201344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:23:08,298][139965] Avg episode reward: [(0, '1982.773')]
[37m[1m[2025-07-03 23:23:08,394][139965] Saving new best policy, reward=1982.773!
[36m[2025-07-03 23:23:13,345][139965] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 166.6). Total num frames: 196608. Throughput: 0: 147.4. Samples: 202176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:23:13,346][139965] Avg episode reward: [(0, '1978.475')]
[36m[2025-07-03 23:23:18,284][139965] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 166.7). Total num frames: 196608. Throughput: 0: 148.7. Samples: 202640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:23:18,284][139965] Avg episode reward: [(0, '2064.390')]
[37m[1m[2025-07-03 23:23:18,388][139965] Saving new best policy, reward=2064.390!
[36m[2025-07-03 23:23:23,326][139965] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 166.6). Total num frames: 196608. Throughput: 0: 147.4. Samples: 203520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:23:23,327][139965] Avg episode reward: [(0, '2075.852')]
[37m[1m[2025-07-03 23:23:23,428][139965] Saving new best policy, reward=2075.852!
[36m[2025-07-03 23:23:28,333][139965] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 166.6). Total num frames: 196608. Throughput: 0: 147.1. Samples: 204432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:23:28,333][139965] Avg episode reward: [(0, '2193.650')]
[37m[1m[2025-07-03 23:23:28,431][139965] Saving new best policy, reward=2193.650!
[36m[2025-07-03 23:23:33,324][139965] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 196608. Throughput: 0: 147.3. Samples: 204832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:23:33,325][139965] Avg episode reward: [(0, '2194.909')]
[37m[1m[2025-07-03 23:23:33,459][139965] Saving new best policy, reward=2194.909!
[36m[2025-07-03 23:23:38,336][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 196608. Throughput: 0: 147.9. Samples: 205776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:23:38,337][139965] Avg episode reward: [(0, '2293.412')]
[37m[1m[2025-07-03 23:23:38,480][139965] Saving new best policy, reward=2293.412!
[36m[2025-07-03 23:23:43,343][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 196608. Throughput: 0: 146.6. Samples: 206640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:23:43,343][139965] Avg episode reward: [(0, '2336.708')]
[37m[1m[2025-07-03 23:23:43,449][139965] Saving new best policy, reward=2336.708!
[36m[2025-07-03 23:23:48,356][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 196608. Throughput: 0: 147.8. Samples: 207120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:23:48,357][139965] Avg episode reward: [(0, '2310.003')]
[36m[2025-07-03 23:23:53,287][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 196608. Throughput: 0: 147.6. Samples: 207984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:23:53,287][139965] Avg episode reward: [(0, '2227.185')]
[36m[2025-07-03 23:23:58,315][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 196608. Throughput: 0: 149.8. Samples: 208912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:23:58,316][139965] Avg episode reward: [(0, '2239.737')]
[37m[1m[2025-07-03 23:23:58,478][139965] Saving ./train_dir/gate_config_3_7/checkpoint_p0/checkpoint_000000384_196608.pth...
[36m[2025-07-03 23:23:58,485][139965] Removing ./train_dir/gate_config_3_7/checkpoint_p0/checkpoint_000000224_114688.pth
[36m[2025-07-03 23:24:03,299][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 196608. Throughput: 0: 148.9. Samples: 209344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:24:03,300][139965] Avg episode reward: [(0, '2269.239')]
[33m[1453835 ms][navigation_task_gate] - WARNING : Gate Navigation Curriculum Level: 3, Progress: 0.0 (navigation_task_gate.py:469)
[33m[1453835 ms][navigation_task_gate] - WARNING : 
[33mSuccess Rate: 0.0
[33mCrash Rate: 0.5556640625
[33mTimeout Rate: 0.4443359375 (navigation_task_gate.py:472)
[36m[2025-07-03 23:24:08,308][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 196608. Throughput: 0: 150.1. Samples: 210272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:24:08,308][139965] Avg episode reward: [(0, '2207.391')]
[36m[2025-07-03 23:24:13,365][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.0). Total num frames: 196608. Throughput: 0: 148.5. Samples: 211120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:24:13,366][139965] Avg episode reward: [(0, '2181.477')]
[36m[2025-07-03 23:24:18,302][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 196608. Throughput: 0: 149.4. Samples: 211552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:24:18,302][139965] Avg episode reward: [(0, '2180.235')]
[36m[2025-07-03 23:24:23,268][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 196608. Throughput: 0: 149.6. Samples: 212496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:24:23,268][139965] Avg episode reward: [(0, '2203.391')]
[36m[2025-07-03 23:24:28,322][139965] Fps is (10 sec: 1635.0, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 212992. Throughput: 0: 148.7. Samples: 213328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:24:28,323][139965] Avg episode reward: [(0, '2163.519')]
[36m[2025-07-03 23:24:33,275][139965] Fps is (10 sec: 1637.3, 60 sec: 273.3, 300 sec: 166.6). Total num frames: 212992. Throughput: 0: 148.2. Samples: 213776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:24:33,275][139965] Avg episode reward: [(0, '2204.489')]
[36m[2025-07-03 23:24:38,373][139965] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 166.6). Total num frames: 212992. Throughput: 0: 149.8. Samples: 214736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:24:38,374][139965] Avg episode reward: [(0, '2212.649')]
[36m[2025-07-03 23:24:43,323][139965] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 166.6). Total num frames: 212992. Throughput: 0: 149.0. Samples: 215616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:24:43,323][139965] Avg episode reward: [(0, '2212.272')]
[36m[2025-07-03 23:24:48,308][139965] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 166.7). Total num frames: 212992. Throughput: 0: 148.9. Samples: 216048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:24:48,308][139965] Avg episode reward: [(0, '2244.837')]
[36m[2025-07-03 23:24:53,328][139965] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 166.7). Total num frames: 212992. Throughput: 0: 148.6. Samples: 216960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:24:53,329][139965] Avg episode reward: [(0, '2353.958')]
[37m[1m[2025-07-03 23:24:53,412][139965] Saving new best policy, reward=2353.958!
[36m[2025-07-03 23:24:58,357][139965] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 166.6). Total num frames: 212992. Throughput: 0: 150.4. Samples: 217888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:24:58,358][139965] Avg episode reward: [(0, '2395.134')]
[37m[1m[2025-07-03 23:24:58,480][139965] Saving new best policy, reward=2395.134!
[36m[2025-07-03 23:25:03,278][139965] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 166.6). Total num frames: 212992. Throughput: 0: 150.5. Samples: 218320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:25:03,278][139965] Avg episode reward: [(0, '2439.303')]
[37m[1m[2025-07-03 23:25:03,365][139965] Saving new best policy, reward=2439.303!
[36m[2025-07-03 23:25:08,302][139965] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 212992. Throughput: 0: 148.2. Samples: 219168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:25:08,302][139965] Avg episode reward: [(0, '2410.971')]
[36m[2025-07-03 23:25:13,285][139965] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 166.6). Total num frames: 212992. Throughput: 0: 151.2. Samples: 220128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:25:13,285][139965] Avg episode reward: [(0, '2477.994')]
[37m[1m[2025-07-03 23:25:13,374][139965] Saving new best policy, reward=2477.994!
[36m[2025-07-03 23:25:18,353][139965] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 166.6). Total num frames: 212992. Throughput: 0: 150.8. Samples: 220576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:25:18,354][139965] Avg episode reward: [(0, '2481.851')]
[37m[1m[2025-07-03 23:25:18,506][139965] Saving new best policy, reward=2481.851!
[36m[2025-07-03 23:25:23,277][139965] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 166.7). Total num frames: 212992. Throughput: 0: 150.7. Samples: 221504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:25:23,277][139965] Avg episode reward: [(0, '2525.145')]
[37m[1m[2025-07-03 23:25:23,379][139965] Saving new best policy, reward=2525.145!
[36m[2025-07-03 23:25:28,286][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.7). Total num frames: 212992. Throughput: 0: 150.9. Samples: 222400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:25:28,286][139965] Avg episode reward: [(0, '2568.100')]
[37m[1m[2025-07-03 23:25:28,406][139965] Saving new best policy, reward=2568.100!
[36m[2025-07-03 23:25:33,307][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 212992. Throughput: 0: 150.0. Samples: 222800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:25:33,308][139965] Avg episode reward: [(0, '2618.591')]
[37m[1m[2025-07-03 23:25:33,428][139965] Saving new best policy, reward=2618.591!
[36m[2025-07-03 23:25:38,408][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 212992. Throughput: 0: 149.8. Samples: 223712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:25:38,409][139965] Avg episode reward: [(0, '2565.328')]
[36m[2025-07-03 23:25:43,330][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 212992. Throughput: 0: 149.8. Samples: 224624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:25:43,330][139965] Avg episode reward: [(0, '2613.573')]
[36m[2025-07-03 23:25:48,301][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 212992. Throughput: 0: 150.0. Samples: 225072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:25:48,302][139965] Avg episode reward: [(0, '2622.368')]
[37m[1m[2025-07-03 23:25:48,447][139965] Saving new best policy, reward=2622.368!
[36m[2025-07-03 23:25:53,345][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 212992. Throughput: 0: 149.9. Samples: 225920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:25:53,345][139965] Avg episode reward: [(0, '2623.487')]
[37m[1m[2025-07-03 23:25:53,436][139965] Saving new best policy, reward=2623.487!
[36m[2025-07-03 23:25:58,329][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 212992. Throughput: 0: 149.5. Samples: 226864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:25:58,329][139965] Avg episode reward: [(0, '2585.478')]
[37m[1m[2025-07-03 23:25:58,420][139965] Saving ./train_dir/gate_config_3_7/checkpoint_p0/checkpoint_000000416_212992.pth...
[36m[2025-07-03 23:25:58,426][139965] Removing ./train_dir/gate_config_3_7/checkpoint_p0/checkpoint_000000256_131072.pth
[36m[2025-07-03 23:26:03,273][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 212992. Throughput: 0: 151.4. Samples: 227376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:26:03,273][139965] Avg episode reward: [(0, '2563.147')]
[36m[2025-07-03 23:26:08,287][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 212992. Throughput: 0: 149.7. Samples: 228240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:26:08,287][139965] Avg episode reward: [(0, '2589.351')]
[36m[2025-07-03 23:26:13,344][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 212992. Throughput: 0: 151.3. Samples: 229216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:26:13,344][139965] Avg episode reward: [(0, '2595.910')]
[36m[2025-07-03 23:26:18,334][139965] Fps is (10 sec: 1630.7, 60 sec: 273.2, 300 sec: 166.6). Total num frames: 229376. Throughput: 0: 151.4. Samples: 229616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:26:18,334][139965] Avg episode reward: [(0, '2609.058')]
[36m[2025-07-03 23:26:23,296][139965] Fps is (10 sec: 1646.2, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 229376. Throughput: 0: 152.6. Samples: 230560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:26:23,297][139965] Avg episode reward: [(0, '2605.317')]
[36m[2025-07-03 23:26:28,296][139965] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 229376. Throughput: 0: 152.3. Samples: 231472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:26:28,296][139965] Avg episode reward: [(0, '2560.498')]
[36m[2025-07-03 23:26:33,354][139965] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 166.6). Total num frames: 229376. Throughput: 0: 152.4. Samples: 231936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:26:33,355][139965] Avg episode reward: [(0, '2505.580')]
[36m[2025-07-03 23:26:38,260][139965] Fps is (10 sec: 0.0, 60 sec: 273.7, 300 sec: 166.6). Total num frames: 229376. Throughput: 0: 155.0. Samples: 232880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:26:38,260][139965] Avg episode reward: [(0, '2508.790')]
[36m[2025-07-03 23:26:43,295][139965] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 166.6). Total num frames: 229376. Throughput: 0: 153.0. Samples: 233744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:26:43,295][139965] Avg episode reward: [(0, '2461.891')]
[36m[2025-07-03 23:26:48,307][139965] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 229376. Throughput: 0: 151.7. Samples: 234208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:26:48,308][139965] Avg episode reward: [(0, '2442.798')]
[36m[2025-07-03 23:26:53,333][139965] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 229376. Throughput: 0: 151.7. Samples: 235072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:26:53,333][139965] Avg episode reward: [(0, '2338.930')]
[36m[2025-07-03 23:26:58,305][139965] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 166.6). Total num frames: 229376. Throughput: 0: 150.2. Samples: 235968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:26:58,305][139965] Avg episode reward: [(0, '2291.945')]
[36m[2025-07-03 23:27:03,277][139965] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 166.7). Total num frames: 229376. Throughput: 0: 151.3. Samples: 236416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:27:03,278][139965] Avg episode reward: [(0, '2362.878')]
[36m[2025-07-03 23:27:08,259][139965] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 166.6). Total num frames: 229376. Throughput: 0: 151.2. Samples: 237360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:27:08,259][139965] Avg episode reward: [(0, '2384.487')]
[36m[2025-07-03 23:27:13,306][139965] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 166.6). Total num frames: 229376. Throughput: 0: 150.7. Samples: 238256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:27:13,306][139965] Avg episode reward: [(0, '2310.606')]
[36m[2025-07-03 23:27:18,302][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 229376. Throughput: 0: 150.6. Samples: 238704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:27:18,303][139965] Avg episode reward: [(0, '2292.044')]
[36m[2025-07-03 23:27:23,325][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 229376. Throughput: 0: 149.5. Samples: 239616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:27:23,325][139965] Avg episode reward: [(0, '2251.738')]
[36m[2025-07-03 23:27:28,289][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 229376. Throughput: 0: 148.6. Samples: 240432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:27:28,289][139965] Avg episode reward: [(0, '2268.154')]
[36m[2025-07-03 23:27:33,277][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 229376. Throughput: 0: 147.7. Samples: 240848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:27:33,278][139965] Avg episode reward: [(0, '2272.352')]
[36m[2025-07-03 23:27:38,323][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 229376. Throughput: 0: 145.8. Samples: 241632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:27:38,323][139965] Avg episode reward: [(0, '2310.554')]
[36m[2025-07-03 23:27:43,316][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 229376. Throughput: 0: 144.0. Samples: 242448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:27:43,317][139965] Avg episode reward: [(0, '2291.950')]
[36m[2025-07-03 23:27:48,326][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 229376. Throughput: 0: 143.5. Samples: 242880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:27:48,326][139965] Avg episode reward: [(0, '2316.746')]
[36m[2025-07-03 23:27:53,275][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 229376. Throughput: 0: 142.5. Samples: 243776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:27:53,275][139965] Avg episode reward: [(0, '2352.648')]
[36m[2025-07-03 23:27:58,334][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 229376. Throughput: 0: 140.0. Samples: 244560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:27:58,334][139965] Avg episode reward: [(0, '2363.882')]
[37m[1m[2025-07-03 23:27:58,439][139965] Saving ./train_dir/gate_config_3_7/checkpoint_p0/checkpoint_000000448_229376.pth...
[36m[2025-07-03 23:27:58,446][139965] Removing ./train_dir/gate_config_3_7/checkpoint_p0/checkpoint_000000288_147456.pth
[GIF] Episode 600 truncated - saving GIFs (every 100 episodes)
[GIF] Saved drone depth: ./gif_episodes/episode_0006_drone_depth.gif
[GIF] Saved drone segmentation: ./gif_episodes/episode_0006_drone_seg.gif
[GIF] Saved static depth: ./gif_episodes/episode_0006_static_depth.gif
[GIF] Saved static segmentation: ./gif_episodes/episode_0006_static_seg.gif
[GIF] Saved merged dual camera: ./gif_episodes/episode_0006_merged_dual_camera.gif
[36m[2025-07-03 23:28:03,367][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 229376. Throughput: 0: 137.4. Samples: 244896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 23:28:03,367][139965] Avg episode reward: [(0, '2319.369')]
[36m[2025-07-03 23:28:08,598][139965] Fps is (10 sec: 1596.3, 60 sec: 271.5, 300 sec: 166.5). Total num frames: 245760. Throughput: 0: 131.5. Samples: 245568. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:28:08,598][139965] Avg episode reward: [(0, '2384.264')]
[36m[2025-07-03 23:28:13,361][139965] Fps is (10 sec: 1639.3, 60 sec: 272.8, 300 sec: 166.6). Total num frames: 245760. Throughput: 0: 128.9. Samples: 246240. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:28:13,362][139965] Avg episode reward: [(0, '2418.162')]
[36m[2025-07-03 23:28:18,329][139965] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 166.6). Total num frames: 245760. Throughput: 0: 126.8. Samples: 246560. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:28:18,330][139965] Avg episode reward: [(0, '2418.304')]
[36m[2025-07-03 23:28:23,317][139965] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 245760. Throughput: 0: 125.9. Samples: 247296. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:28:23,321][139965] Avg episode reward: [(0, '2407.716')]
[36m[2025-07-03 23:28:28,386][139965] Fps is (10 sec: 0.0, 60 sec: 272.6, 300 sec: 166.6). Total num frames: 245760. Throughput: 0: 121.8. Samples: 247936. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:28:28,386][139965] Avg episode reward: [(0, '2404.988')]
[36m[2025-07-03 23:28:33,360][139965] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 166.6). Total num frames: 245760. Throughput: 0: 119.7. Samples: 248272. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:28:33,361][139965] Avg episode reward: [(0, '2456.547')]
[36m[2025-07-03 23:28:38,350][139965] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 166.6). Total num frames: 245760. Throughput: 0: 115.7. Samples: 248992. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:28:38,351][139965] Avg episode reward: [(0, '2448.820')]
[36m[2025-07-03 23:28:43,264][139965] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 166.7). Total num frames: 245760. Throughput: 0: 113.2. Samples: 249648. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:28:43,264][139965] Avg episode reward: [(0, '2508.256')]
[36m[2025-07-03 23:28:48,354][139965] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 166.6). Total num frames: 245760. Throughput: 0: 112.7. Samples: 249968. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:28:48,355][139965] Avg episode reward: [(0, '2466.975')]
[33m[1737908 ms][IGE_viewer_control] - WARNING : Camera follow: True (IGE_viewer_control.py:217)
[36m[2025-07-03 23:28:53,296][139965] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 245760. Throughput: 0: 112.8. Samples: 250608. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:28:53,296][139965] Avg episode reward: [(0, '2473.248')]
[36m[2025-07-03 23:28:58,300][139965] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 166.6). Total num frames: 245760. Throughput: 0: 112.2. Samples: 251280. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:28:58,301][139965] Avg episode reward: [(0, '2442.105')]
[36m[2025-07-03 23:29:03,271][139965] Fps is (10 sec: 0.0, 60 sec: 273.5, 300 sec: 166.6). Total num frames: 245760. Throughput: 0: 112.5. Samples: 251616. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:29:03,271][139965] Avg episode reward: [(0, '2451.298')]
[36m[2025-07-03 23:29:08,386][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 245760. Throughput: 0: 110.4. Samples: 252272. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:29:08,387][139965] Avg episode reward: [(0, '2460.894')]
[36m[2025-07-03 23:29:13,313][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 245760. Throughput: 0: 111.5. Samples: 252944. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:29:13,314][139965] Avg episode reward: [(0, '2418.479')]
[36m[2025-07-03 23:29:18,265][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 245760. Throughput: 0: 113.7. Samples: 253376. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:29:18,266][139965] Avg episode reward: [(0, '2390.722')]
[36m[2025-07-03 23:29:23,319][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 245760. Throughput: 0: 114.9. Samples: 254160. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:29:23,320][139965] Avg episode reward: [(0, '2402.009')]
[36m[2025-07-03 23:29:28,352][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.0). Total num frames: 245760. Throughput: 0: 117.8. Samples: 254960. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:29:28,352][139965] Avg episode reward: [(0, '2503.748')]
[36m[2025-07-03 23:29:33,320][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.1). Total num frames: 245760. Throughput: 0: 119.6. Samples: 255344. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:29:33,321][139965] Avg episode reward: [(0, '2534.783')]
[36m[2025-07-03 23:29:38,406][139965] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 111.0). Total num frames: 245760. Throughput: 0: 122.7. Samples: 256144. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-03 23:29:38,407][139965] Avg episode reward: [(0, '2504.267')]
[37m[1m[2025-07-03 23:29:43,073][139965] Keyboard interrupt detected in the event loop EvtLoop [Runner_EvtLoop, process=main process 139965], exiting...
[37m[1m[2025-07-03 23:29:43,073][139965] Runner profile tree view:
[37m[1mmain_loop: 1777.1895
[37m[1m[2025-07-03 23:29:43,073][139965] Collected {0: 245760}, FPS: 138.3