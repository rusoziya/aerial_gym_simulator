Importing module 'gym_38' (/home/ziyar/aerialgym/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_38.so)
Setting GYM_USD_PLUG_INFO_PATH to /home/ziyar/aerialgym/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json
[36m[2025-07-01 20:33:36,656][166323] Queried available GPUs: 0
[37m[1m[2025-07-01 20:33:36,656][166323] Environment var CUDA_VISIBLE_DEVICES is 0
PyTorch version 1.13.1
Device count 1
/home/ziyar/aerialgym/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/_bindings/src/gymtorch
ninja: no work to do.
Warp 1.0.0-beta.5 initialized:
   CUDA Toolkit: 11.5, Driver: 12.4
   Devices:
     "cpu"    | x86_64
     "cuda:0" | NVIDIA GeForce RTX 4080 Laptop GPU (sm_89)
   Kernel cache: /home/ziyar/.cache/warp/1.0.0-beta.5
[SUBPROCESS] Setting headless mode to: False
[SUBPROCESS] DCE task action_space_dim: 3
[SUBPROCESS] Target Sample Factory action space: 3D
[SUBPROCESS] Setting num_envs to 16 based on env_agents=16
[SUBPROCESS] Set SF_ENV_AGENTS=16 environment variable
[SUBPROCESS] DCE config batch_size: 2048
[SUBPROCESS] Using ORIGINAL DCE CONFIG (16 environments - maximum performance)
Registered quad_with_obstacles and dce_navigation_task in subprocess
[isaacgym:gymutil.py] Unknown args:  ['--env=quad_with_obstacles', '--train_for_env_steps=100000000', '--experiment=HIGH_CONFIG_16ENV_2', '--async_rl=True', '--use_env_info_cache=False', '--normalize_input=True']
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:0
GPU Pipeline: enabled
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/torch/utils/cpp_extension.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
  from pkg_resources import packaging  # type: ignore[attr-defined]
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
Using /home/ziyar/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Emitting ninja build file /home/ziyar/.cache/torch_extensions/py38_cu117/gymtorch/build.ninja...
Building extension module gymtorch...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module gymtorch...
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/networkx/classes/graph.py:23: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  from collections import Mapping
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/networkx/classes/reportviews.py:95: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  from collections import Mapping, Set, Iterable
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/networkx/readwrite/graphml.py:346: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.int, "int"), (np.int8, "int"),
/home/ziyar/aerialgym/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/torch_utils.py:135: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  def get_axis_params(value, axis_idx, x_value=0., dtype=np.float, n_dims=3):
[37m[1763 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task] - INFO : DCE Navigation Task - Headless mode: False (dce_navigation_task.py:17)
[37m[1763 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task] - INFO : Found SF_ENV_AGENTS environment variable: 16 (dce_navigation_task.py:27)
[37m[1763 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task] - INFO : Detected env_agents=16 from environment - setting environment count. (dce_navigation_task.py:33)
[37m[1763 ms][base_task] - INFO : Setting seed: 412930549 (base_task.py:38)
[37m[1763 ms][navigation_task] - INFO : Building environment for navigation task. (navigation_task.py:44)
[37m[1764 ms][navigation_task] - INFO : Sim Name: base_sim, Env Name: env_with_obstacles, Robot Name: lmf2, Controller Name: lmf2_velocity_control (navigation_task.py:45)
[37m[1764 ms][env_manager] - INFO : Populating environments. (env_manager.py:73)
[37m[1764 ms][env_manager] - INFO : Creating simulation instance. (env_manager.py:87)
[37m[1764 ms][env_manager] - INFO : Instantiating IGE object. (env_manager.py:88)
[37m[1764 ms][IsaacGymEnvManager] - INFO : Creating Isaac Gym Environment (IGE_env_manager.py:41)
[37m[1764 ms][IsaacGymEnvManager] - INFO : Acquiring gym object (IGE_env_manager.py:73)
[37m[1764 ms][IsaacGymEnvManager] - INFO : Acquired gym object (IGE_env_manager.py:75)
[37m[1765 ms][IsaacGymEnvManager] - INFO : Fixing devices (IGE_env_manager.py:89)
[37m[1765 ms][IsaacGymEnvManager] - INFO : Using GPU pipeline for simulation. (IGE_env_manager.py:102)
[37m[1765 ms][IsaacGymEnvManager] - INFO : Sim Device type: cuda, Sim Device ID: 0 (IGE_env_manager.py:105)
[37m[1765 ms][IsaacGymEnvManager] - INFO : Graphics Device ID: 0 (IGE_env_manager.py:119)
[37m[1765 ms][IsaacGymEnvManager] - INFO : Creating Isaac Gym Simulation Object (IGE_env_manager.py:120)
[33m[1765 ms][IsaacGymEnvManager] - WARNING : If you have set the CUDA_VISIBLE_DEVICES environment variable, please ensure that you set it
[33mto a particular one that works for your system to use the viewer or Isaac Gym cameras.
[33mIf you want to run parallel simulations on multiple GPUs with camera sensors,
[33mplease disable Isaac Gym and use warp (by setting use_warp=True), set the viewer to headless. (IGE_env_manager.py:127)
[33m[1765 ms][IsaacGymEnvManager] - WARNING : If you see a segfault in the next lines, it is because of the discrepancy between the CUDA device and the graphics device.
[33mPlease ensure that the CUDA device and the graphics device are the same. (IGE_env_manager.py:132)
*** Can't create empty tensor
WARNING: allocation matrix is not full rank. Rank: 4
creating render graph
Module warp.utils load on device 'cuda:0' took 1.40 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_camera_kernels load on device 'cuda:0' took 8.13 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_stereo_camera_kernels load on device 'cuda:0' took 11.37 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_lidar_kernels load on device 'cuda:0' took 5.49 ms
finishing capture of render graph
Encoder network initialized.
Defined encoder.
[ImgDecoder] Starting create_model
[ImgDecoder] Done with create_model
Defined decoder.
Loading weights from file:  /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/utils/vae/weights/ICRA_test_set_more_sim_data_kld_beta_3_LD_64_epoch_49.pth
[AerialGymVecEnv] Forced action space shape: (3,)
[AerialGymVecEnv] is_multiagent: True, num_agents: 16
[make_aerialgym_env] Final action space shape: (3,)
[make_aerialgym_env] Action space: Box(-1.0, 1.0, (3,), float32)
[37m[2738 ms][IsaacGymEnvManager] - INFO : Created Isaac Gym Simulation Object (IGE_env_manager.py:136)
[37m[2738 ms][IsaacGymEnvManager] - INFO : Created Isaac Gym Environment (IGE_env_manager.py:43)
[37m[2947 ms][env_manager] - INFO : IGE object instantiated. (env_manager.py:109)
[37m[2947 ms][env_manager] - INFO : Creating warp environment. (env_manager.py:112)
[37m[2947 ms][env_manager] - INFO : Warp environment created. (env_manager.py:114)
[37m[2947 ms][env_manager] - INFO : Creating robot manager. (env_manager.py:118)
[37m[2947 ms][BaseRobot] - INFO : [DONE] Initializing controller (base_robot.py:26)
[37m[2947 ms][BaseRobot] - INFO : Initializing controller lmf2_velocity_control (base_robot.py:29)
[33m[2947 ms][base_multirotor] - WARNING : Creating 16 multirotors. (base_multirotor.py:32)
[37m[2947 ms][env_manager] - INFO : [DONE] Creating robot manager. (env_manager.py:123)
[37m[2947 ms][env_manager] - INFO : [DONE] Creating simulation instance. (env_manager.py:125)
[37m[2947 ms][asset_loader] - INFO : Loading asset: model.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2947 ms][asset_loader] - INFO : Loading asset: panel.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2949 ms][asset_loader] - INFO : Loading asset: cuboidal_rod.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2950 ms][asset_loader] - INFO : Loading asset: 1_x_1_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2951 ms][asset_loader] - INFO : Loading asset: gate.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2954 ms][asset_loader] - INFO : Loading asset: left_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2955 ms][asset_loader] - INFO : Loading asset: right_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2956 ms][asset_loader] - INFO : Loading asset: back_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2956 ms][asset_loader] - INFO : Loading asset: front_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2957 ms][asset_loader] - INFO : Loading asset: bottom_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2958 ms][asset_loader] - INFO : Loading asset: top_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2959 ms][asset_loader] - INFO : Loading asset: small_cube.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2960 ms][asset_loader] - INFO : Loading asset: 0_5_x_0_5_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2962 ms][env_manager] - INFO : Populating environment 0 (env_manager.py:179)
[33m[3329 ms][robot_manager] - WARNING : 
[33mRobot mass: 1.2400000467896461,
[33mInertia: tensor([[0.0134, 0.0000, 0.0000],
[33m        [0.0000, 0.0144, 0.0000],
[33m        [0.0000, 0.0000, 0.0138]], device='cuda:0'),
[33mRobot COM: tensor([[0., 0., 0., 1.]], device='cuda:0') (robot_manager.py:437)
[33m[3329 ms][robot_manager] - WARNING : Calculated robot mass and inertia for this robot. This code assumes that your robot is the same across environments. (robot_manager.py:440)
[31m[3329 ms][robot_manager] - CRITICAL : If your robot differs across environments you need to perform this computation for each different robot here. (robot_manager.py:443)
[37m[3354 ms][env_manager] - INFO : [DONE] Populating environments. (env_manager.py:75)
[33m[3361 ms][IsaacGymEnvManager] - WARNING : Headless: False (IGE_env_manager.py:424)
[37m[3362 ms][IsaacGymEnvManager] - INFO : Creating viewer (IGE_env_manager.py:426)
[33m[3434 ms][IGE_viewer_control] - WARNING : Instructions for using the viewer with the keyboard:
[33mESC: Quit
[33mV: Toggle Viewer Sync
[33mS: Sync Frame Time
[33mF: Toggle Camera Follow
[33mP: Toggle Camera Follow Type
[33mR: Reset All Environments
[33mUP: Switch Target Environment Up
[33mDOWN: Switch Target Environment Down
[33mSPACE: Pause Simulation
[33m (IGE_viewer_control.py:153)
[37m[3434 ms][IsaacGymEnvManager] - INFO : Created viewer (IGE_env_manager.py:432)
[33m[3469 ms][asset_manager] - WARNING : Number of obstacles to be kept in the environment: 9 (asset_manager.py:32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/control/motor_model.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(self.min_thrust, device=self.device, dtype=torch.float32).expand(
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/control/motor_model.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(self.max_thrust, device=self.device, dtype=torch.float32).expand(
[33m[3666 ms][control_allocation] - WARNING : Control allocation does not account for actuator limits. This leads to suboptimal allocation (control_allocation.py:48)
[37m[3667 ms][WarpSensor] - INFO : Camera sensor initialized (warp_sensor.py:50)
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.
  logger.warn(
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.
  logger.warn(
[36m[2025-07-01 20:33:41,116][166428] Env info: EnvInfo(obs_space=Dict('obs': Box(-inf, inf, (81,), float32)), action_space=Box(-1.0, 1.0, (3,), float32), num_agents=16, gpu_actions=True, gpu_observations=True, action_splits=None, all_discrete=None, frameskip=1, reward_shaping_scheme=None, env_info_protocol_version=1)
[33m[2025-07-01 20:33:41,829][166323] In serial mode all components run on the same process. Only use async_rl and serial mode together for debugging.
[36m[2025-07-01 20:33:41,829][166323] Starting experiment with the following configuration:
[36mhelp=False
[36malgo=APPO
[36menv=quad_with_obstacles
[36mexperiment=HIGH_CONFIG_16ENV_2
[36mtrain_dir=/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir
[36mrestart_behavior=resume
[36mdevice=gpu
[36mseed=None
[36mnum_policies=1
[36masync_rl=True
[36mserial_mode=True
[36mbatched_sampling=True
[36mnum_batches_to_accumulate=2
[36mworker_num_splits=1
[36mpolicy_workers_per_policy=1
[36mmax_policy_lag=1000
[36mnum_workers=1
[36mnum_envs_per_worker=1
[36mbatch_size=2048
[36mnum_batches_per_epoch=8
[36mnum_epochs=4
[36mrollout=32
[36mrecurrence=32
[36mshuffle_minibatches=False
[36mgamma=0.98
[36mreward_scale=0.1
[36mreward_clip=1000.0
[36mvalue_bootstrap=True
[36mnormalize_returns=True
[36mexploration_loss_coeff=0.001
[36mvalue_loss_coeff=2.0
[36mkl_loss_coeff=0.1
[36mexploration_loss=entropy
[36mgae_lambda=0.95
[36mppo_clip_ratio=0.2
[36mppo_clip_value=1.0
[36mwith_vtrace=False
[36mvtrace_rho=1.0
[36mvtrace_c=1.0
[36moptimizer=adam
[36madam_eps=1e-06
[36madam_beta1=0.9
[36madam_beta2=0.999
[36mmax_grad_norm=1.0
[36mlearning_rate=0.0003
[36mlr_schedule=kl_adaptive_epoch
[36mlr_schedule_kl_threshold=0.016
[36mlr_adaptive_min=1e-06
[36mlr_adaptive_max=0.01
[36mobs_subtract_mean=0.0
[36mobs_scale=1.0
[36mnormalize_input=True
[36mnormalize_input_keys=None
[36mdecorrelate_experience_max_seconds=0
[36mdecorrelate_envs_on_one_worker=True
[36mactor_worker_gpus=[0]
[36mset_workers_cpu_affinity=True
[36mforce_envs_single_thread=False
[36mdefault_niceness=0
[36mlog_to_file=True
[36mexperiment_summaries_interval=10
[36mflush_summaries_interval=30
[36mstats_avg=100
[36msummaries_use_frameskip=True
[36mheartbeat_interval=20
[36mheartbeat_reporting_interval=180
[36mtrain_for_env_steps=100000000
[36mtrain_for_seconds=10000000000
[36msave_every_sec=120
[36mkeep_checkpoints=2
[36mload_checkpoint_kind=latest
[36msave_milestones_sec=-1
[36msave_best_every_sec=5
[36msave_best_metric=reward
[36msave_best_after=5000000
[36mbenchmark=False
[36mencoder_mlp_layers=[512, 256, 64]
[36mencoder_conv_architecture=convnet_simple
[36mencoder_conv_mlp_layers=[]
[36muse_rnn=True
[36mrnn_size=64
[36mrnn_type=gru
[36mrnn_num_layers=1
[36mdecoder_mlp_layers=[]
[36mnonlinearity=elu
[36mpolicy_initialization=torch_default
[36mpolicy_init_gain=1.0
[36mactor_critic_share_weights=True
[36madaptive_stddev=True
[36mcontinuous_tanh_scale=0.0
[36minitial_stddev=1.0
[36muse_env_info_cache=False
[36menv_gpu_actions=True
[36menv_gpu_observations=True
[36menv_frameskip=1
[36menv_framestack=1
[36mpixel_format=CHW
[36muse_record_episode_statistics=False
[36mwith_wandb=True
[36mwandb_user=ziya-ruso-ucl
[36mwandb_project=vae_rl_navigation
[36mwandb_group=dce_navigation_training
[36mwandb_job_type=SF
[36mwandb_tags=['aerial_gym', 'dce', 'navigation', 'sample_factory']
[36mwith_pbt=False
[36mpbt_mix_policies_in_one_env=True
[36mpbt_period_env_steps=5000000
[36mpbt_start_mutation=20000000
[36mpbt_replace_fraction=0.3
[36mpbt_mutation_rate=0.15
[36mpbt_replace_reward_gap=0.1
[36mpbt_replace_reward_gap_absolute=1e-06
[36mpbt_optimize_gamma=False
[36mpbt_target_objective=true_objective
[36mpbt_perturb_min=1.1
[36mpbt_perturb_max=1.5
[36menv_agents=16
[36mheadless=False
[36mobs_key=obs
[36msubtask=None
[36mige_api_version=preview4
[36meval_stats=False
[36maction_space_dim=3
[36mcommand_line=--env=quad_with_obstacles --train_for_env_steps=100000000 --experiment=HIGH_CONFIG_16ENV_2 --async_rl=True --use_env_info_cache=False --normalize_input=True --headless=False
[36mcli_args={'env': 'quad_with_obstacles', 'experiment': 'HIGH_CONFIG_16ENV_2', 'async_rl': True, 'normalize_input': True, 'train_for_env_steps': 100000000, 'use_env_info_cache': False, 'headless': False}
[36mgit_hash=7f35eed17f2afcde33e3a7aec669b48e9e8e34cd
[36mgit_repo_name=https://github.com/ntnu-arl/aerial_gym_simulator.git
[36mwandb_unique_id=HIGH_CONFIG_16ENV_2_20250701_203333_331994
[36m[2025-07-01 20:33:41,829][166323] Saving configuration to /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/config.json...
[36m[2025-07-01 20:33:42,022][166323] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[36m[2025-07-01 20:33:42,023][166323] Rollout worker 0 uses device cuda:0
[36m[2025-07-01 20:33:42,040][166323] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[37m[1m[2025-07-01 20:33:42,040][166323] InferenceWorker_p0-w0: min num requests: 1
[36m[2025-07-01 20:33:42,041][166323] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[37m[1m[2025-07-01 20:33:42,042][166323] Starting seed is not provided
[36m[2025-07-01 20:33:42,042][166323] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[36m[2025-07-01 20:33:42,042][166323] Initializing actor-critic model on device cuda:0
[36m[2025-07-01 20:33:42,043][166323] RunningMeanStd input shape: (81,)
[36m[2025-07-01 20:33:42,044][166323] RunningMeanStd input shape: (1,)
[36m[2025-07-01 20:33:42,069][166323] Created Actor Critic model with architecture:
[36m[2025-07-01 20:33:42,069][166323] ActorCriticSharedWeights(
[36m  (obs_normalizer): ObservationNormalizer(
[36m    (running_mean_std): RunningMeanStdDictInPlace(
[36m      (running_mean_std): ModuleDict(
[36m        (obs): RunningMeanStdInPlace()
[36m      )
[36m    )
[36m  )
[36m  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)
[36m  (encoder): MultiInputEncoder(
[36m    (encoders): ModuleDict(
[36m      (obs): MlpEncoder(
[36m        (mlp_head): RecursiveScriptModule(
[36m          original_name=Sequential
[36m          (0): RecursiveScriptModule(original_name=Linear)
[36m          (1): RecursiveScriptModule(original_name=ELU)
[36m          (2): RecursiveScriptModule(original_name=Linear)
[36m          (3): RecursiveScriptModule(original_name=ELU)
[36m          (4): RecursiveScriptModule(original_name=Linear)
[36m          (5): RecursiveScriptModule(original_name=ELU)
[36m        )
[36m      )
[36m    )
[36m  )
[36m  (core): ModelCoreRNN(
[36m    (core): GRU(64, 64)
[36m  )
[36m  (decoder): MlpDecoder(
[36m    (mlp): Identity()
[36m  )
[36m  (critic_linear): Linear(in_features=64, out_features=1, bias=True)
[36m  (action_parameterization): ActionParameterizationDefault(
[36m    (distribution_linear): Linear(in_features=64, out_features=6, bias=True)
[36m  )
[36m)
[36m[2025-07-01 20:33:42,500][166323] Using optimizer <class 'torch.optim.adam.Adam'>
[33m[2025-07-01 20:33:42,501][166323] No checkpoints found
[36m[2025-07-01 20:33:42,501][166323] Did not load from checkpoint, starting from scratch!
[36m[2025-07-01 20:33:42,501][166323] Initialized policy 0 weights for model version 0
[36m[2025-07-01 20:33:42,501][166323] LearnerWorker_p0 finished initialization!
[36m[2025-07-01 20:33:42,501][166323] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[37m[1m[2025-07-01 20:33:42,507][166323] Inference worker 0-0 is ready!
[37m[1m[2025-07-01 20:33:42,507][166323] All inference workers are ready! Signal rollout workers to start!
[36m[2025-07-01 20:33:42,508][166323] EnvRunner 0-0 uses policy 0
[37m[11028 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task] - INFO : DCE Navigation Task - Headless mode: False (dce_navigation_task.py:17)
[37m[11028 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task] - INFO : Found SF_ENV_AGENTS environment variable: 16 (dce_navigation_task.py:27)
[37m[11028 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task] - INFO : Detected env_agents=16 from environment - setting environment count. (dce_navigation_task.py:33)
[37m[11028 ms][base_task] - INFO : Setting seed: 30133563 (base_task.py:38)
[37m[11028 ms][navigation_task] - INFO : Building environment for navigation task. (navigation_task.py:44)
[37m[11029 ms][navigation_task] - INFO : Sim Name: base_sim, Env Name: env_with_obstacles, Robot Name: lmf2, Controller Name: lmf2_velocity_control (navigation_task.py:45)
[37m[11029 ms][env_manager] - INFO : Populating environments. (env_manager.py:73)
[37m[11029 ms][env_manager] - INFO : Creating simulation instance. (env_manager.py:87)
[37m[11029 ms][env_manager] - INFO : Instantiating IGE object. (env_manager.py:88)
[37m[11029 ms][IsaacGymEnvManager] - INFO : Creating Isaac Gym Environment (IGE_env_manager.py:41)
[37m[11029 ms][IsaacGymEnvManager] - INFO : Acquiring gym object (IGE_env_manager.py:73)
[37m[11029 ms][IsaacGymEnvManager] - INFO : Acquired gym object (IGE_env_manager.py:75)
[isaacgym:gymutil.py] Unknown args:  ['--env=quad_with_obstacles', '--train_for_env_steps=100000000', '--experiment=HIGH_CONFIG_16ENV_2', '--async_rl=True', '--use_env_info_cache=False', '--normalize_input=True']
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:0
GPU Pipeline: enabled
*** Can't create empty tensor
WARNING: allocation matrix is not full rank. Rank: 4
creating render graph
Module warp.utils load on device 'cuda:0' took 1.55 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_camera_kernels load on device 'cuda:0' took 7.80 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_stereo_camera_kernels load on device 'cuda:0' took 11.53 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_lidar_kernels load on device 'cuda:0' took 5.69 ms
finishing capture of render graph
Encoder network initialized.
Defined encoder.
[ImgDecoder] Starting create_model
[ImgDecoder] Done with create_model
Defined decoder.
Loading weights from file:  /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/utils/vae/weights/ICRA_test_set_more_sim_data_kld_beta_3_LD_64_epoch_49.pth
[AerialGymVecEnv] Forced action space shape: (3,)
[AerialGymVecEnv] is_multiagent: True, num_agents: 16
[make_aerialgym_env] Final action space shape: (3,)
[make_aerialgym_env] Action space: Box(-1.0, 1.0, (3,), float32)
[37m[11030 ms][IsaacGymEnvManager] - INFO : Fixing devices (IGE_env_manager.py:89)
[37m[11030 ms][IsaacGymEnvManager] - INFO : Using GPU pipeline for simulation. (IGE_env_manager.py:102)
[37m[11030 ms][IsaacGymEnvManager] - INFO : Sim Device type: cuda, Sim Device ID: 0 (IGE_env_manager.py:105)
[37m[11030 ms][IsaacGymEnvManager] - INFO : Graphics Device ID: 0 (IGE_env_manager.py:119)
[37m[11030 ms][IsaacGymEnvManager] - INFO : Creating Isaac Gym Simulation Object (IGE_env_manager.py:120)
[33m[11030 ms][IsaacGymEnvManager] - WARNING : If you have set the CUDA_VISIBLE_DEVICES environment variable, please ensure that you set it
[33mto a particular one that works for your system to use the viewer or Isaac Gym cameras.
[33mIf you want to run parallel simulations on multiple GPUs with camera sensors,
[33mplease disable Isaac Gym and use warp (by setting use_warp=True), set the viewer to headless. (IGE_env_manager.py:127)
[33m[11030 ms][IsaacGymEnvManager] - WARNING : If you see a segfault in the next lines, it is because of the discrepancy between the CUDA device and the graphics device.
[33mPlease ensure that the CUDA device and the graphics device are the same. (IGE_env_manager.py:132)
[37m[12009 ms][IsaacGymEnvManager] - INFO : Created Isaac Gym Simulation Object (IGE_env_manager.py:136)
[37m[12009 ms][IsaacGymEnvManager] - INFO : Created Isaac Gym Environment (IGE_env_manager.py:43)
[37m[12213 ms][env_manager] - INFO : IGE object instantiated. (env_manager.py:109)
[37m[12213 ms][env_manager] - INFO : Creating warp environment. (env_manager.py:112)
[37m[12213 ms][env_manager] - INFO : Warp environment created. (env_manager.py:114)
[37m[12213 ms][env_manager] - INFO : Creating robot manager. (env_manager.py:118)
[37m[12213 ms][BaseRobot] - INFO : [DONE] Initializing controller (base_robot.py:26)
[37m[12213 ms][BaseRobot] - INFO : Initializing controller lmf2_velocity_control (base_robot.py:29)
[33m[12213 ms][base_multirotor] - WARNING : Creating 16 multirotors. (base_multirotor.py:32)
[37m[12213 ms][env_manager] - INFO : [DONE] Creating robot manager. (env_manager.py:123)
[37m[12213 ms][env_manager] - INFO : [DONE] Creating simulation instance. (env_manager.py:125)
[37m[12214 ms][asset_loader] - INFO : Loading asset: model.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[12214 ms][asset_loader] - INFO : Loading asset: panel.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[12216 ms][asset_loader] - INFO : Loading asset: 1_x_1_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[12218 ms][asset_loader] - INFO : Loading asset: 0_5_x_0_5_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[12219 ms][asset_loader] - INFO : Loading asset: gate.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[12222 ms][asset_loader] - INFO : Loading asset: small_cube.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[12223 ms][asset_loader] - INFO : Loading asset: left_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[12224 ms][asset_loader] - INFO : Loading asset: right_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[12225 ms][asset_loader] - INFO : Loading asset: back_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[12225 ms][asset_loader] - INFO : Loading asset: front_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[12226 ms][asset_loader] - INFO : Loading asset: bottom_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[12227 ms][asset_loader] - INFO : Loading asset: top_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[12228 ms][asset_loader] - INFO : Loading asset: cuboidal_rod.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[12230 ms][env_manager] - INFO : Populating environment 0 (env_manager.py:179)
[33m[12247 ms][robot_manager] - WARNING : 
[33mRobot mass: 1.2400000467896461,
[33mInertia: tensor([[0.0134, 0.0000, 0.0000],
[33m        [0.0000, 0.0144, 0.0000],
[33m        [0.0000, 0.0000, 0.0138]], device='cuda:0'),
[33mRobot COM: tensor([[0., 0., 0., 1.]], device='cuda:0') (robot_manager.py:437)
[33m[12247 ms][robot_manager] - WARNING : Calculated robot mass and inertia for this robot. This code assumes that your robot is the same across environments. (robot_manager.py:440)
[31m[12247 ms][robot_manager] - CRITICAL : If your robot differs across environments you need to perform this computation for each different robot here. (robot_manager.py:443)
[37m[12271 ms][env_manager] - INFO : [DONE] Populating environments. (env_manager.py:75)
[33m[12278 ms][IsaacGymEnvManager] - WARNING : Headless: False (IGE_env_manager.py:424)
[37m[12278 ms][IsaacGymEnvManager] - INFO : Creating viewer (IGE_env_manager.py:426)
[33m[12354 ms][IGE_viewer_control] - WARNING : Instructions for using the viewer with the keyboard:
[33mESC: Quit
[33mV: Toggle Viewer Sync
[33mS: Sync Frame Time
[33mF: Toggle Camera Follow
[33mP: Toggle Camera Follow Type
[33mR: Reset All Environments
[33mUP: Switch Target Environment Up
[33mDOWN: Switch Target Environment Down
[33mSPACE: Pause Simulation
[33m (IGE_viewer_control.py:153)
[37m[12354 ms][IsaacGymEnvManager] - INFO : Created viewer (IGE_env_manager.py:432)
[33m[12391 ms][asset_manager] - WARNING : Number of obstacles to be kept in the environment: 9 (asset_manager.py:32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/control/motor_model.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(self.min_thrust, device=self.device, dtype=torch.float32).expand(
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/control/motor_model.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(self.max_thrust, device=self.device, dtype=torch.float32).expand(
[33m[12590 ms][control_allocation] - WARNING : Control allocation does not account for actuator limits. This leads to suboptimal allocation (control_allocation.py:48)
[37m[12591 ms][WarpSensor] - INFO : Camera sensor initialized (warp_sensor.py:50)
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.
  logger.warn(
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.
  logger.warn(
[31m[15142 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15142 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15],
[31m       device='cuda:0') (navigation_task.py:196)
[31m[15143 ms][navigation_task] - CRITICAL : Time at crash: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0',
[31m       dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:33:47,587][166323] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 0. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 20:33:47,587][166323] Avg episode reward: [(0, '-100.000')]
[33m[17265 ms][IGE_viewer_control] - WARNING : Camera follow: True (IGE_viewer_control.py:217)
[36m[2025-07-01 20:33:51,016][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 46.7. Samples: 160. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 20:33:51,017][166323] Avg episode reward: [(0, '-89.471')]
[33m[20748 ms][IGE_viewer_control] - WARNING : Camera follow: False (IGE_viewer_control.py:217)
[36m[2025-07-01 20:33:55,960][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 196.8. Samples: 1648. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 20:33:55,960][166323] Avg episode reward: [(0, '-91.237')]
[36m[2025-07-01 20:34:00,993][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 249.4. Samples: 3344. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 20:34:00,993][166323] Avg episode reward: [(0, '-96.269')]
[37m[1m[2025-07-01 20:34:02,141][166323] Heartbeat connected on Batcher_0
[37m[1m[2025-07-01 20:34:02,142][166323] Heartbeat connected on LearnerWorker_p0
[37m[1m[2025-07-01 20:34:02,142][166323] Heartbeat connected on InferenceWorker_p0-w0
[37m[1m[2025-07-01 20:34:02,142][166323] Heartbeat connected on RolloutWorker_w0
[36m[2025-07-01 20:34:05,977][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 226.2. Samples: 4160. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 20:34:05,977][166323] Avg episode reward: [(0, '-99.806')]
[36m[2025-07-01 20:34:10,985][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 246.2. Samples: 5760. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 20:34:10,985][166323] Avg episode reward: [(0, '-99.021')]
[33m[41194 ms][IGE_viewer_control] - WARNING : Camera follow: True (IGE_viewer_control.py:217)
[31m[41253 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[41253 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[41253 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[33m[42436 ms][IGE_viewer_control] - WARNING : Camera follow: False (IGE_viewer_control.py:217)
[36m[2025-07-01 20:34:15,985][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 265.4. Samples: 7536. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 20:34:15,985][166323] Avg episode reward: [(0, '-97.272')]
[36m[2025-07-01 20:34:20,958][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 251.2. Samples: 8384. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 20:34:20,959][166323] Avg episode reward: [(0, '-96.002')]
[36m[2025-07-01 20:34:25,974][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 264.3. Samples: 10144. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 20:34:25,975][166323] Avg episode reward: [(0, '-99.594')]
[36m[2025-07-01 20:34:30,958][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 271.5. Samples: 11776. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 20:34:30,958][166323] Avg episode reward: [(0, '-99.936')]
[31m[60650 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[60651 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[60651 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:34:35,947][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 278.1. Samples: 12656. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 20:34:35,948][166323] Avg episode reward: [(0, '-98.305')]
[33m[66183 ms][IGE_viewer_control] - WARNING : Camera follow: True (IGE_viewer_control.py:217)
[33m[66752 ms][IGE_viewer_control] - WARNING : Camera follow: False (IGE_viewer_control.py:217)
[31m[67120 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[67120 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[67121 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:34:40,987][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 283.6. Samples: 14416. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 20:34:40,987][166323] Avg episode reward: [(0, '-98.875')]
[36m[2025-07-01 20:34:45,965][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 283.9. Samples: 16112. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 20:34:45,966][166323] Avg episode reward: [(0, '-100.656')]
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/torch/nn/modules/module.py:1194: UserWarning: operator() profile_node %104 : int[] = prim::profile_ivalue(%102)
 does not have profile information (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
[36m[2025-07-01 20:34:50,952][166323] Fps is (10 sec: 1644.0, 60 sec: 273.4, 300 sec: 258.6). Total num frames: 16384. Throughput: 0: 282.5. Samples: 16864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:34:50,953][166323] Avg episode reward: [(0, '-98.988')]
[36m[2025-07-01 20:34:55,954][166323] Fps is (10 sec: 1640.2, 60 sec: 273.1, 300 sec: 239.6). Total num frames: 16384. Throughput: 0: 285.7. Samples: 18608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:34:55,955][166323] Avg episode reward: [(0, '-97.078')]
[31m[84864 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[84865 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[84865 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:35:00,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 223.3). Total num frames: 16384. Throughput: 0: 283.8. Samples: 20304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:35:00,968][166323] Avg episode reward: [(0, '-100.113')]
[31m[93746 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[93747 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[93747 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:35:05,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 209.0). Total num frames: 16384. Throughput: 0: 283.9. Samples: 21168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:35:05,988][166323] Avg episode reward: [(0, '-99.562')]
[36m[2025-07-01 20:35:10,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 196.5). Total num frames: 16384. Throughput: 0: 283.4. Samples: 22896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:35:10,971][166323] Avg episode reward: [(0, '-97.470')]
[36m[2025-07-01 20:35:15,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 185.3). Total num frames: 16384. Throughput: 0: 282.8. Samples: 24512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:35:15,987][166323] Avg episode reward: [(0, '-99.403')]
[31m[105812 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[105812 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[105813 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:35:20,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 175.5). Total num frames: 16384. Throughput: 0: 283.8. Samples: 25424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:35:20,945][166323] Avg episode reward: [(0, '-98.922')]
[31m[114314 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[114315 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[114315 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:35:25,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.5). Total num frames: 16384. Throughput: 0: 279.5. Samples: 26992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:35:25,975][166323] Avg episode reward: [(0, '-101.759')]
[36m[2025-07-01 20:35:31,000][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 158.4). Total num frames: 16384. Throughput: 0: 278.5. Samples: 28656. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:35:31,000][166323] Avg episode reward: [(0, '-97.520')]
[31m[124213 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[124213 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[124213 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:35:35,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 151.2). Total num frames: 16384. Throughput: 0: 280.5. Samples: 29488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:35:35,953][166323] Avg episode reward: [(0, '-98.455')]
[37m[1m[2025-07-01 20:35:36,049][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000000032_16384.pth...
[36m[2025-07-01 20:35:40,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 144.5). Total num frames: 16384. Throughput: 0: 277.2. Samples: 31088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:35:40,980][166323] Avg episode reward: [(0, '-100.269')]
[36m[2025-07-01 20:35:46,379][166323] Fps is (10 sec: 1571.5, 60 sec: 542.4, 300 sec: 275.8). Total num frames: 32768. Throughput: 0: 274.1. Samples: 32752. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 20:35:46,379][166323] Avg episode reward: [(0, '-98.247')]
[36m[2025-07-01 20:35:50,984][166323] Fps is (10 sec: 1637.5, 60 sec: 272.9, 300 sec: 265.5). Total num frames: 32768. Throughput: 0: 272.0. Samples: 33408. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 20:35:50,985][166323] Avg episode reward: [(0, '-99.473')]
[36m[2025-07-01 20:35:55,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 255.3). Total num frames: 32768. Throughput: 0: 270.7. Samples: 35072. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 20:35:55,952][166323] Avg episode reward: [(0, '-99.838')]
[36m[2025-07-01 20:36:00,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 245.6). Total num frames: 32768. Throughput: 0: 270.3. Samples: 36672. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 20:36:00,982][166323] Avg episode reward: [(0, '-97.301')]
[31m[149701 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[149701 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[149702 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:36:06,002][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 236.7). Total num frames: 32768. Throughput: 0: 264.6. Samples: 37344. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 20:36:06,002][166323] Avg episode reward: [(0, '-98.177')]
[33m[156638 ms][IGE_viewer_control] - WARNING : Camera follow: True (IGE_viewer_control.py:217)
[33m[157616 ms][IGE_viewer_control] - WARNING : Camera follow: False (IGE_viewer_control.py:217)
[36m[2025-07-01 20:36:11,038][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 228.4). Total num frames: 32768. Throughput: 0: 254.2. Samples: 38448. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 20:36:11,038][166323] Avg episode reward: [(0, '-98.569')]
[36m[2025-07-01 20:36:15,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 220.8). Total num frames: 32768. Throughput: 0: 248.9. Samples: 39856. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 20:36:15,997][166323] Avg episode reward: [(0, '-98.545')]
[36m[2025-07-01 20:36:21,028][166323] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 213.6). Total num frames: 32768. Throughput: 0: 246.7. Samples: 40608. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 20:36:21,029][166323] Avg episode reward: [(0, '-99.949')]
[36m[2025-07-01 20:36:25,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 206.9). Total num frames: 32768. Throughput: 0: 245.5. Samples: 42128. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 20:36:25,950][166323] Avg episode reward: [(0, '-98.759')]
[36m[2025-07-01 20:36:30,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 200.5). Total num frames: 32768. Throughput: 0: 247.9. Samples: 43808. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 20:36:30,984][166323] Avg episode reward: [(0, '-98.014')]
[36m[2025-07-01 20:36:35,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 194.6). Total num frames: 32768. Throughput: 0: 249.9. Samples: 44656. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 20:36:35,994][166323] Avg episode reward: [(0, '-98.300')]
[33m[185588 ms][IGE_viewer_control] - WARNING : Camera follow: True (IGE_viewer_control.py:217)
[36m[2025-07-01 20:36:40,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 189.0). Total num frames: 32768. Throughput: 0: 250.9. Samples: 46368. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 20:36:40,973][166323] Avg episode reward: [(0, '-100.667')]
[36m[2025-07-01 20:36:45,975][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 183.7). Total num frames: 32768. Throughput: 0: 255.7. Samples: 48176. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 20:36:45,975][166323] Avg episode reward: [(0, '-99.209')]
[36m[2025-07-01 20:36:50,991][166323] Fps is (10 sec: 1635.4, 60 sec: 273.0, 300 sec: 268.0). Total num frames: 49152. Throughput: 0: 260.0. Samples: 49040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:36:50,992][166323] Avg episode reward: [(0, '-101.886')]
[36m[2025-07-01 20:36:55,980][166323] Fps is (10 sec: 1637.6, 60 sec: 272.9, 300 sec: 260.9). Total num frames: 49152. Throughput: 0: 272.0. Samples: 50672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:36:55,980][166323] Avg episode reward: [(0, '-98.344')]
[36m[2025-07-01 20:37:00,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 254.2). Total num frames: 49152. Throughput: 0: 281.1. Samples: 52496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:37:00,961][166323] Avg episode reward: [(0, '-96.484')]
[31m[210168 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[210168 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[210168 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:37:05,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 247.7). Total num frames: 49152. Throughput: 0: 283.3. Samples: 53344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:37:05,986][166323] Avg episode reward: [(0, '-98.097')]
[36m[2025-07-01 20:37:10,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 241.7). Total num frames: 49152. Throughput: 0: 287.9. Samples: 55088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:37:10,971][166323] Avg episode reward: [(0, '-99.044')]
[36m[2025-07-01 20:37:15,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 235.9). Total num frames: 49152. Throughput: 0: 287.8. Samples: 56752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:37:15,961][166323] Avg episode reward: [(0, '-97.293')]
[36m[2025-07-01 20:37:20,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 230.3). Total num frames: 49152. Throughput: 0: 288.1. Samples: 57616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:37:20,973][166323] Avg episode reward: [(0, '-97.823')]
[36m[2025-07-01 20:37:25,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 225.1). Total num frames: 49152. Throughput: 0: 288.9. Samples: 59360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:37:25,947][166323] Avg episode reward: [(0, '-95.470')]
[36m[2025-07-01 20:37:30,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 220.0). Total num frames: 49152. Throughput: 0: 286.2. Samples: 61056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:37:30,972][166323] Avg episode reward: [(0, '-96.396')]
[31m[239780 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[239780 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[239780 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:37:36,003][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 215.2). Total num frames: 49152. Throughput: 0: 287.2. Samples: 61968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:37:36,003][166323] Avg episode reward: [(0, '-98.903')]
[37m[1m[2025-07-01 20:37:36,055][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000000096_49152.pth...
[36m[2025-07-01 20:37:40,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 210.6). Total num frames: 49152. Throughput: 0: 289.0. Samples: 63680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:37:40,991][166323] Avg episode reward: [(0, '-101.075')]
[33m[252872 ms][navigation_task] - WARNING : Curriculum Level: 36, Curriculum progress fraction: 0.0 (navigation_task.py:262)
[33m[252872 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.0
[33mCrash Rate: 0.9995119571685791
[33mTimeout Rate: 0.00048804294783622026 (navigation_task.py:265)
[33m[252872 ms][navigation_task] - WARNING : 
[33mSuccesses: 0
[33mCrashes : 2048
[33mTimeouts: 1 (navigation_task.py:268)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/task/navigation_task/navigation_task.py:275: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.infos["curriculum/success_rate"] = torch.tensor(success_rate, dtype=torch.float32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/task/navigation_task/navigation_task.py:276: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.infos["curriculum/crash_rate"] = torch.tensor(crash_rate, dtype=torch.float32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/task/navigation_task/navigation_task.py:277: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.infos["curriculum/timeout_rate"] = torch.tensor(timeout_rate, dtype=torch.float32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/task/navigation_task/navigation_task.py:278: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.infos["curriculum/total_successes"] = torch.tensor(self.success_aggregate, dtype=torch.float32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/task/navigation_task/navigation_task.py:279: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.infos["curriculum/total_crashes"] = torch.tensor(self.crashes_aggregate, dtype=torch.float32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/task/navigation_task/navigation_task.py:280: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.infos["curriculum/total_timeouts"] = torch.tensor(self.timeouts_aggregate, dtype=torch.float32)
[36m[2025-07-01 20:37:45,953][166323] Fps is (10 sec: 1646.6, 60 sec: 546.3, 300 sec: 274.9). Total num frames: 65536. Throughput: 0: 288.8. Samples: 65488. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 20:37:45,953][166323] Avg episode reward: [(0, '-98.217')]
[36m[2025-07-01 20:37:51,003][166323] Fps is (10 sec: 1636.3, 60 sec: 273.0, 300 sec: 269.2). Total num frames: 65536. Throughput: 0: 289.0. Samples: 66352. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 20:37:51,004][166323] Avg episode reward: [(0, '-97.652')]
[36m[2025-07-01 20:37:55,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 263.9). Total num frames: 65536. Throughput: 0: 291.3. Samples: 68192. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 20:37:55,958][166323] Avg episode reward: [(0, '-96.393')]
[36m[2025-07-01 20:38:00,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 258.7). Total num frames: 65536. Throughput: 0: 291.3. Samples: 69856. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 20:38:00,945][166323] Avg episode reward: [(0, '-98.789')]
[36m[2025-07-01 20:38:05,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 253.6). Total num frames: 65536. Throughput: 0: 291.2. Samples: 70720. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 20:38:05,971][166323] Avg episode reward: [(0, '-96.157')]
[36m[2025-07-01 20:38:10,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 248.8). Total num frames: 65536. Throughput: 0: 291.9. Samples: 72496. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 20:38:10,951][166323] Avg episode reward: [(0, '-96.953')]
[36m[2025-07-01 20:38:15,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 244.2). Total num frames: 65536. Throughput: 0: 289.8. Samples: 74096. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 20:38:15,970][166323] Avg episode reward: [(0, '-99.845')]
[36m[2025-07-01 20:38:20,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 239.7). Total num frames: 65536. Throughput: 0: 289.5. Samples: 74992. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 20:38:20,994][166323] Avg episode reward: [(0, '-96.106')]
[31m[291910 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[291910 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[291911 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:38:25,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 235.4). Total num frames: 65536. Throughput: 0: 291.3. Samples: 76784. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 20:38:25,981][166323] Avg episode reward: [(0, '-97.203')]
[36m[2025-07-01 20:38:30,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 231.3). Total num frames: 65536. Throughput: 0: 289.1. Samples: 78496. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 20:38:30,950][166323] Avg episode reward: [(0, '-97.103')]
[36m[2025-07-01 20:38:35,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 227.3). Total num frames: 65536. Throughput: 0: 290.8. Samples: 79424. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 20:38:35,955][166323] Avg episode reward: [(0, '-99.670')]
[31m[308264 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[308265 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[308265 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[308909 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[308909 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[308909 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:38:40,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 223.4). Total num frames: 65536. Throughput: 0: 287.3. Samples: 81120. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 20:38:40,963][166323] Avg episode reward: [(0, '-100.987')]
[36m[2025-07-01 20:38:45,953][166323] Fps is (10 sec: 1638.8, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 81920. Throughput: 0: 287.2. Samples: 82784. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 20:38:45,953][166323] Avg episode reward: [(0, '-96.800')]
[31m[318498 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[318499 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[318499 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:38:50,971][166323] Fps is (10 sec: 1637.2, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 81920. Throughput: 0: 287.6. Samples: 83664. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 20:38:50,971][166323] Avg episode reward: [(0, '-96.459')]
[36m[2025-07-01 20:38:55,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 81920. Throughput: 0: 290.6. Samples: 85584. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 20:38:55,990][166323] Avg episode reward: [(0, '-96.678')]
[36m[2025-07-01 20:39:01,003][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 81920. Throughput: 0: 291.0. Samples: 87200. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 20:39:01,004][166323] Avg episode reward: [(0, '-95.518')]
[31m[330802 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[330803 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[330803 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[333652 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[333652 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[333652 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:39:05,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 81920. Throughput: 0: 292.4. Samples: 88144. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 20:39:05,969][166323] Avg episode reward: [(0, '-95.677')]
[36m[2025-07-01 20:39:10,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 81920. Throughput: 0: 291.6. Samples: 89904. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 20:39:10,969][166323] Avg episode reward: [(0, '-94.981')]
[36m[2025-07-01 20:39:15,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 81920. Throughput: 0: 294.9. Samples: 91776. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 20:39:15,987][166323] Avg episode reward: [(0, '-95.851')]
[36m[2025-07-01 20:39:20,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 81920. Throughput: 0: 295.0. Samples: 92704. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 20:39:20,979][166323] Avg episode reward: [(0, '-96.788')]
[36m[2025-07-01 20:39:25,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 81920. Throughput: 0: 299.1. Samples: 94576. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 20:39:25,955][166323] Avg episode reward: [(0, '-94.870')]
[31m[355363 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[355364 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[355364 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:39:30,976][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 81920. Throughput: 0: 300.3. Samples: 96304. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 20:39:30,976][166323] Avg episode reward: [(0, '-98.976')]
[36m[2025-07-01 20:39:35,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 81920. Throughput: 0: 300.7. Samples: 97200. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 20:39:35,981][166323] Avg episode reward: [(0, '-97.305')]
[37m[1m[2025-07-01 20:39:36,031][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000000160_81920.pth...
[36m[2025-07-01 20:39:36,035][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000000032_16384.pth
[36m[2025-07-01 20:39:40,963][166323] Fps is (10 sec: 1640.6, 60 sec: 546.1, 300 sec: 333.2). Total num frames: 98304. Throughput: 0: 296.7. Samples: 98928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:39:40,963][166323] Avg episode reward: [(0, '-98.098')]
[36m[2025-07-01 20:39:45,966][166323] Fps is (10 sec: 1640.8, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 98304. Throughput: 0: 301.0. Samples: 100736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:39:45,967][166323] Avg episode reward: [(0, '-90.450')]
[36m[2025-07-01 20:39:50,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 98304. Throughput: 0: 299.3. Samples: 101616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:39:50,986][166323] Avg episode reward: [(0, '-92.651')]
[36m[2025-07-01 20:39:55,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 98304. Throughput: 0: 299.0. Samples: 103360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:39:55,979][166323] Avg episode reward: [(0, '-90.813')]
[36m[2025-07-01 20:40:00,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 98304. Throughput: 0: 296.6. Samples: 105120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:40:00,976][166323] Avg episode reward: [(0, '-92.713')]
[36m[2025-07-01 20:40:05,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 98304. Throughput: 0: 294.9. Samples: 105968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:40:05,957][166323] Avg episode reward: [(0, '-95.854')]
[36m[2025-07-01 20:40:10,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 98304. Throughput: 0: 294.1. Samples: 107808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:40:10,947][166323] Avg episode reward: [(0, '-95.115')]
[36m[2025-07-01 20:40:15,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 98304. Throughput: 0: 294.5. Samples: 109552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:40:15,964][166323] Avg episode reward: [(0, '-93.139')]
[36m[2025-07-01 20:40:20,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 98304. Throughput: 0: 294.8. Samples: 110464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:40:20,967][166323] Avg episode reward: [(0, '-97.616')]
[36m[2025-07-01 20:40:25,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 98304. Throughput: 0: 295.8. Samples: 112240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:40:25,972][166323] Avg episode reward: [(0, '-95.957')]
[36m[2025-07-01 20:40:30,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 98304. Throughput: 0: 294.8. Samples: 114000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:40:30,956][166323] Avg episode reward: [(0, '-96.126')]
[36m[2025-07-01 20:40:35,970][166323] Fps is (10 sec: 1638.7, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 114688. Throughput: 0: 294.9. Samples: 114880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:40:35,970][166323] Avg episode reward: [(0, '-89.084')]
[36m[2025-07-01 20:40:40,977][166323] Fps is (10 sec: 1635.0, 60 sec: 273.0, 300 sec: 278.1). Total num frames: 114688. Throughput: 0: 297.3. Samples: 116736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:40:40,977][166323] Avg episode reward: [(0, '-87.641')]
[36m[2025-07-01 20:40:45,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 114688. Throughput: 0: 298.6. Samples: 118560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:40:45,979][166323] Avg episode reward: [(0, '-92.063')]
[31m[438083 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[438083 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[438083 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:40:50,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 114688. Throughput: 0: 298.4. Samples: 119408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:40:50,999][166323] Avg episode reward: [(0, '-75.766')]
[36m[2025-07-01 20:40:55,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 114688. Throughput: 0: 292.3. Samples: 120976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:40:55,999][166323] Avg episode reward: [(0, '-77.137')]
[36m[2025-07-01 20:41:00,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 114688. Throughput: 0: 297.0. Samples: 122912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:41:00,952][166323] Avg episode reward: [(0, '-82.514')]
[36m[2025-07-01 20:41:05,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 114688. Throughput: 0: 295.9. Samples: 123776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:41:05,958][166323] Avg episode reward: [(0, '-73.937')]
[36m[2025-07-01 20:41:10,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 114688. Throughput: 0: 296.1. Samples: 125568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:41:10,983][166323] Avg episode reward: [(0, '-88.053')]
[36m[2025-07-01 20:41:15,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.8). Total num frames: 114688. Throughput: 0: 294.7. Samples: 127264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:41:15,969][166323] Avg episode reward: [(0, '-84.549')]
[36m[2025-07-01 20:41:20,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 114688. Throughput: 0: 293.9. Samples: 128112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:41:20,988][166323] Avg episode reward: [(0, '-81.765')]
[36m[2025-07-01 20:41:26,002][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 114688. Throughput: 0: 290.7. Samples: 129824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:41:26,003][166323] Avg episode reward: [(0, '-85.051')]
[36m[2025-07-01 20:41:30,971][166323] Fps is (10 sec: 1641.1, 60 sec: 546.0, 300 sec: 333.3). Total num frames: 131072. Throughput: 0: 287.3. Samples: 131488. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 20:41:30,971][166323] Avg episode reward: [(0, '-73.695')]
[36m[2025-07-01 20:41:35,943][166323] Fps is (10 sec: 1648.2, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 131072. Throughput: 0: 286.2. Samples: 132272. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 20:41:35,943][166323] Avg episode reward: [(0, '-79.277')]
[37m[1m[2025-07-01 20:41:35,994][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000000256_131072.pth...
[36m[2025-07-01 20:41:35,998][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000000096_49152.pth
[31m[486040 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[486041 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[486041 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:41:40,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 131072. Throughput: 0: 287.8. Samples: 133920. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 20:41:40,970][166323] Avg episode reward: [(0, '-78.661')]
[36m[2025-07-01 20:41:45,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 131072. Throughput: 0: 282.5. Samples: 135632. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 20:41:45,972][166323] Avg episode reward: [(0, '-66.700')]
[36m[2025-07-01 20:41:50,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 131072. Throughput: 0: 283.5. Samples: 136528. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 20:41:50,945][166323] Avg episode reward: [(0, '-70.426')]
[36m[2025-07-01 20:41:55,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 131072. Throughput: 0: 284.1. Samples: 138352. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 20:41:55,978][166323] Avg episode reward: [(0, '-67.154')]
[36m[2025-07-01 20:42:00,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 131072. Throughput: 0: 288.2. Samples: 140240. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 20:42:01,000][166323] Avg episode reward: [(0, '-68.926')]
[36m[2025-07-01 20:42:05,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 131072. Throughput: 0: 289.1. Samples: 141120. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 20:42:05,977][166323] Avg episode reward: [(0, '-73.955')]
[36m[2025-07-01 20:42:10,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 131072. Throughput: 0: 291.8. Samples: 142944. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 20:42:10,960][166323] Avg episode reward: [(0, '-79.424')]
[36m[2025-07-01 20:42:15,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 131072. Throughput: 0: 296.7. Samples: 144832. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 20:42:15,947][166323] Avg episode reward: [(0, '-76.038')]
[36m[2025-07-01 20:42:20,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 131072. Throughput: 0: 301.8. Samples: 145856. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 20:42:20,954][166323] Avg episode reward: [(0, '-66.952')]
[36m[2025-07-01 20:42:25,976][166323] Fps is (10 sec: 1633.6, 60 sec: 546.4, 300 sec: 333.2). Total num frames: 147456. Throughput: 0: 303.2. Samples: 147568. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 20:42:25,977][166323] Avg episode reward: [(0, '-69.965')]
[36m[2025-07-01 20:42:30,985][166323] Fps is (10 sec: 1633.2, 60 sec: 273.0, 300 sec: 333.3). Total num frames: 147456. Throughput: 0: 301.8. Samples: 149216. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 20:42:30,986][166323] Avg episode reward: [(0, '-56.581')]
[36m[2025-07-01 20:42:35,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 147456. Throughput: 0: 301.6. Samples: 150112. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 20:42:35,982][166323] Avg episode reward: [(0, '-53.345')]
[36m[2025-07-01 20:42:40,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 147456. Throughput: 0: 301.8. Samples: 151936. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 20:42:40,981][166323] Avg episode reward: [(0, '-43.639')]
[36m[2025-07-01 20:42:45,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 147456. Throughput: 0: 299.6. Samples: 153712. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 20:42:45,966][166323] Avg episode reward: [(0, '-51.150')]
[36m[2025-07-01 20:42:50,973][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 147456. Throughput: 0: 301.5. Samples: 154688. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 20:42:50,974][166323] Avg episode reward: [(0, '-41.677')]
[36m[2025-07-01 20:42:55,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 147456. Throughput: 0: 298.9. Samples: 156400. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 20:42:55,981][166323] Avg episode reward: [(0, '-44.727')]
[36m[2025-07-01 20:43:00,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 147456. Throughput: 0: 295.9. Samples: 158160. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 20:43:00,986][166323] Avg episode reward: [(0, '-48.031')]
[36m[2025-07-01 20:43:05,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 147456. Throughput: 0: 293.0. Samples: 159040. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 20:43:05,951][166323] Avg episode reward: [(0, '-48.157')]
[36m[2025-07-01 20:43:10,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 147456. Throughput: 0: 292.0. Samples: 160704. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 20:43:10,958][166323] Avg episode reward: [(0, '-47.394')]
[36m[2025-07-01 20:43:15,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 147456. Throughput: 0: 295.1. Samples: 162496. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 20:43:15,982][166323] Avg episode reward: [(0, '-48.790')]
[36m[2025-07-01 20:43:20,975][166323] Fps is (10 sec: 1635.5, 60 sec: 545.9, 300 sec: 333.2). Total num frames: 163840. Throughput: 0: 293.4. Samples: 163312. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 20:43:20,975][166323] Avg episode reward: [(0, '-52.627')]
[36m[2025-07-01 20:43:25,957][166323] Fps is (10 sec: 1642.5, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 163840. Throughput: 0: 291.0. Samples: 165024. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 20:43:25,958][166323] Avg episode reward: [(0, '-52.109')]
[36m[2025-07-01 20:43:30,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 163840. Throughput: 0: 290.7. Samples: 166800. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 20:43:30,983][166323] Avg episode reward: [(0, '-56.021')]
[36m[2025-07-01 20:43:35,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 163840. Throughput: 0: 290.4. Samples: 167760. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 20:43:35,989][166323] Avg episode reward: [(0, '-44.717')]
[37m[1m[2025-07-01 20:43:36,060][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000000320_163840.pth...
[36m[2025-07-01 20:43:36,066][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000000160_81920.pth
[36m[2025-07-01 20:43:40,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 163840. Throughput: 0: 291.4. Samples: 169504. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 20:43:40,953][166323] Avg episode reward: [(0, '-41.941')]
[36m[2025-07-01 20:43:45,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 163840. Throughput: 0: 291.8. Samples: 171280. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 20:43:45,945][166323] Avg episode reward: [(0, '-42.003')]
[36m[2025-07-01 20:43:50,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 163840. Throughput: 0: 293.2. Samples: 172240. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 20:43:50,970][166323] Avg episode reward: [(0, '-38.235')]
[31m[622397 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[622397 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[622397 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[33m[624347 ms][navigation_task] - WARNING : Curriculum Level: 36, Curriculum progress fraction: 0.0 (navigation_task.py:262)
[33m[624347 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.00537109375
[33mCrash Rate: 0.8837890625
[33mTimeout Rate: 0.11083984375 (navigation_task.py:265)
[33m[624347 ms][navigation_task] - WARNING : 
[33mSuccesses: 11
[33mCrashes : 1810
[33mTimeouts: 227 (navigation_task.py:268)
[36m[2025-07-01 20:43:55,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 163840. Throughput: 0: 290.8. Samples: 173792. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 20:43:55,960][166323] Avg episode reward: [(0, '-39.138')]
[36m[2025-07-01 20:44:00,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 163840. Throughput: 0: 291.8. Samples: 175632. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 20:44:00,993][166323] Avg episode reward: [(0, '-46.921')]
[31m[630712 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[630712 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[630712 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:44:05,983][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 163840. Throughput: 0: 292.6. Samples: 176480. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 20:44:05,983][166323] Avg episode reward: [(0, '-39.870')]
[36m[2025-07-01 20:44:10,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 163840. Throughput: 0: 293.1. Samples: 178224. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 20:44:10,987][166323] Avg episode reward: [(0, '-37.393')]
[36m[2025-07-01 20:44:15,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 163840. Throughput: 0: 292.4. Samples: 179952. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 20:44:15,969][166323] Avg episode reward: [(0, '-43.113')]
[36m[2025-07-01 20:44:20,972][166323] Fps is (10 sec: 1640.9, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 180224. Throughput: 0: 290.6. Samples: 180832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:44:20,972][166323] Avg episode reward: [(0, '-27.289')]
[36m[2025-07-01 20:44:25,962][166323] Fps is (10 sec: 1639.5, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 180224. Throughput: 0: 289.0. Samples: 182512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:44:25,962][166323] Avg episode reward: [(0, '-33.027')]
[36m[2025-07-01 20:44:30,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 180224. Throughput: 0: 291.1. Samples: 184384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:44:30,966][166323] Avg episode reward: [(0, '-26.064')]
[36m[2025-07-01 20:44:35,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 180224. Throughput: 0: 288.3. Samples: 185216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:44:35,977][166323] Avg episode reward: [(0, '-23.970')]
[31m[667504 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[667505 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[667505 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:44:40,979][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 180224. Throughput: 0: 295.7. Samples: 187104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:44:40,979][166323] Avg episode reward: [(0, '-29.472')]
[31m[674386 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[674386 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[674387 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:44:45,979][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 180224. Throughput: 0: 293.8. Samples: 188848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:44:45,979][166323] Avg episode reward: [(0, '-28.415')]
[31m[675909 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[675910 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[675910 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:44:50,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 180224. Throughput: 0: 294.8. Samples: 189744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:44:50,979][166323] Avg episode reward: [(0, '-24.479')]
[36m[2025-07-01 20:44:55,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 180224. Throughput: 0: 294.9. Samples: 191488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:44:55,961][166323] Avg episode reward: [(0, '-31.193')]
[36m[2025-07-01 20:45:00,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 180224. Throughput: 0: 296.1. Samples: 193280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:45:00,973][166323] Avg episode reward: [(0, '-34.221')]
[31m[693968 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[693968 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[693968 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:45:05,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 180224. Throughput: 0: 297.7. Samples: 194224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:45:05,956][166323] Avg episode reward: [(0, '-15.227')]
[36m[2025-07-01 20:45:10,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 180224. Throughput: 0: 302.8. Samples: 196144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:45:10,977][166323] Avg episode reward: [(0, '-19.153')]
[36m[2025-07-01 20:45:15,966][166323] Fps is (10 sec: 1636.7, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 196608. Throughput: 0: 302.2. Samples: 197984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:45:15,966][166323] Avg episode reward: [(0, '-17.174')]
[36m[2025-07-01 20:45:20,972][166323] Fps is (10 sec: 1639.2, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 196608. Throughput: 0: 303.3. Samples: 198864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:45:20,972][166323] Avg episode reward: [(0, '-21.024')]
[36m[2025-07-01 20:45:25,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 196608. Throughput: 0: 300.3. Samples: 200624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:45:26,000][166323] Avg episode reward: [(0, '-14.140')]
[36m[2025-07-01 20:45:30,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 196608. Throughput: 0: 302.4. Samples: 202448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:45:30,958][166323] Avg episode reward: [(0, '-13.490')]
[36m[2025-07-01 20:45:35,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 196608. Throughput: 0: 301.0. Samples: 203280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:45:35,956][166323] Avg episode reward: [(0, '-19.251')]
[37m[1m[2025-07-01 20:45:36,005][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000000384_196608.pth...
[36m[2025-07-01 20:45:36,008][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000000256_131072.pth
[36m[2025-07-01 20:45:40,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 196608. Throughput: 0: 300.4. Samples: 205008. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:45:40,971][166323] Avg episode reward: [(0, '-17.075')]
[36m[2025-07-01 20:45:45,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 196608. Throughput: 0: 301.3. Samples: 206832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:45:45,956][166323] Avg episode reward: [(0, '-15.770')]
[31m[739322 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[739323 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[739323 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:45:50,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 196608. Throughput: 0: 301.9. Samples: 207808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:45:50,953][166323] Avg episode reward: [(0, '-11.464')]
[36m[2025-07-01 20:45:55,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 196608. Throughput: 0: 300.2. Samples: 209648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:45:55,956][166323] Avg episode reward: [(0, '-12.842')]
[36m[2025-07-01 20:46:00,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 196608. Throughput: 0: 298.6. Samples: 211424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:46:00,975][166323] Avg episode reward: [(0, '-10.779')]
[36m[2025-07-01 20:46:05,951][166323] Fps is (10 sec: 1639.2, 60 sec: 546.2, 300 sec: 333.3). Total num frames: 212992. Throughput: 0: 299.5. Samples: 212336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:46:05,951][166323] Avg episode reward: [(0, '-7.587')]
[36m[2025-07-01 20:46:10,971][166323] Fps is (10 sec: 1639.1, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 212992. Throughput: 0: 300.6. Samples: 214144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:46:10,971][166323] Avg episode reward: [(0, '-13.234')]
[36m[2025-07-01 20:46:15,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 212992. Throughput: 0: 300.3. Samples: 215968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:46:15,984][166323] Avg episode reward: [(0, '-8.564')]
[31m[766156 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[766156 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[766156 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:46:20,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 212992. Throughput: 0: 303.7. Samples: 216944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:46:20,944][166323] Avg episode reward: [(0, '-16.467')]
[36m[2025-07-01 20:46:25,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 212992. Throughput: 0: 306.6. Samples: 218800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:46:25,956][166323] Avg episode reward: [(0, '-16.948')]
[36m[2025-07-01 20:46:30,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 212992. Throughput: 0: 303.1. Samples: 220480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:46:30,985][166323] Avg episode reward: [(0, '-21.672')]
[36m[2025-07-01 20:46:35,998][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 212992. Throughput: 0: 300.8. Samples: 221360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:46:35,999][166323] Avg episode reward: [(0, '-19.974')]
[36m[2025-07-01 20:46:40,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 212992. Throughput: 0: 295.8. Samples: 222960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:46:40,957][166323] Avg episode reward: [(0, '-16.732')]
[36m[2025-07-01 20:46:45,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 212992. Throughput: 0: 294.9. Samples: 224688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:46:45,951][166323] Avg episode reward: [(0, '-20.142')]
[36m[2025-07-01 20:46:50,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 212992. Throughput: 0: 295.6. Samples: 225648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:46:50,982][166323] Avg episode reward: [(0, '-5.814')]
[36m[2025-07-01 20:46:55,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 212992. Throughput: 0: 293.7. Samples: 227360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:46:55,962][166323] Avg episode reward: [(0, '-17.960')]
[36m[2025-07-01 20:47:00,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 212992. Throughput: 0: 293.3. Samples: 229168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:47:00,985][166323] Avg episode reward: [(0, '-19.056')]
[36m[2025-07-01 20:47:05,987][166323] Fps is (10 sec: 1634.4, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 229376. Throughput: 0: 290.2. Samples: 230016. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 20:47:05,987][166323] Avg episode reward: [(0, '-20.618')]
[36m[2025-07-01 20:47:10,999][166323] Fps is (10 sec: 1636.1, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 229376. Throughput: 0: 288.4. Samples: 231792. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 20:47:10,999][166323] Avg episode reward: [(0, '-21.021')]
[36m[2025-07-01 20:47:15,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 229376. Throughput: 0: 291.2. Samples: 233584. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 20:47:15,985][166323] Avg episode reward: [(0, '-19.878')]
[36m[2025-07-01 20:47:20,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 229376. Throughput: 0: 292.5. Samples: 234512. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 20:47:20,968][166323] Avg episode reward: [(0, '-21.924')]
[36m[2025-07-01 20:47:25,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 229376. Throughput: 0: 296.0. Samples: 236288. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 20:47:25,980][166323] Avg episode reward: [(0, '-4.238')]
[36m[2025-07-01 20:47:30,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 229376. Throughput: 0: 297.3. Samples: 238064. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 20:47:30,944][166323] Avg episode reward: [(0, '-6.717')]
[36m[2025-07-01 20:47:35,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 229376. Throughput: 0: 297.1. Samples: 239008. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 20:47:35,956][166323] Avg episode reward: [(0, '-7.195')]
[37m[1m[2025-07-01 20:47:36,026][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000000448_229376.pth...
[36m[2025-07-01 20:47:36,034][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000000320_163840.pth
[36m[2025-07-01 20:47:40,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 229376. Throughput: 0: 297.7. Samples: 240752. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 20:47:40,953][166323] Avg episode reward: [(0, '-9.544')]
[36m[2025-07-01 20:47:45,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 229376. Throughput: 0: 295.6. Samples: 242464. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 20:47:45,970][166323] Avg episode reward: [(0, '-5.759')]
[36m[2025-07-01 20:47:50,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 229376. Throughput: 0: 293.8. Samples: 243232. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 20:47:50,977][166323] Avg episode reward: [(0, '-10.769')]
[36m[2025-07-01 20:47:55,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 229376. Throughput: 0: 293.8. Samples: 245008. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 20:47:55,981][166323] Avg episode reward: [(0, '-14.523')]
[36m[2025-07-01 20:48:01,012][166323] Fps is (10 sec: 1632.7, 60 sec: 545.9, 300 sec: 333.2). Total num frames: 245760. Throughput: 0: 291.0. Samples: 246688. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 20:48:01,012][166323] Avg episode reward: [(0, '-11.696')]
[36m[2025-07-01 20:48:05,979][166323] Fps is (10 sec: 1638.7, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 245760. Throughput: 0: 290.8. Samples: 247600. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 20:48:05,979][166323] Avg episode reward: [(0, '-4.408')]
[36m[2025-07-01 20:48:10,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 245760. Throughput: 0: 289.0. Samples: 249296. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 20:48:10,984][166323] Avg episode reward: [(0, '-8.040')]
[31m[879837 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[879837 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[879837 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:48:15,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 245760. Throughput: 0: 290.2. Samples: 251136. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 20:48:15,984][166323] Avg episode reward: [(0, '-16.918')]
[36m[2025-07-01 20:48:20,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 245760. Throughput: 0: 289.8. Samples: 252048. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 20:48:20,957][166323] Avg episode reward: [(0, '-8.240')]
[36m[2025-07-01 20:48:25,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 245760. Throughput: 0: 289.6. Samples: 253792. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 20:48:25,987][166323] Avg episode reward: [(0, '-3.448')]
[36m[2025-07-01 20:48:30,972][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 245760. Throughput: 0: 290.5. Samples: 255536. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 20:48:30,972][166323] Avg episode reward: [(0, '-11.381')]
[36m[2025-07-01 20:48:35,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 245760. Throughput: 0: 291.8. Samples: 256368. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 20:48:35,989][166323] Avg episode reward: [(0, '-5.922')]
[36m[2025-07-01 20:48:40,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 245760. Throughput: 0: 289.6. Samples: 258032. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 20:48:40,952][166323] Avg episode reward: [(0, '1.751')]
[36m[2025-07-01 20:48:45,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 245760. Throughput: 0: 291.7. Samples: 259808. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 20:48:45,987][166323] Avg episode reward: [(0, '9.981')]
[36m[2025-07-01 20:48:51,007][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 245760. Throughput: 0: 290.7. Samples: 260688. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 20:48:51,007][166323] Avg episode reward: [(0, '-3.133')]
[36m[2025-07-01 20:48:55,985][166323] Fps is (10 sec: 1638.8, 60 sec: 546.1, 300 sec: 333.2). Total num frames: 262144. Throughput: 0: 290.1. Samples: 262352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:48:55,985][166323] Avg episode reward: [(0, '3.234')]
[31m[927348 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[927348 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[927348 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:49:00,984][166323] Fps is (10 sec: 1642.1, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 262144. Throughput: 0: 288.7. Samples: 264128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:49:00,984][166323] Avg episode reward: [(0, '-0.044')]
[36m[2025-07-01 20:49:05,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 262144. Throughput: 0: 288.3. Samples: 265024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:49:05,962][166323] Avg episode reward: [(0, '4.302')]
[36m[2025-07-01 20:49:10,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 262144. Throughput: 0: 288.4. Samples: 266768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:49:10,980][166323] Avg episode reward: [(0, '0.099')]
[36m[2025-07-01 20:49:15,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 262144. Throughput: 0: 288.8. Samples: 268528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:49:15,952][166323] Avg episode reward: [(0, '-4.361')]
[36m[2025-07-01 20:49:20,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 262144. Throughput: 0: 291.2. Samples: 269472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:49:20,989][166323] Avg episode reward: [(0, '11.530')]
[36m[2025-07-01 20:49:25,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 262144. Throughput: 0: 294.8. Samples: 271296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:49:25,947][166323] Avg episode reward: [(0, '1.717')]
[36m[2025-07-01 20:49:30,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 262144. Throughput: 0: 294.7. Samples: 273056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:49:30,947][166323] Avg episode reward: [(0, '1.714')]
[36m[2025-07-01 20:49:35,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 262144. Throughput: 0: 293.8. Samples: 273904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:49:35,986][166323] Avg episode reward: [(0, '-1.537')]
[37m[1m[2025-07-01 20:49:36,046][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000000512_262144.pth...
[36m[2025-07-01 20:49:36,050][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000000384_196608.pth
[36m[2025-07-01 20:49:40,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 262144. Throughput: 0: 299.4. Samples: 275824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:49:40,988][166323] Avg episode reward: [(0, '-3.354')]
[31m[973123 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[973124 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[973124 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:49:45,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 262144. Throughput: 0: 300.1. Samples: 277632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:49:45,977][166323] Avg episode reward: [(0, '-13.404')]
[36m[2025-07-01 20:49:50,961][166323] Fps is (10 sec: 1642.9, 60 sec: 546.6, 300 sec: 333.2). Total num frames: 278528. Throughput: 0: 300.8. Samples: 278560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:49:50,961][166323] Avg episode reward: [(0, '-8.516')]
[31m[979767 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[979767 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[979768 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:49:55,949][166323] Fps is (10 sec: 1642.9, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 278528. Throughput: 0: 298.9. Samples: 280208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:49:55,950][166323] Avg episode reward: [(0, '0.294')]
[36m[2025-07-01 20:50:00,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 278528. Throughput: 0: 300.5. Samples: 282048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:50:00,950][166323] Avg episode reward: [(0, '1.999')]
[36m[2025-07-01 20:50:05,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 278528. Throughput: 0: 299.8. Samples: 282960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:50:05,980][166323] Avg episode reward: [(0, '3.438')]
[36m[2025-07-01 20:50:10,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 278528. Throughput: 0: 296.8. Samples: 284656. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:50:10,958][166323] Avg episode reward: [(0, '5.773')]
[36m[2025-07-01 20:50:15,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 278528. Throughput: 0: 297.7. Samples: 286464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:50:15,992][166323] Avg episode reward: [(0, '-1.122')]
[36m[2025-07-01 20:50:20,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 278528. Throughput: 0: 296.9. Samples: 287264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:50:20,978][166323] Avg episode reward: [(0, '-6.542')]
[36m[2025-07-01 20:50:26,012][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 278528. Throughput: 0: 291.4. Samples: 288944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:50:26,013][166323] Avg episode reward: [(0, '-4.419')]
[36m[2025-07-01 20:50:30,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 278528. Throughput: 0: 291.7. Samples: 290752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:50:30,948][166323] Avg episode reward: [(0, '-8.389')]
[36m[2025-07-01 20:50:35,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 278528. Throughput: 0: 289.2. Samples: 291584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:50:35,990][166323] Avg episode reward: [(0, '-3.128')]
[36m[2025-07-01 20:50:40,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 278528. Throughput: 0: 293.7. Samples: 293424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:50:40,954][166323] Avg episode reward: [(0, '-0.257')]
[36m[2025-07-01 20:50:45,962][166323] Fps is (10 sec: 1643.0, 60 sec: 546.3, 300 sec: 333.2). Total num frames: 294912. Throughput: 0: 287.9. Samples: 295008. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:50:45,962][166323] Avg episode reward: [(0, '10.579')]
[31m[1037534 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1037534 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[1037535 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:50:50,973][166323] Fps is (10 sec: 1635.1, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 294912. Throughput: 0: 288.0. Samples: 295920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:50:50,974][166323] Avg episode reward: [(0, '1.097')]
[36m[2025-07-01 20:50:55,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 294912. Throughput: 0: 291.3. Samples: 297760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:50:55,944][166323] Avg episode reward: [(0, '-2.330')]
[36m[2025-07-01 20:51:00,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 294912. Throughput: 0: 292.1. Samples: 299600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:51:00,960][166323] Avg episode reward: [(0, '2.331')]
[36m[2025-07-01 20:51:06,002][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 294912. Throughput: 0: 295.0. Samples: 300544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:51:06,002][166323] Avg episode reward: [(0, '-0.712')]
[36m[2025-07-01 20:51:10,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 294912. Throughput: 0: 296.4. Samples: 302272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:51:10,986][166323] Avg episode reward: [(0, '0.413')]
[36m[2025-07-01 20:51:15,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 294912. Throughput: 0: 297.1. Samples: 304128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:51:15,969][166323] Avg episode reward: [(0, '9.210')]
[36m[2025-07-01 20:51:20,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 294912. Throughput: 0: 301.7. Samples: 305152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:51:20,961][166323] Avg episode reward: [(0, '11.480')]
[36m[2025-07-01 20:51:25,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 294912. Throughput: 0: 296.8. Samples: 306784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:51:25,973][166323] Avg episode reward: [(0, '10.048')]
[36m[2025-07-01 20:51:30,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 294912. Throughput: 0: 304.2. Samples: 308704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:51:30,981][166323] Avg episode reward: [(0, '3.102')]
[36m[2025-07-01 20:51:35,997][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 294912. Throughput: 0: 302.1. Samples: 309520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:51:35,998][166323] Avg episode reward: [(0, '3.919')]
[37m[1m[2025-07-01 20:51:36,065][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000000576_294912.pth...
[36m[2025-07-01 20:51:36,069][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000000448_229376.pth
[31m[1087715 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1087715 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[1087715 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:51:40,955][166323] Fps is (10 sec: 1642.6, 60 sec: 546.1, 300 sec: 333.2). Total num frames: 311296. Throughput: 0: 298.2. Samples: 311184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:51:40,955][166323] Avg episode reward: [(0, '-1.419')]
[31m[1093051 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1093051 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[1093052 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:51:45,967][166323] Fps is (10 sec: 1643.4, 60 sec: 273.0, 300 sec: 333.3). Total num frames: 311296. Throughput: 0: 299.3. Samples: 313072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:51:45,967][166323] Avg episode reward: [(0, '-2.236')]
[36m[2025-07-01 20:51:50,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 311296. Throughput: 0: 297.7. Samples: 313936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:51:50,987][166323] Avg episode reward: [(0, '0.451')]
[36m[2025-07-01 20:51:55,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 311296. Throughput: 0: 298.6. Samples: 315712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:51:55,990][166323] Avg episode reward: [(0, '-0.627')]
[36m[2025-07-01 20:52:00,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 311296. Throughput: 0: 298.4. Samples: 317552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:52:00,958][166323] Avg episode reward: [(0, '3.229')]
[36m[2025-07-01 20:52:05,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 311296. Throughput: 0: 294.5. Samples: 318400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:52:05,948][166323] Avg episode reward: [(0, '4.607')]
[36m[2025-07-01 20:52:10,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 311296. Throughput: 0: 297.3. Samples: 320160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:52:10,970][166323] Avg episode reward: [(0, '11.967')]
[36m[2025-07-01 20:52:15,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 311296. Throughput: 0: 293.3. Samples: 321904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:52:15,980][166323] Avg episode reward: [(0, '19.630')]
[36m[2025-07-01 20:52:20,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 311296. Throughput: 0: 296.1. Samples: 322832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:52:20,954][166323] Avg episode reward: [(0, '15.391')]
[36m[2025-07-01 20:52:25,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 311296. Throughput: 0: 300.9. Samples: 324720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:52:25,946][166323] Avg episode reward: [(0, '6.919')]
[36m[2025-07-01 20:52:30,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 311296. Throughput: 0: 294.3. Samples: 326320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:52:30,988][166323] Avg episode reward: [(0, '9.073')]
[36m[2025-07-01 20:52:35,968][166323] Fps is (10 sec: 1634.7, 60 sec: 546.4, 300 sec: 333.2). Total num frames: 327680. Throughput: 0: 297.7. Samples: 327328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:52:35,969][166323] Avg episode reward: [(0, '10.861')]
[36m[2025-07-01 20:52:40,974][166323] Fps is (10 sec: 1640.6, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 327680. Throughput: 0: 298.1. Samples: 329120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:52:40,974][166323] Avg episode reward: [(0, '1.932')]
[36m[2025-07-01 20:52:45,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 327680. Throughput: 0: 301.3. Samples: 331120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:52:45,991][166323] Avg episode reward: [(0, '2.940')]
[36m[2025-07-01 20:52:50,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 327680. Throughput: 0: 302.3. Samples: 332016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:52:50,983][166323] Avg episode reward: [(0, '7.151')]
[36m[2025-07-01 20:52:55,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 327680. Throughput: 0: 304.3. Samples: 333856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:52:55,983][166323] Avg episode reward: [(0, '16.111')]
[36m[2025-07-01 20:53:00,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 327680. Throughput: 0: 305.9. Samples: 335664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:53:00,965][166323] Avg episode reward: [(0, '9.029')]
[36m[2025-07-01 20:53:05,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 327680. Throughput: 0: 304.7. Samples: 336544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:53:05,949][166323] Avg episode reward: [(0, '18.286')]
[36m[2025-07-01 20:53:10,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 327680. Throughput: 0: 304.7. Samples: 338432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:53:10,952][166323] Avg episode reward: [(0, '14.023')]
[36m[2025-07-01 20:53:15,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 327680. Throughput: 0: 308.7. Samples: 340208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:53:15,974][166323] Avg episode reward: [(0, '14.599')]
[31m[1185094 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1185095 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[1185095 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:53:20,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 327680. Throughput: 0: 305.7. Samples: 341088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:53:20,976][166323] Avg episode reward: [(0, '13.578')]
[36m[2025-07-01 20:53:25,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 327680. Throughput: 0: 304.7. Samples: 342832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:53:25,981][166323] Avg episode reward: [(0, '14.563')]
[36m[2025-07-01 20:53:30,991][166323] Fps is (10 sec: 1635.9, 60 sec: 546.1, 300 sec: 333.2). Total num frames: 344064. Throughput: 0: 299.4. Samples: 344592. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-01 20:53:30,991][166323] Avg episode reward: [(0, '7.840')]
[36m[2025-07-01 20:53:35,978][166323] Fps is (10 sec: 1638.8, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 344064. Throughput: 0: 299.8. Samples: 345504. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-01 20:53:35,978][166323] Avg episode reward: [(0, '2.183')]
[37m[1m[2025-07-01 20:53:36,036][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000000672_344064.pth...
[36m[2025-07-01 20:53:36,040][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000000512_262144.pth
[33m[1207691 ms][navigation_task] - WARNING : Curriculum Level: 36, Curriculum progress fraction: 0.0 (navigation_task.py:262)
[33m[1207692 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.0537109375
[33mCrash Rate: 0.36328125
[33mTimeout Rate: 0.5830078125 (navigation_task.py:265)
[33m[1207692 ms][navigation_task] - WARNING : 
[33mSuccesses: 110
[33mCrashes : 744
[33mTimeouts: 1194 (navigation_task.py:268)
[31m[1209197 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1209197 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[1209198 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:53:40,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 344064. Throughput: 0: 295.5. Samples: 347152. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-01 20:53:40,973][166323] Avg episode reward: [(0, '6.813')]
[36m[2025-07-01 20:53:45,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 344064. Throughput: 0: 294.7. Samples: 348928. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-01 20:53:45,969][166323] Avg episode reward: [(0, '3.898')]
[36m[2025-07-01 20:53:50,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 344064. Throughput: 0: 296.9. Samples: 349904. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-01 20:53:50,951][166323] Avg episode reward: [(0, '2.458')]
[36m[2025-07-01 20:53:55,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 344064. Throughput: 0: 290.8. Samples: 351520. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-01 20:53:55,952][166323] Avg episode reward: [(0, '5.521')]
[36m[2025-07-01 20:54:00,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 344064. Throughput: 0: 292.2. Samples: 353360. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-01 20:54:00,979][166323] Avg episode reward: [(0, '0.767')]
[36m[2025-07-01 20:54:05,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 344064. Throughput: 0: 291.9. Samples: 354224. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-01 20:54:05,982][166323] Avg episode reward: [(0, '9.642')]
[31m[1236416 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1236417 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[1236417 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[1237276 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1237276 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[1237277 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:54:10,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 344064. Throughput: 0: 291.9. Samples: 355968. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-01 20:54:10,988][166323] Avg episode reward: [(0, '11.347')]
[36m[2025-07-01 20:54:15,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 344064. Throughput: 0: 293.8. Samples: 357808. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-01 20:54:15,981][166323] Avg episode reward: [(0, '15.311')]
[36m[2025-07-01 20:54:20,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 344064. Throughput: 0: 294.0. Samples: 358736. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-01 20:54:20,985][166323] Avg episode reward: [(0, '9.541')]
[36m[2025-07-01 20:54:25,992][166323] Fps is (10 sec: 1636.5, 60 sec: 546.0, 300 sec: 333.2). Total num frames: 360448. Throughput: 0: 297.8. Samples: 360560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:54:25,992][166323] Avg episode reward: [(0, '20.002')]
[36m[2025-07-01 20:54:30,979][166323] Fps is (10 sec: 1639.4, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 360448. Throughput: 0: 297.2. Samples: 362304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:54:30,979][166323] Avg episode reward: [(0, '19.127')]
[36m[2025-07-01 20:54:35,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 360448. Throughput: 0: 293.8. Samples: 363136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:54:35,988][166323] Avg episode reward: [(0, '16.187')]
[36m[2025-07-01 20:54:40,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 360448. Throughput: 0: 299.4. Samples: 364992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:54:40,955][166323] Avg episode reward: [(0, '20.280')]
[36m[2025-07-01 20:54:45,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 360448. Throughput: 0: 299.7. Samples: 366848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:54:45,982][166323] Avg episode reward: [(0, '28.702')]
[36m[2025-07-01 20:54:50,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 360448. Throughput: 0: 299.7. Samples: 367712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:54:50,988][166323] Avg episode reward: [(0, '19.517')]
[36m[2025-07-01 20:54:55,983][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 360448. Throughput: 0: 300.8. Samples: 369504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:54:55,983][166323] Avg episode reward: [(0, '18.910')]
[36m[2025-07-01 20:55:00,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 360448. Throughput: 0: 299.8. Samples: 371296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:55:00,970][166323] Avg episode reward: [(0, '18.709')]
[36m[2025-07-01 20:55:05,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 360448. Throughput: 0: 296.9. Samples: 372096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:55:05,988][166323] Avg episode reward: [(0, '9.476')]
[36m[2025-07-01 20:55:10,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 360448. Throughput: 0: 296.1. Samples: 373872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:55:10,957][166323] Avg episode reward: [(0, '21.011')]
[36m[2025-07-01 20:55:15,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 360448. Throughput: 0: 296.2. Samples: 375632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:55:15,969][166323] Avg episode reward: [(0, '18.930')]
[31m[1306101 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1306101 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[1306101 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:55:20,988][166323] Fps is (10 sec: 1633.4, 60 sec: 546.1, 300 sec: 333.3). Total num frames: 376832. Throughput: 0: 296.9. Samples: 376496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:55:20,988][166323] Avg episode reward: [(0, '21.633')]
[36m[2025-07-01 20:55:25,944][166323] Fps is (10 sec: 1642.4, 60 sec: 273.3, 300 sec: 333.2). Total num frames: 376832. Throughput: 0: 295.9. Samples: 378304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:55:25,945][166323] Avg episode reward: [(0, '15.824')]
[36m[2025-07-01 20:55:30,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 376832. Throughput: 0: 296.6. Samples: 380192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:55:30,968][166323] Avg episode reward: [(0, '22.208')]
[36m[2025-07-01 20:55:35,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 376832. Throughput: 0: 296.5. Samples: 381056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:55:35,985][166323] Avg episode reward: [(0, '19.870')]
[37m[1m[2025-07-01 20:55:36,049][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000000736_376832.pth...
[36m[2025-07-01 20:55:36,053][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000000576_294912.pth
[36m[2025-07-01 20:55:40,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 376832. Throughput: 0: 297.6. Samples: 382896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:55:40,983][166323] Avg episode reward: [(0, '4.837')]
[36m[2025-07-01 20:55:45,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 376832. Throughput: 0: 298.0. Samples: 384704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:55:45,967][166323] Avg episode reward: [(0, '15.840')]
[31m[1336236 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1336236 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[1336237 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:55:50,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 376832. Throughput: 0: 299.4. Samples: 385568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:55:50,990][166323] Avg episode reward: [(0, '11.869')]
[36m[2025-07-01 20:55:55,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 376832. Throughput: 0: 300.5. Samples: 387392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:55:55,948][166323] Avg episode reward: [(0, '15.338')]
[31m[1346820 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1346821 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[1346821 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:56:00,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 376832. Throughput: 0: 300.2. Samples: 389136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:56:00,958][166323] Avg episode reward: [(0, '21.339')]
[31m[1352737 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1352737 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[1352737 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:56:05,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 376832. Throughput: 0: 302.5. Samples: 390096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:56:05,949][166323] Avg episode reward: [(0, '22.394')]
[36m[2025-07-01 20:56:10,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 376832. Throughput: 0: 304.2. Samples: 392000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:56:10,963][166323] Avg episode reward: [(0, '34.336')]
[36m[2025-07-01 20:56:15,961][166323] Fps is (10 sec: 1636.4, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 393216. Throughput: 0: 300.5. Samples: 393712. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 20:56:15,962][166323] Avg episode reward: [(0, '36.550')]
[36m[2025-07-01 20:56:21,004][166323] Fps is (10 sec: 1631.6, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 393216. Throughput: 0: 300.7. Samples: 394592. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 20:56:21,005][166323] Avg episode reward: [(0, '47.420')]
[36m[2025-07-01 20:56:25,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 393216. Throughput: 0: 299.6. Samples: 396368. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 20:56:25,944][166323] Avg episode reward: [(0, '28.933')]
[36m[2025-07-01 20:56:30,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 393216. Throughput: 0: 298.2. Samples: 398128. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 20:56:30,986][166323] Avg episode reward: [(0, '22.955')]
[36m[2025-07-01 20:56:35,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 393216. Throughput: 0: 297.3. Samples: 398944. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 20:56:35,978][166323] Avg episode reward: [(0, '18.446')]
[36m[2025-07-01 20:56:40,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 393216. Throughput: 0: 293.9. Samples: 400624. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 20:56:40,973][166323] Avg episode reward: [(0, '16.512')]
[36m[2025-07-01 20:56:45,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 393216. Throughput: 0: 295.5. Samples: 402432. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 20:56:45,955][166323] Avg episode reward: [(0, '13.503')]
[36m[2025-07-01 20:56:50,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 393216. Throughput: 0: 295.2. Samples: 403392. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 20:56:50,984][166323] Avg episode reward: [(0, '10.915')]
[31m[1401769 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1401769 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[1401770 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:56:55,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 393216. Throughput: 0: 288.2. Samples: 404976. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 20:56:55,987][166323] Avg episode reward: [(0, '22.215')]
[36m[2025-07-01 20:57:00,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 393216. Throughput: 0: 290.0. Samples: 406768. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 20:57:00,986][166323] Avg episode reward: [(0, '22.919')]
[36m[2025-07-01 20:57:05,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 393216. Throughput: 0: 291.1. Samples: 407680. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 20:57:05,959][166323] Avg episode reward: [(0, '27.240')]
[31m[1414944 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1414945 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[1414945 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:57:10,985][166323] Fps is (10 sec: 1638.6, 60 sec: 545.9, 300 sec: 333.2). Total num frames: 409600. Throughput: 0: 289.2. Samples: 409392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:57:10,985][166323] Avg episode reward: [(0, '27.352')]
[36m[2025-07-01 20:57:15,974][166323] Fps is (10 sec: 1635.9, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 409600. Throughput: 0: 288.8. Samples: 411120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:57:15,974][166323] Avg episode reward: [(0, '25.169')]
[36m[2025-07-01 20:57:20,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 333.2). Total num frames: 409600. Throughput: 0: 289.6. Samples: 411968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:57:20,954][166323] Avg episode reward: [(0, '24.362')]
[36m[2025-07-01 20:57:25,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.3). Total num frames: 409600. Throughput: 0: 293.8. Samples: 413840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:57:25,958][166323] Avg episode reward: [(0, '16.121')]
[36m[2025-07-01 20:57:30,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 409600. Throughput: 0: 293.3. Samples: 415632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:57:30,960][166323] Avg episode reward: [(0, '27.541')]
[36m[2025-07-01 20:57:35,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 409600. Throughput: 0: 293.5. Samples: 416592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:57:35,963][166323] Avg episode reward: [(0, '31.870')]
[37m[1m[2025-07-01 20:57:36,013][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000000800_409600.pth...
[36m[2025-07-01 20:57:36,017][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000000672_344064.pth
[36m[2025-07-01 20:57:40,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 409600. Throughput: 0: 299.0. Samples: 418432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:57:40,987][166323] Avg episode reward: [(0, '26.713')]
[36m[2025-07-01 20:57:45,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 409600. Throughput: 0: 298.4. Samples: 420192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:57:45,968][166323] Avg episode reward: [(0, '30.373')]
[31m[1456985 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1456985 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[1456986 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:57:50,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 409600. Throughput: 0: 298.1. Samples: 421104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:57:50,989][166323] Avg episode reward: [(0, '27.604')]
[36m[2025-07-01 20:57:55,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 409600. Throughput: 0: 302.0. Samples: 422976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:57:55,967][166323] Avg episode reward: [(0, '35.958')]
[36m[2025-07-01 20:58:00,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 409600. Throughput: 0: 303.7. Samples: 424784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 20:58:00,965][166323] Avg episode reward: [(0, '38.231')]
[36m[2025-07-01 20:58:05,970][166323] Fps is (10 sec: 1637.8, 60 sec: 546.0, 300 sec: 333.2). Total num frames: 425984. Throughput: 0: 306.4. Samples: 425760. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 20:58:05,971][166323] Avg episode reward: [(0, '32.757')]
[36m[2025-07-01 20:58:10,947][166323] Fps is (10 sec: 1641.3, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 425984. Throughput: 0: 303.4. Samples: 427488. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 20:58:10,948][166323] Avg episode reward: [(0, '36.284')]
[36m[2025-07-01 20:58:15,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 425984. Throughput: 0: 302.7. Samples: 429248. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 20:58:15,946][166323] Avg episode reward: [(0, '29.610')]
[36m[2025-07-01 20:58:20,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 425984. Throughput: 0: 302.5. Samples: 430208. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 20:58:20,970][166323] Avg episode reward: [(0, '28.062')]
[36m[2025-07-01 20:58:25,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 425984. Throughput: 0: 302.8. Samples: 432048. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 20:58:25,958][166323] Avg episode reward: [(0, '32.734')]
[36m[2025-07-01 20:58:30,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 425984. Throughput: 0: 304.5. Samples: 433888. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 20:58:30,944][166323] Avg episode reward: [(0, '22.062')]
[31m[1502946 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1502946 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[1502946 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:58:35,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 425984. Throughput: 0: 305.6. Samples: 434848. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 20:58:35,962][166323] Avg episode reward: [(0, '35.333')]
[36m[2025-07-01 20:58:40,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 425984. Throughput: 0: 303.3. Samples: 436624. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 20:58:40,960][166323] Avg episode reward: [(0, '25.455')]
[36m[2025-07-01 20:58:45,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 425984. Throughput: 0: 302.8. Samples: 438416. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 20:58:45,978][166323] Avg episode reward: [(0, '29.859')]
[36m[2025-07-01 20:58:50,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 425984. Throughput: 0: 301.6. Samples: 439328. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 20:58:50,950][166323] Avg episode reward: [(0, '32.848')]
[36m[2025-07-01 20:58:55,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 425984. Throughput: 0: 300.1. Samples: 441008. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 20:58:55,993][166323] Avg episode reward: [(0, '20.599')]
[36m[2025-07-01 20:59:00,949][166323] Fps is (10 sec: 1638.6, 60 sec: 546.3, 300 sec: 333.3). Total num frames: 442368. Throughput: 0: 300.1. Samples: 442752. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 20:59:00,949][166323] Avg episode reward: [(0, '30.709')]
[36m[2025-07-01 20:59:05,982][166323] Fps is (10 sec: 1640.1, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 442368. Throughput: 0: 294.7. Samples: 443472. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 20:59:05,982][166323] Avg episode reward: [(0, '14.916')]
[31m[1534967 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1534967 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[1534968 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 20:59:10,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 442368. Throughput: 0: 293.5. Samples: 445264. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 20:59:10,981][166323] Avg episode reward: [(0, '18.752')]
[36m[2025-07-01 20:59:15,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.3). Total num frames: 442368. Throughput: 0: 291.1. Samples: 446992. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 20:59:15,958][166323] Avg episode reward: [(0, '19.478')]
[36m[2025-07-01 20:59:20,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 442368. Throughput: 0: 289.8. Samples: 447888. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 20:59:20,954][166323] Avg episode reward: [(0, '29.719')]
[36m[2025-07-01 20:59:26,000][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 442368. Throughput: 0: 289.9. Samples: 449680. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 20:59:26,000][166323] Avg episode reward: [(0, '31.385')]
[36m[2025-07-01 20:59:30,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 442368. Throughput: 0: 289.8. Samples: 451456. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 20:59:30,981][166323] Avg episode reward: [(0, '25.739')]
[36m[2025-07-01 20:59:35,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 442368. Throughput: 0: 288.5. Samples: 452320. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 20:59:35,981][166323] Avg episode reward: [(0, '21.743')]
[37m[1m[2025-07-01 20:59:36,038][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000000864_442368.pth...
[36m[2025-07-01 20:59:36,042][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000000736_376832.pth
[36m[2025-07-01 20:59:40,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 442368. Throughput: 0: 289.8. Samples: 454048. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 20:59:40,996][166323] Avg episode reward: [(0, '22.120')]
[36m[2025-07-01 20:59:45,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 442368. Throughput: 0: 291.6. Samples: 455888. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 20:59:45,990][166323] Avg episode reward: [(0, '23.520')]
[36m[2025-07-01 20:59:50,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 442368. Throughput: 0: 297.5. Samples: 456848. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 20:59:50,948][166323] Avg episode reward: [(0, '18.224')]
[36m[2025-07-01 20:59:55,768][166323] Early stopping after 3 epochs (24 sgd steps), loss delta 0.0000008
[36m[2025-07-01 20:59:55,959][166323] Fps is (10 sec: 1643.4, 60 sec: 546.4, 300 sec: 333.2). Total num frames: 458752. Throughput: 0: 293.5. Samples: 458464. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 20:59:55,960][166323] Avg episode reward: [(0, '13.969')]
[36m[2025-07-01 21:00:00,978][166323] Fps is (10 sec: 1633.4, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 458752. Throughput: 0: 292.8. Samples: 460176. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 21:00:00,979][166323] Avg episode reward: [(0, '14.342')]
[36m[2025-07-01 21:00:05,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 458752. Throughput: 0: 295.1. Samples: 461168. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 21:00:05,951][166323] Avg episode reward: [(0, '25.585')]
[31m[1598371 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1598371 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[1598371 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:00:10,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 458752. Throughput: 0: 296.2. Samples: 462992. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 21:00:10,943][166323] Avg episode reward: [(0, '30.804')]
[36m[2025-07-01 21:00:15,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 458752. Throughput: 0: 296.9. Samples: 464816. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 21:00:15,975][166323] Avg episode reward: [(0, '32.393')]
[36m[2025-07-01 21:00:20,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 458752. Throughput: 0: 295.3. Samples: 465600. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 21:00:20,958][166323] Avg episode reward: [(0, '32.160')]
[36m[2025-07-01 21:00:25,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 458752. Throughput: 0: 296.4. Samples: 467376. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 21:00:25,965][166323] Avg episode reward: [(0, '34.927')]
[36m[2025-07-01 21:00:30,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 458752. Throughput: 0: 293.1. Samples: 469072. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 21:00:30,969][166323] Avg episode reward: [(0, '48.421')]
[36m[2025-07-01 21:00:35,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 458752. Throughput: 0: 290.7. Samples: 469936. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 21:00:35,964][166323] Avg episode reward: [(0, '46.207')]
[36m[2025-07-01 21:00:40,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 458752. Throughput: 0: 295.1. Samples: 471744. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 21:00:40,954][166323] Avg episode reward: [(0, '49.823')]
[36m[2025-07-01 21:00:45,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 458752. Throughput: 0: 298.5. Samples: 473600. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 21:00:45,951][166323] Avg episode reward: [(0, '48.564')]
[36m[2025-07-01 21:00:50,972][166323] Fps is (10 sec: 1635.4, 60 sec: 545.9, 300 sec: 333.2). Total num frames: 475136. Throughput: 0: 297.1. Samples: 474544. Policy #0 lag: (min: 0.0, avg: 0.2, max: 24.0)
[36m[2025-07-01 21:00:50,972][166323] Avg episode reward: [(0, '57.935')]
[36m[2025-07-01 21:00:55,988][166323] Fps is (10 sec: 1632.4, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 475136. Throughput: 0: 294.1. Samples: 476240. Policy #0 lag: (min: 0.0, avg: 0.2, max: 24.0)
[36m[2025-07-01 21:00:55,988][166323] Avg episode reward: [(0, '46.181')]
[36m[2025-07-01 21:01:00,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 475136. Throughput: 0: 292.6. Samples: 477984. Policy #0 lag: (min: 0.0, avg: 0.2, max: 24.0)
[36m[2025-07-01 21:01:00,980][166323] Avg episode reward: [(0, '32.469')]
[31m[1653151 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1653152 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[1653152 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:01:05,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 475136. Throughput: 0: 294.5. Samples: 478864. Policy #0 lag: (min: 0.0, avg: 0.2, max: 24.0)
[36m[2025-07-01 21:01:05,990][166323] Avg episode reward: [(0, '43.547')]
[31m[1656942 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1656942 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[1656942 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:01:10,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 475136. Throughput: 0: 292.2. Samples: 480528. Policy #0 lag: (min: 0.0, avg: 0.2, max: 24.0)
[36m[2025-07-01 21:01:10,968][166323] Avg episode reward: [(0, '31.096')]
[36m[2025-07-01 21:01:15,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 475136. Throughput: 0: 292.6. Samples: 482240. Policy #0 lag: (min: 0.0, avg: 0.2, max: 24.0)
[36m[2025-07-01 21:01:15,971][166323] Avg episode reward: [(0, '18.067')]
[36m[2025-07-01 21:01:20,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 475136. Throughput: 0: 293.6. Samples: 483152. Policy #0 lag: (min: 0.0, avg: 0.2, max: 24.0)
[36m[2025-07-01 21:01:20,983][166323] Avg episode reward: [(0, '24.624')]
[36m[2025-07-01 21:01:25,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 475136. Throughput: 0: 295.1. Samples: 485024. Policy #0 lag: (min: 0.0, avg: 0.2, max: 24.0)
[36m[2025-07-01 21:01:25,953][166323] Avg episode reward: [(0, '35.327')]
[36m[2025-07-01 21:01:30,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 475136. Throughput: 0: 293.9. Samples: 486832. Policy #0 lag: (min: 0.0, avg: 0.2, max: 24.0)
[36m[2025-07-01 21:01:30,976][166323] Avg episode reward: [(0, '34.546')]
[31m[1684396 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1684396 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[1684396 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:01:35,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 475136. Throughput: 0: 294.0. Samples: 487776. Policy #0 lag: (min: 0.0, avg: 0.2, max: 24.0)
[36m[2025-07-01 21:01:35,976][166323] Avg episode reward: [(0, '31.467')]
[37m[1m[2025-07-01 21:01:36,025][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000000920_475136.pth...
[36m[2025-07-01 21:01:36,029][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000000800_409600.pth
[36m[2025-07-01 21:01:40,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 475136. Throughput: 0: 295.9. Samples: 489552. Policy #0 lag: (min: 0.0, avg: 0.2, max: 24.0)
[36m[2025-07-01 21:01:40,971][166323] Avg episode reward: [(0, '46.082')]
[36m[2025-07-01 21:01:45,981][166323] Fps is (10 sec: 1637.6, 60 sec: 545.9, 300 sec: 333.2). Total num frames: 491520. Throughput: 0: 299.4. Samples: 491456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:01:45,981][166323] Avg episode reward: [(0, '50.021')]
[36m[2025-07-01 21:01:50,958][166323] Fps is (10 sec: 1640.5, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 491520. Throughput: 0: 300.7. Samples: 492384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:01:50,959][166323] Avg episode reward: [(0, '65.182')]
[36m[2025-07-01 21:01:56,011][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 491520. Throughput: 0: 300.9. Samples: 494080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:01:56,011][166323] Avg episode reward: [(0, '60.963')]
[31m[1709351 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1709351 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[1709352 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:02:00,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 491520. Throughput: 0: 301.8. Samples: 495824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:02:00,977][166323] Avg episode reward: [(0, '66.020')]
[36m[2025-07-01 21:02:05,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 491520. Throughput: 0: 299.0. Samples: 496608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:02:05,985][166323] Avg episode reward: [(0, '43.483')]
[36m[2025-07-01 21:02:10,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 491520. Throughput: 0: 296.3. Samples: 498368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:02:10,987][166323] Avg episode reward: [(0, '54.630')]
[36m[2025-07-01 21:02:15,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 491520. Throughput: 0: 294.7. Samples: 500096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:02:15,983][166323] Avg episode reward: [(0, '62.116')]
[36m[2025-07-01 21:02:20,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 491520. Throughput: 0: 295.8. Samples: 501088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:02:20,979][166323] Avg episode reward: [(0, '54.941')]
[36m[2025-07-01 21:02:25,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 491520. Throughput: 0: 296.6. Samples: 502896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:02:25,968][166323] Avg episode reward: [(0, '57.796')]
[36m[2025-07-01 21:02:30,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 491520. Throughput: 0: 294.3. Samples: 504688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:02:30,949][166323] Avg episode reward: [(0, '56.452')]
[36m[2025-07-01 21:02:35,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 491520. Throughput: 0: 293.3. Samples: 505584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:02:35,966][166323] Avg episode reward: [(0, '67.635')]
[36m[2025-07-01 21:02:40,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 491520. Throughput: 0: 292.8. Samples: 507248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:02:40,985][166323] Avg episode reward: [(0, '55.566')]
[36m[2025-07-01 21:02:46,000][166323] Fps is (10 sec: 1632.8, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 507904. Throughput: 0: 291.8. Samples: 508960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:02:46,001][166323] Avg episode reward: [(0, '54.374')]
[36m[2025-07-01 21:02:50,987][166323] Fps is (10 sec: 1638.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 507904. Throughput: 0: 293.7. Samples: 509824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:02:50,988][166323] Avg episode reward: [(0, '67.476')]
[36m[2025-07-01 21:02:55,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 333.2). Total num frames: 507904. Throughput: 0: 293.1. Samples: 511552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:02:55,966][166323] Avg episode reward: [(0, '58.069')]
[36m[2025-07-01 21:03:00,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 507904. Throughput: 0: 295.1. Samples: 513376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:03:00,990][166323] Avg episode reward: [(0, '55.457')]
[36m[2025-07-01 21:03:05,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 507904. Throughput: 0: 291.3. Samples: 514192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:03:05,957][166323] Avg episode reward: [(0, '70.601')]
[36m[2025-07-01 21:03:10,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 507904. Throughput: 0: 293.1. Samples: 516080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:03:10,948][166323] Avg episode reward: [(0, '69.051')]
[36m[2025-07-01 21:03:15,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 507904. Throughput: 0: 292.9. Samples: 517872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:03:15,955][166323] Avg episode reward: [(0, '84.449')]
[36m[2025-07-01 21:03:20,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 507904. Throughput: 0: 294.8. Samples: 518848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:03:20,959][166323] Avg episode reward: [(0, '82.647')]
[36m[2025-07-01 21:03:25,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 507904. Throughput: 0: 297.9. Samples: 520640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:03:25,946][166323] Avg episode reward: [(0, '93.483')]
[31m[1798043 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1798043 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[1798043 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:03:30,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 507904. Throughput: 0: 299.1. Samples: 522416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:03:30,985][166323] Avg episode reward: [(0, '111.845')]
[36m[2025-07-01 21:03:35,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 507904. Throughput: 0: 299.2. Samples: 523280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:03:35,965][166323] Avg episode reward: [(0, '98.835')]
[37m[1m[2025-07-01 21:03:36,029][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000000984_507904.pth...
[36m[2025-07-01 21:03:36,033][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000000864_442368.pth
[33m[1804955 ms][navigation_task] - WARNING : Curriculum Level: 36, Curriculum progress fraction: 0.0 (navigation_task.py:262)
[33m[1804955 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.24609375
[33mCrash Rate: 0.30517578125
[33mTimeout Rate: 0.44873046875 (navigation_task.py:265)
[33m[1804955 ms][navigation_task] - WARNING : 
[33mSuccesses: 504
[33mCrashes : 625
[33mTimeouts: 919 (navigation_task.py:268)
[36m[2025-07-01 21:03:40,976][166323] Fps is (10 sec: 1639.8, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 524288. Throughput: 0: 298.6. Samples: 524992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:03:40,976][166323] Avg episode reward: [(0, '99.682')]
[31m[1810277 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1810278 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[1810278 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[1811445 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1811445 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[1811445 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:03:45,944][166323] Fps is (10 sec: 1641.7, 60 sec: 273.3, 300 sec: 333.2). Total num frames: 524288. Throughput: 0: 298.3. Samples: 526784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:03:45,944][166323] Avg episode reward: [(0, '79.926')]
[36m[2025-07-01 21:03:50,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 524288. Throughput: 0: 298.8. Samples: 527648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:03:50,994][166323] Avg episode reward: [(0, '52.019')]
[36m[2025-07-01 21:03:55,998][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 524288. Throughput: 0: 293.7. Samples: 529312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:03:55,998][166323] Avg episode reward: [(0, '35.072')]
[36m[2025-07-01 21:04:00,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 524288. Throughput: 0: 290.4. Samples: 530944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:04:00,970][166323] Avg episode reward: [(0, '26.419')]
[36m[2025-07-01 21:04:05,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 524288. Throughput: 0: 286.0. Samples: 531728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:04:05,996][166323] Avg episode reward: [(0, '28.773')]
[36m[2025-07-01 21:04:10,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 524288. Throughput: 0: 287.5. Samples: 533584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:04:10,970][166323] Avg episode reward: [(0, '30.635')]
[36m[2025-07-01 21:04:15,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 524288. Throughput: 0: 288.1. Samples: 535376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:04:15,963][166323] Avg episode reward: [(0, '31.690')]
[36m[2025-07-01 21:04:20,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 524288. Throughput: 0: 289.6. Samples: 536320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:04:20,994][166323] Avg episode reward: [(0, '55.741')]
[31m[1851561 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1851562 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[1851562 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:04:25,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 524288. Throughput: 0: 290.1. Samples: 538048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:04:25,987][166323] Avg episode reward: [(0, '47.656')]
[36m[2025-07-01 21:04:30,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 524288. Throughput: 0: 290.9. Samples: 539888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:04:30,984][166323] Avg episode reward: [(0, '56.778')]
[36m[2025-07-01 21:04:35,970][166323] Fps is (10 sec: 1641.1, 60 sec: 546.1, 300 sec: 333.3). Total num frames: 540672. Throughput: 0: 290.3. Samples: 540704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:04:35,971][166323] Avg episode reward: [(0, '60.736')]
[36m[2025-07-01 21:04:40,994][166323] Fps is (10 sec: 1636.7, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 540672. Throughput: 0: 292.6. Samples: 542480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:04:40,994][166323] Avg episode reward: [(0, '59.457')]
[36m[2025-07-01 21:04:45,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 540672. Throughput: 0: 292.3. Samples: 544096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:04:45,967][166323] Avg episode reward: [(0, '67.968')]
[36m[2025-07-01 21:04:50,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 540672. Throughput: 0: 293.0. Samples: 544912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:04:50,985][166323] Avg episode reward: [(0, '77.934')]
[36m[2025-07-01 21:04:55,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 540672. Throughput: 0: 293.3. Samples: 546784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:04:55,979][166323] Avg episode reward: [(0, '82.539')]
[36m[2025-07-01 21:05:00,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 540672. Throughput: 0: 290.1. Samples: 548432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:05:00,968][166323] Avg episode reward: [(0, '88.259')]
[31m[1892556 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1892556 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[1892556 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:05:05,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 540672. Throughput: 0: 288.6. Samples: 549296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:05:05,951][166323] Avg episode reward: [(0, '90.891')]
[36m[2025-07-01 21:05:10,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 540672. Throughput: 0: 286.5. Samples: 550928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:05:10,943][166323] Avg episode reward: [(0, '88.691')]
[36m[2025-07-01 21:05:15,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 540672. Throughput: 0: 287.2. Samples: 552800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:05:15,944][166323] Avg episode reward: [(0, '94.700')]
[36m[2025-07-01 21:05:20,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 540672. Throughput: 0: 288.7. Samples: 553696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:05:20,967][166323] Avg episode reward: [(0, '82.489')]
[36m[2025-07-01 21:05:25,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 540672. Throughput: 0: 290.7. Samples: 555552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:05:25,954][166323] Avg episode reward: [(0, '79.648')]
[36m[2025-07-01 21:05:30,952][166323] Fps is (10 sec: 1640.7, 60 sec: 546.4, 300 sec: 333.2). Total num frames: 557056. Throughput: 0: 294.5. Samples: 557344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:05:30,953][166323] Avg episode reward: [(0, '73.617')]
[36m[2025-07-01 21:05:36,000][166323] Fps is (10 sec: 1631.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 557056. Throughput: 0: 296.1. Samples: 558240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:05:36,000][166323] Avg episode reward: [(0, '73.668')]
[37m[1m[2025-07-01 21:05:36,051][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000001080_557056.pth...
[36m[2025-07-01 21:05:36,055][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000000920_475136.pth
[36m[2025-07-01 21:05:40,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 557056. Throughput: 0: 293.1. Samples: 559968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:05:40,964][166323] Avg episode reward: [(0, '67.543')]
[36m[2025-07-01 21:05:45,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 557056. Throughput: 0: 298.4. Samples: 561856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:05:45,956][166323] Avg episode reward: [(0, '62.672')]
[36m[2025-07-01 21:05:50,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 557056. Throughput: 0: 299.3. Samples: 562768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:05:50,955][166323] Avg episode reward: [(0, '57.228')]
[36m[2025-07-01 21:05:55,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 557056. Throughput: 0: 303.1. Samples: 564576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:05:55,974][166323] Avg episode reward: [(0, '58.742')]
[31m[1948048 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1948049 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[1948049 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:06:00,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 557056. Throughput: 0: 300.7. Samples: 566336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:06:00,962][166323] Avg episode reward: [(0, '64.023')]
[36m[2025-07-01 21:06:05,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 557056. Throughput: 0: 300.6. Samples: 567216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:06:05,945][166323] Avg episode reward: [(0, '68.500')]
[36m[2025-07-01 21:06:10,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 557056. Throughput: 0: 298.3. Samples: 568976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:06:10,956][166323] Avg episode reward: [(0, '77.399')]
[36m[2025-07-01 21:06:15,977][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 557056. Throughput: 0: 297.4. Samples: 570736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:06:15,977][166323] Avg episode reward: [(0, '79.872')]
[36m[2025-07-01 21:06:21,005][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 557056. Throughput: 0: 297.6. Samples: 571632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:06:21,005][166323] Avg episode reward: [(0, '98.964')]
[36m[2025-07-01 21:06:25,961][166323] Fps is (10 sec: 1641.0, 60 sec: 546.1, 300 sec: 333.3). Total num frames: 573440. Throughput: 0: 296.6. Samples: 573312. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 21:06:25,961][166323] Avg episode reward: [(0, '98.708')]
[36m[2025-07-01 21:06:30,975][166323] Fps is (10 sec: 1643.4, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 573440. Throughput: 0: 290.7. Samples: 574944. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 21:06:30,975][166323] Avg episode reward: [(0, '119.375')]
[36m[2025-07-01 21:06:35,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 573440. Throughput: 0: 288.1. Samples: 575744. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 21:06:35,996][166323] Avg episode reward: [(0, '107.155')]
[36m[2025-07-01 21:06:40,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 573440. Throughput: 0: 287.8. Samples: 577520. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 21:06:40,949][166323] Avg episode reward: [(0, '91.761')]
[36m[2025-07-01 21:06:45,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 573440. Throughput: 0: 287.1. Samples: 579264. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 21:06:45,996][166323] Avg episode reward: [(0, '98.624')]
[36m[2025-07-01 21:06:50,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 573440. Throughput: 0: 285.8. Samples: 580080. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 21:06:50,958][166323] Avg episode reward: [(0, '103.259')]
[36m[2025-07-01 21:06:55,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 573440. Throughput: 0: 286.1. Samples: 581856. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 21:06:55,977][166323] Avg episode reward: [(0, '102.797')]
[31m[2007229 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2007229 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[2007229 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:07:00,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 573440. Throughput: 0: 289.4. Samples: 583760. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 21:07:00,975][166323] Avg episode reward: [(0, '81.985')]
[36m[2025-07-01 21:07:05,978][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 573440. Throughput: 0: 291.7. Samples: 584752. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 21:07:05,979][166323] Avg episode reward: [(0, '91.797')]
[36m[2025-07-01 21:07:10,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 573440. Throughput: 0: 291.9. Samples: 586448. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 21:07:10,958][166323] Avg episode reward: [(0, '103.843')]
[36m[2025-07-01 21:07:15,997][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 573440. Throughput: 0: 296.4. Samples: 588288. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 21:07:15,998][166323] Avg episode reward: [(0, '86.646')]
[36m[2025-07-01 21:07:21,130][166323] Fps is (10 sec: 1610.8, 60 sec: 545.0, 300 sec: 333.1). Total num frames: 589824. Throughput: 0: 297.4. Samples: 589168. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 21:07:21,130][166323] Avg episode reward: [(0, '88.186')]
[36m[2025-07-01 21:07:25,980][166323] Fps is (10 sec: 1641.2, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 589824. Throughput: 0: 294.6. Samples: 590784. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 21:07:25,980][166323] Avg episode reward: [(0, '88.334')]
[31m[2036216 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2036217 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[2036217 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:07:30,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 589824. Throughput: 0: 294.0. Samples: 592480. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 21:07:30,949][166323] Avg episode reward: [(0, '116.189')]
[36m[2025-07-01 21:07:35,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 589824. Throughput: 0: 294.2. Samples: 593328. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 21:07:35,985][166323] Avg episode reward: [(0, '113.360')]
[37m[1m[2025-07-01 21:07:36,063][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000001144_589824.pth...
[36m[2025-07-01 21:07:36,067][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000000984_507904.pth
[36m[2025-07-01 21:07:40,997][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 589824. Throughput: 0: 293.6. Samples: 595072. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 21:07:40,997][166323] Avg episode reward: [(0, '112.289')]
[36m[2025-07-01 21:07:45,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 589824. Throughput: 0: 291.5. Samples: 596880. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 21:07:45,977][166323] Avg episode reward: [(0, '111.182')]
[36m[2025-07-01 21:07:50,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 589824. Throughput: 0: 290.6. Samples: 597824. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 21:07:50,954][166323] Avg episode reward: [(0, '98.092')]
[31m[2064385 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2064385 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[2064385 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:07:55,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 589824. Throughput: 0: 293.3. Samples: 599648. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 21:07:55,968][166323] Avg episode reward: [(0, '92.901')]
[36m[2025-07-01 21:08:00,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 589824. Throughput: 0: 293.9. Samples: 601504. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 21:08:00,969][166323] Avg episode reward: [(0, '87.328')]
[36m[2025-07-01 21:08:05,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 589824. Throughput: 0: 293.6. Samples: 602336. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 21:08:05,986][166323] Avg episode reward: [(0, '85.386')]
[36m[2025-07-01 21:08:10,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 589824. Throughput: 0: 296.9. Samples: 604144. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 21:08:10,977][166323] Avg episode reward: [(0, '82.861')]
[36m[2025-07-01 21:08:15,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 589824. Throughput: 0: 297.3. Samples: 605872. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 21:08:15,988][166323] Avg episode reward: [(0, '83.379')]
[36m[2025-07-01 21:08:20,980][166323] Fps is (10 sec: 1638.0, 60 sec: 273.8, 300 sec: 333.2). Total num frames: 606208. Throughput: 0: 296.6. Samples: 606672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:08:20,980][166323] Avg episode reward: [(0, '91.660')]
[36m[2025-07-01 21:08:25,949][166323] Fps is (10 sec: 1644.8, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 606208. Throughput: 0: 298.6. Samples: 608496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:08:25,949][166323] Avg episode reward: [(0, '92.929')]
[36m[2025-07-01 21:08:30,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 606208. Throughput: 0: 299.3. Samples: 610352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:08:30,996][166323] Avg episode reward: [(0, '105.173')]
[36m[2025-07-01 21:08:36,000][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 606208. Throughput: 0: 296.6. Samples: 611184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:08:36,000][166323] Avg episode reward: [(0, '107.745')]
[36m[2025-07-01 21:08:40,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 606208. Throughput: 0: 294.8. Samples: 612912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:08:40,955][166323] Avg episode reward: [(0, '113.656')]
[36m[2025-07-01 21:08:45,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 606208. Throughput: 0: 295.5. Samples: 614800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:08:45,970][166323] Avg episode reward: [(0, '116.974')]
[36m[2025-07-01 21:08:50,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 606208. Throughput: 0: 298.2. Samples: 615744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:08:50,956][166323] Avg episode reward: [(0, '142.340')]
[31m[2120241 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2120241 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[2120241 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:08:55,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 606208. Throughput: 0: 299.6. Samples: 617632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:08:55,995][166323] Avg episode reward: [(0, '111.424')]
[36m[2025-07-01 21:09:00,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 606208. Throughput: 0: 300.0. Samples: 619360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:09:00,954][166323] Avg episode reward: [(0, '120.655')]
[36m[2025-07-01 21:09:05,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 606208. Throughput: 0: 302.8. Samples: 620304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:09:05,996][166323] Avg episode reward: [(0, '117.033')]
[31m[2138217 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2138217 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[2138217 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:09:11,009][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 606208. Throughput: 0: 301.8. Samples: 622096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:09:11,009][166323] Avg episode reward: [(0, '119.079')]
[36m[2025-07-01 21:09:15,968][166323] Fps is (10 sec: 1643.0, 60 sec: 546.3, 300 sec: 333.3). Total num frames: 622592. Throughput: 0: 299.2. Samples: 623808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:09:15,968][166323] Avg episode reward: [(0, '106.296')]
[36m[2025-07-01 21:09:20,964][166323] Fps is (10 sec: 1645.7, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 622592. Throughput: 0: 300.0. Samples: 624672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:09:20,964][166323] Avg episode reward: [(0, '109.059')]
[36m[2025-07-01 21:09:25,979][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 622592. Throughput: 0: 302.8. Samples: 626544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:09:25,980][166323] Avg episode reward: [(0, '119.459')]
[31m[2156562 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2156563 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[2156564 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:09:30,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 622592. Throughput: 0: 298.2. Samples: 628224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:09:30,985][166323] Avg episode reward: [(0, '106.510')]
[36m[2025-07-01 21:09:35,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 622592. Throughput: 0: 296.8. Samples: 629104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:09:35,971][166323] Avg episode reward: [(0, '105.931')]
[37m[1m[2025-07-01 21:09:36,043][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000001208_622592.pth...
[36m[2025-07-01 21:09:36,047][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000001080_557056.pth
[36m[2025-07-01 21:09:40,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 622592. Throughput: 0: 293.6. Samples: 630832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:09:40,954][166323] Avg episode reward: [(0, '111.055')]
[31m[2173337 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2173337 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[2173337 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:09:45,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 622592. Throughput: 0: 295.2. Samples: 632640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:09:45,945][166323] Avg episode reward: [(0, '109.612')]
[36m[2025-07-01 21:09:50,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 622592. Throughput: 0: 293.5. Samples: 633504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:09:50,973][166323] Avg episode reward: [(0, '100.676')]
[36m[2025-07-01 21:09:55,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 622592. Throughput: 0: 295.4. Samples: 635376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:09:55,973][166323] Avg episode reward: [(0, '109.085')]
[31m[2187843 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2187843 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[2187843 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:10:00,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 622592. Throughput: 0: 294.6. Samples: 637072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:10:00,991][166323] Avg episode reward: [(0, '118.861')]
[36m[2025-07-01 21:10:05,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 622592. Throughput: 0: 294.1. Samples: 637904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:10:05,958][166323] Avg episode reward: [(0, '122.007')]
[36m[2025-07-01 21:10:10,973][166323] Fps is (10 sec: 1641.3, 60 sec: 546.5, 300 sec: 333.2). Total num frames: 638976. Throughput: 0: 288.4. Samples: 639520. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 21:10:10,973][166323] Avg episode reward: [(0, '112.975')]
[31m[2202907 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2202907 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[2202907 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:10:15,945][166323] Fps is (10 sec: 1640.5, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 638976. Throughput: 0: 290.4. Samples: 641280. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 21:10:15,945][166323] Avg episode reward: [(0, '132.510')]
[36m[2025-07-01 21:10:20,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 638976. Throughput: 0: 290.2. Samples: 642160. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 21:10:20,960][166323] Avg episode reward: [(0, '122.604')]
[36m[2025-07-01 21:10:25,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 638976. Throughput: 0: 288.5. Samples: 643824. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 21:10:25,993][166323] Avg episode reward: [(0, '128.181')]
[36m[2025-07-01 21:10:30,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 638976. Throughput: 0: 285.7. Samples: 645504. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 21:10:30,970][166323] Avg episode reward: [(0, '123.589')]
[36m[2025-07-01 21:10:35,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 638976. Throughput: 0: 286.0. Samples: 646368. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 21:10:35,955][166323] Avg episode reward: [(0, '138.413')]
[36m[2025-07-01 21:10:40,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 638976. Throughput: 0: 282.3. Samples: 648080. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 21:10:40,977][166323] Avg episode reward: [(0, '155.217')]
[36m[2025-07-01 21:10:45,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 638976. Throughput: 0: 284.3. Samples: 649856. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 21:10:45,956][166323] Avg episode reward: [(0, '142.360')]
[36m[2025-07-01 21:10:50,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 638976. Throughput: 0: 284.3. Samples: 650704. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 21:10:50,977][166323] Avg episode reward: [(0, '131.661')]
[36m[2025-07-01 21:10:55,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 638976. Throughput: 0: 287.7. Samples: 652464. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 21:10:55,968][166323] Avg episode reward: [(0, '142.386')]
[36m[2025-07-01 21:11:00,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 638976. Throughput: 0: 285.3. Samples: 654128. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 21:11:00,975][166323] Avg episode reward: [(0, '134.228')]
[36m[2025-07-01 21:11:05,946][166323] Fps is (10 sec: 1641.9, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 655360. Throughput: 0: 283.8. Samples: 654928. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 21:11:05,946][166323] Avg episode reward: [(0, '112.810')]
[36m[2025-07-01 21:11:10,949][166323] Fps is (10 sec: 1642.7, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 655360. Throughput: 0: 286.1. Samples: 656688. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 21:11:10,949][166323] Avg episode reward: [(0, '115.864')]
[36m[2025-07-01 21:11:15,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.3). Total num frames: 655360. Throughput: 0: 289.4. Samples: 658528. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 21:11:15,969][166323] Avg episode reward: [(0, '121.033')]
[36m[2025-07-01 21:11:20,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 655360. Throughput: 0: 288.4. Samples: 659344. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 21:11:20,955][166323] Avg episode reward: [(0, '114.006')]
[36m[2025-07-01 21:11:25,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 655360. Throughput: 0: 289.9. Samples: 661120. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 21:11:25,958][166323] Avg episode reward: [(0, '95.495')]
[36m[2025-07-01 21:11:30,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 655360. Throughput: 0: 288.3. Samples: 662832. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 21:11:30,971][166323] Avg episode reward: [(0, '127.262')]
[36m[2025-07-01 21:11:35,997][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 655360. Throughput: 0: 288.6. Samples: 663696. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 21:11:35,997][166323] Avg episode reward: [(0, '131.577')]
[37m[1m[2025-07-01 21:11:36,055][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000001272_655360.pth...
[36m[2025-07-01 21:11:36,059][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000001144_589824.pth
[36m[2025-07-01 21:11:40,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 655360. Throughput: 0: 285.6. Samples: 665312. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 21:11:40,960][166323] Avg episode reward: [(0, '122.245')]
[36m[2025-07-01 21:11:46,003][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 655360. Throughput: 0: 287.1. Samples: 667056. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 21:11:46,003][166323] Avg episode reward: [(0, '139.490')]
[36m[2025-07-01 21:11:50,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 655360. Throughput: 0: 288.7. Samples: 667920. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 21:11:50,950][166323] Avg episode reward: [(0, '135.833')]
[36m[2025-07-01 21:11:55,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 655360. Throughput: 0: 290.5. Samples: 669760. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 21:11:55,946][166323] Avg episode reward: [(0, '138.725')]
[36m[2025-07-01 21:12:00,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 655360. Throughput: 0: 290.0. Samples: 671584. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 21:12:00,988][166323] Avg episode reward: [(0, '118.133')]
[36m[2025-07-01 21:12:05,981][166323] Fps is (10 sec: 1632.6, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 671744. Throughput: 0: 290.3. Samples: 672416. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 21:12:05,982][166323] Avg episode reward: [(0, '114.193')]
[36m[2025-07-01 21:12:10,968][166323] Fps is (10 sec: 1641.8, 60 sec: 273.0, 300 sec: 333.3). Total num frames: 671744. Throughput: 0: 294.3. Samples: 674368. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 21:12:10,968][166323] Avg episode reward: [(0, '118.251')]
[36m[2025-07-01 21:12:15,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.8). Total num frames: 671744. Throughput: 0: 295.4. Samples: 676128. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 21:12:15,984][166323] Avg episode reward: [(0, '134.061')]
[36m[2025-07-01 21:12:20,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 671744. Throughput: 0: 295.2. Samples: 676976. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 21:12:20,976][166323] Avg episode reward: [(0, '148.483')]
[31m[2333496 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2333496 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[2333496 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:12:25,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 671744. Throughput: 0: 297.9. Samples: 678720. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 21:12:25,975][166323] Avg episode reward: [(0, '131.813')]
[36m[2025-07-01 21:12:30,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 671744. Throughput: 0: 299.5. Samples: 680528. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 21:12:30,981][166323] Avg episode reward: [(0, '147.764')]
[36m[2025-07-01 21:12:35,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 671744. Throughput: 0: 302.4. Samples: 681536. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 21:12:35,972][166323] Avg episode reward: [(0, '120.302')]
[36m[2025-07-01 21:12:41,006][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 671744. Throughput: 0: 299.7. Samples: 683264. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 21:12:41,007][166323] Avg episode reward: [(0, '123.083')]
[36m[2025-07-01 21:12:45,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 671744. Throughput: 0: 298.4. Samples: 685008. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 21:12:45,975][166323] Avg episode reward: [(0, '107.860')]
[36m[2025-07-01 21:12:50,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 671744. Throughput: 0: 299.9. Samples: 685904. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 21:12:50,956][166323] Avg episode reward: [(0, '111.276')]
[36m[2025-07-01 21:12:55,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 671744. Throughput: 0: 295.1. Samples: 687648. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 21:12:55,971][166323] Avg episode reward: [(0, '110.512')]
[36m[2025-07-01 21:13:00,964][166323] Fps is (10 sec: 1637.1, 60 sec: 546.4, 300 sec: 333.3). Total num frames: 688128. Throughput: 0: 293.5. Samples: 689328. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-01 21:13:00,964][166323] Avg episode reward: [(0, '120.917')]
[36m[2025-07-01 21:13:05,946][166323] Fps is (10 sec: 1642.5, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 688128. Throughput: 0: 292.8. Samples: 690144. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-01 21:13:05,946][166323] Avg episode reward: [(0, '147.528')]
[36m[2025-07-01 21:13:10,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 688128. Throughput: 0: 292.1. Samples: 691856. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-01 21:13:10,953][166323] Avg episode reward: [(0, '155.510')]
[36m[2025-07-01 21:13:15,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 688128. Throughput: 0: 291.2. Samples: 693632. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-01 21:13:15,988][166323] Avg episode reward: [(0, '146.400')]
[36m[2025-07-01 21:13:20,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 688128. Throughput: 0: 289.0. Samples: 694544. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-01 21:13:20,983][166323] Avg episode reward: [(0, '136.712')]
[36m[2025-07-01 21:13:25,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 688128. Throughput: 0: 293.0. Samples: 696432. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-01 21:13:25,944][166323] Avg episode reward: [(0, '130.989')]
[36m[2025-07-01 21:13:30,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 688128. Throughput: 0: 293.5. Samples: 698224. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-01 21:13:30,998][166323] Avg episode reward: [(0, '123.749')]
[31m[2401694 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2401694 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[2401694 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[33m[2402735 ms][navigation_task] - WARNING : Curriculum Level: 36, Curriculum progress fraction: 0.0 (navigation_task.py:262)
[33m[2402735 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.5078125
[33mCrash Rate: 0.31640625
[33mTimeout Rate: 0.17578125 (navigation_task.py:265)
[33m[2402735 ms][navigation_task] - WARNING : 
[33mSuccesses: 1040
[33mCrashes : 648
[33mTimeouts: 360 (navigation_task.py:268)
[36m[2025-07-01 21:13:35,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 688128. Throughput: 0: 293.3. Samples: 699104. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-01 21:13:35,954][166323] Avg episode reward: [(0, '107.416')]
[37m[1m[2025-07-01 21:13:36,006][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000001336_688128.pth...
[36m[2025-07-01 21:13:36,011][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000001208_622592.pth
[36m[2025-07-01 21:13:40,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 688128. Throughput: 0: 291.1. Samples: 700752. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-01 21:13:40,979][166323] Avg episode reward: [(0, '124.691')]
[36m[2025-07-01 21:13:45,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 688128. Throughput: 0: 294.3. Samples: 702576. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-01 21:13:45,981][166323] Avg episode reward: [(0, '124.982')]
[36m[2025-07-01 21:13:50,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 688128. Throughput: 0: 296.2. Samples: 703488. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-01 21:13:50,994][166323] Avg episode reward: [(0, '137.079')]
[36m[2025-07-01 21:13:55,966][166323] Fps is (10 sec: 1640.8, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 704512. Throughput: 0: 297.5. Samples: 705248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:13:55,966][166323] Avg episode reward: [(0, '144.023')]
[36m[2025-07-01 21:14:00,962][166323] Fps is (10 sec: 1643.7, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 704512. Throughput: 0: 298.5. Samples: 707056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:14:00,962][166323] Avg episode reward: [(0, '150.431')]
[36m[2025-07-01 21:14:05,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.3). Total num frames: 704512. Throughput: 0: 298.8. Samples: 707984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:14:05,959][166323] Avg episode reward: [(0, '169.687')]
[36m[2025-07-01 21:14:10,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 704512. Throughput: 0: 295.1. Samples: 709712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:14:10,948][166323] Avg episode reward: [(0, '149.910')]
[36m[2025-07-01 21:14:15,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 704512. Throughput: 0: 293.6. Samples: 711424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:14:15,952][166323] Avg episode reward: [(0, '147.192')]
[36m[2025-07-01 21:14:20,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 704512. Throughput: 0: 293.7. Samples: 712320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:14:20,953][166323] Avg episode reward: [(0, '152.279')]
[36m[2025-07-01 21:14:25,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 704512. Throughput: 0: 296.3. Samples: 714080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:14:25,965][166323] Avg episode reward: [(0, '154.339')]
[36m[2025-07-01 21:14:30,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 704512. Throughput: 0: 297.1. Samples: 715936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:14:30,947][166323] Avg episode reward: [(0, '135.928')]
[36m[2025-07-01 21:14:35,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 704512. Throughput: 0: 295.0. Samples: 716752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:14:35,958][166323] Avg episode reward: [(0, '129.635')]
[36m[2025-07-01 21:14:40,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 704512. Throughput: 0: 294.8. Samples: 718512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:14:40,959][166323] Avg episode reward: [(0, '137.177')]
[36m[2025-07-01 21:14:45,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 704512. Throughput: 0: 291.9. Samples: 720192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:14:45,958][166323] Avg episode reward: [(0, '156.007')]
[36m[2025-07-01 21:14:50,958][166323] Fps is (10 sec: 1638.5, 60 sec: 546.5, 300 sec: 333.2). Total num frames: 720896. Throughput: 0: 291.6. Samples: 721104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:14:50,959][166323] Avg episode reward: [(0, '171.286')]
[36m[2025-07-01 21:14:55,987][166323] Fps is (10 sec: 1633.7, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 720896. Throughput: 0: 290.2. Samples: 722784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:14:55,987][166323] Avg episode reward: [(0, '173.753')]
[36m[2025-07-01 21:15:00,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 720896. Throughput: 0: 288.4. Samples: 724400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:15:00,944][166323] Avg episode reward: [(0, '169.391')]
[36m[2025-07-01 21:15:05,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 720896. Throughput: 0: 289.0. Samples: 725328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:15:05,968][166323] Avg episode reward: [(0, '162.555')]
[36m[2025-07-01 21:15:10,978][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 720896. Throughput: 0: 288.3. Samples: 727056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:15:10,978][166323] Avg episode reward: [(0, '175.058')]
[36m[2025-07-01 21:15:15,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 720896. Throughput: 0: 286.5. Samples: 728832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:15:15,957][166323] Avg episode reward: [(0, '149.017')]
[36m[2025-07-01 21:15:20,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 720896. Throughput: 0: 288.2. Samples: 729728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:15:20,976][166323] Avg episode reward: [(0, '145.532')]
[36m[2025-07-01 21:15:25,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 720896. Throughput: 0: 288.0. Samples: 731472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:15:25,955][166323] Avg episode reward: [(0, '146.763')]
[36m[2025-07-01 21:15:30,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 720896. Throughput: 0: 288.4. Samples: 733168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:15:30,945][166323] Avg episode reward: [(0, '148.073')]
[36m[2025-07-01 21:15:35,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 720896. Throughput: 0: 288.8. Samples: 734096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:15:35,952][166323] Avg episode reward: [(0, '146.216')]
[37m[1m[2025-07-01 21:15:36,002][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000001400_720896.pth...
[36m[2025-07-01 21:15:36,006][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000001272_655360.pth
[36m[2025-07-01 21:15:40,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 720896. Throughput: 0: 292.6. Samples: 735952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:15:40,986][166323] Avg episode reward: [(0, '151.472')]
[31m[2531584 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2531584 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[2531585 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:15:45,977][166323] Fps is (10 sec: 1634.3, 60 sec: 546.0, 300 sec: 333.2). Total num frames: 737280. Throughput: 0: 294.5. Samples: 737664. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 21:15:45,977][166323] Avg episode reward: [(0, '161.907')]
[31m[2537145 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2537145 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[2537146 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:15:50,975][166323] Fps is (10 sec: 1640.2, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 737280. Throughput: 0: 294.7. Samples: 738592. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 21:15:50,976][166323] Avg episode reward: [(0, '163.919')]
[36m[2025-07-01 21:15:55,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 737280. Throughput: 0: 295.4. Samples: 740352. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 21:15:55,991][166323] Avg episode reward: [(0, '160.411')]
[36m[2025-07-01 21:16:00,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 737280. Throughput: 0: 293.3. Samples: 742032. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 21:16:00,969][166323] Avg episode reward: [(0, '176.381')]
[31m[2549626 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2549626 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[2549627 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:16:05,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 737280. Throughput: 0: 294.6. Samples: 742976. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 21:16:05,946][166323] Avg episode reward: [(0, '157.547')]
[36m[2025-07-01 21:16:10,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 737280. Throughput: 0: 295.8. Samples: 744784. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 21:16:10,958][166323] Avg episode reward: [(0, '157.440')]
[31m[2560390 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2560390 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[2560390 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:16:15,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 737280. Throughput: 0: 296.4. Samples: 746512. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 21:16:15,964][166323] Avg episode reward: [(0, '149.354')]
[36m[2025-07-01 21:16:20,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 737280. Throughput: 0: 293.7. Samples: 747312. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 21:16:20,955][166323] Avg episode reward: [(0, '142.265')]
[36m[2025-07-01 21:16:26,007][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 737280. Throughput: 0: 292.1. Samples: 749104. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 21:16:26,007][166323] Avg episode reward: [(0, '147.501')]
[36m[2025-07-01 21:16:30,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 737280. Throughput: 0: 293.2. Samples: 750864. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 21:16:30,994][166323] Avg episode reward: [(0, '162.032')]
[36m[2025-07-01 21:16:35,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 737280. Throughput: 0: 291.5. Samples: 751712. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 21:16:35,982][166323] Avg episode reward: [(0, '185.530')]
[36m[2025-07-01 21:16:40,956][166323] Fps is (10 sec: 1644.5, 60 sec: 546.4, 300 sec: 333.3). Total num frames: 753664. Throughput: 0: 293.9. Samples: 753568. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 21:16:40,957][166323] Avg episode reward: [(0, '176.392')]
[36m[2025-07-01 21:16:46,013][166323] Fps is (10 sec: 1633.2, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 753664. Throughput: 0: 292.7. Samples: 755216. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 21:16:46,014][166323] Avg episode reward: [(0, '176.867')]
[36m[2025-07-01 21:16:50,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 753664. Throughput: 0: 290.6. Samples: 756064. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 21:16:50,983][166323] Avg episode reward: [(0, '178.507')]
[36m[2025-07-01 21:16:55,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 753664. Throughput: 0: 290.4. Samples: 757856. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 21:16:55,965][166323] Avg episode reward: [(0, '151.073')]
[31m[2605307 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2605307 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[2605307 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:17:00,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 753664. Throughput: 0: 292.5. Samples: 759680. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 21:17:00,982][166323] Avg episode reward: [(0, '129.192')]
[36m[2025-07-01 21:17:05,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 753664. Throughput: 0: 295.8. Samples: 760624. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 21:17:05,962][166323] Avg episode reward: [(0, '139.887')]
[36m[2025-07-01 21:17:10,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 753664. Throughput: 0: 295.8. Samples: 762400. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 21:17:10,964][166323] Avg episode reward: [(0, '131.593')]
[36m[2025-07-01 21:17:15,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 753664. Throughput: 0: 295.9. Samples: 764176. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 21:17:15,983][166323] Avg episode reward: [(0, '110.262')]
[36m[2025-07-01 21:17:20,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 753664. Throughput: 0: 297.3. Samples: 765088. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 21:17:20,973][166323] Avg episode reward: [(0, '118.166')]
[36m[2025-07-01 21:17:25,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 753664. Throughput: 0: 296.6. Samples: 766912. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 21:17:25,943][166323] Avg episode reward: [(0, '124.532')]
[36m[2025-07-01 21:17:30,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 753664. Throughput: 0: 302.5. Samples: 768816. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 21:17:30,971][166323] Avg episode reward: [(0, '112.042')]
[36m[2025-07-01 21:17:35,985][166323] Fps is (10 sec: 1631.4, 60 sec: 546.1, 300 sec: 333.3). Total num frames: 770048. Throughput: 0: 303.3. Samples: 769712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:17:35,986][166323] Avg episode reward: [(0, '109.554')]
[37m[1m[2025-07-01 21:17:36,059][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000001496_770048.pth...
[36m[2025-07-01 21:17:36,064][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000001336_688128.pth
[36m[2025-07-01 21:17:40,953][166323] Fps is (10 sec: 1641.3, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 770048. Throughput: 0: 300.9. Samples: 771392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:17:40,953][166323] Avg episode reward: [(0, '118.684')]
[36m[2025-07-01 21:17:45,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 770048. Throughput: 0: 298.7. Samples: 773120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:17:45,981][166323] Avg episode reward: [(0, '126.071')]
[36m[2025-07-01 21:17:50,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 770048. Throughput: 0: 296.9. Samples: 773984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:17:50,965][166323] Avg episode reward: [(0, '133.610')]
[36m[2025-07-01 21:17:55,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 770048. Throughput: 0: 295.9. Samples: 775712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:17:55,952][166323] Avg episode reward: [(0, '134.279')]
[36m[2025-07-01 21:18:00,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 770048. Throughput: 0: 297.0. Samples: 777536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:18:00,960][166323] Avg episode reward: [(0, '146.754')]
[36m[2025-07-01 21:18:05,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 770048. Throughput: 0: 297.4. Samples: 778464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:18:05,952][166323] Avg episode reward: [(0, '137.836')]
[36m[2025-07-01 21:18:10,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 770048. Throughput: 0: 297.2. Samples: 780288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:18:10,957][166323] Avg episode reward: [(0, '163.146')]
[36m[2025-07-01 21:18:15,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 770048. Throughput: 0: 292.2. Samples: 781968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:18:15,975][166323] Avg episode reward: [(0, '177.418')]
[36m[2025-07-01 21:18:21,007][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 770048. Throughput: 0: 293.6. Samples: 782928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:18:21,007][166323] Avg episode reward: [(0, '150.073')]
[36m[2025-07-01 21:18:25,979][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 770048. Throughput: 0: 294.6. Samples: 784656. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:18:25,979][166323] Avg episode reward: [(0, '170.604')]
[36m[2025-07-01 21:18:30,949][166323] Fps is (10 sec: 1647.9, 60 sec: 546.3, 300 sec: 333.2). Total num frames: 786432. Throughput: 0: 297.5. Samples: 786496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:18:30,949][166323] Avg episode reward: [(0, '182.600')]
[36m[2025-07-01 21:18:35,948][166323] Fps is (10 sec: 1643.4, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 786432. Throughput: 0: 298.1. Samples: 787392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:18:35,948][166323] Avg episode reward: [(0, '184.999')]
[36m[2025-07-01 21:18:40,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 786432. Throughput: 0: 298.4. Samples: 789152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:18:40,990][166323] Avg episode reward: [(0, '156.279')]
[36m[2025-07-01 21:18:45,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 786432. Throughput: 0: 296.0. Samples: 790864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:18:45,987][166323] Avg episode reward: [(0, '161.394')]
[36m[2025-07-01 21:18:50,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 786432. Throughput: 0: 295.8. Samples: 791776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:18:50,958][166323] Avg episode reward: [(0, '180.008')]
[36m[2025-07-01 21:18:55,983][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 786432. Throughput: 0: 295.6. Samples: 793600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:18:55,984][166323] Avg episode reward: [(0, '171.436')]
[36m[2025-07-01 21:19:00,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 786432. Throughput: 0: 298.4. Samples: 795392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:19:00,958][166323] Avg episode reward: [(0, '172.527')]
[36m[2025-07-01 21:19:05,979][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 786432. Throughput: 0: 296.7. Samples: 796272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:19:05,979][166323] Avg episode reward: [(0, '177.424')]
[36m[2025-07-01 21:19:10,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 786432. Throughput: 0: 295.4. Samples: 797952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:19:10,982][166323] Avg episode reward: [(0, '188.688')]
[36m[2025-07-01 21:19:15,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 786432. Throughput: 0: 292.1. Samples: 799648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:19:15,982][166323] Avg episode reward: [(0, '179.156')]
[36m[2025-07-01 21:19:20,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 786432. Throughput: 0: 293.0. Samples: 800576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:19:20,946][166323] Avg episode reward: [(0, '167.208')]
[36m[2025-07-01 21:19:25,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 786432. Throughput: 0: 292.1. Samples: 802288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:19:25,960][166323] Avg episode reward: [(0, '176.348')]
[36m[2025-07-01 21:19:30,977][166323] Fps is (10 sec: 1633.3, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 802816. Throughput: 0: 294.8. Samples: 804128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:19:30,977][166323] Avg episode reward: [(0, '180.022')]
[36m[2025-07-01 21:19:35,951][166323] Fps is (10 sec: 1639.8, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 802816. Throughput: 0: 295.5. Samples: 805072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:19:35,951][166323] Avg episode reward: [(0, '192.743')]
[37m[1m[2025-07-01 21:19:36,010][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000001560_802816.pth...
[36m[2025-07-01 21:19:36,014][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000001400_720896.pth
[36m[2025-07-01 21:19:40,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 802816. Throughput: 0: 294.8. Samples: 806864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:19:40,972][166323] Avg episode reward: [(0, '184.756')]
[36m[2025-07-01 21:19:45,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 802816. Throughput: 0: 293.6. Samples: 808608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:19:45,966][166323] Avg episode reward: [(0, '180.837')]
[36m[2025-07-01 21:19:50,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 802816. Throughput: 0: 294.0. Samples: 809504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:19:50,983][166323] Avg episode reward: [(0, '180.636')]
[36m[2025-07-01 21:19:55,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 802816. Throughput: 0: 294.8. Samples: 811216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:19:55,968][166323] Avg episode reward: [(0, '177.604')]
[36m[2025-07-01 21:20:00,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 802816. Throughput: 0: 294.2. Samples: 812880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:20:00,956][166323] Avg episode reward: [(0, '169.013')]
[36m[2025-07-01 21:20:05,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 802816. Throughput: 0: 293.7. Samples: 813808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:20:05,999][166323] Avg episode reward: [(0, '172.469')]
[31m[2798012 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2798012 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[2798013 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:20:10,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 802816. Throughput: 0: 296.7. Samples: 815648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:20:10,985][166323] Avg episode reward: [(0, '185.078')]
[36m[2025-07-01 21:20:15,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 802816. Throughput: 0: 293.8. Samples: 817344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:20:15,964][166323] Avg episode reward: [(0, '192.649')]
[36m[2025-07-01 21:20:20,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 802816. Throughput: 0: 293.8. Samples: 818304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:20:20,982][166323] Avg episode reward: [(0, '182.221')]
[36m[2025-07-01 21:20:25,957][166323] Fps is (10 sec: 1639.4, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 819200. Throughput: 0: 294.8. Samples: 820128. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 21:20:25,957][166323] Avg episode reward: [(0, '182.559')]
[36m[2025-07-01 21:20:30,978][166323] Fps is (10 sec: 1639.1, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 819200. Throughput: 0: 295.4. Samples: 821904. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 21:20:30,978][166323] Avg episode reward: [(0, '172.739')]
[36m[2025-07-01 21:20:35,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 819200. Throughput: 0: 295.8. Samples: 822816. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 21:20:35,987][166323] Avg episode reward: [(0, '158.137')]
[36m[2025-07-01 21:20:40,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 819200. Throughput: 0: 297.0. Samples: 824576. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 21:20:40,949][166323] Avg episode reward: [(0, '170.404')]
[36m[2025-07-01 21:20:45,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 819200. Throughput: 0: 300.2. Samples: 826384. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 21:20:45,945][166323] Avg episode reward: [(0, '173.810')]
[36m[2025-07-01 21:20:50,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 819200. Throughput: 0: 299.9. Samples: 827296. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 21:20:50,966][166323] Avg episode reward: [(0, '181.701')]
[36m[2025-07-01 21:20:55,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 819200. Throughput: 0: 298.4. Samples: 829072. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 21:20:55,974][166323] Avg episode reward: [(0, '188.631')]
[36m[2025-07-01 21:21:00,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 819200. Throughput: 0: 297.9. Samples: 830752. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 21:21:00,968][166323] Avg episode reward: [(0, '214.968')]
[36m[2025-07-01 21:21:05,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 819200. Throughput: 0: 296.0. Samples: 831616. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 21:21:05,961][166323] Avg episode reward: [(0, '207.212')]
[36m[2025-07-01 21:21:10,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 819200. Throughput: 0: 296.0. Samples: 833456. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 21:21:10,987][166323] Avg episode reward: [(0, '195.500')]
[31m[2861436 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2861436 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[2861437 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:21:15,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 819200. Throughput: 0: 298.6. Samples: 835344. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 21:21:15,994][166323] Avg episode reward: [(0, '198.132')]
[36m[2025-07-01 21:21:20,956][166323] Fps is (10 sec: 1643.5, 60 sec: 546.4, 300 sec: 333.3). Total num frames: 835584. Throughput: 0: 296.4. Samples: 836144. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:21:20,956][166323] Avg episode reward: [(0, '201.289')]
[36m[2025-07-01 21:21:25,954][166323] Fps is (10 sec: 1645.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 835584. Throughput: 0: 296.5. Samples: 837920. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:21:25,954][166323] Avg episode reward: [(0, '185.125')]
[36m[2025-07-01 21:21:30,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 835584. Throughput: 0: 293.3. Samples: 839584. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:21:30,948][166323] Avg episode reward: [(0, '150.633')]
[36m[2025-07-01 21:21:35,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 835584. Throughput: 0: 291.9. Samples: 840432. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:21:35,970][166323] Avg episode reward: [(0, '179.290')]
[37m[1m[2025-07-01 21:21:36,018][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000001624_835584.pth...
[36m[2025-07-01 21:21:36,022][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000001496_770048.pth
[36m[2025-07-01 21:21:40,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 835584. Throughput: 0: 291.2. Samples: 842176. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:21:40,969][166323] Avg episode reward: [(0, '177.257')]
[36m[2025-07-01 21:21:45,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 835584. Throughput: 0: 294.6. Samples: 844016. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:21:45,990][166323] Avg episode reward: [(0, '177.861')]
[36m[2025-07-01 21:21:50,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 835584. Throughput: 0: 295.6. Samples: 844928. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:21:50,991][166323] Avg episode reward: [(0, '178.859')]
[36m[2025-07-01 21:21:55,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 835584. Throughput: 0: 295.8. Samples: 846752. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:21:55,943][166323] Avg episode reward: [(0, '180.718')]
[36m[2025-07-01 21:22:00,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 835584. Throughput: 0: 297.6. Samples: 848720. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:22:00,948][166323] Avg episode reward: [(0, '167.729')]
[36m[2025-07-01 21:22:05,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 835584. Throughput: 0: 297.6. Samples: 849536. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:22:05,963][166323] Avg episode reward: [(0, '164.661')]
[36m[2025-07-01 21:22:10,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 835584. Throughput: 0: 295.8. Samples: 851232. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:22:10,962][166323] Avg episode reward: [(0, '162.288')]
[36m[2025-07-01 21:22:15,966][166323] Fps is (10 sec: 1637.8, 60 sec: 546.4, 300 sec: 333.2). Total num frames: 851968. Throughput: 0: 297.8. Samples: 852992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:22:15,967][166323] Avg episode reward: [(0, '153.128')]
[36m[2025-07-01 21:22:20,961][166323] Fps is (10 sec: 1638.5, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 851968. Throughput: 0: 299.1. Samples: 853888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:22:20,961][166323] Avg episode reward: [(0, '156.303')]
[36m[2025-07-01 21:22:25,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 851968. Throughput: 0: 302.1. Samples: 855776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:22:25,990][166323] Avg episode reward: [(0, '172.638')]
[31m[2937206 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2937206 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[2937207 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:22:30,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 851968. Throughput: 0: 299.6. Samples: 857488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:22:30,953][166323] Avg episode reward: [(0, '175.706')]
[36m[2025-07-01 21:22:35,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 851968. Throughput: 0: 299.0. Samples: 858368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:22:35,944][166323] Avg episode reward: [(0, '185.965')]
[36m[2025-07-01 21:22:40,998][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 851968. Throughput: 0: 293.0. Samples: 859952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:22:40,998][166323] Avg episode reward: [(0, '197.790')]
[36m[2025-07-01 21:22:45,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 851968. Throughput: 0: 289.2. Samples: 861744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:22:45,987][166323] Avg episode reward: [(0, '196.747')]
[36m[2025-07-01 21:22:50,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 851968. Throughput: 0: 289.5. Samples: 862560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:22:50,951][166323] Avg episode reward: [(0, '187.178')]
[36m[2025-07-01 21:22:55,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 851968. Throughput: 0: 292.3. Samples: 864384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:22:55,952][166323] Avg episode reward: [(0, '184.648')]
[36m[2025-07-01 21:23:00,979][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 851968. Throughput: 0: 293.6. Samples: 866208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:23:00,979][166323] Avg episode reward: [(0, '195.801')]
[36m[2025-07-01 21:23:05,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 851968. Throughput: 0: 292.2. Samples: 867040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:23:05,974][166323] Avg episode reward: [(0, '192.516')]
[36m[2025-07-01 21:23:10,947][166323] Fps is (10 sec: 1643.6, 60 sec: 546.3, 300 sec: 333.3). Total num frames: 868352. Throughput: 0: 292.9. Samples: 868944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:23:10,948][166323] Avg episode reward: [(0, '194.072')]
[36m[2025-07-01 21:23:15,952][166323] Fps is (10 sec: 1642.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 868352. Throughput: 0: 295.1. Samples: 870768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:23:15,952][166323] Avg episode reward: [(0, '203.383')]
[36m[2025-07-01 21:23:20,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 868352. Throughput: 0: 295.6. Samples: 871680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:23:20,982][166323] Avg episode reward: [(0, '213.354')]
[36m[2025-07-01 21:23:25,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 868352. Throughput: 0: 299.4. Samples: 873408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:23:25,945][166323] Avg episode reward: [(0, '232.429')]
[36m[2025-07-01 21:23:30,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 868352. Throughput: 0: 302.3. Samples: 875344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:23:30,970][166323] Avg episode reward: [(0, '206.187')]
[36m[2025-07-01 21:23:35,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 868352. Throughput: 0: 305.9. Samples: 876336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:23:35,992][166323] Avg episode reward: [(0, '194.780')]
[37m[1m[2025-07-01 21:23:36,043][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000001688_868352.pth...
[36m[2025-07-01 21:23:36,047][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000001560_802816.pth
[33m[3007900 ms][navigation_task] - WARNING : Curriculum Level: 36, Curriculum progress fraction: 0.0 (navigation_task.py:262)
[33m[3007900 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.654296875
[33mCrash Rate: 0.271484375
[33mTimeout Rate: 0.07421875 (navigation_task.py:265)
[33m[3007900 ms][navigation_task] - WARNING : 
[33mSuccesses: 1340
[33mCrashes : 556
[33mTimeouts: 152 (navigation_task.py:268)
[36m[2025-07-01 21:23:40,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 868352. Throughput: 0: 304.0. Samples: 878064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:23:40,951][166323] Avg episode reward: [(0, '187.020')]
[36m[2025-07-01 21:23:45,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 868352. Throughput: 0: 303.1. Samples: 879840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:23:45,960][166323] Avg episode reward: [(0, '172.701')]
[36m[2025-07-01 21:23:50,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 868352. Throughput: 0: 303.5. Samples: 880704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:23:50,994][166323] Avg episode reward: [(0, '175.989')]
[36m[2025-07-01 21:23:55,979][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 868352. Throughput: 0: 300.9. Samples: 882496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:23:55,980][166323] Avg episode reward: [(0, '170.937')]
[36m[2025-07-01 21:24:00,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 868352. Throughput: 0: 299.9. Samples: 884272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:24:00,975][166323] Avg episode reward: [(0, '173.151')]
[36m[2025-07-01 21:24:05,967][166323] Fps is (10 sec: 1640.4, 60 sec: 546.2, 300 sec: 333.3). Total num frames: 884736. Throughput: 0: 297.0. Samples: 885040. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 21:24:05,968][166323] Avg episode reward: [(0, '191.106')]
[36m[2025-07-01 21:24:10,960][166323] Fps is (10 sec: 1640.9, 60 sec: 273.0, 300 sec: 333.3). Total num frames: 884736. Throughput: 0: 295.4. Samples: 886704. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 21:24:10,960][166323] Avg episode reward: [(0, '193.770')]
[36m[2025-07-01 21:24:15,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 884736. Throughput: 0: 288.1. Samples: 888304. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 21:24:15,956][166323] Avg episode reward: [(0, '197.837')]
[36m[2025-07-01 21:24:20,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 884736. Throughput: 0: 286.8. Samples: 889232. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 21:24:20,955][166323] Avg episode reward: [(0, '184.904')]
[36m[2025-07-01 21:24:25,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 884736. Throughput: 0: 285.5. Samples: 890912. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 21:24:25,959][166323] Avg episode reward: [(0, '196.618')]
[36m[2025-07-01 21:24:30,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 884736. Throughput: 0: 285.6. Samples: 892688. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 21:24:30,949][166323] Avg episode reward: [(0, '210.899')]
[36m[2025-07-01 21:24:35,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 884736. Throughput: 0: 287.0. Samples: 893616. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 21:24:35,986][166323] Avg episode reward: [(0, '184.784')]
[36m[2025-07-01 21:24:40,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 884736. Throughput: 0: 288.7. Samples: 895488. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 21:24:40,986][166323] Avg episode reward: [(0, '188.178')]
[36m[2025-07-01 21:24:45,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 884736. Throughput: 0: 287.5. Samples: 897200. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 21:24:45,945][166323] Avg episode reward: [(0, '190.936')]
[36m[2025-07-01 21:24:50,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 884736. Throughput: 0: 292.1. Samples: 898192. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 21:24:50,995][166323] Avg episode reward: [(0, '190.492')]
[36m[2025-07-01 21:24:55,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 884736. Throughput: 0: 295.1. Samples: 899984. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-01 21:24:55,959][166323] Avg episode reward: [(0, '192.984')]
[36m[2025-07-01 21:25:00,983][166323] Fps is (10 sec: 1639.4, 60 sec: 546.1, 300 sec: 333.3). Total num frames: 901120. Throughput: 0: 299.6. Samples: 901792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:25:00,984][166323] Avg episode reward: [(0, '190.992')]
[36m[2025-07-01 21:25:05,967][166323] Fps is (10 sec: 1637.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 901120. Throughput: 0: 297.9. Samples: 902640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:25:05,967][166323] Avg episode reward: [(0, '178.047')]
[36m[2025-07-01 21:25:10,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 901120. Throughput: 0: 300.0. Samples: 904416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:25:10,975][166323] Avg episode reward: [(0, '158.606')]
[36m[2025-07-01 21:25:16,001][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 901120. Throughput: 0: 301.2. Samples: 906256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:25:16,001][166323] Avg episode reward: [(0, '145.741')]
[36m[2025-07-01 21:25:20,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 901120. Throughput: 0: 299.6. Samples: 907088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:25:20,954][166323] Avg episode reward: [(0, '124.324')]
[36m[2025-07-01 21:25:25,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 901120. Throughput: 0: 295.2. Samples: 908768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:25:25,978][166323] Avg episode reward: [(0, '134.151')]
[36m[2025-07-01 21:25:31,003][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 901120. Throughput: 0: 297.2. Samples: 910592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:25:31,003][166323] Avg episode reward: [(0, '141.588')]
[36m[2025-07-01 21:25:35,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 901120. Throughput: 0: 295.2. Samples: 911472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:25:35,976][166323] Avg episode reward: [(0, '125.081')]
[37m[1m[2025-07-01 21:25:36,027][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000001752_901120.pth...
[36m[2025-07-01 21:25:36,031][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000001624_835584.pth
[36m[2025-07-01 21:25:40,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 901120. Throughput: 0: 291.2. Samples: 913088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:25:40,961][166323] Avg episode reward: [(0, '131.104')]
[36m[2025-07-01 21:25:45,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 901120. Throughput: 0: 289.3. Samples: 914800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:25:45,946][166323] Avg episode reward: [(0, '137.741')]
[36m[2025-07-01 21:25:50,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 901120. Throughput: 0: 292.5. Samples: 915808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:25:50,979][166323] Avg episode reward: [(0, '134.151')]
[36m[2025-07-01 21:25:55,987][166323] Fps is (10 sec: 1631.6, 60 sec: 545.9, 300 sec: 333.2). Total num frames: 917504. Throughput: 0: 294.3. Samples: 917664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:25:55,987][166323] Avg episode reward: [(0, '106.523')]
[36m[2025-07-01 21:26:00,959][166323] Fps is (10 sec: 1641.7, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 917504. Throughput: 0: 292.2. Samples: 919392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:26:00,960][166323] Avg episode reward: [(0, '115.312')]
[36m[2025-07-01 21:26:05,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 917504. Throughput: 0: 292.7. Samples: 920256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:26:05,949][166323] Avg episode reward: [(0, '119.749')]
[36m[2025-07-01 21:26:10,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 917504. Throughput: 0: 295.3. Samples: 922064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:26:10,996][166323] Avg episode reward: [(0, '123.476')]
[36m[2025-07-01 21:26:15,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 917504. Throughput: 0: 296.0. Samples: 923904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:26:15,977][166323] Avg episode reward: [(0, '137.867')]
[36m[2025-07-01 21:26:20,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 917504. Throughput: 0: 293.9. Samples: 924688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:26:20,949][166323] Avg episode reward: [(0, '162.470')]
[36m[2025-07-01 21:26:25,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 917504. Throughput: 0: 296.2. Samples: 926416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:26:25,959][166323] Avg episode reward: [(0, '173.495')]
[36m[2025-07-01 21:26:30,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 917504. Throughput: 0: 299.4. Samples: 928272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:26:30,947][166323] Avg episode reward: [(0, '168.860')]
[36m[2025-07-01 21:26:35,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 917504. Throughput: 0: 294.4. Samples: 929056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:26:35,984][166323] Avg episode reward: [(0, '197.808')]
[36m[2025-07-01 21:26:40,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 917504. Throughput: 0: 290.3. Samples: 930720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:26:40,961][166323] Avg episode reward: [(0, '208.906')]
[36m[2025-07-01 21:26:46,010][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 917504. Throughput: 0: 291.2. Samples: 932512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:26:46,010][166323] Avg episode reward: [(0, '208.677')]
[36m[2025-07-01 21:26:50,999][166323] Fps is (10 sec: 1632.1, 60 sec: 546.0, 300 sec: 333.2). Total num frames: 933888. Throughput: 0: 290.2. Samples: 933328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:26:50,999][166323] Avg episode reward: [(0, '216.142')]
[31m[3201026 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[3201027 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[3201027 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[3204404 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[3204404 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[3204405 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:26:56,011][166323] Fps is (10 sec: 1638.2, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 933888. Throughput: 0: 286.8. Samples: 934976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:26:56,012][166323] Avg episode reward: [(0, '211.290')]
[36m[2025-07-01 21:27:01,017][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 333.2). Total num frames: 933888. Throughput: 0: 285.6. Samples: 936768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:27:01,018][166323] Avg episode reward: [(0, '210.757')]
[36m[2025-07-01 21:27:05,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 933888. Throughput: 0: 288.3. Samples: 937664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:27:05,965][166323] Avg episode reward: [(0, '188.510')]
[36m[2025-07-01 21:27:11,003][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 933888. Throughput: 0: 290.2. Samples: 939488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:27:11,003][166323] Avg episode reward: [(0, '170.802')]
[36m[2025-07-01 21:27:15,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 933888. Throughput: 0: 289.0. Samples: 941280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:27:15,957][166323] Avg episode reward: [(0, '163.942')]
[31m[3229210 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[3229210 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[3229210 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:27:20,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 933888. Throughput: 0: 288.0. Samples: 942016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:27:20,988][166323] Avg episode reward: [(0, '161.736')]
[36m[2025-07-01 21:27:25,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 933888. Throughput: 0: 291.2. Samples: 943824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:27:25,966][166323] Avg episode reward: [(0, '156.942')]
[36m[2025-07-01 21:27:30,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 933888. Throughput: 0: 290.1. Samples: 945552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:27:30,961][166323] Avg episode reward: [(0, '150.330')]
[36m[2025-07-01 21:27:35,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 933888. Throughput: 0: 293.3. Samples: 946512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:27:35,943][166323] Avg episode reward: [(0, '165.353')]
[37m[1m[2025-07-01 21:27:36,007][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000001816_933888.pth...
[36m[2025-07-01 21:27:36,011][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000001688_868352.pth
[36m[2025-07-01 21:27:40,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 933888. Throughput: 0: 295.0. Samples: 948240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:27:40,969][166323] Avg episode reward: [(0, '174.938')]
[36m[2025-07-01 21:27:45,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 933888. Throughput: 0: 295.8. Samples: 950064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:27:45,972][166323] Avg episode reward: [(0, '158.662')]
[36m[2025-07-01 21:27:50,954][166323] Fps is (10 sec: 1640.8, 60 sec: 273.3, 300 sec: 333.2). Total num frames: 950272. Throughput: 0: 293.4. Samples: 950864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:27:50,954][166323] Avg episode reward: [(0, '136.938')]
[36m[2025-07-01 21:27:55,960][166323] Fps is (10 sec: 1640.2, 60 sec: 273.3, 300 sec: 333.3). Total num frames: 950272. Throughput: 0: 292.2. Samples: 952624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:27:55,961][166323] Avg episode reward: [(0, '136.396')]
[36m[2025-07-01 21:28:00,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 950272. Throughput: 0: 293.8. Samples: 954512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:28:00,994][166323] Avg episode reward: [(0, '144.118')]
[36m[2025-07-01 21:28:05,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 950272. Throughput: 0: 297.6. Samples: 955408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:28:05,986][166323] Avg episode reward: [(0, '127.728')]
[36m[2025-07-01 21:28:10,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 950272. Throughput: 0: 299.2. Samples: 957280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:28:10,944][166323] Avg episode reward: [(0, '126.599')]
[36m[2025-07-01 21:28:15,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 950272. Throughput: 0: 298.8. Samples: 959008. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:28:15,995][166323] Avg episode reward: [(0, '141.822')]
[36m[2025-07-01 21:28:20,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 950272. Throughput: 0: 295.7. Samples: 959824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:28:20,963][166323] Avg episode reward: [(0, '145.142')]
[36m[2025-07-01 21:28:25,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 950272. Throughput: 0: 296.6. Samples: 961584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:28:25,963][166323] Avg episode reward: [(0, '136.718')]
[36m[2025-07-01 21:28:30,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 950272. Throughput: 0: 295.9. Samples: 963376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:28:30,953][166323] Avg episode reward: [(0, '128.812')]
[36m[2025-07-01 21:28:35,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 950272. Throughput: 0: 295.3. Samples: 964160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:28:35,987][166323] Avg episode reward: [(0, '123.898')]
[36m[2025-07-01 21:28:40,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 950272. Throughput: 0: 295.5. Samples: 965920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:28:40,960][166323] Avg episode reward: [(0, '132.736')]
[36m[2025-07-01 21:28:45,971][166323] Fps is (10 sec: 1640.8, 60 sec: 546.1, 300 sec: 333.3). Total num frames: 966656. Throughput: 0: 288.1. Samples: 967472. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 21:28:45,972][166323] Avg episode reward: [(0, '133.198')]
[36m[2025-07-01 21:28:50,984][166323] Fps is (10 sec: 1634.4, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 966656. Throughput: 0: 287.7. Samples: 968352. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 21:28:50,984][166323] Avg episode reward: [(0, '145.953')]
[36m[2025-07-01 21:28:55,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 966656. Throughput: 0: 286.6. Samples: 970176. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 21:28:55,947][166323] Avg episode reward: [(0, '173.341')]
[36m[2025-07-01 21:29:00,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 966656. Throughput: 0: 288.4. Samples: 971984. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 21:29:00,992][166323] Avg episode reward: [(0, '178.501')]
[36m[2025-07-01 21:29:05,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 966656. Throughput: 0: 290.2. Samples: 972880. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 21:29:05,950][166323] Avg episode reward: [(0, '188.591')]
[36m[2025-07-01 21:29:10,975][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 966656. Throughput: 0: 288.6. Samples: 974576. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 21:29:10,975][166323] Avg episode reward: [(0, '203.993')]
[36m[2025-07-01 21:29:15,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 966656. Throughput: 0: 287.3. Samples: 976304. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 21:29:15,947][166323] Avg episode reward: [(0, '194.822')]
[36m[2025-07-01 21:29:20,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 966656. Throughput: 0: 288.9. Samples: 977152. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 21:29:20,962][166323] Avg episode reward: [(0, '169.630')]
[36m[2025-07-01 21:29:25,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 966656. Throughput: 0: 289.7. Samples: 978960. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 21:29:25,965][166323] Avg episode reward: [(0, '177.258')]
[36m[2025-07-01 21:29:30,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 966656. Throughput: 0: 293.6. Samples: 980688. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 21:29:30,985][166323] Avg episode reward: [(0, '181.127')]
[36m[2025-07-01 21:29:35,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 966656. Throughput: 0: 292.8. Samples: 981520. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 21:29:35,957][166323] Avg episode reward: [(0, '181.569')]
[37m[1m[2025-07-01 21:29:36,023][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000001880_966656.pth...
[36m[2025-07-01 21:29:36,028][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000001752_901120.pth
[36m[2025-07-01 21:29:40,989][166323] Fps is (10 sec: 1637.7, 60 sec: 545.9, 300 sec: 333.2). Total num frames: 983040. Throughput: 0: 290.2. Samples: 983248. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-01 21:29:40,989][166323] Avg episode reward: [(0, '188.250')]
[31m[3374387 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[3374387 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[3374387 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:29:45,944][166323] Fps is (10 sec: 1640.4, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 983040. Throughput: 0: 294.0. Samples: 985200. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-01 21:29:45,944][166323] Avg episode reward: [(0, '185.376')]
[36m[2025-07-01 21:29:50,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 983040. Throughput: 0: 292.1. Samples: 986032. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-01 21:29:50,975][166323] Avg episode reward: [(0, '199.104')]
[36m[2025-07-01 21:29:55,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 983040. Throughput: 0: 295.2. Samples: 987856. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-01 21:29:55,961][166323] Avg episode reward: [(0, '214.526')]
[36m[2025-07-01 21:30:00,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 983040. Throughput: 0: 295.0. Samples: 989584. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-01 21:30:00,968][166323] Avg episode reward: [(0, '208.154')]
[36m[2025-07-01 21:30:05,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 983040. Throughput: 0: 295.5. Samples: 990448. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-01 21:30:05,956][166323] Avg episode reward: [(0, '194.592')]
[36m[2025-07-01 21:30:10,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 983040. Throughput: 0: 297.6. Samples: 992352. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-01 21:30:10,960][166323] Avg episode reward: [(0, '207.613')]
[36m[2025-07-01 21:30:15,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 983040. Throughput: 0: 297.1. Samples: 994048. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-01 21:30:15,952][166323] Avg episode reward: [(0, '207.123')]
[36m[2025-07-01 21:30:20,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 983040. Throughput: 0: 298.9. Samples: 994976. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-01 21:30:20,974][166323] Avg episode reward: [(0, '212.482')]
[36m[2025-07-01 21:30:25,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 983040. Throughput: 0: 300.3. Samples: 996752. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-01 21:30:25,955][166323] Avg episode reward: [(0, '191.079')]
[36m[2025-07-01 21:30:30,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 983040. Throughput: 0: 297.2. Samples: 998576. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-01 21:30:30,949][166323] Avg episode reward: [(0, '201.175')]
[36m[2025-07-01 21:30:35,955][166323] Fps is (10 sec: 1638.5, 60 sec: 546.1, 300 sec: 333.2). Total num frames: 999424. Throughput: 0: 298.8. Samples: 999472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:30:35,955][166323] Avg episode reward: [(0, '186.517')]
[36m[2025-07-01 21:30:40,970][166323] Fps is (10 sec: 1634.8, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 999424. Throughput: 0: 296.8. Samples: 1001216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:30:40,971][166323] Avg episode reward: [(0, '170.453')]
[36m[2025-07-01 21:30:46,015][166323] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 333.2). Total num frames: 999424. Throughput: 0: 296.9. Samples: 1002960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:30:46,015][166323] Avg episode reward: [(0, '164.265')]
[31m[3435998 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[3435999 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[3435999 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:30:50,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 999424. Throughput: 0: 295.8. Samples: 1003760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:30:50,964][166323] Avg episode reward: [(0, '176.854')]
[36m[2025-07-01 21:30:55,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 999424. Throughput: 0: 290.2. Samples: 1005408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:30:55,945][166323] Avg episode reward: [(0, '169.731')]
[36m[2025-07-01 21:31:00,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 999424. Throughput: 0: 292.0. Samples: 1007200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:31:00,999][166323] Avg episode reward: [(0, '150.812')]
[36m[2025-07-01 21:31:05,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 999424. Throughput: 0: 292.2. Samples: 1008128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:31:05,988][166323] Avg episode reward: [(0, '156.484')]
[36m[2025-07-01 21:31:10,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 999424. Throughput: 0: 294.0. Samples: 1009984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:31:10,959][166323] Avg episode reward: [(0, '141.447')]
[36m[2025-07-01 21:31:15,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 999424. Throughput: 0: 289.2. Samples: 1011600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:31:15,990][166323] Avg episode reward: [(0, '154.784')]
[36m[2025-07-01 21:31:20,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 999424. Throughput: 0: 287.8. Samples: 1012432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:31:20,979][166323] Avg episode reward: [(0, '144.700')]
[36m[2025-07-01 21:31:25,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 999424. Throughput: 0: 284.1. Samples: 1014000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:31:25,963][166323] Avg episode reward: [(0, '163.112')]
[36m[2025-07-01 21:31:30,958][166323] Fps is (10 sec: 1641.8, 60 sec: 546.0, 300 sec: 333.3). Total num frames: 1015808. Throughput: 0: 286.6. Samples: 1015840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:31:30,958][166323] Avg episode reward: [(0, '182.640')]
[31m[3481081 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[3481081 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[3481082 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:31:35,960][166323] Fps is (10 sec: 1638.9, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1015808. Throughput: 0: 288.0. Samples: 1016720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:31:35,960][166323] Avg episode reward: [(0, '192.006')]
[37m[1m[2025-07-01 21:31:36,031][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000001976_1015808.pth...
[36m[2025-07-01 21:31:36,035][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000001816_933888.pth
[36m[2025-07-01 21:31:40,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 1015808. Throughput: 0: 288.4. Samples: 1018384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:31:40,945][166323] Avg episode reward: [(0, '198.877')]
[36m[2025-07-01 21:31:45,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 1015808. Throughput: 0: 291.8. Samples: 1020320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:31:45,966][166323] Avg episode reward: [(0, '204.676')]
[36m[2025-07-01 21:31:50,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 1015808. Throughput: 0: 290.1. Samples: 1021168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:31:50,944][166323] Avg episode reward: [(0, '219.699')]
[36m[2025-07-01 21:31:55,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 1015808. Throughput: 0: 283.9. Samples: 1022768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:31:55,995][166323] Avg episode reward: [(0, '206.509')]
[36m[2025-07-01 21:32:00,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1015808. Throughput: 0: 286.2. Samples: 1024480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:32:00,991][166323] Avg episode reward: [(0, '189.789')]
[36m[2025-07-01 21:32:05,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1015808. Throughput: 0: 287.7. Samples: 1025376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:32:05,969][166323] Avg episode reward: [(0, '196.499')]
[36m[2025-07-01 21:32:10,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1015808. Throughput: 0: 294.1. Samples: 1027232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:32:10,954][166323] Avg episode reward: [(0, '207.484')]
[36m[2025-07-01 21:32:15,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1015808. Throughput: 0: 293.7. Samples: 1029056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:32:15,955][166323] Avg episode reward: [(0, '197.053')]
[36m[2025-07-01 21:32:20,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1015808. Throughput: 0: 293.0. Samples: 1029904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:32:20,955][166323] Avg episode reward: [(0, '189.066')]
[36m[2025-07-01 21:32:25,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1015808. Throughput: 0: 294.6. Samples: 1031648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:32:25,977][166323] Avg episode reward: [(0, '189.742')]
[36m[2025-07-01 21:32:30,968][166323] Fps is (10 sec: 1636.2, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1032192. Throughput: 0: 289.0. Samples: 1033328. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 21:32:30,968][166323] Avg episode reward: [(0, '195.378')]
[36m[2025-07-01 21:32:35,959][166323] Fps is (10 sec: 1641.3, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 1032192. Throughput: 0: 290.4. Samples: 1034240. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 21:32:35,959][166323] Avg episode reward: [(0, '193.961')]
[36m[2025-07-01 21:32:40,972][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 1032192. Throughput: 0: 295.6. Samples: 1036064. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 21:32:40,973][166323] Avg episode reward: [(0, '192.772')]
[36m[2025-07-01 21:32:45,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1032192. Throughput: 0: 298.2. Samples: 1037888. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 21:32:45,958][166323] Avg episode reward: [(0, '187.849')]
[36m[2025-07-01 21:32:50,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1032192. Throughput: 0: 299.4. Samples: 1038848. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 21:32:50,968][166323] Avg episode reward: [(0, '193.005')]
[36m[2025-07-01 21:32:55,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1032192. Throughput: 0: 297.2. Samples: 1040608. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 21:32:55,960][166323] Avg episode reward: [(0, '206.573')]
[36m[2025-07-01 21:33:00,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1032192. Throughput: 0: 298.9. Samples: 1042512. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 21:33:00,978][166323] Avg episode reward: [(0, '197.915')]
[36m[2025-07-01 21:33:05,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1032192. Throughput: 0: 300.6. Samples: 1043440. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 21:33:05,979][166323] Avg episode reward: [(0, '204.978')]
[36m[2025-07-01 21:33:10,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1032192. Throughput: 0: 303.2. Samples: 1045296. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 21:33:10,985][166323] Avg episode reward: [(0, '204.610')]
[31m[3580228 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[3580229 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[3580229 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:33:15,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1032192. Throughput: 0: 305.4. Samples: 1047072. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 21:33:15,977][166323] Avg episode reward: [(0, '185.297')]
[36m[2025-07-01 21:33:20,945][166323] Fps is (10 sec: 1644.8, 60 sec: 546.2, 300 sec: 333.3). Total num frames: 1048576. Throughput: 0: 305.9. Samples: 1048000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:33:20,946][166323] Avg episode reward: [(0, '192.224')]
[36m[2025-07-01 21:33:25,985][166323] Fps is (10 sec: 1637.2, 60 sec: 546.1, 300 sec: 333.2). Total num frames: 1048576. Throughput: 0: 306.8. Samples: 1049872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:33:25,985][166323] Avg episode reward: [(0, '205.974')]
[36m[2025-07-01 21:33:30,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1048576. Throughput: 0: 305.3. Samples: 1051632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:33:30,981][166323] Avg episode reward: [(0, '204.429')]
[36m[2025-07-01 21:33:35,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 1048576. Throughput: 0: 304.5. Samples: 1052544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:33:35,953][166323] Avg episode reward: [(0, '210.222')]
[37m[1m[2025-07-01 21:33:36,005][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000002040_1048576.pth...
[36m[2025-07-01 21:33:36,009][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000001880_966656.pth
[36m[2025-07-01 21:33:40,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1048576. Throughput: 0: 302.2. Samples: 1054208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:33:40,966][166323] Avg episode reward: [(0, '203.015')]
[36m[2025-07-01 21:33:45,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1048576. Throughput: 0: 301.1. Samples: 1056064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:33:45,990][166323] Avg episode reward: [(0, '231.450')]
[33m[3616449 ms][navigation_task] - WARNING : Curriculum Level: 36, Curriculum progress fraction: 0.0 (navigation_task.py:262)
[33m[3616449 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.6435546875
[33mCrash Rate: 0.2958984375
[33mTimeout Rate: 0.060546875 (navigation_task.py:265)
[33m[3616449 ms][navigation_task] - WARNING : 
[33mSuccesses: 1318
[33mCrashes : 606
[33mTimeouts: 124 (navigation_task.py:268)
[36m[2025-07-01 21:33:50,997][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 1048576. Throughput: 0: 297.8. Samples: 1056848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:33:50,998][166323] Avg episode reward: [(0, '230.311')]
[36m[2025-07-01 21:33:55,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1048576. Throughput: 0: 296.2. Samples: 1058624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:33:55,979][166323] Avg episode reward: [(0, '211.757')]
[36m[2025-07-01 21:34:00,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1048576. Throughput: 0: 296.5. Samples: 1060416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:34:00,979][166323] Avg episode reward: [(0, '206.783')]
[36m[2025-07-01 21:34:05,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1048576. Throughput: 0: 293.9. Samples: 1061232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:34:05,960][166323] Avg episode reward: [(0, '194.294')]
[31m[3635753 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[3635753 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[3635753 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:34:10,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1048576. Throughput: 0: 293.4. Samples: 1063072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:34:10,971][166323] Avg episode reward: [(0, '191.091')]
[36m[2025-07-01 21:34:15,947][166323] Fps is (10 sec: 1640.6, 60 sec: 546.4, 300 sec: 333.3). Total num frames: 1064960. Throughput: 0: 296.0. Samples: 1064944. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 21:34:15,947][166323] Avg episode reward: [(0, '180.970')]
[31m[3648625 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[3648625 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[3648625 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:34:20,981][166323] Fps is (10 sec: 1636.7, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 1064960. Throughput: 0: 295.6. Samples: 1065856. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 21:34:20,982][166323] Avg episode reward: [(0, '177.953')]
[36m[2025-07-01 21:34:25,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 1064960. Throughput: 0: 301.1. Samples: 1067760. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 21:34:25,979][166323] Avg episode reward: [(0, '162.998')]
[36m[2025-07-01 21:34:30,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1064960. Throughput: 0: 300.7. Samples: 1069600. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 21:34:30,999][166323] Avg episode reward: [(0, '179.038')]
[36m[2025-07-01 21:34:35,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1064960. Throughput: 0: 303.7. Samples: 1070512. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 21:34:35,992][166323] Avg episode reward: [(0, '206.778')]
[36m[2025-07-01 21:34:40,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1064960. Throughput: 0: 304.8. Samples: 1072336. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 21:34:40,961][166323] Avg episode reward: [(0, '197.674')]
[36m[2025-07-01 21:34:45,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1064960. Throughput: 0: 303.9. Samples: 1074096. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 21:34:45,999][166323] Avg episode reward: [(0, '179.165')]
[36m[2025-07-01 21:34:50,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1064960. Throughput: 0: 304.2. Samples: 1074928. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 21:34:50,989][166323] Avg episode reward: [(0, '196.627')]
[36m[2025-07-01 21:34:55,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1064960. Throughput: 0: 302.9. Samples: 1076704. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 21:34:55,983][166323] Avg episode reward: [(0, '207.006')]
[36m[2025-07-01 21:35:00,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1064960. Throughput: 0: 300.7. Samples: 1078480. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 21:35:00,956][166323] Avg episode reward: [(0, '203.413')]
[36m[2025-07-01 21:35:05,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1064960. Throughput: 0: 299.8. Samples: 1079344. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 21:35:05,975][166323] Avg episode reward: [(0, '193.100')]
[36m[2025-07-01 21:35:10,963][166323] Fps is (10 sec: 1637.2, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 1081344. Throughput: 0: 298.4. Samples: 1081184. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-01 21:35:10,963][166323] Avg episode reward: [(0, '197.828')]
[36m[2025-07-01 21:35:15,992][166323] Fps is (10 sec: 1635.7, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 1081344. Throughput: 0: 298.0. Samples: 1083008. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-01 21:35:15,992][166323] Avg episode reward: [(0, '223.329')]
[36m[2025-07-01 21:35:20,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1081344. Throughput: 0: 296.6. Samples: 1083856. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-01 21:35:20,989][166323] Avg episode reward: [(0, '217.629')]
[36m[2025-07-01 21:35:25,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 1081344. Throughput: 0: 296.8. Samples: 1085696. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-01 21:35:25,973][166323] Avg episode reward: [(0, '218.637')]
[36m[2025-07-01 21:35:30,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1081344. Throughput: 0: 298.1. Samples: 1087504. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-01 21:35:30,984][166323] Avg episode reward: [(0, '234.304')]
[36m[2025-07-01 21:35:35,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 1081344. Throughput: 0: 299.3. Samples: 1088384. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-01 21:35:35,951][166323] Avg episode reward: [(0, '223.393')]
[37m[1m[2025-07-01 21:35:36,001][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000002104_1081344.pth...
[36m[2025-07-01 21:35:36,005][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000001976_1015808.pth
[36m[2025-07-01 21:35:40,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 1081344. Throughput: 0: 298.9. Samples: 1090144. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-01 21:35:40,948][166323] Avg episode reward: [(0, '227.633')]
[36m[2025-07-01 21:35:46,012][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 1081344. Throughput: 0: 297.6. Samples: 1091888. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-01 21:35:46,012][166323] Avg episode reward: [(0, '222.276')]
[36m[2025-07-01 21:35:50,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1081344. Throughput: 0: 299.3. Samples: 1092816. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-01 21:35:50,982][166323] Avg episode reward: [(0, '214.964')]
[31m[3740896 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[3740896 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[3740896 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:35:55,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1081344. Throughput: 0: 298.2. Samples: 1094608. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-01 21:35:55,986][166323] Avg episode reward: [(0, '191.184')]
[36m[2025-07-01 21:36:01,012][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 1081344. Throughput: 0: 298.9. Samples: 1096464. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-01 21:36:01,013][166323] Avg episode reward: [(0, '196.796')]
[36m[2025-07-01 21:36:05,970][166323] Fps is (10 sec: 1641.0, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 1097728. Throughput: 0: 297.7. Samples: 1097248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:36:05,971][166323] Avg episode reward: [(0, '192.572')]
[36m[2025-07-01 21:36:10,950][166323] Fps is (10 sec: 1648.6, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 1097728. Throughput: 0: 295.3. Samples: 1098976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:36:10,951][166323] Avg episode reward: [(0, '201.618')]
[36m[2025-07-01 21:36:15,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 1097728. Throughput: 0: 295.9. Samples: 1100816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:36:15,971][166323] Avg episode reward: [(0, '223.787')]
[36m[2025-07-01 21:36:20,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 1097728. Throughput: 0: 296.5. Samples: 1101728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:36:20,963][166323] Avg episode reward: [(0, '238.171')]
[36m[2025-07-01 21:36:25,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1097728. Throughput: 0: 297.7. Samples: 1103552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:36:25,993][166323] Avg episode reward: [(0, '244.085')]
[36m[2025-07-01 21:36:30,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1097728. Throughput: 0: 300.7. Samples: 1105408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:36:30,966][166323] Avg episode reward: [(0, '241.266')]
[36m[2025-07-01 21:36:35,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1097728. Throughput: 0: 299.2. Samples: 1106272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:36:35,962][166323] Avg episode reward: [(0, '233.310')]
[36m[2025-07-01 21:36:40,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1097728. Throughput: 0: 299.1. Samples: 1108064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:36:40,971][166323] Avg episode reward: [(0, '219.933')]
[36m[2025-07-01 21:36:45,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 1097728. Throughput: 0: 296.6. Samples: 1109792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:36:45,945][166323] Avg episode reward: [(0, '211.612')]
[36m[2025-07-01 21:36:50,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1097728. Throughput: 0: 297.7. Samples: 1110640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:36:50,963][166323] Avg episode reward: [(0, '219.659')]
[31m[3800358 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[3800358 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[3800358 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:36:55,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1097728. Throughput: 0: 298.7. Samples: 1112432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:36:55,999][166323] Avg episode reward: [(0, '197.940')]
[36m[2025-07-01 21:37:00,956][166323] Fps is (10 sec: 1639.5, 60 sec: 546.6, 300 sec: 333.2). Total num frames: 1114112. Throughput: 0: 297.0. Samples: 1114176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:37:00,956][166323] Avg episode reward: [(0, '218.087')]
[36m[2025-07-01 21:37:05,966][166323] Fps is (10 sec: 1643.7, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 1114112. Throughput: 0: 293.7. Samples: 1114944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:37:05,966][166323] Avg episode reward: [(0, '216.684')]
[36m[2025-07-01 21:37:10,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 1114112. Throughput: 0: 295.1. Samples: 1116832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:37:10,995][166323] Avg episode reward: [(0, '233.369')]
[36m[2025-07-01 21:37:15,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 1114112. Throughput: 0: 293.0. Samples: 1118592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:37:15,963][166323] Avg episode reward: [(0, '238.439')]
[36m[2025-07-01 21:37:20,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 1114112. Throughput: 0: 293.4. Samples: 1119472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:37:20,958][166323] Avg episode reward: [(0, '202.374')]
[36m[2025-07-01 21:37:25,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1114112. Throughput: 0: 293.4. Samples: 1121264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:37:25,963][166323] Avg episode reward: [(0, '206.793')]
[36m[2025-07-01 21:37:30,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1114112. Throughput: 0: 294.7. Samples: 1123056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:37:30,948][166323] Avg episode reward: [(0, '204.141')]
[36m[2025-07-01 21:37:35,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1114112. Throughput: 0: 294.1. Samples: 1123872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:37:35,961][166323] Avg episode reward: [(0, '219.576')]
[37m[1m[2025-07-01 21:37:36,037][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000002168_1114112.pth...
[36m[2025-07-01 21:37:36,044][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000002040_1048576.pth
[36m[2025-07-01 21:37:40,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1114112. Throughput: 0: 290.7. Samples: 1125504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:37:40,958][166323] Avg episode reward: [(0, '210.158')]
[36m[2025-07-01 21:37:45,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1114112. Throughput: 0: 289.3. Samples: 1127200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:37:45,981][166323] Avg episode reward: [(0, '215.879')]
[36m[2025-07-01 21:37:50,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1114112. Throughput: 0: 293.4. Samples: 1128144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:37:50,954][166323] Avg episode reward: [(0, '244.165')]
[36m[2025-07-01 21:37:55,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 1114112. Throughput: 0: 293.0. Samples: 1130000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:37:55,945][166323] Avg episode reward: [(0, '228.949')]
[36m[2025-07-01 21:38:00,953][166323] Fps is (10 sec: 1638.6, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 1130496. Throughput: 0: 293.4. Samples: 1131792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:38:00,953][166323] Avg episode reward: [(0, '248.190')]
[36m[2025-07-01 21:38:05,955][166323] Fps is (10 sec: 1636.7, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 1130496. Throughput: 0: 293.4. Samples: 1132672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:38:05,955][166323] Avg episode reward: [(0, '245.410')]
[36m[2025-07-01 21:38:10,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 333.3). Total num frames: 1130496. Throughput: 0: 294.5. Samples: 1134512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:38:10,946][166323] Avg episode reward: [(0, '236.495')]
[36m[2025-07-01 21:38:15,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1130496. Throughput: 0: 292.6. Samples: 1136224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:38:15,946][166323] Avg episode reward: [(0, '217.335')]
[31m[3889282 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[3889283 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[3889283 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:38:20,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1130496. Throughput: 0: 294.1. Samples: 1137104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:38:20,957][166323] Avg episode reward: [(0, '181.680')]
[31m[3890863 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[3890863 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[3890864 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:38:25,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1130496. Throughput: 0: 295.0. Samples: 1138784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:38:25,976][166323] Avg episode reward: [(0, '196.902')]
[36m[2025-07-01 21:38:30,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1130496. Throughput: 0: 297.9. Samples: 1140608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:38:30,991][166323] Avg episode reward: [(0, '198.297')]
[36m[2025-07-01 21:38:35,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1130496. Throughput: 0: 298.1. Samples: 1141568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:38:35,993][166323] Avg episode reward: [(0, '216.978')]
[36m[2025-07-01 21:38:40,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1130496. Throughput: 0: 297.1. Samples: 1143376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:38:40,967][166323] Avg episode reward: [(0, '212.370')]
[36m[2025-07-01 21:38:45,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1130496. Throughput: 0: 297.1. Samples: 1145168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:38:45,976][166323] Avg episode reward: [(0, '214.383')]
[36m[2025-07-01 21:38:50,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1130496. Throughput: 0: 296.9. Samples: 1146032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:38:50,956][166323] Avg episode reward: [(0, '228.863')]
[36m[2025-07-01 21:38:55,962][166323] Fps is (10 sec: 1640.6, 60 sec: 546.0, 300 sec: 333.3). Total num frames: 1146880. Throughput: 0: 295.7. Samples: 1147824. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:38:55,963][166323] Avg episode reward: [(0, '222.305')]
[36m[2025-07-01 21:39:00,987][166323] Fps is (10 sec: 1633.3, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 1146880. Throughput: 0: 298.7. Samples: 1149680. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:39:00,987][166323] Avg episode reward: [(0, '208.843')]
[36m[2025-07-01 21:39:06,008][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 333.2). Total num frames: 1146880. Throughput: 0: 298.7. Samples: 1150560. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:39:06,008][166323] Avg episode reward: [(0, '206.853')]
[36m[2025-07-01 21:39:10,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1146880. Throughput: 0: 302.4. Samples: 1152384. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:39:10,957][166323] Avg episode reward: [(0, '204.290')]
[36m[2025-07-01 21:39:15,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1146880. Throughput: 0: 301.3. Samples: 1154160. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:39:15,966][166323] Avg episode reward: [(0, '211.785')]
[36m[2025-07-01 21:39:20,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1146880. Throughput: 0: 298.4. Samples: 1154992. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:39:20,980][166323] Avg episode reward: [(0, '221.937')]
[36m[2025-07-01 21:39:25,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1146880. Throughput: 0: 296.0. Samples: 1156688. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:39:25,946][166323] Avg episode reward: [(0, '221.027')]
[36m[2025-07-01 21:39:30,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1146880. Throughput: 0: 294.5. Samples: 1158416. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:39:30,955][166323] Avg episode reward: [(0, '215.474')]
[36m[2025-07-01 21:39:35,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1146880. Throughput: 0: 295.8. Samples: 1159344. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:39:35,954][166323] Avg episode reward: [(0, '242.696')]
[37m[1m[2025-07-01 21:39:36,026][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000002232_1146880.pth...
[36m[2025-07-01 21:39:36,030][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000002104_1081344.pth
[36m[2025-07-01 21:39:40,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1146880. Throughput: 0: 294.3. Samples: 1161072. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:39:40,975][166323] Avg episode reward: [(0, '251.203')]
[36m[2025-07-01 21:39:45,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1146880. Throughput: 0: 289.2. Samples: 1162688. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:39:45,973][166323] Avg episode reward: [(0, '237.970')]
[36m[2025-07-01 21:39:50,964][166323] Fps is (10 sec: 1640.2, 60 sec: 546.1, 300 sec: 333.3). Total num frames: 1163264. Throughput: 0: 286.9. Samples: 1163456. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:39:50,964][166323] Avg episode reward: [(0, '228.860')]
[31m[3982175 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[3982175 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[3982176 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:39:55,975][166323] Fps is (10 sec: 1638.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1163264. Throughput: 0: 287.2. Samples: 1165312. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:39:55,976][166323] Avg episode reward: [(0, '230.740')]
[36m[2025-07-01 21:40:00,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 1163264. Throughput: 0: 287.9. Samples: 1167120. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:40:00,976][166323] Avg episode reward: [(0, '234.786')]
[36m[2025-07-01 21:40:05,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1163264. Throughput: 0: 287.3. Samples: 1167920. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:40:05,977][166323] Avg episode reward: [(0, '214.813')]
[36m[2025-07-01 21:40:10,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1163264. Throughput: 0: 290.4. Samples: 1169760. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:40:10,965][166323] Avg episode reward: [(0, '196.896')]
[36m[2025-07-01 21:40:15,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1163264. Throughput: 0: 292.6. Samples: 1171584. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:40:15,958][166323] Avg episode reward: [(0, '211.954')]
[36m[2025-07-01 21:40:20,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1163264. Throughput: 0: 292.6. Samples: 1172512. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:40:20,952][166323] Avg episode reward: [(0, '220.403')]
[36m[2025-07-01 21:40:25,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1163264. Throughput: 0: 294.5. Samples: 1174320. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:40:25,961][166323] Avg episode reward: [(0, '227.054')]
[31m[4016191 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4016191 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[4016191 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:40:31,002][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 1163264. Throughput: 0: 296.7. Samples: 1176048. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:40:31,003][166323] Avg episode reward: [(0, '211.228')]
[36m[2025-07-01 21:40:35,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1163264. Throughput: 0: 300.2. Samples: 1176960. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:40:35,953][166323] Avg episode reward: [(0, '226.416')]
[36m[2025-07-01 21:40:40,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1163264. Throughput: 0: 299.1. Samples: 1178768. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 21:40:40,965][166323] Avg episode reward: [(0, '235.190')]
[36m[2025-07-01 21:40:45,951][166323] Fps is (10 sec: 1638.7, 60 sec: 546.3, 300 sec: 333.3). Total num frames: 1179648. Throughput: 0: 297.4. Samples: 1180496. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 21:40:45,951][166323] Avg episode reward: [(0, '231.665')]
[36m[2025-07-01 21:40:50,971][166323] Fps is (10 sec: 1637.4, 60 sec: 273.0, 300 sec: 333.3). Total num frames: 1179648. Throughput: 0: 299.8. Samples: 1181408. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 21:40:50,971][166323] Avg episode reward: [(0, '224.065')]
[36m[2025-07-01 21:40:55,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.3). Total num frames: 1179648. Throughput: 0: 300.7. Samples: 1183296. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 21:40:55,982][166323] Avg episode reward: [(0, '217.432')]
[36m[2025-07-01 21:41:00,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1179648. Throughput: 0: 297.6. Samples: 1184976. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 21:41:00,956][166323] Avg episode reward: [(0, '218.571')]
[36m[2025-07-01 21:41:05,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1179648. Throughput: 0: 295.2. Samples: 1185808. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 21:41:05,986][166323] Avg episode reward: [(0, '217.064')]
[36m[2025-07-01 21:41:10,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1179648. Throughput: 0: 295.8. Samples: 1187632. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 21:41:10,965][166323] Avg episode reward: [(0, '219.066')]
[36m[2025-07-01 21:41:15,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1179648. Throughput: 0: 297.4. Samples: 1189424. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 21:41:15,979][166323] Avg episode reward: [(0, '220.896')]
[36m[2025-07-01 21:41:20,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1179648. Throughput: 0: 298.2. Samples: 1190384. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 21:41:20,970][166323] Avg episode reward: [(0, '221.396')]
[36m[2025-07-01 21:41:25,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1179648. Throughput: 0: 299.0. Samples: 1192224. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 21:41:25,967][166323] Avg episode reward: [(0, '215.629')]
[36m[2025-07-01 21:41:30,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1179648. Throughput: 0: 300.3. Samples: 1194016. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 21:41:30,973][166323] Avg episode reward: [(0, '237.484')]
[36m[2025-07-01 21:41:35,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1179648. Throughput: 0: 300.6. Samples: 1194928. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 21:41:35,945][166323] Avg episode reward: [(0, '226.496')]
[37m[1m[2025-07-01 21:41:36,000][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000002296_1179648.pth...
[36m[2025-07-01 21:41:36,008][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000002168_1114112.pth
[36m[2025-07-01 21:41:40,974][166323] Fps is (10 sec: 1638.1, 60 sec: 546.0, 300 sec: 333.2). Total num frames: 1196032. Throughput: 0: 299.1. Samples: 1196752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:41:40,975][166323] Avg episode reward: [(0, '239.577')]
[36m[2025-07-01 21:41:45,971][166323] Fps is (10 sec: 1634.1, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1196032. Throughput: 0: 299.3. Samples: 1198448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:41:45,972][166323] Avg episode reward: [(0, '247.154')]
[36m[2025-07-01 21:41:51,004][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 1196032. Throughput: 0: 301.0. Samples: 1199360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:41:51,005][166323] Avg episode reward: [(0, '229.888')]
[36m[2025-07-01 21:41:55,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1196032. Throughput: 0: 302.6. Samples: 1201248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:41:55,963][166323] Avg episode reward: [(0, '257.667')]
[36m[2025-07-01 21:42:00,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1196032. Throughput: 0: 305.5. Samples: 1203168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:42:00,963][166323] Avg episode reward: [(0, '255.780')]
[36m[2025-07-01 21:42:05,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1196032. Throughput: 0: 305.7. Samples: 1204144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:42:05,975][166323] Avg episode reward: [(0, '248.679')]
[36m[2025-07-01 21:42:10,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1196032. Throughput: 0: 303.5. Samples: 1205888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:42:10,987][166323] Avg episode reward: [(0, '234.200')]
[36m[2025-07-01 21:42:15,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1196032. Throughput: 0: 302.4. Samples: 1207616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:42:15,947][166323] Avg episode reward: [(0, '233.147')]
[36m[2025-07-01 21:42:20,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1196032. Throughput: 0: 300.5. Samples: 1208464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:42:20,983][166323] Avg episode reward: [(0, '234.422')]
[36m[2025-07-01 21:42:25,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1196032. Throughput: 0: 300.0. Samples: 1210256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:42:25,984][166323] Avg episode reward: [(0, '225.386')]
[36m[2025-07-01 21:42:30,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1196032. Throughput: 0: 301.7. Samples: 1212032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:42:30,991][166323] Avg episode reward: [(0, '230.585')]
[31m[4140069 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4140069 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[4140070 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:42:35,958][166323] Fps is (10 sec: 1642.6, 60 sec: 546.0, 300 sec: 333.2). Total num frames: 1212416. Throughput: 0: 299.0. Samples: 1212800. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 21:42:35,959][166323] Avg episode reward: [(0, '228.441')]
[36m[2025-07-01 21:42:40,978][166323] Fps is (10 sec: 1640.5, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 1212416. Throughput: 0: 295.7. Samples: 1214560. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 21:42:40,978][166323] Avg episode reward: [(0, '233.265')]
[36m[2025-07-01 21:42:45,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 1212416. Throughput: 0: 290.1. Samples: 1216224. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 21:42:45,969][166323] Avg episode reward: [(0, '226.695')]
[36m[2025-07-01 21:42:50,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 333.2). Total num frames: 1212416. Throughput: 0: 286.3. Samples: 1217024. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 21:42:50,959][166323] Avg episode reward: [(0, '253.152')]
[36m[2025-07-01 21:42:55,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1212416. Throughput: 0: 285.9. Samples: 1218752. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 21:42:55,984][166323] Avg episode reward: [(0, '233.674')]
[36m[2025-07-01 21:43:00,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1212416. Throughput: 0: 288.0. Samples: 1220576. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 21:43:00,955][166323] Avg episode reward: [(0, '222.741')]
[31m[4173598 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4173598 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[4173598 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:43:05,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1212416. Throughput: 0: 290.2. Samples: 1221520. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 21:43:05,971][166323] Avg episode reward: [(0, '215.172')]
[31m[4179104 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4179105 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[4179105 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:43:10,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1212416. Throughput: 0: 289.6. Samples: 1223280. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 21:43:10,957][166323] Avg episode reward: [(0, '197.024')]
[31m[4183739 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4183740 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[4183741 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:43:15,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1212416. Throughput: 0: 291.2. Samples: 1225136. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 21:43:15,989][166323] Avg episode reward: [(0, '218.178')]
[36m[2025-07-01 21:43:20,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1212416. Throughput: 0: 296.5. Samples: 1226144. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 21:43:20,960][166323] Avg episode reward: [(0, '200.932')]
[36m[2025-07-01 21:43:25,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1212416. Throughput: 0: 296.2. Samples: 1227888. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 21:43:25,975][166323] Avg episode reward: [(0, '205.903')]
[36m[2025-07-01 21:43:30,956][166323] Fps is (10 sec: 1639.2, 60 sec: 546.4, 300 sec: 333.3). Total num frames: 1228800. Throughput: 0: 293.4. Samples: 1229424. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 21:43:30,956][166323] Avg episode reward: [(0, '206.459')]
[36m[2025-07-01 21:43:35,948][166323] Fps is (10 sec: 1642.8, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 1228800. Throughput: 0: 296.6. Samples: 1230368. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 21:43:35,948][166323] Avg episode reward: [(0, '204.822')]
[37m[1m[2025-07-01 21:43:36,031][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000002392_1228800.pth...
[36m[2025-07-01 21:43:36,036][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000002232_1146880.pth
[36m[2025-07-01 21:43:40,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 1228800. Throughput: 0: 295.6. Samples: 1232048. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 21:43:40,969][166323] Avg episode reward: [(0, '202.850')]
[36m[2025-07-01 21:43:45,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 1228800. Throughput: 0: 292.7. Samples: 1233744. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 21:43:45,946][166323] Avg episode reward: [(0, '217.286')]
[36m[2025-07-01 21:43:50,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1228800. Throughput: 0: 293.5. Samples: 1234736. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 21:43:50,996][166323] Avg episode reward: [(0, '229.847')]
[36m[2025-07-01 21:43:55,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1228800. Throughput: 0: 297.1. Samples: 1236656. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 21:43:55,981][166323] Avg episode reward: [(0, '217.348')]
[36m[2025-07-01 21:44:00,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1228800. Throughput: 0: 296.2. Samples: 1238464. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 21:44:00,985][166323] Avg episode reward: [(0, '219.501')]
[33m[4229762 ms][navigation_task] - WARNING : Curriculum Level: 38, Curriculum progress fraction: 0.14285714285714285 (navigation_task.py:262)
[33m[4229763 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.7294921875
[33mCrash Rate: 0.2236328125
[33mTimeout Rate: 0.046875 (navigation_task.py:265)
[33m[4229763 ms][navigation_task] - WARNING : 
[33mSuccesses: 1494
[33mCrashes : 458
[33mTimeouts: 96 (navigation_task.py:268)
[36m[2025-07-01 21:44:05,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1228800. Throughput: 0: 292.5. Samples: 1239312. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 21:44:05,984][166323] Avg episode reward: [(0, '233.279')]
[36m[2025-07-01 21:44:10,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1228800. Throughput: 0: 293.4. Samples: 1241088. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 21:44:10,958][166323] Avg episode reward: [(0, '251.501')]
[36m[2025-07-01 21:44:15,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1228800. Throughput: 0: 300.7. Samples: 1242960. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 21:44:15,977][166323] Avg episode reward: [(0, '247.393')]
[36m[2025-07-01 21:44:20,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1228800. Throughput: 0: 298.9. Samples: 1243824. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 21:44:20,959][166323] Avg episode reward: [(0, '263.095')]
[36m[2025-07-01 21:44:25,953][166323] Fps is (10 sec: 1642.3, 60 sec: 546.3, 300 sec: 333.2). Total num frames: 1245184. Throughput: 0: 303.4. Samples: 1245696. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 21:44:25,953][166323] Avg episode reward: [(0, '259.427')]
[36m[2025-07-01 21:44:30,970][166323] Fps is (10 sec: 1636.6, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1245184. Throughput: 0: 303.1. Samples: 1247392. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 21:44:30,970][166323] Avg episode reward: [(0, '260.803')]
[36m[2025-07-01 21:44:35,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1245184. Throughput: 0: 299.6. Samples: 1248208. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 21:44:35,968][166323] Avg episode reward: [(0, '253.184')]
[36m[2025-07-01 21:44:40,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1245184. Throughput: 0: 294.4. Samples: 1249904. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 21:44:40,976][166323] Avg episode reward: [(0, '253.300')]
[36m[2025-07-01 21:44:45,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1245184. Throughput: 0: 294.9. Samples: 1251728. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 21:44:45,963][166323] Avg episode reward: [(0, '266.424')]
[31m[4274527 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4274527 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[4274527 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:44:50,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1245184. Throughput: 0: 296.2. Samples: 1252640. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 21:44:50,985][166323] Avg episode reward: [(0, '252.666')]
[36m[2025-07-01 21:44:55,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1245184. Throughput: 0: 299.1. Samples: 1254544. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 21:44:55,952][166323] Avg episode reward: [(0, '267.457')]
[36m[2025-07-01 21:45:00,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1245184. Throughput: 0: 297.8. Samples: 1256352. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 21:45:00,947][166323] Avg episode reward: [(0, '291.017')]
[36m[2025-07-01 21:45:05,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1245184. Throughput: 0: 298.1. Samples: 1257232. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 21:45:05,945][166323] Avg episode reward: [(0, '313.142')]
[36m[2025-07-01 21:45:10,997][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1245184. Throughput: 0: 296.6. Samples: 1259056. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 21:45:10,997][166323] Avg episode reward: [(0, '320.153')]
[36m[2025-07-01 21:45:15,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1245184. Throughput: 0: 299.3. Samples: 1260864. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 21:45:15,980][166323] Avg episode reward: [(0, '301.430')]
[36m[2025-07-01 21:45:20,983][166323] Fps is (10 sec: 1640.6, 60 sec: 545.9, 300 sec: 333.2). Total num frames: 1261568. Throughput: 0: 301.4. Samples: 1261776. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 21:45:20,983][166323] Avg episode reward: [(0, '326.385')]
[31m[4313406 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4313406 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[4313406 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:45:25,969][166323] Fps is (10 sec: 1640.2, 60 sec: 273.0, 300 sec: 333.3). Total num frames: 1261568. Throughput: 0: 304.0. Samples: 1263584. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 21:45:25,969][166323] Avg episode reward: [(0, '337.117')]
[36m[2025-07-01 21:45:30,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1261568. Throughput: 0: 305.3. Samples: 1265472. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 21:45:30,983][166323] Avg episode reward: [(0, '337.874')]
[36m[2025-07-01 21:45:35,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 1261568. Throughput: 0: 305.6. Samples: 1266384. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 21:45:35,961][166323] Avg episode reward: [(0, '337.311')]
[37m[1m[2025-07-01 21:45:36,013][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000002456_1261568.pth...
[36m[2025-07-01 21:45:36,017][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000002296_1179648.pth
[36m[2025-07-01 21:45:40,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1261568. Throughput: 0: 302.0. Samples: 1268144. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 21:45:40,982][166323] Avg episode reward: [(0, '321.607')]
[31m[4334110 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4334110 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[4334111 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:45:45,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1261568. Throughput: 0: 303.7. Samples: 1270032. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 21:45:45,986][166323] Avg episode reward: [(0, '323.402')]
[36m[2025-07-01 21:45:50,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1261568. Throughput: 0: 303.3. Samples: 1270896. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 21:45:50,998][166323] Avg episode reward: [(0, '304.077')]
[36m[2025-07-01 21:45:55,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1261568. Throughput: 0: 301.9. Samples: 1272640. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 21:45:55,990][166323] Avg episode reward: [(0, '306.648')]
[36m[2025-07-01 21:46:00,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1261568. Throughput: 0: 300.9. Samples: 1274400. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 21:46:00,967][166323] Avg episode reward: [(0, '299.756')]
[36m[2025-07-01 21:46:05,973][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1261568. Throughput: 0: 298.7. Samples: 1275216. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 21:46:05,974][166323] Avg episode reward: [(0, '304.632')]
[36m[2025-07-01 21:46:10,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1261568. Throughput: 0: 300.1. Samples: 1277088. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 21:46:10,964][166323] Avg episode reward: [(0, '294.602')]
[36m[2025-07-01 21:46:15,958][166323] Fps is (10 sec: 1641.0, 60 sec: 546.3, 300 sec: 333.2). Total num frames: 1277952. Throughput: 0: 297.4. Samples: 1278848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:46:15,958][166323] Avg episode reward: [(0, '281.512')]
[36m[2025-07-01 21:46:20,946][166323] Fps is (10 sec: 1641.4, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 1277952. Throughput: 0: 298.1. Samples: 1279792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:46:20,946][166323] Avg episode reward: [(0, '279.536')]
[36m[2025-07-01 21:46:26,004][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 1277952. Throughput: 0: 296.7. Samples: 1281504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:46:26,004][166323] Avg episode reward: [(0, '290.096')]
[36m[2025-07-01 21:46:30,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 1277952. Throughput: 0: 296.5. Samples: 1283376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:46:30,984][166323] Avg episode reward: [(0, '281.320')]
[36m[2025-07-01 21:46:35,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1277952. Throughput: 0: 297.0. Samples: 1284256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:46:35,983][166323] Avg episode reward: [(0, '282.884')]
[36m[2025-07-01 21:46:40,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1277952. Throughput: 0: 297.5. Samples: 1286016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:46:40,957][166323] Avg episode reward: [(0, '304.167')]
[36m[2025-07-01 21:46:45,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1277952. Throughput: 0: 299.4. Samples: 1287872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:46:45,960][166323] Avg episode reward: [(0, '306.188')]
[36m[2025-07-01 21:46:50,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1277952. Throughput: 0: 301.8. Samples: 1288800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:46:50,990][166323] Avg episode reward: [(0, '320.281')]
[36m[2025-07-01 21:46:55,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1277952. Throughput: 0: 300.8. Samples: 1290624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:46:55,962][166323] Avg episode reward: [(0, '322.695')]
[36m[2025-07-01 21:47:00,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1277952. Throughput: 0: 299.3. Samples: 1292320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:47:00,970][166323] Avg episode reward: [(0, '303.290')]
[36m[2025-07-01 21:47:05,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1277952. Throughput: 0: 296.8. Samples: 1293152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:47:05,963][166323] Avg episode reward: [(0, '312.844')]
[36m[2025-07-01 21:47:10,996][166323] Fps is (10 sec: 1634.2, 60 sec: 545.8, 300 sec: 333.2). Total num frames: 1294336. Throughput: 0: 296.6. Samples: 1294848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:47:10,996][166323] Avg episode reward: [(0, '305.618')]
[31m[4420326 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4420326 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[4420326 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:47:15,968][166323] Fps is (10 sec: 1637.5, 60 sec: 273.0, 300 sec: 333.3). Total num frames: 1294336. Throughput: 0: 296.6. Samples: 1296720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:47:15,968][166323] Avg episode reward: [(0, '309.735')]
[36m[2025-07-01 21:47:20,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 1294336. Throughput: 0: 294.7. Samples: 1297504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:47:20,943][166323] Avg episode reward: [(0, '301.467')]
[31m[4431438 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4431438 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[4431439 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:47:25,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 333.3). Total num frames: 1294336. Throughput: 0: 295.8. Samples: 1299328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:47:25,958][166323] Avg episode reward: [(0, '305.988')]
[36m[2025-07-01 21:47:30,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1294336. Throughput: 0: 293.0. Samples: 1301056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:47:30,954][166323] Avg episode reward: [(0, '328.385')]
[36m[2025-07-01 21:47:35,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1294336. Throughput: 0: 292.1. Samples: 1301936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:47:35,953][166323] Avg episode reward: [(0, '321.272')]
[37m[1m[2025-07-01 21:47:36,011][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000002520_1294336.pth...
[36m[2025-07-01 21:47:36,015][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000002392_1228800.pth
[36m[2025-07-01 21:47:40,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1294336. Throughput: 0: 290.0. Samples: 1303680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:47:40,985][166323] Avg episode reward: [(0, '317.739')]
[36m[2025-07-01 21:47:45,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1294336. Throughput: 0: 294.3. Samples: 1305568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:47:45,983][166323] Avg episode reward: [(0, '308.209')]
[36m[2025-07-01 21:47:50,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1294336. Throughput: 0: 297.1. Samples: 1306528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:47:50,978][166323] Avg episode reward: [(0, '307.864')]
[36m[2025-07-01 21:47:55,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1294336. Throughput: 0: 300.6. Samples: 1308368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:47:55,978][166323] Avg episode reward: [(0, '322.663')]
[36m[2025-07-01 21:48:00,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1294336. Throughput: 0: 297.2. Samples: 1310096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:48:00,976][166323] Avg episode reward: [(0, '308.932')]
[36m[2025-07-01 21:48:05,992][166323] Fps is (10 sec: 1636.2, 60 sec: 545.9, 300 sec: 333.2). Total num frames: 1310720. Throughput: 0: 298.3. Samples: 1310944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:48:05,992][166323] Avg episode reward: [(0, '283.807')]
[36m[2025-07-01 21:48:10,982][166323] Fps is (10 sec: 1637.3, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 1310720. Throughput: 0: 296.7. Samples: 1312688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:48:10,982][166323] Avg episode reward: [(0, '282.247')]
[36m[2025-07-01 21:48:15,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1310720. Throughput: 0: 299.1. Samples: 1314528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:48:15,993][166323] Avg episode reward: [(0, '270.666')]
[36m[2025-07-01 21:48:20,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.3). Total num frames: 1310720. Throughput: 0: 300.4. Samples: 1315456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:48:20,958][166323] Avg episode reward: [(0, '284.544')]
[36m[2025-07-01 21:48:25,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1310720. Throughput: 0: 299.4. Samples: 1317152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:48:25,985][166323] Avg episode reward: [(0, '273.974')]
[36m[2025-07-01 21:48:30,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1310720. Throughput: 0: 296.7. Samples: 1318912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:48:30,964][166323] Avg episode reward: [(0, '297.219')]
[36m[2025-07-01 21:48:35,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1310720. Throughput: 0: 294.5. Samples: 1319776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:48:35,968][166323] Avg episode reward: [(0, '297.414')]
[36m[2025-07-01 21:48:40,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1310720. Throughput: 0: 291.9. Samples: 1321504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:48:40,981][166323] Avg episode reward: [(0, '294.767')]
[31m[4512264 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4512264 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[4512264 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:48:45,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1310720. Throughput: 0: 291.0. Samples: 1323184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:48:45,959][166323] Avg episode reward: [(0, '312.594')]
[36m[2025-07-01 21:48:50,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1310720. Throughput: 0: 293.4. Samples: 1324144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:48:50,974][166323] Avg episode reward: [(0, '309.987')]
[36m[2025-07-01 21:48:55,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1310720. Throughput: 0: 294.4. Samples: 1325936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:48:55,979][166323] Avg episode reward: [(0, '313.729')]
[36m[2025-07-01 21:49:00,971][166323] Fps is (10 sec: 1638.9, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 1327104. Throughput: 0: 294.2. Samples: 1327760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:49:00,971][166323] Avg episode reward: [(0, '315.366')]
[36m[2025-07-01 21:49:05,958][166323] Fps is (10 sec: 1641.7, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 1327104. Throughput: 0: 295.5. Samples: 1328752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:49:05,958][166323] Avg episode reward: [(0, '340.146')]
[36m[2025-07-01 21:49:10,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 1327104. Throughput: 0: 298.9. Samples: 1330592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:49:10,954][166323] Avg episode reward: [(0, '342.906')]
[36m[2025-07-01 21:49:15,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 1327104. Throughput: 0: 297.6. Samples: 1332304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:49:15,962][166323] Avg episode reward: [(0, '324.895')]
[36m[2025-07-01 21:49:21,009][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 1327104. Throughput: 0: 296.6. Samples: 1333136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:49:21,009][166323] Avg episode reward: [(0, '344.917')]
[36m[2025-07-01 21:49:25,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1327104. Throughput: 0: 296.9. Samples: 1334864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:49:25,986][166323] Avg episode reward: [(0, '322.152')]
[36m[2025-07-01 21:49:30,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1327104. Throughput: 0: 298.7. Samples: 1336624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:49:30,951][166323] Avg episode reward: [(0, '324.240')]
[36m[2025-07-01 21:49:35,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1327104. Throughput: 0: 296.0. Samples: 1337456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:49:35,955][166323] Avg episode reward: [(0, '301.967')]
[37m[1m[2025-07-01 21:49:36,006][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000002584_1327104.pth...
[36m[2025-07-01 21:49:36,009][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000002456_1261568.pth
[36m[2025-07-01 21:49:40,997][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1327104. Throughput: 0: 293.2. Samples: 1339136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:49:40,997][166323] Avg episode reward: [(0, '323.586')]
[36m[2025-07-01 21:49:45,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1327104. Throughput: 0: 291.8. Samples: 1340896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:49:45,987][166323] Avg episode reward: [(0, '320.401')]
[36m[2025-07-01 21:49:50,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1327104. Throughput: 0: 290.1. Samples: 1341808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:49:50,969][166323] Avg episode reward: [(0, '315.542')]
[36m[2025-07-01 21:49:55,969][166323] Fps is (10 sec: 1641.5, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 1343488. Throughput: 0: 289.0. Samples: 1343600. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 21:49:55,969][166323] Avg episode reward: [(0, '318.203')]
[36m[2025-07-01 21:50:01,010][166323] Fps is (10 sec: 1631.6, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 1343488. Throughput: 0: 291.2. Samples: 1345424. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 21:50:01,011][166323] Avg episode reward: [(0, '312.109')]
[36m[2025-07-01 21:50:05,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.3). Total num frames: 1343488. Throughput: 0: 292.5. Samples: 1346288. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 21:50:05,973][166323] Avg episode reward: [(0, '333.721')]
[36m[2025-07-01 21:50:10,980][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 1343488. Throughput: 0: 294.4. Samples: 1348112. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 21:50:10,980][166323] Avg episode reward: [(0, '321.102')]
[36m[2025-07-01 21:50:15,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1343488. Throughput: 0: 296.5. Samples: 1349968. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 21:50:15,958][166323] Avg episode reward: [(0, '339.730')]
[36m[2025-07-01 21:50:20,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1343488. Throughput: 0: 297.1. Samples: 1350832. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 21:50:20,976][166323] Avg episode reward: [(0, '334.795')]
[36m[2025-07-01 21:50:25,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1343488. Throughput: 0: 296.5. Samples: 1352480. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 21:50:25,999][166323] Avg episode reward: [(0, '323.402')]
[31m[4617301 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4617301 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[4617302 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:50:30,977][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1343488. Throughput: 0: 293.8. Samples: 1354112. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 21:50:30,978][166323] Avg episode reward: [(0, '309.688')]
[36m[2025-07-01 21:50:35,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1343488. Throughput: 0: 292.4. Samples: 1354960. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 21:50:35,956][166323] Avg episode reward: [(0, '309.193')]
[36m[2025-07-01 21:50:40,997][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1343488. Throughput: 0: 291.0. Samples: 1356704. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 21:50:40,998][166323] Avg episode reward: [(0, '325.408')]
[36m[2025-07-01 21:50:45,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1343488. Throughput: 0: 287.9. Samples: 1358368. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 21:50:45,966][166323] Avg episode reward: [(0, '306.981')]
[36m[2025-07-01 21:50:50,993][166323] Fps is (10 sec: 1639.2, 60 sec: 545.9, 300 sec: 333.2). Total num frames: 1359872. Throughput: 0: 288.9. Samples: 1359296. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 21:50:50,993][166323] Avg episode reward: [(0, '303.097')]
[36m[2025-07-01 21:50:55,981][166323] Fps is (10 sec: 1636.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1359872. Throughput: 0: 289.8. Samples: 1361152. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 21:50:55,981][166323] Avg episode reward: [(0, '293.187')]
[36m[2025-07-01 21:51:01,016][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1359872. Throughput: 0: 287.3. Samples: 1362912. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 21:51:01,016][166323] Avg episode reward: [(0, '323.032')]
[36m[2025-07-01 21:51:05,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1359872. Throughput: 0: 287.6. Samples: 1363776. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 21:51:05,981][166323] Avg episode reward: [(0, '324.829')]
[36m[2025-07-01 21:51:10,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1359872. Throughput: 0: 293.4. Samples: 1365680. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 21:51:10,983][166323] Avg episode reward: [(0, '347.551')]
[36m[2025-07-01 21:51:15,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1359872. Throughput: 0: 300.6. Samples: 1367632. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 21:51:15,951][166323] Avg episode reward: [(0, '334.477')]
[36m[2025-07-01 21:51:20,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1359872. Throughput: 0: 301.2. Samples: 1368512. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 21:51:20,949][166323] Avg episode reward: [(0, '360.619')]
[36m[2025-07-01 21:51:25,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1359872. Throughput: 0: 300.3. Samples: 1370208. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 21:51:25,968][166323] Avg episode reward: [(0, '349.698')]
[36m[2025-07-01 21:51:30,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1359872. Throughput: 0: 304.0. Samples: 1372048. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 21:51:30,959][166323] Avg episode reward: [(0, '324.498')]
[36m[2025-07-01 21:51:35,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1359872. Throughput: 0: 305.7. Samples: 1373040. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 21:51:35,951][166323] Avg episode reward: [(0, '343.357')]
[37m[1m[2025-07-01 21:51:36,045][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000002648_1359872.pth...
[36m[2025-07-01 21:51:36,051][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000002520_1294336.pth
[36m[2025-07-01 21:51:40,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1359872. Throughput: 0: 302.0. Samples: 1374736. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 21:51:40,966][166323] Avg episode reward: [(0, '319.665')]
[36m[2025-07-01 21:51:45,992][166323] Fps is (10 sec: 1631.7, 60 sec: 545.9, 300 sec: 333.2). Total num frames: 1376256. Throughput: 0: 302.4. Samples: 1376512. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 21:51:45,992][166323] Avg episode reward: [(0, '327.092')]
[36m[2025-07-01 21:51:50,972][166323] Fps is (10 sec: 1637.4, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 1376256. Throughput: 0: 300.9. Samples: 1377312. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 21:51:50,972][166323] Avg episode reward: [(0, '303.473')]
[36m[2025-07-01 21:51:55,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1376256. Throughput: 0: 295.7. Samples: 1378992. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 21:51:55,998][166323] Avg episode reward: [(0, '310.338')]
[36m[2025-07-01 21:52:00,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 333.2). Total num frames: 1376256. Throughput: 0: 291.1. Samples: 1380736. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 21:52:00,966][166323] Avg episode reward: [(0, '309.600')]
[36m[2025-07-01 21:52:05,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1376256. Throughput: 0: 293.6. Samples: 1381728. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 21:52:05,970][166323] Avg episode reward: [(0, '313.037')]
[31m[4719094 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4719095 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[4719095 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:52:10,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1376256. Throughput: 0: 295.5. Samples: 1383504. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 21:52:10,967][166323] Avg episode reward: [(0, '306.555')]
[36m[2025-07-01 21:52:15,979][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1376256. Throughput: 0: 295.3. Samples: 1385344. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 21:52:15,979][166323] Avg episode reward: [(0, '287.057')]
[36m[2025-07-01 21:52:20,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1376256. Throughput: 0: 293.0. Samples: 1386224. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 21:52:20,951][166323] Avg episode reward: [(0, '302.814')]
[36m[2025-07-01 21:52:25,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1376256. Throughput: 0: 293.2. Samples: 1387936. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 21:52:25,993][166323] Avg episode reward: [(0, '295.654')]
[36m[2025-07-01 21:52:30,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1376256. Throughput: 0: 291.9. Samples: 1389632. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 21:52:30,946][166323] Avg episode reward: [(0, '299.290')]
[36m[2025-07-01 21:52:35,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1376256. Throughput: 0: 293.1. Samples: 1390496. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 21:52:35,961][166323] Avg episode reward: [(0, '286.905')]
[36m[2025-07-01 21:52:41,118][166323] Fps is (10 sec: 1610.6, 60 sec: 544.8, 300 sec: 333.1). Total num frames: 1392640. Throughput: 0: 293.6. Samples: 1392240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:52:41,118][166323] Avg episode reward: [(0, '301.261')]
[36m[2025-07-01 21:52:45,997][166323] Fps is (10 sec: 1632.5, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1392640. Throughput: 0: 293.8. Samples: 1393968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:52:45,997][166323] Avg episode reward: [(0, '310.415')]
[36m[2025-07-01 21:52:50,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1392640. Throughput: 0: 291.1. Samples: 1394832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:52:50,981][166323] Avg episode reward: [(0, '303.981')]
[31m[4759788 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4759788 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[4759788 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:52:55,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 1392640. Throughput: 0: 292.2. Samples: 1396656. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:52:55,973][166323] Avg episode reward: [(0, '300.650')]
[31m[4769163 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4769163 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[4769163 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:53:00,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1392640. Throughput: 0: 293.3. Samples: 1398544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:53:00,978][166323] Avg episode reward: [(0, '287.384')]
[36m[2025-07-01 21:53:05,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1392640. Throughput: 0: 294.8. Samples: 1399488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:53:05,943][166323] Avg episode reward: [(0, '294.843')]
[36m[2025-07-01 21:53:10,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1392640. Throughput: 0: 297.6. Samples: 1401328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:53:10,994][166323] Avg episode reward: [(0, '267.113')]
[36m[2025-07-01 21:53:15,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1392640. Throughput: 0: 300.4. Samples: 1403152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:53:15,952][166323] Avg episode reward: [(0, '274.754')]
[36m[2025-07-01 21:53:20,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1392640. Throughput: 0: 299.8. Samples: 1403984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:53:20,948][166323] Avg episode reward: [(0, '285.264')]
[31m[4791194 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4791194 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[4791194 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:53:25,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1392640. Throughput: 0: 301.2. Samples: 1405744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:53:25,957][166323] Avg episode reward: [(0, '258.590')]
[36m[2025-07-01 21:53:30,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1392640. Throughput: 0: 298.7. Samples: 1407408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:53:30,992][166323] Avg episode reward: [(0, '265.839')]
[36m[2025-07-01 21:53:35,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1392640. Throughput: 0: 298.9. Samples: 1408272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:53:35,947][166323] Avg episode reward: [(0, '279.634')]
[37m[1m[2025-07-01 21:53:36,039][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000002712_1392640.pth...
[36m[2025-07-01 21:53:36,047][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000002584_1327104.pth
[36m[2025-07-01 21:53:40,957][166323] Fps is (10 sec: 1644.2, 60 sec: 273.8, 300 sec: 333.2). Total num frames: 1409024. Throughput: 0: 294.9. Samples: 1409920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:53:40,957][166323] Avg episode reward: [(0, '259.316')]
[36m[2025-07-01 21:53:45,962][166323] Fps is (10 sec: 1635.8, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 1409024. Throughput: 0: 294.9. Samples: 1411808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:53:45,963][166323] Avg episode reward: [(0, '298.799')]
[36m[2025-07-01 21:53:50,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 1409024. Throughput: 0: 293.9. Samples: 1412720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:53:50,963][166323] Avg episode reward: [(0, '308.218')]
[36m[2025-07-01 21:53:55,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1409024. Throughput: 0: 296.7. Samples: 1414672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:53:55,965][166323] Avg episode reward: [(0, '327.256')]
[36m[2025-07-01 21:54:00,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1409024. Throughput: 0: 294.0. Samples: 1416384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:54:00,959][166323] Avg episode reward: [(0, '337.249')]
[36m[2025-07-01 21:54:05,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1409024. Throughput: 0: 296.5. Samples: 1417328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:54:05,952][166323] Avg episode reward: [(0, '338.537')]
[36m[2025-07-01 21:54:10,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 1409024. Throughput: 0: 295.9. Samples: 1419056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:54:10,946][166323] Avg episode reward: [(0, '365.353')]
[31m[4844364 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4844364 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[4844364 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:54:16,016][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 1409024. Throughput: 0: 299.2. Samples: 1420880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:54:16,017][166323] Avg episode reward: [(0, '370.019')]
[36m[2025-07-01 21:54:20,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1409024. Throughput: 0: 298.7. Samples: 1421728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:54:20,994][166323] Avg episode reward: [(0, '371.608')]
[33m[4850487 ms][navigation_task] - WARNING : Curriculum Level: 40, Curriculum progress fraction: 0.2857142857142857 (navigation_task.py:262)
[33m[4850488 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.763671875
[33mCrash Rate: 0.2001953125
[33mTimeout Rate: 0.0361328125 (navigation_task.py:265)
[33m[4850488 ms][navigation_task] - WARNING : 
[33mSuccesses: 1564
[33mCrashes : 410
[33mTimeouts: 74 (navigation_task.py:268)
[36m[2025-07-01 21:54:25,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1409024. Throughput: 0: 299.9. Samples: 1423424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:54:25,988][166323] Avg episode reward: [(0, '373.428')]
[36m[2025-07-01 21:54:30,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 1409024. Throughput: 0: 294.5. Samples: 1425056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:54:30,944][166323] Avg episode reward: [(0, '373.317')]
[36m[2025-07-01 21:54:35,969][166323] Fps is (10 sec: 1641.4, 60 sec: 545.9, 300 sec: 333.3). Total num frames: 1425408. Throughput: 0: 291.9. Samples: 1425856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:54:35,970][166323] Avg episode reward: [(0, '375.484')]
[36m[2025-07-01 21:54:40,943][166323] Fps is (10 sec: 1638.5, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 1425408. Throughput: 0: 287.1. Samples: 1427584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:54:40,943][166323] Avg episode reward: [(0, '361.600')]
[36m[2025-07-01 21:54:45,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1425408. Throughput: 0: 292.2. Samples: 1429536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:54:45,973][166323] Avg episode reward: [(0, '359.033')]
[36m[2025-07-01 21:54:50,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1425408. Throughput: 0: 291.4. Samples: 1430448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:54:50,974][166323] Avg episode reward: [(0, '361.937')]
[36m[2025-07-01 21:54:55,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1425408. Throughput: 0: 293.5. Samples: 1432272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:54:55,970][166323] Avg episode reward: [(0, '370.400')]
[36m[2025-07-01 21:55:00,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1425408. Throughput: 0: 295.8. Samples: 1434176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:55:00,962][166323] Avg episode reward: [(0, '348.659')]
[36m[2025-07-01 21:55:05,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1425408. Throughput: 0: 298.9. Samples: 1435168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:55:05,961][166323] Avg episode reward: [(0, '356.815')]
[36m[2025-07-01 21:55:10,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1425408. Throughput: 0: 302.5. Samples: 1437024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:55:10,954][166323] Avg episode reward: [(0, '348.091')]
[36m[2025-07-01 21:55:15,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 1425408. Throughput: 0: 304.1. Samples: 1438752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:55:15,976][166323] Avg episode reward: [(0, '345.023')]
[36m[2025-07-01 21:55:20,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1425408. Throughput: 0: 306.8. Samples: 1439664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:55:20,978][166323] Avg episode reward: [(0, '345.262')]
[31m[4914366 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4914366 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[4914366 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[4914415 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4914415 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[4914415 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:55:25,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1425408. Throughput: 0: 306.4. Samples: 1441376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:55:25,953][166323] Avg episode reward: [(0, '327.819')]
[36m[2025-07-01 21:55:30,970][166323] Fps is (10 sec: 1639.7, 60 sec: 545.9, 300 sec: 333.2). Total num frames: 1441792. Throughput: 0: 302.2. Samples: 1443136. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 21:55:30,971][166323] Avg episode reward: [(0, '311.049')]
[36m[2025-07-01 21:55:35,989][166323] Fps is (10 sec: 1632.5, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1441792. Throughput: 0: 302.1. Samples: 1444048. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 21:55:35,989][166323] Avg episode reward: [(0, '330.992')]
[37m[1m[2025-07-01 21:55:36,062][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000002808_1441792.pth...
[36m[2025-07-01 21:55:36,066][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000002648_1359872.pth
[36m[2025-07-01 21:55:40,978][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 1441792. Throughput: 0: 301.8. Samples: 1445856. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 21:55:40,979][166323] Avg episode reward: [(0, '322.006')]
[36m[2025-07-01 21:55:45,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1441792. Throughput: 0: 299.6. Samples: 1447664. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 21:55:45,988][166323] Avg episode reward: [(0, '356.358')]
[36m[2025-07-01 21:55:50,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1441792. Throughput: 0: 299.0. Samples: 1448624. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 21:55:50,963][166323] Avg episode reward: [(0, '374.381')]
[31m[4944053 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4944054 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[4944054 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:55:55,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1441792. Throughput: 0: 299.6. Samples: 1450512. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 21:55:55,968][166323] Avg episode reward: [(0, '373.570')]
[36m[2025-07-01 21:56:00,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1441792. Throughput: 0: 302.7. Samples: 1452368. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 21:56:00,953][166323] Avg episode reward: [(0, '395.403')]
[36m[2025-07-01 21:56:05,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1441792. Throughput: 0: 301.3. Samples: 1453216. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 21:56:05,959][166323] Avg episode reward: [(0, '375.720')]
[36m[2025-07-01 21:56:10,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1441792. Throughput: 0: 301.5. Samples: 1454944. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 21:56:10,956][166323] Avg episode reward: [(0, '364.775')]
[36m[2025-07-01 21:56:15,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1441792. Throughput: 0: 300.6. Samples: 1456656. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 21:56:15,950][166323] Avg episode reward: [(0, '346.011')]
[36m[2025-07-01 21:56:20,961][166323] Fps is (10 sec: 1637.6, 60 sec: 546.3, 300 sec: 333.2). Total num frames: 1458176. Throughput: 0: 301.0. Samples: 1457584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:56:20,961][166323] Avg episode reward: [(0, '332.505')]
[36m[2025-07-01 21:56:26,013][166323] Fps is (10 sec: 1628.1, 60 sec: 545.6, 300 sec: 333.2). Total num frames: 1458176. Throughput: 0: 297.4. Samples: 1459248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:56:26,013][166323] Avg episode reward: [(0, '356.433')]
[36m[2025-07-01 21:56:30,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 1458176. Throughput: 0: 295.4. Samples: 1460944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:56:30,945][166323] Avg episode reward: [(0, '325.247')]
[36m[2025-07-01 21:56:35,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 1458176. Throughput: 0: 295.0. Samples: 1461904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:56:35,976][166323] Avg episode reward: [(0, '315.846')]
[36m[2025-07-01 21:56:40,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1458176. Throughput: 0: 293.6. Samples: 1463728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:56:40,981][166323] Avg episode reward: [(0, '321.999')]
[36m[2025-07-01 21:56:45,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1458176. Throughput: 0: 293.2. Samples: 1465568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:56:45,977][166323] Avg episode reward: [(0, '325.809')]
[36m[2025-07-01 21:56:50,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1458176. Throughput: 0: 294.7. Samples: 1466480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:56:50,974][166323] Avg episode reward: [(0, '323.135')]
[36m[2025-07-01 21:56:55,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1458176. Throughput: 0: 294.7. Samples: 1468208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:56:55,957][166323] Avg episode reward: [(0, '318.354')]
[36m[2025-07-01 21:57:00,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1458176. Throughput: 0: 292.6. Samples: 1469824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:57:00,958][166323] Avg episode reward: [(0, '333.431')]
[36m[2025-07-01 21:57:05,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1458176. Throughput: 0: 293.1. Samples: 1470784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:57:05,996][166323] Avg episode reward: [(0, '348.901')]
[31m[5017696 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5017697 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[5017697 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[5017756 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5017757 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[5017757 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:57:10,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1458176. Throughput: 0: 295.8. Samples: 1472544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:57:10,962][166323] Avg episode reward: [(0, '338.194')]
[36m[2025-07-01 21:57:16,002][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 1458176. Throughput: 0: 296.2. Samples: 1474288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:57:16,002][166323] Avg episode reward: [(0, '358.057')]
[36m[2025-07-01 21:57:20,958][166323] Fps is (10 sec: 1639.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 1474560. Throughput: 0: 293.8. Samples: 1475120. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-01 21:57:20,958][166323] Avg episode reward: [(0, '368.640')]
[31m[5033542 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5033543 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[5033543 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:57:25,949][166323] Fps is (10 sec: 1647.1, 60 sec: 273.4, 300 sec: 333.2). Total num frames: 1474560. Throughput: 0: 291.4. Samples: 1476832. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-01 21:57:25,949][166323] Avg episode reward: [(0, '357.972')]
[36m[2025-07-01 21:57:30,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 1474560. Throughput: 0: 288.0. Samples: 1478528. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-01 21:57:30,981][166323] Avg episode reward: [(0, '371.074')]
[36m[2025-07-01 21:57:35,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 1474560. Throughput: 0: 287.4. Samples: 1479408. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-01 21:57:35,957][166323] Avg episode reward: [(0, '351.759')]
[37m[1m[2025-07-01 21:57:36,029][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000002872_1474560.pth...
[36m[2025-07-01 21:57:36,035][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000002712_1392640.pth
[36m[2025-07-01 21:57:40,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1474560. Throughput: 0: 287.4. Samples: 1481152. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-01 21:57:40,988][166323] Avg episode reward: [(0, '361.069')]
[36m[2025-07-01 21:57:45,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1474560. Throughput: 0: 291.9. Samples: 1482960. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-01 21:57:45,956][166323] Avg episode reward: [(0, '332.093')]
[36m[2025-07-01 21:57:50,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1474560. Throughput: 0: 289.1. Samples: 1483792. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-01 21:57:50,984][166323] Avg episode reward: [(0, '328.925')]
[36m[2025-07-01 21:57:55,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1474560. Throughput: 0: 286.9. Samples: 1485456. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-01 21:57:55,967][166323] Avg episode reward: [(0, '319.179')]
[36m[2025-07-01 21:58:01,010][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 1474560. Throughput: 0: 288.7. Samples: 1487280. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-01 21:58:01,011][166323] Avg episode reward: [(0, '304.823')]
[36m[2025-07-01 21:58:05,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1474560. Throughput: 0: 291.7. Samples: 1488256. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-01 21:58:05,988][166323] Avg episode reward: [(0, '331.843')]
[36m[2025-07-01 21:58:10,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1474560. Throughput: 0: 295.7. Samples: 1490144. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-01 21:58:10,968][166323] Avg episode reward: [(0, '334.677')]
[36m[2025-07-01 21:58:15,998][166323] Fps is (10 sec: 1636.7, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 1490944. Throughput: 0: 297.5. Samples: 1491920. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 21:58:15,998][166323] Avg episode reward: [(0, '382.419')]
[36m[2025-07-01 21:58:20,960][166323] Fps is (10 sec: 1639.6, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 1490944. Throughput: 0: 296.5. Samples: 1492752. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 21:58:20,961][166323] Avg episode reward: [(0, '373.623')]
[36m[2025-07-01 21:58:25,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.3). Total num frames: 1490944. Throughput: 0: 297.4. Samples: 1494528. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 21:58:25,959][166323] Avg episode reward: [(0, '386.894')]
[36m[2025-07-01 21:58:31,000][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1490944. Throughput: 0: 297.3. Samples: 1496352. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 21:58:31,000][166323] Avg episode reward: [(0, '407.395')]
[36m[2025-07-01 21:58:35,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1490944. Throughput: 0: 296.9. Samples: 1497152. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 21:58:35,987][166323] Avg episode reward: [(0, '382.712')]
[36m[2025-07-01 21:58:40,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1490944. Throughput: 0: 296.4. Samples: 1498800. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 21:58:40,985][166323] Avg episode reward: [(0, '380.834')]
[36m[2025-07-01 21:58:45,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1490944. Throughput: 0: 295.6. Samples: 1500576. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 21:58:45,994][166323] Avg episode reward: [(0, '358.534')]
[36m[2025-07-01 21:58:50,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1490944. Throughput: 0: 293.0. Samples: 1501440. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 21:58:50,979][166323] Avg episode reward: [(0, '374.256')]
[36m[2025-07-01 21:58:55,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1490944. Throughput: 0: 293.3. Samples: 1503344. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 21:58:55,979][166323] Avg episode reward: [(0, '379.878')]
[36m[2025-07-01 21:59:00,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 1490944. Throughput: 0: 293.2. Samples: 1505104. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 21:59:00,959][166323] Avg episode reward: [(0, '394.201')]
[36m[2025-07-01 21:59:05,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1490944. Throughput: 0: 295.0. Samples: 1506032. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 21:59:05,973][166323] Avg episode reward: [(0, '373.941')]
[36m[2025-07-01 21:59:10,970][166323] Fps is (10 sec: 1636.7, 60 sec: 546.1, 300 sec: 333.3). Total num frames: 1507328. Throughput: 0: 297.2. Samples: 1507904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:59:10,970][166323] Avg episode reward: [(0, '395.757')]
[36m[2025-07-01 21:59:15,977][166323] Fps is (10 sec: 1637.6, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 1507328. Throughput: 0: 297.0. Samples: 1509712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:59:15,978][166323] Avg episode reward: [(0, '380.310')]
[36m[2025-07-01 21:59:20,997][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 1507328. Throughput: 0: 299.0. Samples: 1510608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:59:20,997][166323] Avg episode reward: [(0, '363.453')]
[36m[2025-07-01 21:59:25,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1507328. Throughput: 0: 303.4. Samples: 1512448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:59:25,974][166323] Avg episode reward: [(0, '367.715')]
[31m[5159056 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5159056 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[5159056 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 21:59:30,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1507328. Throughput: 0: 303.7. Samples: 1514240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:59:30,983][166323] Avg episode reward: [(0, '371.381')]
[36m[2025-07-01 21:59:35,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 1507328. Throughput: 0: 302.8. Samples: 1515056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:59:35,944][166323] Avg episode reward: [(0, '428.633')]
[37m[1m[2025-07-01 21:59:35,992][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000002936_1507328.pth...
[36m[2025-07-01 21:59:35,996][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000002808_1441792.pth
[36m[2025-07-01 21:59:40,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1507328. Throughput: 0: 299.6. Samples: 1516816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:59:40,945][166323] Avg episode reward: [(0, '414.279')]
[36m[2025-07-01 21:59:45,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1507328. Throughput: 0: 300.0. Samples: 1518608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:59:45,975][166323] Avg episode reward: [(0, '448.257')]
[36m[2025-07-01 21:59:50,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1507328. Throughput: 0: 299.4. Samples: 1519504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:59:50,970][166323] Avg episode reward: [(0, '441.570')]
[36m[2025-07-01 21:59:56,006][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1507328. Throughput: 0: 295.2. Samples: 1521200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 21:59:56,007][166323] Avg episode reward: [(0, '432.734')]
[36m[2025-07-01 22:00:00,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1507328. Throughput: 0: 290.9. Samples: 1522800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:00:00,962][166323] Avg episode reward: [(0, '425.914')]
[36m[2025-07-01 22:00:05,979][166323] Fps is (10 sec: 1642.9, 60 sec: 546.1, 300 sec: 333.2). Total num frames: 1523712. Throughput: 0: 289.9. Samples: 1523648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:00:05,979][166323] Avg episode reward: [(0, '414.556')]
[36m[2025-07-01 22:00:10,954][166323] Fps is (10 sec: 1639.7, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 1523712. Throughput: 0: 287.8. Samples: 1525392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:00:10,954][166323] Avg episode reward: [(0, '406.198')]
[36m[2025-07-01 22:00:15,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1523712. Throughput: 0: 285.1. Samples: 1527072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:00:15,986][166323] Avg episode reward: [(0, '422.386')]
[36m[2025-07-01 22:00:20,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 333.2). Total num frames: 1523712. Throughput: 0: 287.3. Samples: 1527984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:00:20,945][166323] Avg episode reward: [(0, '387.844')]
[36m[2025-07-01 22:00:25,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1523712. Throughput: 0: 288.3. Samples: 1529792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:00:25,954][166323] Avg episode reward: [(0, '409.464')]
[36m[2025-07-01 22:00:30,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1523712. Throughput: 0: 289.5. Samples: 1531632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:00:30,958][166323] Avg episode reward: [(0, '398.611')]
[36m[2025-07-01 22:00:35,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1523712. Throughput: 0: 287.2. Samples: 1532432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:00:35,986][166323] Avg episode reward: [(0, '419.323')]
[36m[2025-07-01 22:00:40,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1523712. Throughput: 0: 290.4. Samples: 1534256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:00:40,960][166323] Avg episode reward: [(0, '430.117')]
[36m[2025-07-01 22:00:45,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1523712. Throughput: 0: 294.7. Samples: 1536064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:00:45,972][166323] Avg episode reward: [(0, '406.762')]
[36m[2025-07-01 22:00:50,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1523712. Throughput: 0: 298.1. Samples: 1537056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:00:50,950][166323] Avg episode reward: [(0, '424.150')]
[36m[2025-07-01 22:00:55,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 1523712. Throughput: 0: 299.0. Samples: 1538848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:00:55,964][166323] Avg episode reward: [(0, '429.757')]
[36m[2025-07-01 22:01:00,969][166323] Fps is (10 sec: 1635.3, 60 sec: 546.1, 300 sec: 333.2). Total num frames: 1540096. Throughput: 0: 300.6. Samples: 1540592. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-01 22:01:00,970][166323] Avg episode reward: [(0, '432.502')]
[36m[2025-07-01 22:01:05,952][166323] Fps is (10 sec: 1640.4, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 1540096. Throughput: 0: 299.7. Samples: 1541472. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-01 22:01:05,952][166323] Avg episode reward: [(0, '416.371')]
[36m[2025-07-01 22:01:10,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 1540096. Throughput: 0: 300.8. Samples: 1543328. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-01 22:01:10,954][166323] Avg episode reward: [(0, '389.033')]
[31m[5260678 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5260678 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[5260679 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:01:15,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1540096. Throughput: 0: 300.0. Samples: 1545136. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-01 22:01:15,975][166323] Avg episode reward: [(0, '380.415')]
[36m[2025-07-01 22:01:20,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1540096. Throughput: 0: 302.3. Samples: 1546032. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-01 22:01:20,970][166323] Avg episode reward: [(0, '354.463')]
[31m[5270705 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5270705 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[5270705 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:01:25,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1540096. Throughput: 0: 300.1. Samples: 1547760. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-01 22:01:25,959][166323] Avg episode reward: [(0, '341.844')]
[36m[2025-07-01 22:01:30,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1540096. Throughput: 0: 299.0. Samples: 1549520. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-01 22:01:30,974][166323] Avg episode reward: [(0, '338.950')]
[31m[5283016 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5283016 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[5283016 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:01:35,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1540096. Throughput: 0: 296.4. Samples: 1550400. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-01 22:01:35,967][166323] Avg episode reward: [(0, '359.293')]
[37m[1m[2025-07-01 22:01:36,015][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000003000_1540096.pth...
[36m[2025-07-01 22:01:36,019][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000002872_1474560.pth
[36m[2025-07-01 22:01:40,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1540096. Throughput: 0: 298.3. Samples: 1552272. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-01 22:01:40,967][166323] Avg episode reward: [(0, '375.144')]
[36m[2025-07-01 22:01:45,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1540096. Throughput: 0: 296.8. Samples: 1553952. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-01 22:01:45,982][166323] Avg episode reward: [(0, '392.447')]
[36m[2025-07-01 22:01:50,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1540096. Throughput: 0: 296.1. Samples: 1554800. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-01 22:01:50,969][166323] Avg episode reward: [(0, '414.796')]
[36m[2025-07-01 22:01:55,957][166323] Fps is (10 sec: 1642.5, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 1556480. Throughput: 0: 291.9. Samples: 1556464. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 22:01:55,957][166323] Avg episode reward: [(0, '455.455')]
[36m[2025-07-01 22:02:00,984][166323] Fps is (10 sec: 1636.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1556480. Throughput: 0: 286.5. Samples: 1558032. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 22:02:00,984][166323] Avg episode reward: [(0, '461.150')]
[31m[5310538 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5310538 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[5310538 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:02:05,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 1556480. Throughput: 0: 289.6. Samples: 1559056. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 22:02:05,944][166323] Avg episode reward: [(0, '452.925')]
[36m[2025-07-01 22:02:10,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.3). Total num frames: 1556480. Throughput: 0: 292.2. Samples: 1560912. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 22:02:10,972][166323] Avg episode reward: [(0, '392.800')]
[31m[5322294 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5322294 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[5322294 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:02:15,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1556480. Throughput: 0: 290.6. Samples: 1562592. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 22:02:15,951][166323] Avg episode reward: [(0, '402.370')]
[36m[2025-07-01 22:02:20,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1556480. Throughput: 0: 289.0. Samples: 1563408. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 22:02:20,970][166323] Avg episode reward: [(0, '383.306')]
[36m[2025-07-01 22:02:25,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1556480. Throughput: 0: 289.6. Samples: 1565312. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 22:02:25,990][166323] Avg episode reward: [(0, '363.999')]
[31m[5338286 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5338287 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[5338287 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:02:30,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1556480. Throughput: 0: 292.3. Samples: 1567104. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 22:02:30,979][166323] Avg episode reward: [(0, '329.104')]
[36m[2025-07-01 22:02:35,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1556480. Throughput: 0: 292.5. Samples: 1567968. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 22:02:35,991][166323] Avg episode reward: [(0, '338.347')]
[36m[2025-07-01 22:02:40,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1556480. Throughput: 0: 298.1. Samples: 1569888. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 22:02:40,984][166323] Avg episode reward: [(0, '360.882')]
[36m[2025-07-01 22:02:45,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1556480. Throughput: 0: 302.2. Samples: 1571632. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 22:02:45,983][166323] Avg episode reward: [(0, '351.098')]
[36m[2025-07-01 22:02:50,960][166323] Fps is (10 sec: 1642.3, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 1572864. Throughput: 0: 301.1. Samples: 1572608. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:02:50,960][166323] Avg episode reward: [(0, '362.165')]
[36m[2025-07-01 22:02:55,949][166323] Fps is (10 sec: 1643.9, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 1572864. Throughput: 0: 300.6. Samples: 1574432. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:02:55,950][166323] Avg episode reward: [(0, '343.017')]
[36m[2025-07-01 22:03:00,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 1572864. Throughput: 0: 301.0. Samples: 1576144. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:03:00,972][166323] Avg episode reward: [(0, '365.073')]
[36m[2025-07-01 22:03:05,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1572864. Throughput: 0: 303.0. Samples: 1577040. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:03:05,964][166323] Avg episode reward: [(0, '355.562')]
[36m[2025-07-01 22:03:10,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1572864. Throughput: 0: 299.9. Samples: 1578800. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:03:10,972][166323] Avg episode reward: [(0, '366.725')]
[36m[2025-07-01 22:03:15,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1572864. Throughput: 0: 300.8. Samples: 1580640. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:03:15,972][166323] Avg episode reward: [(0, '355.684')]
[36m[2025-07-01 22:03:20,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1572864. Throughput: 0: 302.4. Samples: 1581568. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:03:20,961][166323] Avg episode reward: [(0, '336.861')]
[36m[2025-07-01 22:03:25,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 1572864. Throughput: 0: 298.2. Samples: 1583296. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:03:25,944][166323] Avg episode reward: [(0, '368.766')]
[36m[2025-07-01 22:03:30,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1572864. Throughput: 0: 297.2. Samples: 1585008. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:03:30,986][166323] Avg episode reward: [(0, '347.349')]
[36m[2025-07-01 22:03:35,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1572864. Throughput: 0: 298.2. Samples: 1586032. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:03:35,978][166323] Avg episode reward: [(0, '381.572')]
[37m[1m[2025-07-01 22:03:36,029][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000003064_1572864.pth...
[36m[2025-07-01 22:03:36,033][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000002936_1507328.pth
[36m[2025-07-01 22:03:40,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1572864. Throughput: 0: 296.7. Samples: 1587792. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:03:40,975][166323] Avg episode reward: [(0, '394.722')]
[36m[2025-07-01 22:03:45,972][166323] Fps is (10 sec: 1639.3, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 1589248. Throughput: 0: 293.7. Samples: 1589360. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-01 22:03:45,972][166323] Avg episode reward: [(0, '371.236')]
[31m[5415582 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5415583 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[5415583 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:03:50,980][166323] Fps is (10 sec: 1637.5, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1589248. Throughput: 0: 294.3. Samples: 1590288. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-01 22:03:50,981][166323] Avg episode reward: [(0, '372.779')]
[36m[2025-07-01 22:03:55,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1589248. Throughput: 0: 291.6. Samples: 1591920. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-01 22:03:55,957][166323] Avg episode reward: [(0, '367.541')]
[31m[5427516 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5427516 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[5427516 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:04:00,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1589248. Throughput: 0: 289.3. Samples: 1593664. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-01 22:04:00,984][166323] Avg episode reward: [(0, '365.465')]
[36m[2025-07-01 22:04:06,008][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1589248. Throughput: 0: 288.4. Samples: 1594560. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-01 22:04:06,009][166323] Avg episode reward: [(0, '353.452')]
[36m[2025-07-01 22:04:10,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1589248. Throughput: 0: 289.3. Samples: 1596320. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-01 22:04:10,956][166323] Avg episode reward: [(0, '340.994')]
[36m[2025-07-01 22:04:15,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1589248. Throughput: 0: 291.3. Samples: 1598112. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-01 22:04:15,972][166323] Avg episode reward: [(0, '351.896')]
[36m[2025-07-01 22:04:20,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1589248. Throughput: 0: 290.4. Samples: 1599104. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-01 22:04:20,986][166323] Avg episode reward: [(0, '375.282')]
[36m[2025-07-01 22:04:25,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1589248. Throughput: 0: 290.7. Samples: 1600864. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-01 22:04:25,944][166323] Avg episode reward: [(0, '370.199')]
[36m[2025-07-01 22:04:30,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1589248. Throughput: 0: 299.8. Samples: 1602848. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-01 22:04:30,959][166323] Avg episode reward: [(0, '344.823')]
[36m[2025-07-01 22:04:35,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1589248. Throughput: 0: 298.0. Samples: 1603696. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-01 22:04:35,976][166323] Avg episode reward: [(0, '359.799')]
[33m[5468439 ms][navigation_task] - WARNING : Curriculum Level: 42, Curriculum progress fraction: 0.42857142857142855 (navigation_task.py:262)
[33m[5468439 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.73681640625
[33mCrash Rate: 0.2138671875
[33mTimeout Rate: 0.04931640625 (navigation_task.py:265)
[33m[5468439 ms][navigation_task] - WARNING : 
[33mSuccesses: 1509
[33mCrashes : 438
[33mTimeouts: 101 (navigation_task.py:268)
[36m[2025-07-01 22:04:40,998][166323] Fps is (10 sec: 1632.0, 60 sec: 545.9, 300 sec: 333.2). Total num frames: 1605632. Throughput: 0: 300.2. Samples: 1605440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:04:40,998][166323] Avg episode reward: [(0, '371.907')]
[36m[2025-07-01 22:04:45,985][166323] Fps is (10 sec: 1636.8, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1605632. Throughput: 0: 300.1. Samples: 1607168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:04:45,985][166323] Avg episode reward: [(0, '400.636')]
[36m[2025-07-01 22:04:50,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 1605632. Throughput: 0: 299.7. Samples: 1608032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:04:50,964][166323] Avg episode reward: [(0, '381.384')]
[36m[2025-07-01 22:04:55,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 1605632. Throughput: 0: 299.4. Samples: 1609792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:04:55,945][166323] Avg episode reward: [(0, '378.578')]
[36m[2025-07-01 22:05:00,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1605632. Throughput: 0: 303.8. Samples: 1611776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:05:00,956][166323] Avg episode reward: [(0, '404.400')]
[31m[5489927 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5489928 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[5489928 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:05:05,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1605632. Throughput: 0: 300.8. Samples: 1612640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:05:05,989][166323] Avg episode reward: [(0, '412.205')]
[36m[2025-07-01 22:05:10,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1605632. Throughput: 0: 300.5. Samples: 1614400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:05:10,996][166323] Avg episode reward: [(0, '410.247')]
[31m[5503021 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5503022 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[5503022 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:05:15,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1605632. Throughput: 0: 295.8. Samples: 1616160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:05:15,965][166323] Avg episode reward: [(0, '409.093')]
[36m[2025-07-01 22:05:20,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1605632. Throughput: 0: 294.4. Samples: 1616944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:05:20,980][166323] Avg episode reward: [(0, '444.178')]
[31m[5512835 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5512835 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[5512836 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:05:25,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1605632. Throughput: 0: 292.5. Samples: 1618592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:05:25,969][166323] Avg episode reward: [(0, '461.734')]
[36m[2025-07-01 22:05:30,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1605632. Throughput: 0: 293.5. Samples: 1620368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:05:30,964][166323] Avg episode reward: [(0, '473.719')]
[36m[2025-07-01 22:05:35,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1605632. Throughput: 0: 294.4. Samples: 1621280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:05:35,962][166323] Avg episode reward: [(0, '481.540')]
[37m[1m[2025-07-01 22:05:36,184][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000003160_1622016.pth...
[36m[2025-07-01 22:05:36,188][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000003000_1540096.pth
[36m[2025-07-01 22:05:40,974][166323] Fps is (10 sec: 1636.7, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 1622016. Throughput: 0: 296.0. Samples: 1623120. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 22:05:40,974][166323] Avg episode reward: [(0, '466.232')]
[36m[2025-07-01 22:05:45,968][166323] Fps is (10 sec: 1637.4, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 1622016. Throughput: 0: 293.3. Samples: 1624976. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 22:05:45,968][166323] Avg episode reward: [(0, '438.206')]
[36m[2025-07-01 22:05:50,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 1622016. Throughput: 0: 293.6. Samples: 1625840. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 22:05:50,952][166323] Avg episode reward: [(0, '444.294')]
[36m[2025-07-01 22:05:55,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1622016. Throughput: 0: 293.3. Samples: 1627584. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 22:05:55,946][166323] Avg episode reward: [(0, '450.563')]
[31m[5545966 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5545967 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[5545967 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:06:00,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1622016. Throughput: 0: 294.4. Samples: 1629408. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 22:06:00,969][166323] Avg episode reward: [(0, '444.536')]
[36m[2025-07-01 22:06:05,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1622016. Throughput: 0: 295.4. Samples: 1630240. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 22:06:06,004][166323] Avg episode reward: [(0, '436.487')]
[36m[2025-07-01 22:06:11,002][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1622016. Throughput: 0: 296.3. Samples: 1631936. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 22:06:11,003][166323] Avg episode reward: [(0, '424.976')]
[36m[2025-07-01 22:06:15,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1622016. Throughput: 0: 298.9. Samples: 1633824. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 22:06:15,986][166323] Avg episode reward: [(0, '461.159')]
[36m[2025-07-01 22:06:20,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1622016. Throughput: 0: 296.6. Samples: 1634624. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 22:06:20,945][166323] Avg episode reward: [(0, '440.992')]
[36m[2025-07-01 22:06:25,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1622016. Throughput: 0: 293.5. Samples: 1636320. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 22:06:25,954][166323] Avg episode reward: [(0, '410.833')]
[36m[2025-07-01 22:06:30,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1622016. Throughput: 0: 292.6. Samples: 1638144. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 22:06:30,978][166323] Avg episode reward: [(0, '382.043')]
[36m[2025-07-01 22:06:35,955][166323] Fps is (10 sec: 1638.2, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 1638400. Throughput: 0: 292.2. Samples: 1638992. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 22:06:35,955][166323] Avg episode reward: [(0, '395.563')]
[31m[5585673 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5585673 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[5585673 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:06:40,973][166323] Fps is (10 sec: 1639.1, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 1638400. Throughput: 0: 292.4. Samples: 1640752. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 22:06:40,974][166323] Avg episode reward: [(0, '390.057')]
[36m[2025-07-01 22:06:45,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1638400. Throughput: 0: 291.5. Samples: 1642528. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 22:06:45,984][166323] Avg episode reward: [(0, '395.710')]
[36m[2025-07-01 22:06:50,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1638400. Throughput: 0: 291.7. Samples: 1643360. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 22:06:50,965][166323] Avg episode reward: [(0, '402.519')]
[36m[2025-07-01 22:06:55,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1638400. Throughput: 0: 294.9. Samples: 1645200. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 22:06:55,986][166323] Avg episode reward: [(0, '412.647')]
[36m[2025-07-01 22:07:00,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1638400. Throughput: 0: 288.9. Samples: 1646816. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 22:07:00,963][166323] Avg episode reward: [(0, '436.968')]
[36m[2025-07-01 22:07:05,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1638400. Throughput: 0: 291.8. Samples: 1647760. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 22:07:05,960][166323] Avg episode reward: [(0, '447.317')]
[36m[2025-07-01 22:07:10,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 1638400. Throughput: 0: 295.4. Samples: 1649616. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 22:07:10,957][166323] Avg episode reward: [(0, '435.291')]
[36m[2025-07-01 22:07:15,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1638400. Throughput: 0: 294.2. Samples: 1651376. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 22:07:15,962][166323] Avg episode reward: [(0, '435.051')]
[36m[2025-07-01 22:07:20,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1638400. Throughput: 0: 293.5. Samples: 1652208. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 22:07:20,986][166323] Avg episode reward: [(0, '444.562')]
[36m[2025-07-01 22:07:25,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1638400. Throughput: 0: 295.9. Samples: 1654064. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 22:07:25,961][166323] Avg episode reward: [(0, '478.332')]
[36m[2025-07-01 22:07:30,944][166323] Fps is (10 sec: 1645.3, 60 sec: 546.4, 300 sec: 333.3). Total num frames: 1654784. Throughput: 0: 296.1. Samples: 1655840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:07:30,944][166323] Avg episode reward: [(0, '483.105')]
[36m[2025-07-01 22:07:35,971][166323] Fps is (10 sec: 1636.7, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1654784. Throughput: 0: 295.8. Samples: 1656672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:07:35,971][166323] Avg episode reward: [(0, '476.315')]
[37m[1m[2025-07-01 22:07:36,021][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000003224_1654784.pth...
[36m[2025-07-01 22:07:36,025][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000003064_1572864.pth
[36m[2025-07-01 22:07:40,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 1654784. Throughput: 0: 295.6. Samples: 1658496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:07:40,962][166323] Avg episode reward: [(0, '510.545')]
[36m[2025-07-01 22:07:45,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1654784. Throughput: 0: 300.7. Samples: 1660352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:07:45,981][166323] Avg episode reward: [(0, '474.863')]
[36m[2025-07-01 22:07:50,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1654784. Throughput: 0: 298.6. Samples: 1661200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:07:50,976][166323] Avg episode reward: [(0, '483.708')]
[36m[2025-07-01 22:07:55,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1654784. Throughput: 0: 296.5. Samples: 1662960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:07:55,958][166323] Avg episode reward: [(0, '484.470')]
[36m[2025-07-01 22:08:00,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1654784. Throughput: 0: 300.4. Samples: 1664896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:08:00,967][166323] Avg episode reward: [(0, '430.318')]
[31m[5671823 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5671824 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[5671824 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:08:05,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1654784. Throughput: 0: 302.1. Samples: 1665792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:08:05,952][166323] Avg episode reward: [(0, '428.474')]
[36m[2025-07-01 22:08:10,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1654784. Throughput: 0: 300.0. Samples: 1667568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:08:10,977][166323] Avg episode reward: [(0, '397.145')]
[36m[2025-07-01 22:08:15,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1654784. Throughput: 0: 300.0. Samples: 1669344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:08:15,963][166323] Avg episode reward: [(0, '449.749')]
[31m[5688165 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5688165 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[5688166 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:08:20,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1654784. Throughput: 0: 301.9. Samples: 1670256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:08:20,961][166323] Avg episode reward: [(0, '466.799')]
[36m[2025-07-01 22:08:25,952][166323] Fps is (10 sec: 1640.2, 60 sec: 546.2, 300 sec: 333.3). Total num frames: 1671168. Throughput: 0: 298.4. Samples: 1671920. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 22:08:25,952][166323] Avg episode reward: [(0, '468.581')]
[36m[2025-07-01 22:08:30,970][166323] Fps is (10 sec: 1637.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 1671168. Throughput: 0: 298.4. Samples: 1673776. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 22:08:30,970][166323] Avg episode reward: [(0, '498.219')]
[31m[5701437 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5701438 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[5701438 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[5704202 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5704202 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[5704202 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:08:35,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 1671168. Throughput: 0: 297.8. Samples: 1674592. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 22:08:35,949][166323] Avg episode reward: [(0, '502.337')]
[31m[5706354 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5706355 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[5706355 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:08:40,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1671168. Throughput: 0: 297.4. Samples: 1676352. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 22:08:40,988][166323] Avg episode reward: [(0, '520.014')]
[36m[2025-07-01 22:08:45,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1671168. Throughput: 0: 293.6. Samples: 1678112. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 22:08:45,985][166323] Avg episode reward: [(0, '491.439')]
[36m[2025-07-01 22:08:50,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1671168. Throughput: 0: 292.9. Samples: 1678976. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 22:08:50,964][166323] Avg episode reward: [(0, '444.955')]
[36m[2025-07-01 22:08:55,998][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1671168. Throughput: 0: 293.5. Samples: 1680784. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 22:08:55,998][166323] Avg episode reward: [(0, '414.624')]
[36m[2025-07-01 22:09:00,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1671168. Throughput: 0: 293.3. Samples: 1682544. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 22:09:00,967][166323] Avg episode reward: [(0, '439.470')]
[36m[2025-07-01 22:09:06,007][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 1671168. Throughput: 0: 290.9. Samples: 1683360. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 22:09:06,007][166323] Avg episode reward: [(0, '434.642')]
[36m[2025-07-01 22:09:10,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1671168. Throughput: 0: 289.6. Samples: 1684960. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 22:09:10,979][166323] Avg episode reward: [(0, '428.954')]
[36m[2025-07-01 22:09:15,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1671168. Throughput: 0: 292.1. Samples: 1686928. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 22:09:15,996][166323] Avg episode reward: [(0, '434.463')]
[36m[2025-07-01 22:09:20,983][166323] Fps is (10 sec: 1637.8, 60 sec: 545.9, 300 sec: 333.2). Total num frames: 1687552. Throughput: 0: 293.5. Samples: 1687808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:09:20,983][166323] Avg episode reward: [(0, '448.658')]
[36m[2025-07-01 22:09:25,954][166323] Fps is (10 sec: 1645.2, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 1687552. Throughput: 0: 294.6. Samples: 1689600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:09:25,955][166323] Avg episode reward: [(0, '489.419')]
[36m[2025-07-01 22:09:30,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 1687552. Throughput: 0: 294.6. Samples: 1691360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:09:30,947][166323] Avg episode reward: [(0, '482.428')]
[36m[2025-07-01 22:09:35,979][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1687552. Throughput: 0: 294.3. Samples: 1692224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:09:35,979][166323] Avg episode reward: [(0, '471.818')]
[37m[1m[2025-07-01 22:09:36,028][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000003288_1687552.pth...
[36m[2025-07-01 22:09:36,032][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000003160_1622016.pth
[36m[2025-07-01 22:09:40,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1687552. Throughput: 0: 292.6. Samples: 1693936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:09:40,950][166323] Avg episode reward: [(0, '480.766')]
[36m[2025-07-01 22:09:45,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1687552. Throughput: 0: 294.0. Samples: 1695776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:09:45,977][166323] Avg episode reward: [(0, '452.959')]
[36m[2025-07-01 22:09:50,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1687552. Throughput: 0: 296.3. Samples: 1696688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:09:50,984][166323] Avg episode reward: [(0, '480.734')]
[36m[2025-07-01 22:09:55,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 1687552. Throughput: 0: 298.1. Samples: 1698368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:09:55,951][166323] Avg episode reward: [(0, '426.464')]
[36m[2025-07-01 22:10:00,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1687552. Throughput: 0: 295.2. Samples: 1700208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:10:00,978][166323] Avg episode reward: [(0, '403.753')]
[36m[2025-07-01 22:10:05,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1687552. Throughput: 0: 296.2. Samples: 1701136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:10:05,984][166323] Avg episode reward: [(0, '429.960')]
[36m[2025-07-01 22:10:10,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1687552. Throughput: 0: 298.1. Samples: 1703024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:10:10,979][166323] Avg episode reward: [(0, '430.947')]
[36m[2025-07-01 22:10:15,974][166323] Fps is (10 sec: 1640.1, 60 sec: 546.3, 300 sec: 333.2). Total num frames: 1703936. Throughput: 0: 298.8. Samples: 1704816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:10:15,974][166323] Avg episode reward: [(0, '451.307')]
[31m[5805100 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5805100 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[5805101 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:10:21,008][166323] Fps is (10 sec: 1633.7, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1703936. Throughput: 0: 297.4. Samples: 1705616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:10:21,008][166323] Avg episode reward: [(0, '455.077')]
[36m[2025-07-01 22:10:25,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1703936. Throughput: 0: 299.7. Samples: 1707424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:10:25,959][166323] Avg episode reward: [(0, '473.967')]
[36m[2025-07-01 22:10:30,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1703936. Throughput: 0: 300.2. Samples: 1709280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:10:30,958][166323] Avg episode reward: [(0, '496.385')]
[36m[2025-07-01 22:10:35,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1703936. Throughput: 0: 299.0. Samples: 1710144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:10:35,985][166323] Avg episode reward: [(0, '473.028')]
[36m[2025-07-01 22:10:40,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1703936. Throughput: 0: 301.8. Samples: 1711952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:10:40,963][166323] Avg episode reward: [(0, '478.766')]
[36m[2025-07-01 22:10:45,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1703936. Throughput: 0: 300.8. Samples: 1713744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:10:45,979][166323] Avg episode reward: [(0, '486.864')]
[36m[2025-07-01 22:10:50,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1703936. Throughput: 0: 299.8. Samples: 1714624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:10:50,979][166323] Avg episode reward: [(0, '474.855')]
[36m[2025-07-01 22:10:55,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1703936. Throughput: 0: 299.4. Samples: 1716496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:10:55,970][166323] Avg episode reward: [(0, '514.118')]
[36m[2025-07-01 22:11:00,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1703936. Throughput: 0: 296.9. Samples: 1718176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:11:00,979][166323] Avg episode reward: [(0, '507.695')]
[36m[2025-07-01 22:11:05,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1703936. Throughput: 0: 300.1. Samples: 1719104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:11:05,956][166323] Avg episode reward: [(0, '530.591')]
[36m[2025-07-01 22:11:10,988][166323] Fps is (10 sec: 1636.9, 60 sec: 546.1, 300 sec: 333.2). Total num frames: 1720320. Throughput: 0: 298.5. Samples: 1720864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:11:10,988][166323] Avg episode reward: [(0, '519.484')]
[36m[2025-07-01 22:11:15,951][166323] Fps is (10 sec: 1639.3, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 1720320. Throughput: 0: 294.4. Samples: 1722528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:11:15,951][166323] Avg episode reward: [(0, '491.229')]
[36m[2025-07-01 22:11:20,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 333.2). Total num frames: 1720320. Throughput: 0: 293.5. Samples: 1723344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:11:20,964][166323] Avg episode reward: [(0, '492.901')]
[36m[2025-07-01 22:11:25,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 1720320. Throughput: 0: 293.8. Samples: 1725168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:11:25,943][166323] Avg episode reward: [(0, '416.892')]
[31m[5876267 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5876268 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[5876268 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:11:30,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1720320. Throughput: 0: 293.0. Samples: 1726928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:11:30,968][166323] Avg episode reward: [(0, '427.417')]
[36m[2025-07-01 22:11:35,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 1720320. Throughput: 0: 292.9. Samples: 1727792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:11:35,944][166323] Avg episode reward: [(0, '403.005')]
[37m[1m[2025-07-01 22:11:35,993][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000003352_1720320.pth...
[36m[2025-07-01 22:11:35,997][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000003224_1654784.pth
[36m[2025-07-01 22:11:40,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1720320. Throughput: 0: 293.9. Samples: 1729712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:11:40,944][166323] Avg episode reward: [(0, '381.522')]
[36m[2025-07-01 22:11:45,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1720320. Throughput: 0: 299.6. Samples: 1731648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:11:45,951][166323] Avg episode reward: [(0, '399.163')]
[36m[2025-07-01 22:11:50,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1720320. Throughput: 0: 296.6. Samples: 1732464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:11:50,998][166323] Avg episode reward: [(0, '425.276')]
[36m[2025-07-01 22:11:55,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1720320. Throughput: 0: 297.1. Samples: 1734224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:11:55,962][166323] Avg episode reward: [(0, '459.096')]
[36m[2025-07-01 22:12:00,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1720320. Throughput: 0: 301.0. Samples: 1736080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:12:00,979][166323] Avg episode reward: [(0, '464.045')]
[36m[2025-07-01 22:12:05,975][166323] Fps is (10 sec: 1636.2, 60 sec: 546.0, 300 sec: 333.2). Total num frames: 1736704. Throughput: 0: 303.6. Samples: 1737008. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:12:05,976][166323] Avg episode reward: [(0, '489.104')]
[36m[2025-07-01 22:12:10,954][166323] Fps is (10 sec: 1642.4, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 1736704. Throughput: 0: 304.3. Samples: 1738864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:12:10,954][166323] Avg episode reward: [(0, '553.558')]
[36m[2025-07-01 22:12:15,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 1736704. Throughput: 0: 309.5. Samples: 1740848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:12:15,947][166323] Avg episode reward: [(0, '517.853')]
[31m[5926268 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5926269 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[5926269 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:12:20,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1736704. Throughput: 0: 310.8. Samples: 1741792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:12:20,984][166323] Avg episode reward: [(0, '516.562')]
[36m[2025-07-01 22:12:25,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1736704. Throughput: 0: 306.7. Samples: 1743520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:12:25,965][166323] Avg episode reward: [(0, '553.182')]
[31m[5935775 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5935776 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[5935776 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:12:30,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1736704. Throughput: 0: 302.8. Samples: 1745280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:12:30,963][166323] Avg episode reward: [(0, '517.960')]
[36m[2025-07-01 22:12:35,974][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1736704. Throughput: 0: 304.2. Samples: 1746144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:12:35,974][166323] Avg episode reward: [(0, '535.396')]
[31m[5948512 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5948512 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[5948513 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:12:40,973][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1736704. Throughput: 0: 307.1. Samples: 1748048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:12:40,973][166323] Avg episode reward: [(0, '517.045')]
[36m[2025-07-01 22:12:45,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1736704. Throughput: 0: 304.9. Samples: 1749792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:12:45,945][166323] Avg episode reward: [(0, '553.337')]
[36m[2025-07-01 22:12:50,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1736704. Throughput: 0: 301.6. Samples: 1750576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:12:50,963][166323] Avg episode reward: [(0, '562.830')]
[36m[2025-07-01 22:12:55,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1736704. Throughput: 0: 297.6. Samples: 1752256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:12:55,952][166323] Avg episode reward: [(0, '506.805')]
[36m[2025-07-01 22:13:00,946][166323] Fps is (10 sec: 1641.1, 60 sec: 546.4, 300 sec: 333.2). Total num frames: 1753088. Throughput: 0: 293.7. Samples: 1754064. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 22:13:00,947][166323] Avg episode reward: [(0, '498.738')]
[36m[2025-07-01 22:13:05,958][166323] Fps is (10 sec: 1637.4, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 1753088. Throughput: 0: 293.9. Samples: 1755008. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 22:13:05,958][166323] Avg episode reward: [(0, '461.764')]
[31m[5978454 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5978454 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[5978454 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:13:10,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 1753088. Throughput: 0: 296.8. Samples: 1756880. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 22:13:10,981][166323] Avg episode reward: [(0, '409.780')]
[36m[2025-07-01 22:13:15,980][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 1753088. Throughput: 0: 295.0. Samples: 1758560. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 22:13:15,980][166323] Avg episode reward: [(0, '395.212')]
[36m[2025-07-01 22:13:20,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1753088. Throughput: 0: 295.5. Samples: 1759440. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 22:13:20,976][166323] Avg episode reward: [(0, '394.714')]
[36m[2025-07-01 22:13:25,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1753088. Throughput: 0: 291.0. Samples: 1761136. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 22:13:25,955][166323] Avg episode reward: [(0, '463.131')]
[36m[2025-07-01 22:13:30,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1753088. Throughput: 0: 291.0. Samples: 1762896. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 22:13:30,973][166323] Avg episode reward: [(0, '457.413')]
[36m[2025-07-01 22:13:35,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1753088. Throughput: 0: 294.4. Samples: 1763824. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 22:13:35,961][166323] Avg episode reward: [(0, '482.443')]
[37m[1m[2025-07-01 22:13:36,012][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000003416_1753088.pth...
[36m[2025-07-01 22:13:36,018][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000003288_1687552.pth
[36m[2025-07-01 22:13:40,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1753088. Throughput: 0: 298.3. Samples: 1765680. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 22:13:40,952][166323] Avg episode reward: [(0, '537.690')]
[36m[2025-07-01 22:13:45,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1753088. Throughput: 0: 297.9. Samples: 1767472. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 22:13:45,959][166323] Avg episode reward: [(0, '519.956')]
[36m[2025-07-01 22:13:50,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1753088. Throughput: 0: 295.7. Samples: 1768320. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 22:13:50,977][166323] Avg episode reward: [(0, '581.192')]
[36m[2025-07-01 22:13:55,966][166323] Fps is (10 sec: 1637.3, 60 sec: 546.0, 300 sec: 333.2). Total num frames: 1769472. Throughput: 0: 290.9. Samples: 1769968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:13:55,966][166323] Avg episode reward: [(0, '596.993')]
[36m[2025-07-01 22:14:00,980][166323] Fps is (10 sec: 1637.8, 60 sec: 272.9, 300 sec: 333.3). Total num frames: 1769472. Throughput: 0: 294.0. Samples: 1771792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:14:00,980][166323] Avg episode reward: [(0, '585.485')]
[36m[2025-07-01 22:14:05,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 1769472. Throughput: 0: 293.5. Samples: 1772640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:14:05,946][166323] Avg episode reward: [(0, '541.975')]
[36m[2025-07-01 22:14:10,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 1769472. Throughput: 0: 295.5. Samples: 1774432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:14:10,946][166323] Avg episode reward: [(0, '547.141')]
[36m[2025-07-01 22:14:15,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1769472. Throughput: 0: 298.4. Samples: 1776320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:14:15,964][166323] Avg episode reward: [(0, '532.846')]
[31m[6045266 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6045266 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[6045266 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:14:20,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1769472. Throughput: 0: 296.5. Samples: 1777168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:14:20,960][166323] Avg episode reward: [(0, '499.584')]
[36m[2025-07-01 22:14:25,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1769472. Throughput: 0: 292.2. Samples: 1778832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:14:25,955][166323] Avg episode reward: [(0, '471.610')]
[36m[2025-07-01 22:14:30,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1769472. Throughput: 0: 291.1. Samples: 1780576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:14:30,977][166323] Avg episode reward: [(0, '498.427')]
[36m[2025-07-01 22:14:35,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1769472. Throughput: 0: 294.4. Samples: 1781568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:14:35,975][166323] Avg episode reward: [(0, '532.521')]
[36m[2025-07-01 22:14:40,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1769472. Throughput: 0: 296.1. Samples: 1783296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:14:40,971][166323] Avg episode reward: [(0, '511.276')]
[36m[2025-07-01 22:14:46,007][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 1769472. Throughput: 0: 292.8. Samples: 1784976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:14:46,007][166323] Avg episode reward: [(0, '507.056')]
[36m[2025-07-01 22:14:51,006][166323] Fps is (10 sec: 1632.6, 60 sec: 545.9, 300 sec: 333.2). Total num frames: 1785856. Throughput: 0: 292.2. Samples: 1785808. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 22:14:51,006][166323] Avg episode reward: [(0, '527.078')]
[33m[6080406 ms][navigation_task] - WARNING : Curriculum Level: 44, Curriculum progress fraction: 0.5714285714285714 (navigation_task.py:262)
[33m[6080406 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.75634765625
[33mCrash Rate: 0.2109375
[33mTimeout Rate: 0.03271484375 (navigation_task.py:265)
[33m[6080407 ms][navigation_task] - WARNING : 
[33mSuccesses: 1549
[33mCrashes : 432
[33mTimeouts: 67 (navigation_task.py:268)
[36m[2025-07-01 22:14:55,993][166323] Fps is (10 sec: 1640.6, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 1785856. Throughput: 0: 291.6. Samples: 1787568. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 22:14:55,994][166323] Avg episode reward: [(0, '544.561')]
[36m[2025-07-01 22:15:00,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 1785856. Throughput: 0: 290.4. Samples: 1789392. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 22:15:00,977][166323] Avg episode reward: [(0, '546.325')]
[36m[2025-07-01 22:15:05,976][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 1785856. Throughput: 0: 293.2. Samples: 1790368. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 22:15:05,976][166323] Avg episode reward: [(0, '543.074')]
[36m[2025-07-01 22:15:10,976][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1785856. Throughput: 0: 297.5. Samples: 1792224. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 22:15:10,977][166323] Avg episode reward: [(0, '543.461')]
[36m[2025-07-01 22:15:15,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1785856. Throughput: 0: 299.7. Samples: 1794064. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 22:15:15,985][166323] Avg episode reward: [(0, '570.603')]
[36m[2025-07-01 22:15:20,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1785856. Throughput: 0: 299.9. Samples: 1795056. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 22:15:20,952][166323] Avg episode reward: [(0, '575.388')]
[36m[2025-07-01 22:15:25,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1785856. Throughput: 0: 299.1. Samples: 1796752. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 22:15:25,957][166323] Avg episode reward: [(0, '519.399')]
[31m[6116041 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6116041 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[6116041 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[6117859 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6117860 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[6117860 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:15:31,006][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1785856. Throughput: 0: 302.2. Samples: 1798576. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 22:15:31,006][166323] Avg episode reward: [(0, '501.494')]
[36m[2025-07-01 22:15:35,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1785856. Throughput: 0: 302.8. Samples: 1799424. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 22:15:35,976][166323] Avg episode reward: [(0, '492.770')]
[37m[1m[2025-07-01 22:15:36,026][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000003480_1785856.pth...
[36m[2025-07-01 22:15:36,030][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000003352_1720320.pth
[36m[2025-07-01 22:15:40,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1785856. Throughput: 0: 305.2. Samples: 1801296. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 22:15:40,977][166323] Avg episode reward: [(0, '536.916')]
[31m[6131380 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6131380 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[6131380 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:15:45,981][166323] Fps is (10 sec: 1637.6, 60 sec: 546.4, 300 sec: 333.2). Total num frames: 1802240. Throughput: 0: 301.8. Samples: 1802976. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:15:45,981][166323] Avg episode reward: [(0, '531.670')]
[36m[2025-07-01 22:15:50,982][166323] Fps is (10 sec: 1637.6, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 1802240. Throughput: 0: 300.4. Samples: 1803888. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:15:50,991][166323] Avg episode reward: [(0, '519.320')]
[36m[2025-07-01 22:15:55,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 1802240. Throughput: 0: 296.9. Samples: 1805584. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:15:55,968][166323] Avg episode reward: [(0, '565.789')]
[36m[2025-07-01 22:16:00,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 1802240. Throughput: 0: 297.1. Samples: 1807424. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:16:00,961][166323] Avg episode reward: [(0, '571.348')]
[36m[2025-07-01 22:16:05,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1802240. Throughput: 0: 295.1. Samples: 1808336. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:16:05,948][166323] Avg episode reward: [(0, '605.149')]
[36m[2025-07-01 22:16:10,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1802240. Throughput: 0: 297.8. Samples: 1810160. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:16:10,978][166323] Avg episode reward: [(0, '591.666')]
[31m[6163263 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6163263 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[6163263 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:16:15,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1802240. Throughput: 0: 295.5. Samples: 1811856. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:16:15,946][166323] Avg episode reward: [(0, '632.474')]
[36m[2025-07-01 22:16:20,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1802240. Throughput: 0: 296.0. Samples: 1812736. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:16:20,952][166323] Avg episode reward: [(0, '640.280')]
[36m[2025-07-01 22:16:25,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1802240. Throughput: 0: 293.7. Samples: 1814512. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:16:25,976][166323] Avg episode reward: [(0, '605.478')]
[36m[2025-07-01 22:16:30,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 1802240. Throughput: 0: 295.9. Samples: 1816288. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:16:30,963][166323] Avg episode reward: [(0, '567.777')]
[36m[2025-07-01 22:16:35,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1802240. Throughput: 0: 293.3. Samples: 1817088. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:16:35,984][166323] Avg episode reward: [(0, '584.389')]
[36m[2025-07-01 22:16:40,992][166323] Fps is (10 sec: 1633.7, 60 sec: 546.0, 300 sec: 333.2). Total num frames: 1818624. Throughput: 0: 293.9. Samples: 1818816. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:16:40,992][166323] Avg episode reward: [(0, '558.967')]
[36m[2025-07-01 22:16:45,986][166323] Fps is (10 sec: 1638.1, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1818624. Throughput: 0: 292.8. Samples: 1820608. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:16:45,986][166323] Avg episode reward: [(0, '570.029')]
[36m[2025-07-01 22:16:50,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 1818624. Throughput: 0: 292.8. Samples: 1821520. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:16:50,977][166323] Avg episode reward: [(0, '581.391')]
[31m[6201163 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6201163 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[6201164 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:16:56,001][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 1818624. Throughput: 0: 290.7. Samples: 1823248. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:16:56,001][166323] Avg episode reward: [(0, '576.267')]
[36m[2025-07-01 22:17:00,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1818624. Throughput: 0: 293.9. Samples: 1825088. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:17:00,973][166323] Avg episode reward: [(0, '616.530')]
[36m[2025-07-01 22:17:05,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1818624. Throughput: 0: 294.6. Samples: 1826000. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:17:05,982][166323] Avg episode reward: [(0, '548.530')]
[36m[2025-07-01 22:17:10,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1818624. Throughput: 0: 293.1. Samples: 1827696. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:17:10,953][166323] Avg episode reward: [(0, '542.385')]
[36m[2025-07-01 22:17:15,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1818624. Throughput: 0: 289.9. Samples: 1829344. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:17:15,993][166323] Avg episode reward: [(0, '536.284')]
[36m[2025-07-01 22:17:21,000][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 1818624. Throughput: 0: 291.8. Samples: 1830224. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:17:21,001][166323] Avg episode reward: [(0, '537.259')]
[36m[2025-07-01 22:17:25,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1818624. Throughput: 0: 294.2. Samples: 1832048. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:17:25,971][166323] Avg episode reward: [(0, '557.249')]
[36m[2025-07-01 22:17:30,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1818624. Throughput: 0: 292.0. Samples: 1833744. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:17:30,979][166323] Avg episode reward: [(0, '515.459')]
[36m[2025-07-01 22:17:35,979][166323] Fps is (10 sec: 1637.1, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 1835008. Throughput: 0: 293.3. Samples: 1834720. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 22:17:35,979][166323] Avg episode reward: [(0, '534.287')]
[37m[1m[2025-07-01 22:17:36,034][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000003576_1835008.pth...
[36m[2025-07-01 22:17:36,039][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000003416_1753088.pth
[36m[2025-07-01 22:17:40,968][166323] Fps is (10 sec: 1640.2, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 1835008. Throughput: 0: 293.2. Samples: 1836432. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 22:17:40,968][166323] Avg episode reward: [(0, '515.409')]
[36m[2025-07-01 22:17:45,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1835008. Throughput: 0: 290.3. Samples: 1838160. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 22:17:45,997][166323] Avg episode reward: [(0, '526.345')]
[36m[2025-07-01 22:17:50,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 1835008. Throughput: 0: 289.6. Samples: 1839024. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 22:17:50,958][166323] Avg episode reward: [(0, '491.019')]
[36m[2025-07-01 22:17:55,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1835008. Throughput: 0: 288.3. Samples: 1840672. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 22:17:55,966][166323] Avg episode reward: [(0, '494.321')]
[36m[2025-07-01 22:18:00,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1835008. Throughput: 0: 289.8. Samples: 1842384. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 22:18:00,985][166323] Avg episode reward: [(0, '540.518')]
[31m[6272145 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6272145 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[6272145 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:18:05,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1835008. Throughput: 0: 290.2. Samples: 1843280. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 22:18:05,991][166323] Avg episode reward: [(0, '504.538')]
[36m[2025-07-01 22:18:10,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1835008. Throughput: 0: 289.1. Samples: 1845056. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 22:18:10,960][166323] Avg episode reward: [(0, '520.620')]
[36m[2025-07-01 22:18:15,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 1835008. Throughput: 0: 294.2. Samples: 1846976. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 22:18:15,950][166323] Avg episode reward: [(0, '547.883')]
[36m[2025-07-01 22:18:20,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1835008. Throughput: 0: 292.3. Samples: 1847872. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 22:18:20,981][166323] Avg episode reward: [(0, '574.975')]
[36m[2025-07-01 22:18:25,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1835008. Throughput: 0: 293.7. Samples: 1849648. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 22:18:25,960][166323] Avg episode reward: [(0, '557.437')]
[36m[2025-07-01 22:18:30,956][166323] Fps is (10 sec: 1642.5, 60 sec: 546.3, 300 sec: 333.2). Total num frames: 1851392. Throughput: 0: 296.8. Samples: 1851504. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 22:18:30,956][166323] Avg episode reward: [(0, '583.343')]
[36m[2025-07-01 22:18:35,977][166323] Fps is (10 sec: 1635.7, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 1851392. Throughput: 0: 298.5. Samples: 1852464. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 22:18:35,977][166323] Avg episode reward: [(0, '619.413')]
[36m[2025-07-01 22:18:40,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1851392. Throughput: 0: 301.1. Samples: 1854224. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 22:18:40,975][166323] Avg episode reward: [(0, '637.405')]
[36m[2025-07-01 22:18:45,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 1851392. Throughput: 0: 300.5. Samples: 1855904. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 22:18:45,981][166323] Avg episode reward: [(0, '671.098')]
[36m[2025-07-01 22:18:51,010][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 1851392. Throughput: 0: 300.0. Samples: 1856784. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 22:18:51,011][166323] Avg episode reward: [(0, '669.818')]
[36m[2025-07-01 22:18:55,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1851392. Throughput: 0: 299.2. Samples: 1858528. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 22:18:55,983][166323] Avg episode reward: [(0, '662.099')]
[36m[2025-07-01 22:19:00,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1851392. Throughput: 0: 296.3. Samples: 1860320. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 22:19:00,984][166323] Avg episode reward: [(0, '655.461')]
[31m[6330126 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6330126 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[6330126 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:19:05,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1851392. Throughput: 0: 297.4. Samples: 1861248. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 22:19:05,960][166323] Avg episode reward: [(0, '665.505')]
[36m[2025-07-01 22:19:10,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1851392. Throughput: 0: 299.4. Samples: 1863120. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 22:19:10,955][166323] Avg episode reward: [(0, '636.920')]
[36m[2025-07-01 22:19:15,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1851392. Throughput: 0: 296.6. Samples: 1864848. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 22:19:15,950][166323] Avg episode reward: [(0, '615.978')]
[36m[2025-07-01 22:19:20,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1851392. Throughput: 0: 297.6. Samples: 1865856. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 22:19:20,970][166323] Avg episode reward: [(0, '604.087')]
[36m[2025-07-01 22:19:25,992][166323] Fps is (10 sec: 1631.5, 60 sec: 545.8, 300 sec: 333.2). Total num frames: 1867776. Throughput: 0: 296.8. Samples: 1867584. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-01 22:19:25,993][166323] Avg episode reward: [(0, '589.233')]
[36m[2025-07-01 22:19:30,974][166323] Fps is (10 sec: 1637.8, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1867776. Throughput: 0: 300.5. Samples: 1869424. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-01 22:19:30,974][166323] Avg episode reward: [(0, '558.136')]
[36m[2025-07-01 22:19:35,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 1867776. Throughput: 0: 301.1. Samples: 1870320. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-01 22:19:35,967][166323] Avg episode reward: [(0, '538.285')]
[37m[1m[2025-07-01 22:19:36,021][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000003640_1867776.pth...
[36m[2025-07-01 22:19:36,025][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000003480_1785856.pth
[36m[2025-07-01 22:19:40,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 1867776. Throughput: 0: 301.6. Samples: 1872096. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-01 22:19:40,963][166323] Avg episode reward: [(0, '548.888')]
[36m[2025-07-01 22:19:45,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1867776. Throughput: 0: 298.3. Samples: 1873744. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-01 22:19:45,979][166323] Avg episode reward: [(0, '514.604')]
[36m[2025-07-01 22:19:50,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 1867776. Throughput: 0: 299.4. Samples: 1874720. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-01 22:19:50,956][166323] Avg episode reward: [(0, '532.783')]
[36m[2025-07-01 22:19:55,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1867776. Throughput: 0: 298.7. Samples: 1876560. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-01 22:19:55,948][166323] Avg episode reward: [(0, '522.238')]
[36m[2025-07-01 22:20:00,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1867776. Throughput: 0: 300.2. Samples: 1878368. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-01 22:20:00,979][166323] Avg episode reward: [(0, '556.859')]
[36m[2025-07-01 22:20:05,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1867776. Throughput: 0: 297.0. Samples: 1879216. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-01 22:20:05,954][166323] Avg episode reward: [(0, '599.605')]
[36m[2025-07-01 22:20:10,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1867776. Throughput: 0: 297.8. Samples: 1880976. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-01 22:20:10,963][166323] Avg episode reward: [(0, '577.806')]
[36m[2025-07-01 22:20:15,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1867776. Throughput: 0: 298.7. Samples: 1882864. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-01 22:20:15,976][166323] Avg episode reward: [(0, '607.542')]
[36m[2025-07-01 22:20:20,979][166323] Fps is (10 sec: 1635.7, 60 sec: 546.0, 300 sec: 333.2). Total num frames: 1884160. Throughput: 0: 298.9. Samples: 1883776. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:20:20,980][166323] Avg episode reward: [(0, '577.391')]
[36m[2025-07-01 22:20:25,965][166323] Fps is (10 sec: 1640.1, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 1884160. Throughput: 0: 296.9. Samples: 1885456. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:20:25,965][166323] Avg episode reward: [(0, '554.611')]
[36m[2025-07-01 22:20:30,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 1884160. Throughput: 0: 300.6. Samples: 1887264. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:20:30,952][166323] Avg episode reward: [(0, '543.844')]
[36m[2025-07-01 22:20:35,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 1884160. Throughput: 0: 299.1. Samples: 1888192. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:20:35,993][166323] Avg episode reward: [(0, '518.624')]
[36m[2025-07-01 22:20:40,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1884160. Throughput: 0: 298.4. Samples: 1890000. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:20:40,992][166323] Avg episode reward: [(0, '542.710')]
[36m[2025-07-01 22:20:45,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1884160. Throughput: 0: 294.6. Samples: 1891616. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:20:45,949][166323] Avg episode reward: [(0, '567.610')]
[36m[2025-07-01 22:20:50,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1884160. Throughput: 0: 295.8. Samples: 1892528. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:20:50,953][166323] Avg episode reward: [(0, '590.812')]
[36m[2025-07-01 22:20:55,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1884160. Throughput: 0: 295.8. Samples: 1894288. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:20:55,960][166323] Avg episode reward: [(0, '592.010')]
[36m[2025-07-01 22:21:00,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1884160. Throughput: 0: 296.7. Samples: 1896208. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:21:00,948][166323] Avg episode reward: [(0, '585.684')]
[36m[2025-07-01 22:21:06,002][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 1884160. Throughput: 0: 298.5. Samples: 1897216. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:21:06,002][166323] Avg episode reward: [(0, '594.149')]
[31m[6456847 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6456847 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[6456847 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:21:10,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1884160. Throughput: 0: 302.3. Samples: 1899056. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:21:10,952][166323] Avg episode reward: [(0, '537.141')]
[36m[2025-07-01 22:21:15,965][166323] Fps is (10 sec: 1644.5, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 1900544. Throughput: 0: 304.3. Samples: 1900960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:21:15,965][166323] Avg episode reward: [(0, '551.302')]
[36m[2025-07-01 22:21:20,988][166323] Fps is (10 sec: 1632.6, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1900544. Throughput: 0: 304.8. Samples: 1901904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:21:20,988][166323] Avg episode reward: [(0, '531.594')]
[36m[2025-07-01 22:21:25,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 1900544. Throughput: 0: 302.8. Samples: 1903616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:21:25,952][166323] Avg episode reward: [(0, '535.330')]
[36m[2025-07-01 22:21:30,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 1900544. Throughput: 0: 309.0. Samples: 1905520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:21:30,944][166323] Avg episode reward: [(0, '561.472')]
[36m[2025-07-01 22:21:35,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1900544. Throughput: 0: 308.5. Samples: 1906416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:21:35,976][166323] Avg episode reward: [(0, '544.524')]
[37m[1m[2025-07-01 22:21:36,040][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000003704_1900544.pth...
[36m[2025-07-01 22:21:36,047][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000003576_1835008.pth
[36m[2025-07-01 22:21:40,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1900544. Throughput: 0: 309.9. Samples: 1908240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:21:40,977][166323] Avg episode reward: [(0, '597.845')]
[36m[2025-07-01 22:21:45,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1900544. Throughput: 0: 306.0. Samples: 1909984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:21:45,973][166323] Avg episode reward: [(0, '608.117')]
[36m[2025-07-01 22:21:50,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1900544. Throughput: 0: 303.4. Samples: 1910864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:21:50,986][166323] Avg episode reward: [(0, '617.973')]
[36m[2025-07-01 22:21:55,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1900544. Throughput: 0: 301.1. Samples: 1912608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:21:55,966][166323] Avg episode reward: [(0, '661.559')]
[31m[6506141 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6506142 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[6506142 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:22:00,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1900544. Throughput: 0: 300.2. Samples: 1914464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:22:00,951][166323] Avg episode reward: [(0, '655.350')]
[36m[2025-07-01 22:22:05,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1900544. Throughput: 0: 297.2. Samples: 1915280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:22:05,987][166323] Avg episode reward: [(0, '613.102')]
[36m[2025-07-01 22:22:10,958][166323] Fps is (10 sec: 1637.1, 60 sec: 546.1, 300 sec: 333.3). Total num frames: 1916928. Throughput: 0: 299.3. Samples: 1917088. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-01 22:22:10,958][166323] Avg episode reward: [(0, '616.870')]
[36m[2025-07-01 22:22:15,962][166323] Fps is (10 sec: 1642.6, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 1916928. Throughput: 0: 297.8. Samples: 1918928. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-01 22:22:15,962][166323] Avg episode reward: [(0, '595.422')]
[31m[6529291 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6529291 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[6529291 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:22:20,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 1916928. Throughput: 0: 298.8. Samples: 1919856. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-01 22:22:20,955][166323] Avg episode reward: [(0, '559.081')]
[36m[2025-07-01 22:22:25,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1916928. Throughput: 0: 295.2. Samples: 1921520. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-01 22:22:25,967][166323] Avg episode reward: [(0, '543.587')]
[36m[2025-07-01 22:22:30,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1916928. Throughput: 0: 294.0. Samples: 1923216. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-01 22:22:30,982][166323] Avg episode reward: [(0, '547.523')]
[31m[6542724 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6542725 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[6542725 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:22:35,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1916928. Throughput: 0: 294.0. Samples: 1924096. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-01 22:22:35,987][166323] Avg episode reward: [(0, '588.255')]
[36m[2025-07-01 22:22:40,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1916928. Throughput: 0: 294.8. Samples: 1925872. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-01 22:22:40,957][166323] Avg episode reward: [(0, '543.792')]
[36m[2025-07-01 22:22:45,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1916928. Throughput: 0: 293.6. Samples: 1927680. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-01 22:22:45,965][166323] Avg episode reward: [(0, '538.888')]
[36m[2025-07-01 22:22:50,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1916928. Throughput: 0: 295.0. Samples: 1928544. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-01 22:22:50,948][166323] Avg episode reward: [(0, '554.441')]
[31m[6563756 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6563756 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[6563756 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:22:55,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1916928. Throughput: 0: 290.9. Samples: 1930176. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-01 22:22:55,949][166323] Avg episode reward: [(0, '576.397')]
[36m[2025-07-01 22:23:00,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1916928. Throughput: 0: 292.2. Samples: 1932080. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-01 22:23:00,972][166323] Avg episode reward: [(0, '526.284')]
[36m[2025-07-01 22:23:05,947][166323] Fps is (10 sec: 1638.7, 60 sec: 546.5, 300 sec: 333.2). Total num frames: 1933312. Throughput: 0: 291.3. Samples: 1932960. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 22:23:05,947][166323] Avg episode reward: [(0, '557.042')]
[36m[2025-07-01 22:23:10,985][166323] Fps is (10 sec: 1636.2, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 1933312. Throughput: 0: 295.3. Samples: 1934816. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 22:23:10,985][166323] Avg episode reward: [(0, '579.475')]
[36m[2025-07-01 22:23:15,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 1933312. Throughput: 0: 294.0. Samples: 1936448. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 22:23:15,989][166323] Avg episode reward: [(0, '570.075')]
[36m[2025-07-01 22:23:20,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 1933312. Throughput: 0: 294.4. Samples: 1937344. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 22:23:20,986][166323] Avg episode reward: [(0, '593.147')]
[36m[2025-07-01 22:23:25,998][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1933312. Throughput: 0: 294.8. Samples: 1939152. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 22:23:25,998][166323] Avg episode reward: [(0, '599.829')]
[36m[2025-07-01 22:23:30,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1933312. Throughput: 0: 296.9. Samples: 1941040. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 22:23:30,967][166323] Avg episode reward: [(0, '636.694')]
[36m[2025-07-01 22:23:35,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1933312. Throughput: 0: 297.9. Samples: 1941952. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 22:23:35,958][166323] Avg episode reward: [(0, '623.297')]
[37m[1m[2025-07-01 22:23:36,007][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000003768_1933312.pth...
[36m[2025-07-01 22:23:36,011][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000003640_1867776.pth
[36m[2025-07-01 22:23:40,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1933312. Throughput: 0: 301.4. Samples: 1943744. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 22:23:40,963][166323] Avg episode reward: [(0, '636.898')]
[36m[2025-07-01 22:23:45,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1933312. Throughput: 0: 299.4. Samples: 1945552. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 22:23:45,963][166323] Avg episode reward: [(0, '607.225')]
[36m[2025-07-01 22:23:50,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1933312. Throughput: 0: 299.4. Samples: 1946432. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 22:23:50,943][166323] Avg episode reward: [(0, '620.070')]
[36m[2025-07-01 22:23:55,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1933312. Throughput: 0: 297.0. Samples: 1948176. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-01 22:23:55,962][166323] Avg episode reward: [(0, '609.061')]
[36m[2025-07-01 22:24:00,947][166323] Fps is (10 sec: 1637.8, 60 sec: 546.4, 300 sec: 333.2). Total num frames: 1949696. Throughput: 0: 300.4. Samples: 1949952. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:24:00,947][166323] Avg episode reward: [(0, '622.762')]
[36m[2025-07-01 22:24:05,964][166323] Fps is (10 sec: 1638.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1949696. Throughput: 0: 299.2. Samples: 1950800. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:24:05,964][166323] Avg episode reward: [(0, '607.555')]
[36m[2025-07-01 22:24:10,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 1949696. Throughput: 0: 298.8. Samples: 1952592. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:24:10,979][166323] Avg episode reward: [(0, '582.925')]
[36m[2025-07-01 22:24:15,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 1949696. Throughput: 0: 296.4. Samples: 1954384. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:24:15,987][166323] Avg episode reward: [(0, '610.053')]
[36m[2025-07-01 22:24:20,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1949696. Throughput: 0: 295.0. Samples: 1955232. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:24:20,979][166323] Avg episode reward: [(0, '586.860')]
[36m[2025-07-01 22:24:25,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1949696. Throughput: 0: 296.2. Samples: 1957072. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:24:25,965][166323] Avg episode reward: [(0, '567.162')]
[36m[2025-07-01 22:24:30,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1949696. Throughput: 0: 299.3. Samples: 1959024. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:24:30,974][166323] Avg episode reward: [(0, '574.592')]
[31m[6663183 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6663184 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[6663184 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:24:35,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1949696. Throughput: 0: 299.2. Samples: 1959904. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:24:35,974][166323] Avg episode reward: [(0, '569.255')]
[36m[2025-07-01 22:24:40,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1949696. Throughput: 0: 300.6. Samples: 1961712. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:24:40,994][166323] Avg episode reward: [(0, '586.891')]
[36m[2025-07-01 22:24:46,010][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 1949696. Throughput: 0: 300.7. Samples: 1963504. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:24:46,011][166323] Avg episode reward: [(0, '608.389')]
[36m[2025-07-01 22:24:50,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1949696. Throughput: 0: 301.0. Samples: 1964352. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:24:50,986][166323] Avg episode reward: [(0, '634.006')]
[36m[2025-07-01 22:24:55,982][166323] Fps is (10 sec: 1643.0, 60 sec: 545.9, 300 sec: 333.2). Total num frames: 1966080. Throughput: 0: 300.4. Samples: 1966112. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 22:24:55,982][166323] Avg episode reward: [(0, '620.867')]
[33m[6688739 ms][navigation_task] - WARNING : Curriculum Level: 46, Curriculum progress fraction: 0.7142857142857143 (navigation_task.py:262)
[33m[6688739 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.76904296875
[33mCrash Rate: 0.2109375
[33mTimeout Rate: 0.02001953125 (navigation_task.py:265)
[33m[6688739 ms][navigation_task] - WARNING : 
[33mSuccesses: 1575
[33mCrashes : 432
[33mTimeouts: 41 (navigation_task.py:268)
[36m[2025-07-01 22:25:00,962][166323] Fps is (10 sec: 1642.4, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1966080. Throughput: 0: 301.3. Samples: 1967936. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 22:25:00,962][166323] Avg episode reward: [(0, '597.991')]
[36m[2025-07-01 22:25:05,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 1966080. Throughput: 0: 304.5. Samples: 1968928. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 22:25:05,953][166323] Avg episode reward: [(0, '645.485')]
[36m[2025-07-01 22:25:10,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 1966080. Throughput: 0: 303.7. Samples: 1970736. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 22:25:10,950][166323] Avg episode reward: [(0, '594.308')]
[31m[6701975 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6701976 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[6701976 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:25:15,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 1966080. Throughput: 0: 297.1. Samples: 1972384. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 22:25:15,945][166323] Avg episode reward: [(0, '577.592')]
[36m[2025-07-01 22:25:20,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1966080. Throughput: 0: 295.9. Samples: 1973216. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 22:25:20,959][166323] Avg episode reward: [(0, '540.544')]
[36m[2025-07-01 22:25:25,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1966080. Throughput: 0: 293.9. Samples: 1974928. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 22:25:25,962][166323] Avg episode reward: [(0, '574.710')]
[36m[2025-07-01 22:25:30,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1966080. Throughput: 0: 291.7. Samples: 1976624. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 22:25:30,987][166323] Avg episode reward: [(0, '619.836')]
[36m[2025-07-01 22:25:35,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1966080. Throughput: 0: 293.3. Samples: 1977536. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 22:25:35,943][166323] Avg episode reward: [(0, '605.323')]
[37m[1m[2025-07-01 22:25:35,994][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000003832_1966080.pth...
[36m[2025-07-01 22:25:35,998][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000003704_1900544.pth
[36m[2025-07-01 22:25:40,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1966080. Throughput: 0: 294.1. Samples: 1979344. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 22:25:40,978][166323] Avg episode reward: [(0, '629.797')]
[36m[2025-07-01 22:25:45,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 1966080. Throughput: 0: 293.6. Samples: 1981152. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 22:25:45,980][166323] Avg episode reward: [(0, '684.225')]
[31m[6736795 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6736795 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[6736795 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:25:50,961][166323] Fps is (10 sec: 1641.2, 60 sec: 546.4, 300 sec: 333.2). Total num frames: 1982464. Throughput: 0: 290.4. Samples: 1982000. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:25:50,961][166323] Avg episode reward: [(0, '673.979')]
[36m[2025-07-01 22:25:56,018][166323] Fps is (10 sec: 1632.2, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 1982464. Throughput: 0: 288.6. Samples: 1983744. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:25:56,018][166323] Avg episode reward: [(0, '697.742')]
[36m[2025-07-01 22:26:00,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 1982464. Throughput: 0: 295.8. Samples: 1985696. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:26:00,954][166323] Avg episode reward: [(0, '671.781')]
[31m[6749660 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6749660 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[6749660 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:26:05,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 1982464. Throughput: 0: 297.6. Samples: 1986608. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:26:05,963][166323] Avg episode reward: [(0, '687.452')]
[36m[2025-07-01 22:26:10,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1982464. Throughput: 0: 299.4. Samples: 1988400. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:26:10,957][166323] Avg episode reward: [(0, '614.149')]
[36m[2025-07-01 22:26:15,973][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1982464. Throughput: 0: 303.4. Samples: 1990272. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:26:15,973][166323] Avg episode reward: [(0, '620.936')]
[36m[2025-07-01 22:26:20,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1982464. Throughput: 0: 302.9. Samples: 1991168. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:26:20,952][166323] Avg episode reward: [(0, '679.795')]
[36m[2025-07-01 22:26:25,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1982464. Throughput: 0: 301.7. Samples: 1992912. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:26:25,946][166323] Avg episode reward: [(0, '677.853')]
[36m[2025-07-01 22:26:31,016][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1982464. Throughput: 0: 300.9. Samples: 1994704. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:26:31,016][166323] Avg episode reward: [(0, '646.892')]
[36m[2025-07-01 22:26:35,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1982464. Throughput: 0: 301.6. Samples: 1995568. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:26:35,954][166323] Avg episode reward: [(0, '648.664')]
[36m[2025-07-01 22:26:41,001][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1982464. Throughput: 0: 303.4. Samples: 1997392. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:26:41,001][166323] Avg episode reward: [(0, '676.410')]
[36m[2025-07-01 22:26:45,975][166323] Fps is (10 sec: 1634.9, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 1998848. Throughput: 0: 296.4. Samples: 1999040. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 22:26:45,975][166323] Avg episode reward: [(0, '678.318')]
[36m[2025-07-01 22:26:50,989][166323] Fps is (10 sec: 1640.4, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 1998848. Throughput: 0: 294.6. Samples: 1999872. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 22:26:50,989][166323] Avg episode reward: [(0, '631.452')]
[36m[2025-07-01 22:26:55,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 333.2). Total num frames: 1998848. Throughput: 0: 292.7. Samples: 2001568. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 22:26:55,949][166323] Avg episode reward: [(0, '611.935')]
[36m[2025-07-01 22:27:00,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 1998848. Throughput: 0: 294.0. Samples: 2003504. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 22:27:00,981][166323] Avg episode reward: [(0, '617.616')]
[36m[2025-07-01 22:27:05,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 1998848. Throughput: 0: 295.4. Samples: 2004464. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 22:27:05,955][166323] Avg episode reward: [(0, '638.116')]
[31m[6815032 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6815032 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[6815032 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:27:10,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1998848. Throughput: 0: 296.7. Samples: 2006272. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 22:27:10,975][166323] Avg episode reward: [(0, '666.907')]
[36m[2025-07-01 22:27:15,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 1998848. Throughput: 0: 299.2. Samples: 2008160. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 22:27:15,986][166323] Avg episode reward: [(0, '638.919')]
[36m[2025-07-01 22:27:20,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1998848. Throughput: 0: 298.8. Samples: 2009024. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 22:27:20,992][166323] Avg episode reward: [(0, '644.576')]
[36m[2025-07-01 22:27:25,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 1998848. Throughput: 0: 295.9. Samples: 2010704. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 22:27:25,996][166323] Avg episode reward: [(0, '694.053')]
[36m[2025-07-01 22:27:30,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 1998848. Throughput: 0: 299.1. Samples: 2012496. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 22:27:30,970][166323] Avg episode reward: [(0, '659.391')]
[31m[6841269 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6841269 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[6841269 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:27:35,980][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 1998848. Throughput: 0: 300.9. Samples: 2013408. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-01 22:27:35,981][166323] Avg episode reward: [(0, '654.354')]
[37m[1m[2025-07-01 22:27:36,049][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000003896_1998848.pth...
[36m[2025-07-01 22:27:36,053][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000003768_1933312.pth
[36m[2025-07-01 22:27:40,962][166323] Fps is (10 sec: 1639.7, 60 sec: 546.5, 300 sec: 333.2). Total num frames: 2015232. Throughput: 0: 301.8. Samples: 2015152. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:27:40,962][166323] Avg episode reward: [(0, '649.209')]
[36m[2025-07-01 22:27:45,967][166323] Fps is (10 sec: 1640.6, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2015232. Throughput: 0: 297.7. Samples: 2016896. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:27:45,967][166323] Avg episode reward: [(0, '648.739')]
[36m[2025-07-01 22:27:50,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 2015232. Throughput: 0: 297.6. Samples: 2017856. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:27:50,958][166323] Avg episode reward: [(0, '613.830')]
[36m[2025-07-01 22:27:55,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2015232. Throughput: 0: 299.1. Samples: 2019728. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:27:55,960][166323] Avg episode reward: [(0, '622.019')]
[36m[2025-07-01 22:28:00,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2015232. Throughput: 0: 297.0. Samples: 2021520. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:28:00,973][166323] Avg episode reward: [(0, '653.611')]
[36m[2025-07-01 22:28:05,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2015232. Throughput: 0: 297.6. Samples: 2022416. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:28:05,999][166323] Avg episode reward: [(0, '707.505')]
[36m[2025-07-01 22:28:10,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2015232. Throughput: 0: 297.7. Samples: 2024096. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:28:10,987][166323] Avg episode reward: [(0, '674.085')]
[36m[2025-07-01 22:28:16,010][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2015232. Throughput: 0: 298.0. Samples: 2025920. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:28:16,011][166323] Avg episode reward: [(0, '682.966')]
[36m[2025-07-01 22:28:20,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2015232. Throughput: 0: 296.7. Samples: 2026752. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:28:20,953][166323] Avg episode reward: [(0, '692.633')]
[36m[2025-07-01 22:28:25,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 2015232. Throughput: 0: 295.2. Samples: 2028432. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:28:25,952][166323] Avg episode reward: [(0, '718.295')]
[36m[2025-07-01 22:28:30,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2015232. Throughput: 0: 293.7. Samples: 2030112. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:28:30,970][166323] Avg episode reward: [(0, '674.304')]
[36m[2025-07-01 22:28:36,027][166323] Fps is (10 sec: 1626.1, 60 sec: 545.7, 300 sec: 333.2). Total num frames: 2031616. Throughput: 0: 291.8. Samples: 2031008. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:28:36,028][166323] Avg episode reward: [(0, '707.782')]
[36m[2025-07-01 22:28:40,987][166323] Fps is (10 sec: 1635.6, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2031616. Throughput: 0: 286.1. Samples: 2032608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:28:40,987][166323] Avg episode reward: [(0, '676.149')]
[36m[2025-07-01 22:28:45,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2031616. Throughput: 0: 285.6. Samples: 2034368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:28:45,958][166323] Avg episode reward: [(0, '696.613')]
[36m[2025-07-01 22:28:50,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2031616. Throughput: 0: 285.3. Samples: 2035248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:28:50,981][166323] Avg episode reward: [(0, '695.477')]
[36m[2025-07-01 22:28:55,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2031616. Throughput: 0: 291.4. Samples: 2037200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:28:55,959][166323] Avg episode reward: [(0, '665.744')]
[36m[2025-07-01 22:29:00,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2031616. Throughput: 0: 288.2. Samples: 2038880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:29:00,981][166323] Avg episode reward: [(0, '718.051')]
[31m[6929838 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6929838 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[6929838 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:29:05,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2031616. Throughput: 0: 289.6. Samples: 2039792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:29:05,975][166323] Avg episode reward: [(0, '682.486')]
[36m[2025-07-01 22:29:10,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2031616. Throughput: 0: 294.3. Samples: 2041680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:29:10,966][166323] Avg episode reward: [(0, '673.762')]
[36m[2025-07-01 22:29:15,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2031616. Throughput: 0: 297.1. Samples: 2043488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:29:15,991][166323] Avg episode reward: [(0, '714.997')]
[31m[6944925 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6944926 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[6944926 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:29:20,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2031616. Throughput: 0: 296.7. Samples: 2044336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:29:20,944][166323] Avg episode reward: [(0, '714.228')]
[36m[2025-07-01 22:29:25,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2031616. Throughput: 0: 301.0. Samples: 2046144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:29:25,962][166323] Avg episode reward: [(0, '746.813')]
[36m[2025-07-01 22:29:31,046][166323] Fps is (10 sec: 1622.0, 60 sec: 545.4, 300 sec: 333.2). Total num frames: 2048000. Throughput: 0: 300.2. Samples: 2047904. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:29:31,046][166323] Avg episode reward: [(0, '703.105')]
[36m[2025-07-01 22:29:35,972][166323] Fps is (10 sec: 1636.8, 60 sec: 273.3, 300 sec: 333.3). Total num frames: 2048000. Throughput: 0: 300.8. Samples: 2048784. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:29:35,972][166323] Avg episode reward: [(0, '666.400')]
[37m[1m[2025-07-01 22:29:36,023][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000003992_2048000.pth...
[36m[2025-07-01 22:29:36,027][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000003832_1966080.pth
[36m[2025-07-01 22:29:40,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 2048000. Throughput: 0: 298.2. Samples: 2050624. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:29:40,976][166323] Avg episode reward: [(0, '660.139')]
[36m[2025-07-01 22:29:45,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 2048000. Throughput: 0: 300.0. Samples: 2052368. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:29:45,944][166323] Avg episode reward: [(0, '646.036')]
[36m[2025-07-01 22:29:50,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2048000. Throughput: 0: 297.7. Samples: 2053184. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:29:50,961][166323] Avg episode reward: [(0, '671.092')]
[36m[2025-07-01 22:29:55,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2048000. Throughput: 0: 294.1. Samples: 2054912. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:29:55,959][166323] Avg episode reward: [(0, '636.320')]
[36m[2025-07-01 22:30:00,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2048000. Throughput: 0: 296.6. Samples: 2056832. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:30:00,977][166323] Avg episode reward: [(0, '668.730')]
[36m[2025-07-01 22:30:05,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2048000. Throughput: 0: 296.1. Samples: 2057664. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:30:05,953][166323] Avg episode reward: [(0, '670.251')]
[36m[2025-07-01 22:30:10,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2048000. Throughput: 0: 297.3. Samples: 2059520. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:30:10,958][166323] Avg episode reward: [(0, '645.733')]
[36m[2025-07-01 22:30:15,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 2048000. Throughput: 0: 299.7. Samples: 2061360. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:30:15,948][166323] Avg episode reward: [(0, '643.760')]
[36m[2025-07-01 22:30:20,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2048000. Throughput: 0: 299.8. Samples: 2062272. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:30:20,966][166323] Avg episode reward: [(0, '638.441')]
[36m[2025-07-01 22:30:26,040][166323] Fps is (10 sec: 1623.4, 60 sec: 545.4, 300 sec: 333.2). Total num frames: 2064384. Throughput: 0: 298.2. Samples: 2064064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:30:26,040][166323] Avg episode reward: [(0, '619.027')]
[36m[2025-07-01 22:30:30,975][166323] Fps is (10 sec: 1636.9, 60 sec: 273.4, 300 sec: 333.2). Total num frames: 2064384. Throughput: 0: 298.1. Samples: 2065792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:30:30,976][166323] Avg episode reward: [(0, '581.631')]
[36m[2025-07-01 22:30:35,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2064384. Throughput: 0: 299.0. Samples: 2066640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:30:35,965][166323] Avg episode reward: [(0, '633.851')]
[36m[2025-07-01 22:30:40,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2064384. Throughput: 0: 301.4. Samples: 2068480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:30:40,982][166323] Avg episode reward: [(0, '615.997')]
[31m[7034270 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7034270 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[7034271 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:30:45,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2064384. Throughput: 0: 301.3. Samples: 2070384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:30:45,953][166323] Avg episode reward: [(0, '638.853')]
[36m[2025-07-01 22:30:50,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 2064384. Throughput: 0: 301.5. Samples: 2071232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:30:50,949][166323] Avg episode reward: [(0, '651.213')]
[36m[2025-07-01 22:30:55,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2064384. Throughput: 0: 297.0. Samples: 2072896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:30:55,992][166323] Avg episode reward: [(0, '682.064')]
[36m[2025-07-01 22:31:00,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2064384. Throughput: 0: 296.5. Samples: 2074704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:31:00,960][166323] Avg episode reward: [(0, '692.335')]
[36m[2025-07-01 22:31:05,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2064384. Throughput: 0: 295.4. Samples: 2075568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:31:05,975][166323] Avg episode reward: [(0, '699.111')]
[36m[2025-07-01 22:31:10,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2064384. Throughput: 0: 293.8. Samples: 2077264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:31:10,968][166323] Avg episode reward: [(0, '713.055')]
[36m[2025-07-01 22:31:16,025][166323] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 277.6). Total num frames: 2064384. Throughput: 0: 295.5. Samples: 2079104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:31:16,025][166323] Avg episode reward: [(0, '720.542')]
[36m[2025-07-01 22:31:20,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2064384. Throughput: 0: 294.1. Samples: 2079872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:31:20,956][166323] Avg episode reward: [(0, '728.838')]
[36m[2025-07-01 22:31:25,992][166323] Fps is (10 sec: 1643.7, 60 sec: 273.3, 300 sec: 333.3). Total num frames: 2080768. Throughput: 0: 288.3. Samples: 2081456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:31:25,993][166323] Avg episode reward: [(0, '708.558')]
[36m[2025-07-01 22:31:30,970][166323] Fps is (10 sec: 1636.2, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2080768. Throughput: 0: 286.5. Samples: 2083280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:31:30,970][166323] Avg episode reward: [(0, '736.358')]
[36m[2025-07-01 22:31:35,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 2080768. Throughput: 0: 284.8. Samples: 2084048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:31:35,957][166323] Avg episode reward: [(0, '698.942')]
[37m[1m[2025-07-01 22:31:36,053][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000004056_2080768.pth...
[36m[2025-07-01 22:31:36,057][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000003896_1998848.pth
[36m[2025-07-01 22:31:40,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2080768. Throughput: 0: 287.8. Samples: 2085840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:31:40,960][166323] Avg episode reward: [(0, '680.593')]
[36m[2025-07-01 22:31:45,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2080768. Throughput: 0: 289.9. Samples: 2087744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:31:45,948][166323] Avg episode reward: [(0, '632.510')]
[31m[7097818 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7097818 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[7097818 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:31:50,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2080768. Throughput: 0: 291.9. Samples: 2088704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:31:50,970][166323] Avg episode reward: [(0, '668.881')]
[36m[2025-07-01 22:31:55,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2080768. Throughput: 0: 295.6. Samples: 2090560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:31:55,955][166323] Avg episode reward: [(0, '646.202')]
[36m[2025-07-01 22:32:00,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2080768. Throughput: 0: 295.2. Samples: 2092368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:32:00,958][166323] Avg episode reward: [(0, '660.216')]
[31m[7113786 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7113787 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[7113787 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:32:05,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2080768. Throughput: 0: 299.3. Samples: 2093344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:32:05,962][166323] Avg episode reward: [(0, '679.945')]
[36m[2025-07-01 22:32:10,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2080768. Throughput: 0: 305.4. Samples: 2095200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:32:11,000][166323] Avg episode reward: [(0, '688.864')]
[36m[2025-07-01 22:32:15,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 2080768. Throughput: 0: 304.3. Samples: 2096976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:32:15,978][166323] Avg episode reward: [(0, '671.583')]
[36m[2025-07-01 22:32:20,952][166323] Fps is (10 sec: 1646.2, 60 sec: 546.2, 300 sec: 333.3). Total num frames: 2097152. Throughput: 0: 308.3. Samples: 2097920. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 22:32:20,952][166323] Avg episode reward: [(0, '649.764')]
[36m[2025-07-01 22:32:25,946][166323] Fps is (10 sec: 1643.6, 60 sec: 273.3, 300 sec: 333.3). Total num frames: 2097152. Throughput: 0: 306.9. Samples: 2099648. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 22:32:25,946][166323] Avg episode reward: [(0, '697.871')]
[36m[2025-07-01 22:32:30,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2097152. Throughput: 0: 302.3. Samples: 2101360. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 22:32:30,991][166323] Avg episode reward: [(0, '667.526')]
[36m[2025-07-01 22:32:35,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2097152. Throughput: 0: 300.4. Samples: 2102224. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 22:32:35,980][166323] Avg episode reward: [(0, '660.378')]
[36m[2025-07-01 22:32:40,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2097152. Throughput: 0: 300.9. Samples: 2104112. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 22:32:40,987][166323] Avg episode reward: [(0, '658.234')]
[36m[2025-07-01 22:32:45,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2097152. Throughput: 0: 296.8. Samples: 2105728. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 22:32:45,968][166323] Avg episode reward: [(0, '592.752')]
[36m[2025-07-01 22:32:50,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2097152. Throughput: 0: 296.0. Samples: 2106672. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 22:32:50,996][166323] Avg episode reward: [(0, '616.106')]
[36m[2025-07-01 22:32:55,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2097152. Throughput: 0: 294.6. Samples: 2108448. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 22:32:55,974][166323] Avg episode reward: [(0, '558.772')]
[36m[2025-07-01 22:33:00,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2097152. Throughput: 0: 294.5. Samples: 2110224. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 22:33:00,961][166323] Avg episode reward: [(0, '587.027')]
[36m[2025-07-01 22:33:06,016][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 2097152. Throughput: 0: 291.8. Samples: 2111072. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 22:33:06,017][166323] Avg episode reward: [(0, '624.075')]
[36m[2025-07-01 22:33:10,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2097152. Throughput: 0: 295.2. Samples: 2112944. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 22:33:10,988][166323] Avg episode reward: [(0, '620.530')]
[36m[2025-07-01 22:33:15,958][166323] Fps is (10 sec: 1648.1, 60 sec: 546.3, 300 sec: 333.2). Total num frames: 2113536. Throughput: 0: 294.3. Samples: 2114592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:33:15,958][166323] Avg episode reward: [(0, '672.775')]
[36m[2025-07-01 22:33:20,982][166323] Fps is (10 sec: 1639.4, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 2113536. Throughput: 0: 295.8. Samples: 2115536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:33:20,983][166323] Avg episode reward: [(0, '684.106')]
[36m[2025-07-01 22:33:25,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.3). Total num frames: 2113536. Throughput: 0: 293.5. Samples: 2117312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:33:25,955][166323] Avg episode reward: [(0, '703.578')]
[36m[2025-07-01 22:33:31,003][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2113536. Throughput: 0: 298.4. Samples: 2119168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:33:31,003][166323] Avg episode reward: [(0, '659.366')]
[36m[2025-07-01 22:33:35,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2113536. Throughput: 0: 296.6. Samples: 2120016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:33:35,982][166323] Avg episode reward: [(0, '665.821')]
[37m[1m[2025-07-01 22:33:36,031][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000004120_2113536.pth...
[36m[2025-07-01 22:33:36,035][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000003992_2048000.pth
[36m[2025-07-01 22:33:40,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2113536. Throughput: 0: 296.0. Samples: 2121760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:33:40,954][166323] Avg episode reward: [(0, '617.741')]
[36m[2025-07-01 22:33:45,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2113536. Throughput: 0: 295.5. Samples: 2123520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:33:45,957][166323] Avg episode reward: [(0, '599.198')]
[31m[7216916 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7216917 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[7216917 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:33:51,008][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 2113536. Throughput: 0: 297.3. Samples: 2124448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:33:51,009][166323] Avg episode reward: [(0, '639.117')]
[36m[2025-07-01 22:33:55,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2113536. Throughput: 0: 293.3. Samples: 2126144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:33:55,993][166323] Avg episode reward: [(0, '658.845')]
[36m[2025-07-01 22:34:00,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2113536. Throughput: 0: 296.0. Samples: 2127920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:34:00,988][166323] Avg episode reward: [(0, '704.777')]
[36m[2025-07-01 22:34:05,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 2113536. Throughput: 0: 294.6. Samples: 2128784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:34:05,952][166323] Avg episode reward: [(0, '714.693')]
[36m[2025-07-01 22:34:10,977][166323] Fps is (10 sec: 1640.3, 60 sec: 546.2, 300 sec: 333.3). Total num frames: 2129920. Throughput: 0: 292.5. Samples: 2130480. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:34:10,977][166323] Avg episode reward: [(0, '774.086')]
[36m[2025-07-01 22:34:15,983][166323] Fps is (10 sec: 1633.2, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2129920. Throughput: 0: 288.8. Samples: 2132160. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:34:15,983][166323] Avg episode reward: [(0, '802.965')]
[36m[2025-07-01 22:34:20,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 2129920. Throughput: 0: 291.4. Samples: 2133120. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:34:20,949][166323] Avg episode reward: [(0, '767.677')]
[36m[2025-07-01 22:34:25,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.8). Total num frames: 2129920. Throughput: 0: 290.3. Samples: 2134832. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:34:25,980][166323] Avg episode reward: [(0, '780.891')]
[36m[2025-07-01 22:34:30,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2129920. Throughput: 0: 290.3. Samples: 2136592. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:34:30,983][166323] Avg episode reward: [(0, '763.177')]
[36m[2025-07-01 22:34:35,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2129920. Throughput: 0: 289.1. Samples: 2137440. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:34:35,954][166323] Avg episode reward: [(0, '701.540')]
[36m[2025-07-01 22:34:40,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2129920. Throughput: 0: 294.3. Samples: 2139376. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:34:40,960][166323] Avg episode reward: [(0, '644.956')]
[36m[2025-07-01 22:34:45,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2129920. Throughput: 0: 294.7. Samples: 2141184. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:34:45,999][166323] Avg episode reward: [(0, '647.322')]
[36m[2025-07-01 22:34:50,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2129920. Throughput: 0: 294.6. Samples: 2142048. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:34:50,980][166323] Avg episode reward: [(0, '633.507')]
[36m[2025-07-01 22:34:55,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 2129920. Throughput: 0: 296.4. Samples: 2143808. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:34:55,947][166323] Avg episode reward: [(0, '592.250')]
[36m[2025-07-01 22:35:00,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2129920. Throughput: 0: 296.2. Samples: 2145488. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 22:35:00,979][166323] Avg episode reward: [(0, '625.646')]
[31m[7291197 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7291197 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[7291197 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:35:05,986][166323] Fps is (10 sec: 1631.9, 60 sec: 545.8, 300 sec: 333.2). Total num frames: 2146304. Throughput: 0: 293.4. Samples: 2146336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:35:05,987][166323] Avg episode reward: [(0, '702.674')]
[33m[7294778 ms][navigation_task] - WARNING : Curriculum Level: 48, Curriculum progress fraction: 0.8571428571428571 (navigation_task.py:262)
[33m[7294779 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.7587890625
[33mCrash Rate: 0.22412109375
[33mTimeout Rate: 0.01708984375 (navigation_task.py:265)
[33m[7294779 ms][navigation_task] - WARNING : 
[33mSuccesses: 1554
[33mCrashes : 459
[33mTimeouts: 35 (navigation_task.py:268)
[36m[2025-07-01 22:35:10,958][166323] Fps is (10 sec: 1641.9, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 2146304. Throughput: 0: 292.1. Samples: 2147968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:35:10,958][166323] Avg episode reward: [(0, '753.825')]
[36m[2025-07-01 22:35:15,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 2146304. Throughput: 0: 294.2. Samples: 2149824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:35:15,954][166323] Avg episode reward: [(0, '749.558')]
[36m[2025-07-01 22:35:20,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.8). Total num frames: 2146304. Throughput: 0: 295.7. Samples: 2150752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:35:20,969][166323] Avg episode reward: [(0, '791.029')]
[36m[2025-07-01 22:35:25,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2146304. Throughput: 0: 289.8. Samples: 2152416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:35:25,959][166323] Avg episode reward: [(0, '785.608')]
[36m[2025-07-01 22:35:30,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2146304. Throughput: 0: 290.8. Samples: 2154256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:35:30,949][166323] Avg episode reward: [(0, '801.004')]
[31m[7320317 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7320317 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[7320317 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:35:35,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2146304. Throughput: 0: 290.9. Samples: 2155136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:35:35,975][166323] Avg episode reward: [(0, '767.194')]
[37m[1m[2025-07-01 22:35:36,033][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000004184_2146304.pth...
[36m[2025-07-01 22:35:36,037][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000004056_2080768.pth
[36m[2025-07-01 22:35:40,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2146304. Throughput: 0: 291.7. Samples: 2156944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:35:40,975][166323] Avg episode reward: [(0, '764.463')]
[36m[2025-07-01 22:35:45,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 2146304. Throughput: 0: 295.0. Samples: 2158768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:35:45,999][166323] Avg episode reward: [(0, '759.567')]
[36m[2025-07-01 22:35:50,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2146304. Throughput: 0: 296.0. Samples: 2159648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:35:50,961][166323] Avg episode reward: [(0, '779.444')]
[36m[2025-07-01 22:35:55,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2146304. Throughput: 0: 298.6. Samples: 2161408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:35:55,971][166323] Avg episode reward: [(0, '798.542')]
[31m[7348818 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7348818 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[7348818 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:36:00,965][166323] Fps is (10 sec: 1637.6, 60 sec: 546.3, 300 sec: 333.2). Total num frames: 2162688. Throughput: 0: 293.6. Samples: 2163040. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:36:00,965][166323] Avg episode reward: [(0, '795.646')]
[36m[2025-07-01 22:36:05,952][166323] Fps is (10 sec: 1641.3, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 2162688. Throughput: 0: 293.4. Samples: 2163952. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:36:05,953][166323] Avg episode reward: [(0, '852.142')]
[36m[2025-07-01 22:36:10,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.3). Total num frames: 2162688. Throughput: 0: 296.8. Samples: 2165776. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:36:10,969][166323] Avg episode reward: [(0, '853.253')]
[36m[2025-07-01 22:36:15,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2162688. Throughput: 0: 295.4. Samples: 2167552. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:36:15,962][166323] Avg episode reward: [(0, '795.252')]
[36m[2025-07-01 22:36:20,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2162688. Throughput: 0: 295.2. Samples: 2168416. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:36:20,955][166323] Avg episode reward: [(0, '861.821')]
[31m[7374503 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7374503 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[7374503 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:36:25,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2162688. Throughput: 0: 294.7. Samples: 2170208. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:36:25,990][166323] Avg episode reward: [(0, '867.625')]
[36m[2025-07-01 22:36:30,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2162688. Throughput: 0: 290.5. Samples: 2171824. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:36:30,944][166323] Avg episode reward: [(0, '805.385')]
[36m[2025-07-01 22:36:35,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2162688. Throughput: 0: 290.4. Samples: 2172720. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:36:35,979][166323] Avg episode reward: [(0, '740.845')]
[36m[2025-07-01 22:36:40,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2162688. Throughput: 0: 292.3. Samples: 2174560. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:36:40,980][166323] Avg episode reward: [(0, '733.251')]
[36m[2025-07-01 22:36:45,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 2162688. Throughput: 0: 294.9. Samples: 2176304. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:36:45,950][166323] Avg episode reward: [(0, '741.446')]
[36m[2025-07-01 22:36:50,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2162688. Throughput: 0: 293.2. Samples: 2177152. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:36:50,969][166323] Avg episode reward: [(0, '726.100')]
[36m[2025-07-01 22:36:55,968][166323] Fps is (10 sec: 1635.5, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 2179072. Throughput: 0: 289.4. Samples: 2178800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:36:55,968][166323] Avg episode reward: [(0, '725.628')]
[36m[2025-07-01 22:37:00,951][166323] Fps is (10 sec: 1641.4, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2179072. Throughput: 0: 288.1. Samples: 2180512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:37:00,951][166323] Avg episode reward: [(0, '751.961')]
[31m[7414375 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7414375 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[7414375 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:37:05,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.3). Total num frames: 2179072. Throughput: 0: 288.7. Samples: 2181408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:37:05,963][166323] Avg episode reward: [(0, '810.917')]
[36m[2025-07-01 22:37:10,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 2179072. Throughput: 0: 289.4. Samples: 2183232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:37:10,995][166323] Avg episode reward: [(0, '841.755')]
[36m[2025-07-01 22:37:15,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2179072. Throughput: 0: 292.9. Samples: 2185008. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:37:15,954][166323] Avg episode reward: [(0, '807.500')]
[36m[2025-07-01 22:37:20,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2179072. Throughput: 0: 294.1. Samples: 2185952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:37:20,971][166323] Avg episode reward: [(0, '839.436')]
[36m[2025-07-01 22:37:25,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2179072. Throughput: 0: 288.5. Samples: 2187536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:37:25,950][166323] Avg episode reward: [(0, '901.125')]
[36m[2025-07-01 22:37:30,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2179072. Throughput: 0: 287.5. Samples: 2189248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:37:30,967][166323] Avg episode reward: [(0, '890.744')]
[36m[2025-07-01 22:37:35,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2179072. Throughput: 0: 288.0. Samples: 2190112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:37:35,968][166323] Avg episode reward: [(0, '856.306')]
[37m[1m[2025-07-01 22:37:36,018][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000004248_2179072.pth...
[36m[2025-07-01 22:37:36,022][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000004120_2113536.pth
[36m[2025-07-01 22:37:40,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2179072. Throughput: 0: 290.8. Samples: 2191888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:37:40,968][166323] Avg episode reward: [(0, '852.016')]
[31m[7451297 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7451298 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[7451298 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:37:45,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2179072. Throughput: 0: 292.2. Samples: 2193664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:37:45,963][166323] Avg episode reward: [(0, '844.865')]
[36m[2025-07-01 22:37:50,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2179072. Throughput: 0: 292.9. Samples: 2194592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:37:50,971][166323] Avg episode reward: [(0, '878.318')]
[36m[2025-07-01 22:37:55,989][166323] Fps is (10 sec: 1634.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2195456. Throughput: 0: 290.5. Samples: 2196304. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 22:37:55,989][166323] Avg episode reward: [(0, '802.332')]
[36m[2025-07-01 22:38:00,981][166323] Fps is (10 sec: 1636.6, 60 sec: 272.9, 300 sec: 333.3). Total num frames: 2195456. Throughput: 0: 290.7. Samples: 2198096. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 22:38:00,982][166323] Avg episode reward: [(0, '813.489')]
[36m[2025-07-01 22:38:05,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 2195456. Throughput: 0: 289.6. Samples: 2198992. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 22:38:05,995][166323] Avg episode reward: [(0, '798.714')]
[36m[2025-07-01 22:38:10,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2195456. Throughput: 0: 293.1. Samples: 2200736. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 22:38:10,990][166323] Avg episode reward: [(0, '785.926')]
[36m[2025-07-01 22:38:15,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2195456. Throughput: 0: 295.2. Samples: 2202528. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 22:38:15,953][166323] Avg episode reward: [(0, '811.300')]
[36m[2025-07-01 22:38:20,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2195456. Throughput: 0: 297.3. Samples: 2203488. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 22:38:20,957][166323] Avg episode reward: [(0, '791.572')]
[36m[2025-07-01 22:38:25,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2195456. Throughput: 0: 296.3. Samples: 2205216. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 22:38:25,957][166323] Avg episode reward: [(0, '797.685')]
[36m[2025-07-01 22:38:30,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2195456. Throughput: 0: 294.7. Samples: 2206928. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 22:38:30,978][166323] Avg episode reward: [(0, '823.141')]
[36m[2025-07-01 22:38:35,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2195456. Throughput: 0: 293.8. Samples: 2207808. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 22:38:35,949][166323] Avg episode reward: [(0, '764.950')]
[36m[2025-07-01 22:38:40,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2195456. Throughput: 0: 292.9. Samples: 2209472. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 22:38:40,948][166323] Avg episode reward: [(0, '812.288')]
[31m[7512070 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7512070 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[7512070 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:38:45,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2195456. Throughput: 0: 293.3. Samples: 2211296. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-01 22:38:45,992][166323] Avg episode reward: [(0, '797.152')]
[36m[2025-07-01 22:38:50,948][166323] Fps is (10 sec: 1638.3, 60 sec: 546.3, 300 sec: 333.3). Total num frames: 2211840. Throughput: 0: 290.1. Samples: 2212032. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 22:38:50,948][166323] Avg episode reward: [(0, '822.226')]
[36m[2025-07-01 22:38:55,971][166323] Fps is (10 sec: 1641.8, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 2211840. Throughput: 0: 289.2. Samples: 2213744. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 22:38:55,972][166323] Avg episode reward: [(0, '830.705')]
[36m[2025-07-01 22:39:00,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2211840. Throughput: 0: 285.7. Samples: 2215392. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 22:39:00,973][166323] Avg episode reward: [(0, '795.189')]
[31m[7529839 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7529839 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[7529839 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:39:05,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2211840. Throughput: 0: 284.3. Samples: 2216288. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 22:39:05,983][166323] Avg episode reward: [(0, '843.493')]
[36m[2025-07-01 22:39:10,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2211840. Throughput: 0: 285.3. Samples: 2218064. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 22:39:10,985][166323] Avg episode reward: [(0, '769.438')]
[36m[2025-07-01 22:39:15,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2211840. Throughput: 0: 285.1. Samples: 2219760. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 22:39:15,987][166323] Avg episode reward: [(0, '750.552')]
[36m[2025-07-01 22:39:20,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2211840. Throughput: 0: 286.9. Samples: 2220720. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 22:39:20,952][166323] Avg episode reward: [(0, '736.853')]
[36m[2025-07-01 22:39:25,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2211840. Throughput: 0: 289.9. Samples: 2222528. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 22:39:25,979][166323] Avg episode reward: [(0, '733.319')]
[36m[2025-07-01 22:39:30,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2211840. Throughput: 0: 289.5. Samples: 2224320. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 22:39:30,977][166323] Avg episode reward: [(0, '740.768')]
[31m[7560773 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7560774 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[7560774 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:39:35,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2211840. Throughput: 0: 291.3. Samples: 2225152. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 22:39:35,986][166323] Avg episode reward: [(0, '790.464')]
[37m[1m[2025-07-01 22:39:36,079][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000004312_2211840.pth...
[36m[2025-07-01 22:39:36,086][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000004184_2146304.pth
[36m[2025-07-01 22:39:40,979][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2211840. Throughput: 0: 291.2. Samples: 2226848. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-01 22:39:40,979][166323] Avg episode reward: [(0, '747.727')]
[36m[2025-07-01 22:39:45,984][166323] Fps is (10 sec: 1638.7, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 2228224. Throughput: 0: 293.3. Samples: 2228592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:39:45,984][166323] Avg episode reward: [(0, '774.549')]
[36m[2025-07-01 22:39:50,959][166323] Fps is (10 sec: 1641.7, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2228224. Throughput: 0: 292.1. Samples: 2229424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:39:50,959][166323] Avg episode reward: [(0, '812.825')]
[36m[2025-07-01 22:39:55,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 2228224. Throughput: 0: 290.3. Samples: 2231120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:39:55,953][166323] Avg episode reward: [(0, '795.419')]
[36m[2025-07-01 22:40:00,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2228224. Throughput: 0: 290.2. Samples: 2232816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:40:00,976][166323] Avg episode reward: [(0, '793.349')]
[36m[2025-07-01 22:40:05,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2228224. Throughput: 0: 288.9. Samples: 2233728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:40:05,973][166323] Avg episode reward: [(0, '741.532')]
[36m[2025-07-01 22:40:10,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2228224. Throughput: 0: 286.2. Samples: 2235408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:40:10,977][166323] Avg episode reward: [(0, '766.264')]
[31m[7600164 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7600164 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[7600164 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[7603602 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7603602 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[7603603 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:40:15,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2228224. Throughput: 0: 281.0. Samples: 2236960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:40:15,954][166323] Avg episode reward: [(0, '824.613')]
[36m[2025-07-01 22:40:20,978][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2228224. Throughput: 0: 283.1. Samples: 2237888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:40:20,978][166323] Avg episode reward: [(0, '780.242')]
[36m[2025-07-01 22:40:26,000][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 2228224. Throughput: 0: 287.5. Samples: 2239792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:40:26,001][166323] Avg episode reward: [(0, '738.817')]
[36m[2025-07-01 22:40:30,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2228224. Throughput: 0: 287.4. Samples: 2241520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:40:30,967][166323] Avg episode reward: [(0, '777.932')]
[36m[2025-07-01 22:40:35,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2228224. Throughput: 0: 288.5. Samples: 2242416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:40:35,988][166323] Avg episode reward: [(0, '760.717')]
[36m[2025-07-01 22:40:40,990][166323] Fps is (10 sec: 1634.6, 60 sec: 546.0, 300 sec: 333.2). Total num frames: 2244608. Throughput: 0: 292.0. Samples: 2244272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:40:40,990][166323] Avg episode reward: [(0, '732.863')]
[36m[2025-07-01 22:40:45,991][166323] Fps is (10 sec: 1637.8, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2244608. Throughput: 0: 293.9. Samples: 2246048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:40:45,991][166323] Avg episode reward: [(0, '751.066')]
[36m[2025-07-01 22:40:50,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2244608. Throughput: 0: 292.6. Samples: 2246896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:40:50,969][166323] Avg episode reward: [(0, '763.602')]
[36m[2025-07-01 22:40:55,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2244608. Throughput: 0: 293.0. Samples: 2248592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:40:55,975][166323] Avg episode reward: [(0, '771.489')]
[31m[7646277 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7646277 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[7646278 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:41:00,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2244608. Throughput: 0: 299.0. Samples: 2250416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:41:00,963][166323] Avg episode reward: [(0, '782.553')]
[36m[2025-07-01 22:41:06,010][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2244608. Throughput: 0: 299.9. Samples: 2251392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:41:06,010][166323] Avg episode reward: [(0, '806.263')]
[36m[2025-07-01 22:41:10,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2244608. Throughput: 0: 295.0. Samples: 2253056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:41:10,957][166323] Avg episode reward: [(0, '820.563')]
[36m[2025-07-01 22:41:15,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2244608. Throughput: 0: 298.9. Samples: 2254976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:41:15,991][166323] Avg episode reward: [(0, '781.219')]
[36m[2025-07-01 22:41:20,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2244608. Throughput: 0: 299.2. Samples: 2255872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:41:20,962][166323] Avg episode reward: [(0, '801.387')]
[36m[2025-07-01 22:41:25,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 2244608. Throughput: 0: 294.0. Samples: 2257488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:41:25,947][166323] Avg episode reward: [(0, '783.350')]
[31m[7676390 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7676390 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[7676390 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:41:30,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2244608. Throughput: 0: 289.8. Samples: 2259088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:41:30,992][166323] Avg episode reward: [(0, '765.821')]
[36m[2025-07-01 22:41:35,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2244608. Throughput: 0: 289.1. Samples: 2259904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:41:35,957][166323] Avg episode reward: [(0, '737.575')]
[37m[1m[2025-07-01 22:41:36,048][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000004376_2244608.pth...
[36m[2025-07-01 22:41:36,052][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000004248_2179072.pth
[36m[2025-07-01 22:41:40,982][166323] Fps is (10 sec: 1640.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2260992. Throughput: 0: 286.9. Samples: 2261504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:41:40,982][166323] Avg episode reward: [(0, '744.814')]
[36m[2025-07-01 22:41:45,980][166323] Fps is (10 sec: 1634.6, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2260992. Throughput: 0: 289.0. Samples: 2263424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:41:45,981][166323] Avg episode reward: [(0, '746.101')]
[36m[2025-07-01 22:41:50,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2260992. Throughput: 0: 286.2. Samples: 2264256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:41:50,965][166323] Avg episode reward: [(0, '748.628')]
[36m[2025-07-01 22:41:55,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2260992. Throughput: 0: 288.9. Samples: 2266064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:41:55,981][166323] Avg episode reward: [(0, '724.934')]
[36m[2025-07-01 22:42:00,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2260992. Throughput: 0: 285.9. Samples: 2267840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:42:00,993][166323] Avg episode reward: [(0, '758.075')]
[36m[2025-07-01 22:42:05,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 2260992. Throughput: 0: 285.8. Samples: 2268736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:42:05,969][166323] Avg episode reward: [(0, '778.820')]
[36m[2025-07-01 22:42:10,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2260992. Throughput: 0: 286.4. Samples: 2270384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:42:10,974][166323] Avg episode reward: [(0, '764.046')]
[36m[2025-07-01 22:42:15,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2260992. Throughput: 0: 289.8. Samples: 2272128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:42:15,986][166323] Avg episode reward: [(0, '776.693')]
[36m[2025-07-01 22:42:20,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2260992. Throughput: 0: 292.8. Samples: 2273088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:42:20,985][166323] Avg episode reward: [(0, '776.471')]
[36m[2025-07-01 22:42:25,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 2260992. Throughput: 0: 298.9. Samples: 2274960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:42:25,995][166323] Avg episode reward: [(0, '775.094')]
[31m[7737852 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7737852 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[7737852 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:42:30,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2260992. Throughput: 0: 296.5. Samples: 2276768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:42:30,979][166323] Avg episode reward: [(0, '757.593')]
[36m[2025-07-01 22:42:35,978][166323] Fps is (10 sec: 1641.1, 60 sec: 545.9, 300 sec: 333.2). Total num frames: 2277376. Throughput: 0: 297.9. Samples: 2277664. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 22:42:35,978][166323] Avg episode reward: [(0, '775.850')]
[36m[2025-07-01 22:42:40,983][166323] Fps is (10 sec: 1637.7, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2277376. Throughput: 0: 299.7. Samples: 2279552. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 22:42:40,983][166323] Avg episode reward: [(0, '787.327')]
[36m[2025-07-01 22:42:45,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2277376. Throughput: 0: 300.2. Samples: 2281344. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 22:42:45,973][166323] Avg episode reward: [(0, '736.792')]
[36m[2025-07-01 22:42:50,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2277376. Throughput: 0: 300.5. Samples: 2282256. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 22:42:50,965][166323] Avg episode reward: [(0, '742.057')]
[36m[2025-07-01 22:42:55,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2277376. Throughput: 0: 303.1. Samples: 2284016. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 22:42:55,943][166323] Avg episode reward: [(0, '735.035')]
[36m[2025-07-01 22:43:00,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2277376. Throughput: 0: 300.7. Samples: 2285648. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 22:43:00,955][166323] Avg episode reward: [(0, '726.232')]
[36m[2025-07-01 22:43:05,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2277376. Throughput: 0: 299.0. Samples: 2286544. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 22:43:05,993][166323] Avg episode reward: [(0, '694.192')]
[36m[2025-07-01 22:43:10,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2277376. Throughput: 0: 295.6. Samples: 2288256. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 22:43:10,977][166323] Avg episode reward: [(0, '673.892')]
[36m[2025-07-01 22:43:15,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2277376. Throughput: 0: 295.8. Samples: 2290080. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 22:43:15,975][166323] Avg episode reward: [(0, '729.734')]
[36m[2025-07-01 22:43:20,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2277376. Throughput: 0: 296.6. Samples: 2291008. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 22:43:20,972][166323] Avg episode reward: [(0, '736.876')]
[36m[2025-07-01 22:43:25,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 2277376. Throughput: 0: 296.0. Samples: 2292864. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 22:43:25,952][166323] Avg episode reward: [(0, '721.220')]
[36m[2025-07-01 22:43:30,944][166323] Fps is (10 sec: 1643.0, 60 sec: 546.4, 300 sec: 333.2). Total num frames: 2293760. Throughput: 0: 295.7. Samples: 2294640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:43:30,944][166323] Avg episode reward: [(0, '781.777')]
[36m[2025-07-01 22:43:35,963][166323] Fps is (10 sec: 1636.4, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2293760. Throughput: 0: 298.0. Samples: 2295664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:43:35,964][166323] Avg episode reward: [(0, '792.885')]
[37m[1m[2025-07-01 22:43:36,040][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000004472_2293760.pth...
[36m[2025-07-01 22:43:36,044][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000004312_2211840.pth
[36m[2025-07-01 22:43:40,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2293760. Throughput: 0: 296.6. Samples: 2297376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:43:40,983][166323] Avg episode reward: [(0, '798.655')]
[36m[2025-07-01 22:43:45,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2293760. Throughput: 0: 301.0. Samples: 2299200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:43:45,980][166323] Avg episode reward: [(0, '770.203')]
[36m[2025-07-01 22:43:50,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2293760. Throughput: 0: 302.3. Samples: 2300144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:43:50,979][166323] Avg episode reward: [(0, '717.791')]
[36m[2025-07-01 22:43:55,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2293760. Throughput: 0: 304.8. Samples: 2301968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:43:55,960][166323] Avg episode reward: [(0, '712.559')]
[36m[2025-07-01 22:44:00,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2293760. Throughput: 0: 303.5. Samples: 2303744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:44:00,994][166323] Avg episode reward: [(0, '683.508')]
[31m[7833152 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7833152 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[7833152 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:44:05,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 2293760. Throughput: 0: 302.8. Samples: 2304624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:44:05,945][166323] Avg episode reward: [(0, '627.953')]
[36m[2025-07-01 22:44:10,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2293760. Throughput: 0: 301.7. Samples: 2306448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:44:10,975][166323] Avg episode reward: [(0, '622.335')]
[36m[2025-07-01 22:44:15,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2293760. Throughput: 0: 301.8. Samples: 2308224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:44:15,959][166323] Avg episode reward: [(0, '678.170')]
[36m[2025-07-01 22:44:20,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2293760. Throughput: 0: 298.9. Samples: 2309120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:44:20,983][166323] Avg episode reward: [(0, '706.163')]
[31m[7850141 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7850141 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[7850141 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:44:26,003][166323] Fps is (10 sec: 1631.2, 60 sec: 545.7, 300 sec: 333.2). Total num frames: 2310144. Throughput: 0: 299.6. Samples: 2310864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:44:26,003][166323] Avg episode reward: [(0, '755.492')]
[36m[2025-07-01 22:44:30,965][166323] Fps is (10 sec: 1641.4, 60 sec: 273.0, 300 sec: 333.3). Total num frames: 2310144. Throughput: 0: 299.8. Samples: 2312688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:44:30,965][166323] Avg episode reward: [(0, '750.389')]
[36m[2025-07-01 22:44:35,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 2310144. Throughput: 0: 300.6. Samples: 2313664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:44:35,949][166323] Avg episode reward: [(0, '762.907')]
[36m[2025-07-01 22:44:40,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2310144. Throughput: 0: 300.4. Samples: 2315488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:44:40,960][166323] Avg episode reward: [(0, '799.238')]
[36m[2025-07-01 22:44:45,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2310144. Throughput: 0: 302.8. Samples: 2317360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:44:45,955][166323] Avg episode reward: [(0, '750.071')]
[36m[2025-07-01 22:44:50,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2310144. Throughput: 0: 302.1. Samples: 2318224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:44:50,957][166323] Avg episode reward: [(0, '761.799')]
[31m[7881436 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7881437 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[7881437 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:44:55,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2310144. Throughput: 0: 302.3. Samples: 2320048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:44:55,959][166323] Avg episode reward: [(0, '740.225')]
[36m[2025-07-01 22:45:00,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 2310144. Throughput: 0: 305.1. Samples: 2321952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:45:00,953][166323] Avg episode reward: [(0, '696.860')]
[36m[2025-07-01 22:45:05,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 2310144. Throughput: 0: 305.7. Samples: 2322880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:45:05,997][166323] Avg episode reward: [(0, '715.423')]
[31m[7898190 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7898190 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[7898190 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:45:10,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2310144. Throughput: 0: 309.1. Samples: 2324768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:45:10,981][166323] Avg episode reward: [(0, '654.269')]
[36m[2025-07-01 22:45:15,982][166323] Fps is (10 sec: 1640.7, 60 sec: 545.9, 300 sec: 333.2). Total num frames: 2326528. Throughput: 0: 308.1. Samples: 2326560. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 22:45:15,982][166323] Avg episode reward: [(0, '712.937')]
[31m[7904778 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7904779 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[7904779 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:45:20,948][166323] Fps is (10 sec: 1643.9, 60 sec: 546.5, 300 sec: 333.3). Total num frames: 2326528. Throughput: 0: 306.1. Samples: 2327440. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 22:45:20,948][166323] Avg episode reward: [(0, '730.531')]
[36m[2025-07-01 22:45:25,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 2326528. Throughput: 0: 304.3. Samples: 2329184. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 22:45:25,972][166323] Avg episode reward: [(0, '756.981')]
[33m[7915289 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[7915289 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.78173828125
[33mCrash Rate: 0.19677734375
[33mTimeout Rate: 0.021484375 (navigation_task.py:265)
[33m[7915289 ms][navigation_task] - WARNING : 
[33mSuccesses: 1601
[33mCrashes : 403
[33mTimeouts: 44 (navigation_task.py:268)
[31m[7915829 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7915830 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[7915830 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:45:30,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 2326528. Throughput: 0: 298.6. Samples: 2330800. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 22:45:30,968][166323] Avg episode reward: [(0, '747.198')]
[36m[2025-07-01 22:45:35,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2326528. Throughput: 0: 298.0. Samples: 2331632. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 22:45:35,945][166323] Avg episode reward: [(0, '770.090')]
[37m[1m[2025-07-01 22:45:36,028][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000004536_2326528.pth...
[36m[2025-07-01 22:45:36,033][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000004376_2244608.pth
[36m[2025-07-01 22:45:40,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2326528. Throughput: 0: 298.0. Samples: 2333456. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 22:45:40,959][166323] Avg episode reward: [(0, '831.032')]
[36m[2025-07-01 22:45:45,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2326528. Throughput: 0: 292.3. Samples: 2335104. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 22:45:45,948][166323] Avg episode reward: [(0, '829.734')]
[36m[2025-07-01 22:45:50,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2326528. Throughput: 0: 290.0. Samples: 2335920. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 22:45:50,969][166323] Avg episode reward: [(0, '856.416')]
[36m[2025-07-01 22:45:55,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2326528. Throughput: 0: 288.5. Samples: 2337744. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 22:45:55,955][166323] Avg episode reward: [(0, '822.071')]
[36m[2025-07-01 22:46:00,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2326528. Throughput: 0: 288.8. Samples: 2339552. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 22:46:00,971][166323] Avg episode reward: [(0, '869.743')]
[36m[2025-07-01 22:46:05,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2326528. Throughput: 0: 288.9. Samples: 2340448. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 22:46:05,968][166323] Avg episode reward: [(0, '865.003')]
[36m[2025-07-01 22:46:10,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2326528. Throughput: 0: 290.5. Samples: 2342256. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 22:46:10,967][166323] Avg episode reward: [(0, '901.743')]
[31m[7963166 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7963166 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[7963166 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:46:15,993][166323] Fps is (10 sec: 1634.3, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2342912. Throughput: 0: 292.1. Samples: 2343952. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:46:15,993][166323] Avg episode reward: [(0, '865.411')]
[36m[2025-07-01 22:46:20,969][166323] Fps is (10 sec: 1638.1, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2342912. Throughput: 0: 293.9. Samples: 2344864. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:46:20,969][166323] Avg episode reward: [(0, '841.445')]
[36m[2025-07-01 22:46:25,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2342912. Throughput: 0: 293.5. Samples: 2346672. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:46:25,989][166323] Avg episode reward: [(0, '828.165')]
[36m[2025-07-01 22:46:30,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 2342912. Throughput: 0: 296.5. Samples: 2348448. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:46:30,948][166323] Avg episode reward: [(0, '793.300')]
[36m[2025-07-01 22:46:35,970][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2342912. Throughput: 0: 297.9. Samples: 2349328. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:46:35,971][166323] Avg episode reward: [(0, '780.505')]
[36m[2025-07-01 22:46:40,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2342912. Throughput: 0: 296.2. Samples: 2351072. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:46:40,949][166323] Avg episode reward: [(0, '762.661')]
[36m[2025-07-01 22:46:45,975][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2342912. Throughput: 0: 293.7. Samples: 2352768. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:46:45,975][166323] Avg episode reward: [(0, '807.728')]
[36m[2025-07-01 22:46:50,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2342912. Throughput: 0: 294.1. Samples: 2353680. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:46:50,959][166323] Avg episode reward: [(0, '800.066')]
[31m[8004164 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8004164 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[8004165 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:46:55,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2342912. Throughput: 0: 294.8. Samples: 2355520. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:46:55,955][166323] Avg episode reward: [(0, '818.601')]
[31m[8006124 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8006124 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[8006125 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:47:00,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2342912. Throughput: 0: 293.6. Samples: 2357152. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:47:00,952][166323] Avg episode reward: [(0, '833.810')]
[36m[2025-07-01 22:47:05,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2342912. Throughput: 0: 290.5. Samples: 2357936. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 22:47:05,964][166323] Avg episode reward: [(0, '930.241')]
[36m[2025-07-01 22:47:10,964][166323] Fps is (10 sec: 1636.3, 60 sec: 546.2, 300 sec: 333.3). Total num frames: 2359296. Throughput: 0: 289.2. Samples: 2359680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:47:10,965][166323] Avg episode reward: [(0, '905.859')]
[36m[2025-07-01 22:47:15,958][166323] Fps is (10 sec: 1639.4, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 2359296. Throughput: 0: 288.3. Samples: 2361424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:47:15,958][166323] Avg episode reward: [(0, '902.442')]
[36m[2025-07-01 22:47:20,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 2359296. Throughput: 0: 289.6. Samples: 2362368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:47:20,996][166323] Avg episode reward: [(0, '946.004')]
[36m[2025-07-01 22:47:25,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2359296. Throughput: 0: 289.9. Samples: 2364128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:47:25,993][166323] Avg episode reward: [(0, '969.904')]
[36m[2025-07-01 22:47:30,998][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 2359296. Throughput: 0: 290.3. Samples: 2365840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:47:30,998][166323] Avg episode reward: [(0, '907.397')]
[36m[2025-07-01 22:47:35,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2359296. Throughput: 0: 287.8. Samples: 2366640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:47:35,993][166323] Avg episode reward: [(0, '872.688')]
[37m[1m[2025-07-01 22:47:36,051][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000004600_2359296.pth...
[36m[2025-07-01 22:47:36,055][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000004472_2293760.pth
[36m[2025-07-01 22:47:40,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2359296. Throughput: 0: 285.8. Samples: 2368384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:47:40,958][166323] Avg episode reward: [(0, '892.831')]
[36m[2025-07-01 22:47:45,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2359296. Throughput: 0: 289.3. Samples: 2370176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:47:45,966][166323] Avg episode reward: [(0, '847.560')]
[36m[2025-07-01 22:47:50,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2359296. Throughput: 0: 290.5. Samples: 2371008. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:47:50,964][166323] Avg episode reward: [(0, '820.158')]
[36m[2025-07-01 22:47:55,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2359296. Throughput: 0: 290.7. Samples: 2372768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:47:55,980][166323] Avg episode reward: [(0, '791.619')]
[36m[2025-07-01 22:48:00,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2359296. Throughput: 0: 291.5. Samples: 2374544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:48:00,973][166323] Avg episode reward: [(0, '842.589')]
[36m[2025-07-01 22:48:05,962][166323] Fps is (10 sec: 1641.3, 60 sec: 546.1, 300 sec: 333.3). Total num frames: 2375680. Throughput: 0: 291.1. Samples: 2375456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:48:05,962][166323] Avg episode reward: [(0, '786.368')]
[36m[2025-07-01 22:48:10,971][166323] Fps is (10 sec: 1638.6, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2375680. Throughput: 0: 289.9. Samples: 2377168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:48:10,972][166323] Avg episode reward: [(0, '796.769')]
[36m[2025-07-01 22:48:15,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2375680. Throughput: 0: 292.4. Samples: 2378992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:48:15,984][166323] Avg episode reward: [(0, '792.510')]
[36m[2025-07-01 22:48:20,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 333.2). Total num frames: 2375680. Throughput: 0: 295.1. Samples: 2379904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:48:20,943][166323] Avg episode reward: [(0, '834.947')]
[36m[2025-07-01 22:48:25,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2375680. Throughput: 0: 295.7. Samples: 2381696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:48:25,971][166323] Avg episode reward: [(0, '849.328')]
[36m[2025-07-01 22:48:31,002][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2375680. Throughput: 0: 295.2. Samples: 2383472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:48:31,002][166323] Avg episode reward: [(0, '842.428')]
[36m[2025-07-01 22:48:35,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2375680. Throughput: 0: 295.1. Samples: 2384288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:48:35,959][166323] Avg episode reward: [(0, '878.213')]
[36m[2025-07-01 22:48:40,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2375680. Throughput: 0: 295.6. Samples: 2386064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:48:40,963][166323] Avg episode reward: [(0, '833.701')]
[36m[2025-07-01 22:48:45,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2375680. Throughput: 0: 296.5. Samples: 2387888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:48:45,983][166323] Avg episode reward: [(0, '896.529')]
[36m[2025-07-01 22:48:50,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2375680. Throughput: 0: 295.7. Samples: 2388768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:48:50,978][166323] Avg episode reward: [(0, '892.889')]
[36m[2025-07-01 22:48:55,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2375680. Throughput: 0: 295.5. Samples: 2390464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:48:55,967][166323] Avg episode reward: [(0, '888.028')]
[36m[2025-07-01 22:49:00,970][166323] Fps is (10 sec: 1639.7, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 2392064. Throughput: 0: 293.1. Samples: 2392176. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 22:49:00,970][166323] Avg episode reward: [(0, '884.529')]
[36m[2025-07-01 22:49:05,962][166323] Fps is (10 sec: 1639.1, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2392064. Throughput: 0: 291.8. Samples: 2393040. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 22:49:05,962][166323] Avg episode reward: [(0, '803.782')]
[36m[2025-07-01 22:49:10,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2392064. Throughput: 0: 292.4. Samples: 2394848. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 22:49:10,955][166323] Avg episode reward: [(0, '800.039')]
[36m[2025-07-01 22:49:15,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 2392064. Throughput: 0: 292.1. Samples: 2396608. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 22:49:15,966][166323] Avg episode reward: [(0, '811.035')]
[36m[2025-07-01 22:49:20,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2392064. Throughput: 0: 294.1. Samples: 2397520. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 22:49:20,956][166323] Avg episode reward: [(0, '792.676')]
[36m[2025-07-01 22:49:25,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2392064. Throughput: 0: 292.7. Samples: 2399232. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 22:49:25,954][166323] Avg episode reward: [(0, '754.475')]
[36m[2025-07-01 22:49:30,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2392064. Throughput: 0: 293.0. Samples: 2401072. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 22:49:30,975][166323] Avg episode reward: [(0, '706.151')]
[36m[2025-07-01 22:49:35,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2392064. Throughput: 0: 293.9. Samples: 2401984. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 22:49:35,947][166323] Avg episode reward: [(0, '756.383')]
[37m[1m[2025-07-01 22:49:36,028][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000004664_2392064.pth...
[36m[2025-07-01 22:49:36,034][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000004536_2326528.pth
[36m[2025-07-01 22:49:40,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2392064. Throughput: 0: 295.3. Samples: 2403760. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 22:49:40,988][166323] Avg episode reward: [(0, '827.116')]
[36m[2025-07-01 22:49:45,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2392064. Throughput: 0: 296.5. Samples: 2405520. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 22:49:45,974][166323] Avg episode reward: [(0, '785.203')]
[36m[2025-07-01 22:49:50,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2392064. Throughput: 0: 299.4. Samples: 2406512. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 22:49:50,965][166323] Avg episode reward: [(0, '810.029')]
[36m[2025-07-01 22:49:55,953][166323] Fps is (10 sec: 1641.9, 60 sec: 546.3, 300 sec: 333.2). Total num frames: 2408448. Throughput: 0: 302.2. Samples: 2408448. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-01 22:49:55,953][166323] Avg episode reward: [(0, '901.181')]
[36m[2025-07-01 22:50:00,949][166323] Fps is (10 sec: 1641.0, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 2408448. Throughput: 0: 302.7. Samples: 2410224. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-01 22:50:00,949][166323] Avg episode reward: [(0, '947.575')]
[36m[2025-07-01 22:50:05,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2408448. Throughput: 0: 302.4. Samples: 2411136. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-01 22:50:05,976][166323] Avg episode reward: [(0, '924.353')]
[36m[2025-07-01 22:50:10,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2408448. Throughput: 0: 303.9. Samples: 2412912. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-01 22:50:10,970][166323] Avg episode reward: [(0, '920.423')]
[36m[2025-07-01 22:50:15,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2408448. Throughput: 0: 303.0. Samples: 2414704. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-01 22:50:15,965][166323] Avg episode reward: [(0, '947.325')]
[36m[2025-07-01 22:50:20,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2408448. Throughput: 0: 300.5. Samples: 2415520. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-01 22:50:20,988][166323] Avg episode reward: [(0, '921.607')]
[36m[2025-07-01 22:50:25,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2408448. Throughput: 0: 299.9. Samples: 2417248. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-01 22:50:25,970][166323] Avg episode reward: [(0, '796.242')]
[36m[2025-07-01 22:50:30,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2408448. Throughput: 0: 299.3. Samples: 2418992. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-01 22:50:30,987][166323] Avg episode reward: [(0, '764.754')]
[36m[2025-07-01 22:50:35,973][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2408448. Throughput: 0: 296.5. Samples: 2419856. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-01 22:50:35,973][166323] Avg episode reward: [(0, '833.483')]
[36m[2025-07-01 22:50:40,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2408448. Throughput: 0: 294.0. Samples: 2421680. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-01 22:50:40,965][166323] Avg episode reward: [(0, '824.759')]
[36m[2025-07-01 22:50:46,001][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2408448. Throughput: 0: 294.4. Samples: 2423488. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-01 22:50:46,002][166323] Avg episode reward: [(0, '794.531')]
[36m[2025-07-01 22:50:50,972][166323] Fps is (10 sec: 1637.2, 60 sec: 546.1, 300 sec: 333.2). Total num frames: 2424832. Throughput: 0: 294.8. Samples: 2424400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:50:50,973][166323] Avg episode reward: [(0, '831.049')]
[36m[2025-07-01 22:50:56,000][166323] Fps is (10 sec: 1638.5, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 2424832. Throughput: 0: 292.4. Samples: 2426080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:50:56,001][166323] Avg episode reward: [(0, '918.416')]
[36m[2025-07-01 22:51:00,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2424832. Throughput: 0: 292.6. Samples: 2427872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:51:00,969][166323] Avg episode reward: [(0, '975.082')]
[36m[2025-07-01 22:51:05,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2424832. Throughput: 0: 295.7. Samples: 2428816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:51:05,959][166323] Avg episode reward: [(0, '901.603')]
[36m[2025-07-01 22:51:10,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2424832. Throughput: 0: 297.4. Samples: 2430624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:51:10,947][166323] Avg episode reward: [(0, '951.138')]
[31m[8262027 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8262027 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[8262027 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:51:15,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2424832. Throughput: 0: 299.2. Samples: 2432448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:51:15,967][166323] Avg episode reward: [(0, '956.660')]
[36m[2025-07-01 22:51:20,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2424832. Throughput: 0: 300.7. Samples: 2433392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:51:20,989][166323] Avg episode reward: [(0, '974.813')]
[36m[2025-07-01 22:51:25,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2424832. Throughput: 0: 298.9. Samples: 2435136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:51:25,985][166323] Avg episode reward: [(0, '931.690')]
[31m[8277756 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8277757 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[8277757 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:51:30,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2424832. Throughput: 0: 297.7. Samples: 2436880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:51:30,991][166323] Avg episode reward: [(0, '949.676')]
[36m[2025-07-01 22:51:35,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2424832. Throughput: 0: 297.0. Samples: 2437760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:51:35,962][166323] Avg episode reward: [(0, '983.733')]
[37m[1m[2025-07-01 22:51:36,011][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000004728_2424832.pth...
[36m[2025-07-01 22:51:36,015][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000004600_2359296.pth
[36m[2025-07-01 22:51:40,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2424832. Throughput: 0: 300.1. Samples: 2439568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:51:40,950][166323] Avg episode reward: [(0, '971.970')]
[36m[2025-07-01 22:51:45,982][166323] Fps is (10 sec: 1635.2, 60 sec: 546.3, 300 sec: 333.2). Total num frames: 2441216. Throughput: 0: 301.1. Samples: 2441424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:51:45,982][166323] Avg episode reward: [(0, '956.061')]
[36m[2025-07-01 22:51:50,981][166323] Fps is (10 sec: 1633.2, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2441216. Throughput: 0: 301.7. Samples: 2442400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:51:50,982][166323] Avg episode reward: [(0, '908.678')]
[31m[8300291 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8300292 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[8300292 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:51:55,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2441216. Throughput: 0: 301.2. Samples: 2444192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:51:55,995][166323] Avg episode reward: [(0, '919.241')]
[31m[8306008 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8306008 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[8306008 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:52:00,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 2441216. Throughput: 0: 300.9. Samples: 2445984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:52:00,948][166323] Avg episode reward: [(0, '963.072')]
[36m[2025-07-01 22:52:05,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2441216. Throughput: 0: 300.5. Samples: 2446912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:52:05,985][166323] Avg episode reward: [(0, '912.248')]
[36m[2025-07-01 22:52:10,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2441216. Throughput: 0: 304.9. Samples: 2448848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:52:10,959][166323] Avg episode reward: [(0, '864.495')]
[36m[2025-07-01 22:52:15,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2441216. Throughput: 0: 306.5. Samples: 2450672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:52:15,986][166323] Avg episode reward: [(0, '895.510')]
[36m[2025-07-01 22:52:20,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2441216. Throughput: 0: 306.5. Samples: 2451552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:52:20,962][166323] Avg episode reward: [(0, '928.112')]
[36m[2025-07-01 22:52:25,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2441216. Throughput: 0: 303.8. Samples: 2453248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:52:25,977][166323] Avg episode reward: [(0, '965.949')]
[36m[2025-07-01 22:52:30,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2441216. Throughput: 0: 304.2. Samples: 2455104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:52:30,956][166323] Avg episode reward: [(0, '876.581')]
[36m[2025-07-01 22:52:35,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2441216. Throughput: 0: 303.1. Samples: 2456032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:52:35,961][166323] Avg episode reward: [(0, '877.399')]
[36m[2025-07-01 22:52:40,975][166323] Fps is (10 sec: 1635.4, 60 sec: 545.9, 300 sec: 333.2). Total num frames: 2457600. Throughput: 0: 299.5. Samples: 2457664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:52:40,975][166323] Avg episode reward: [(0, '868.684')]
[36m[2025-07-01 22:52:45,988][166323] Fps is (10 sec: 1633.9, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2457600. Throughput: 0: 298.8. Samples: 2459440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:52:45,989][166323] Avg episode reward: [(0, '844.445')]
[36m[2025-07-01 22:52:50,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2457600. Throughput: 0: 300.1. Samples: 2460416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:52:50,990][166323] Avg episode reward: [(0, '886.306')]
[36m[2025-07-01 22:52:55,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 333.3). Total num frames: 2457600. Throughput: 0: 295.9. Samples: 2462160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:52:55,945][166323] Avg episode reward: [(0, '828.958')]
[36m[2025-07-01 22:53:00,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2457600. Throughput: 0: 294.4. Samples: 2463920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:53:00,991][166323] Avg episode reward: [(0, '823.724')]
[36m[2025-07-01 22:53:05,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2457600. Throughput: 0: 295.8. Samples: 2464864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:53:05,962][166323] Avg episode reward: [(0, '849.825')]
[36m[2025-07-01 22:53:10,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2457600. Throughput: 0: 299.1. Samples: 2466704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:53:10,969][166323] Avg episode reward: [(0, '920.987')]
[36m[2025-07-01 22:53:15,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2457600. Throughput: 0: 296.8. Samples: 2468464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:53:15,971][166323] Avg episode reward: [(0, '886.907')]
[36m[2025-07-01 22:53:20,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2457600. Throughput: 0: 296.5. Samples: 2469376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:53:20,962][166323] Avg episode reward: [(0, '843.376')]
[36m[2025-07-01 22:53:25,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2457600. Throughput: 0: 299.1. Samples: 2471120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:53:25,957][166323] Avg episode reward: [(0, '841.880')]
[36m[2025-07-01 22:53:30,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2457600. Throughput: 0: 297.7. Samples: 2472832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:53:30,969][166323] Avg episode reward: [(0, '876.867')]
[36m[2025-07-01 22:53:35,952][166323] Fps is (10 sec: 1639.2, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 2473984. Throughput: 0: 295.4. Samples: 2473696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:53:35,953][166323] Avg episode reward: [(0, '929.739')]
[37m[1m[2025-07-01 22:53:36,003][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000004824_2473984.pth...
[36m[2025-07-01 22:53:36,010][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000004664_2392064.pth
[36m[2025-07-01 22:53:40,972][166323] Fps is (10 sec: 1638.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2473984. Throughput: 0: 294.9. Samples: 2475440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:53:40,972][166323] Avg episode reward: [(0, '946.461')]
[36m[2025-07-01 22:53:45,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 2473984. Throughput: 0: 294.6. Samples: 2477168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:53:45,966][166323] Avg episode reward: [(0, '944.559')]
[36m[2025-07-01 22:53:51,006][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2473984. Throughput: 0: 290.6. Samples: 2477952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:53:51,006][166323] Avg episode reward: [(0, '968.506')]
[36m[2025-07-01 22:53:55,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2473984. Throughput: 0: 291.8. Samples: 2479840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:53:55,990][166323] Avg episode reward: [(0, '970.737')]
[36m[2025-07-01 22:54:00,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 2473984. Throughput: 0: 292.1. Samples: 2481600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:54:00,947][166323] Avg episode reward: [(0, '993.352')]
[36m[2025-07-01 22:54:05,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2473984. Throughput: 0: 291.6. Samples: 2482496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:54:05,961][166323] Avg episode reward: [(0, '896.050')]
[36m[2025-07-01 22:54:10,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2473984. Throughput: 0: 293.6. Samples: 2484336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:54:10,978][166323] Avg episode reward: [(0, '847.068')]
[36m[2025-07-01 22:54:15,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2473984. Throughput: 0: 292.8. Samples: 2486000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:54:15,945][166323] Avg episode reward: [(0, '849.671')]
[36m[2025-07-01 22:54:21,003][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 2473984. Throughput: 0: 295.1. Samples: 2486992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:54:21,003][166323] Avg episode reward: [(0, '885.353')]
[36m[2025-07-01 22:54:25,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2473984. Throughput: 0: 296.2. Samples: 2488768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 22:54:25,975][166323] Avg episode reward: [(0, '886.437')]
[36m[2025-07-01 22:54:31,017][166323] Fps is (10 sec: 1636.1, 60 sec: 545.7, 300 sec: 333.2). Total num frames: 2490368. Throughput: 0: 294.4. Samples: 2490432. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 22:54:31,017][166323] Avg episode reward: [(0, '882.135')]
[31m[8461465 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8461465 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[8461465 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:54:35,945][166323] Fps is (10 sec: 1643.4, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 2490368. Throughput: 0: 296.6. Samples: 2491280. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 22:54:35,945][166323] Avg episode reward: [(0, '886.909')]
[36m[2025-07-01 22:54:40,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2490368. Throughput: 0: 292.3. Samples: 2492992. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 22:54:40,978][166323] Avg episode reward: [(0, '932.379')]
[36m[2025-07-01 22:54:45,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2490368. Throughput: 0: 292.4. Samples: 2494768. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 22:54:45,988][166323] Avg episode reward: [(0, '837.077')]
[36m[2025-07-01 22:54:50,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2490368. Throughput: 0: 292.4. Samples: 2495664. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 22:54:50,988][166323] Avg episode reward: [(0, '815.577')]
[36m[2025-07-01 22:54:55,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2490368. Throughput: 0: 291.2. Samples: 2497440. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 22:54:55,983][166323] Avg episode reward: [(0, '802.501')]
[36m[2025-07-01 22:55:00,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2490368. Throughput: 0: 292.9. Samples: 2499184. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 22:55:00,957][166323] Avg episode reward: [(0, '777.607')]
[36m[2025-07-01 22:55:05,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2490368. Throughput: 0: 291.9. Samples: 2500112. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 22:55:05,945][166323] Avg episode reward: [(0, '762.262')]
[36m[2025-07-01 22:55:11,007][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2490368. Throughput: 0: 288.5. Samples: 2501760. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 22:55:11,007][166323] Avg episode reward: [(0, '766.145')]
[36m[2025-07-01 22:55:15,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2490368. Throughput: 0: 290.3. Samples: 2503488. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 22:55:15,984][166323] Avg episode reward: [(0, '861.064')]
[36m[2025-07-01 22:55:21,005][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2490368. Throughput: 0: 290.1. Samples: 2504352. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 22:55:21,006][166323] Avg episode reward: [(0, '913.932')]
[36m[2025-07-01 22:55:25,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2490368. Throughput: 0: 292.5. Samples: 2506144. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 22:55:25,945][166323] Avg episode reward: [(0, '930.165')]
[36m[2025-07-01 22:55:30,969][166323] Fps is (10 sec: 1644.3, 60 sec: 273.3, 300 sec: 333.2). Total num frames: 2506752. Throughput: 0: 290.2. Samples: 2507824. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:55:30,969][166323] Avg episode reward: [(0, '926.192')]
[36m[2025-07-01 22:55:35,953][166323] Fps is (10 sec: 1637.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2506752. Throughput: 0: 291.8. Samples: 2508784. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:55:35,954][166323] Avg episode reward: [(0, '992.086')]
[37m[1m[2025-07-01 22:55:36,004][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000004888_2506752.pth...
[36m[2025-07-01 22:55:36,009][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000004728_2424832.pth
[36m[2025-07-01 22:55:40,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2506752. Throughput: 0: 290.8. Samples: 2510528. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:55:40,988][166323] Avg episode reward: [(0, '1025.290')]
[36m[2025-07-01 22:55:45,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 2506752. Throughput: 0: 294.1. Samples: 2512416. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:55:45,943][166323] Avg episode reward: [(0, '1015.796')]
[36m[2025-07-01 22:55:50,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2506752. Throughput: 0: 291.2. Samples: 2513216. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:55:50,948][166323] Avg episode reward: [(0, '1019.150')]
[33m[8541607 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[8541608 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.787109375
[33mCrash Rate: 0.18994140625
[33mTimeout Rate: 0.02294921875 (navigation_task.py:265)
[33m[8541608 ms][navigation_task] - WARNING : 
[33mSuccesses: 1612
[33mCrashes : 389
[33mTimeouts: 47 (navigation_task.py:268)
[36m[2025-07-01 22:55:55,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2506752. Throughput: 0: 296.1. Samples: 2515072. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:55:55,968][166323] Avg episode reward: [(0, '931.686')]
[36m[2025-07-01 22:56:00,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2506752. Throughput: 0: 296.8. Samples: 2516848. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:56:00,992][166323] Avg episode reward: [(0, '982.847')]
[36m[2025-07-01 22:56:05,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2506752. Throughput: 0: 296.3. Samples: 2517680. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:56:05,989][166323] Avg episode reward: [(0, '1007.116')]
[36m[2025-07-01 22:56:10,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 2506752. Throughput: 0: 296.9. Samples: 2519504. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:56:10,946][166323] Avg episode reward: [(0, '1023.420')]
[36m[2025-07-01 22:56:15,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2506752. Throughput: 0: 299.1. Samples: 2521280. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:56:15,955][166323] Avg episode reward: [(0, '1007.430')]
[36m[2025-07-01 22:56:20,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 2506752. Throughput: 0: 298.7. Samples: 2522224. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-01 22:56:20,948][166323] Avg episode reward: [(0, '979.560')]
[36m[2025-07-01 22:56:25,987][166323] Fps is (10 sec: 1633.2, 60 sec: 545.8, 300 sec: 333.2). Total num frames: 2523136. Throughput: 0: 297.2. Samples: 2523904. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-01 22:56:25,987][166323] Avg episode reward: [(0, '1033.752')]
[36m[2025-07-01 22:56:30,967][166323] Fps is (10 sec: 1635.2, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2523136. Throughput: 0: 293.2. Samples: 2525616. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-01 22:56:30,968][166323] Avg episode reward: [(0, '1044.807')]
[36m[2025-07-01 22:56:35,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2523136. Throughput: 0: 297.5. Samples: 2526608. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-01 22:56:35,966][166323] Avg episode reward: [(0, '1006.850')]
[36m[2025-07-01 22:56:40,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 2523136. Throughput: 0: 296.7. Samples: 2528416. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-01 22:56:40,946][166323] Avg episode reward: [(0, '939.232')]
[36m[2025-07-01 22:56:45,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2523136. Throughput: 0: 295.0. Samples: 2530112. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-01 22:56:45,962][166323] Avg episode reward: [(0, '920.719')]
[36m[2025-07-01 22:56:50,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2523136. Throughput: 0: 296.0. Samples: 2530992. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-01 22:56:50,966][166323] Avg episode reward: [(0, '1000.456')]
[36m[2025-07-01 22:56:55,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2523136. Throughput: 0: 293.3. Samples: 2532704. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-01 22:56:55,948][166323] Avg episode reward: [(0, '954.076')]
[36m[2025-07-01 22:57:00,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 2523136. Throughput: 0: 294.4. Samples: 2534528. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-01 22:57:00,949][166323] Avg episode reward: [(0, '987.274')]
[36m[2025-07-01 22:57:05,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2523136. Throughput: 0: 291.8. Samples: 2535360. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-01 22:57:05,966][166323] Avg episode reward: [(0, '988.232')]
[36m[2025-07-01 22:57:10,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2523136. Throughput: 0: 294.4. Samples: 2537152. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-01 22:57:10,993][166323] Avg episode reward: [(0, '1010.360')]
[36m[2025-07-01 22:57:15,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2523136. Throughput: 0: 296.6. Samples: 2538960. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-01 22:57:15,954][166323] Avg episode reward: [(0, '1020.404')]
[31m[8628465 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8628466 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[8628466 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:57:20,985][166323] Fps is (10 sec: 1639.6, 60 sec: 545.8, 300 sec: 333.2). Total num frames: 2539520. Throughput: 0: 292.1. Samples: 2539760. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 22:57:20,986][166323] Avg episode reward: [(0, '986.409')]
[36m[2025-07-01 22:57:25,957][166323] Fps is (10 sec: 1637.9, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 2539520. Throughput: 0: 294.3. Samples: 2541664. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 22:57:25,957][166323] Avg episode reward: [(0, '998.573')]
[36m[2025-07-01 22:57:31,006][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 2539520. Throughput: 0: 297.7. Samples: 2543520. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 22:57:31,006][166323] Avg episode reward: [(0, '996.382')]
[36m[2025-07-01 22:57:35,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2539520. Throughput: 0: 298.3. Samples: 2544416. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 22:57:35,975][166323] Avg episode reward: [(0, '1005.757')]
[37m[1m[2025-07-01 22:57:36,028][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000004952_2539520.pth...
[36m[2025-07-01 22:57:36,033][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000004824_2473984.pth
[36m[2025-07-01 22:57:40,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2539520. Throughput: 0: 297.9. Samples: 2546112. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 22:57:40,962][166323] Avg episode reward: [(0, '1026.926')]
[36m[2025-07-01 22:57:45,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2539520. Throughput: 0: 299.9. Samples: 2548032. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 22:57:45,981][166323] Avg episode reward: [(0, '1003.309')]
[31m[8657408 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8657408 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[8657409 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:57:50,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2539520. Throughput: 0: 300.4. Samples: 2548880. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 22:57:50,970][166323] Avg episode reward: [(0, '994.181')]
[36m[2025-07-01 22:57:55,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2539520. Throughput: 0: 302.8. Samples: 2550768. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 22:57:55,962][166323] Avg episode reward: [(0, '978.026')]
[36m[2025-07-01 22:58:00,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2539520. Throughput: 0: 301.7. Samples: 2552544. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 22:58:00,986][166323] Avg episode reward: [(0, '926.795')]
[36m[2025-07-01 22:58:05,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2539520. Throughput: 0: 300.9. Samples: 2553296. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 22:58:05,975][166323] Avg episode reward: [(0, '848.283')]
[36m[2025-07-01 22:58:10,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2539520. Throughput: 0: 296.5. Samples: 2555008. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 22:58:10,957][166323] Avg episode reward: [(0, '846.953')]
[36m[2025-07-01 22:58:15,948][166323] Fps is (10 sec: 1642.7, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 2555904. Throughput: 0: 291.9. Samples: 2556640. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 22:58:15,948][166323] Avg episode reward: [(0, '805.523')]
[36m[2025-07-01 22:58:20,971][166323] Fps is (10 sec: 1636.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2555904. Throughput: 0: 291.2. Samples: 2557520. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 22:58:20,971][166323] Avg episode reward: [(0, '841.782')]
[36m[2025-07-01 22:58:25,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2555904. Throughput: 0: 292.1. Samples: 2559264. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 22:58:25,981][166323] Avg episode reward: [(0, '800.689')]
[36m[2025-07-01 22:58:30,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 2555904. Throughput: 0: 287.4. Samples: 2560960. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 22:58:30,964][166323] Avg episode reward: [(0, '790.448')]
[36m[2025-07-01 22:58:35,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2555904. Throughput: 0: 288.4. Samples: 2561856. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 22:58:35,965][166323] Avg episode reward: [(0, '849.564')]
[36m[2025-07-01 22:58:40,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2555904. Throughput: 0: 287.4. Samples: 2563712. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 22:58:40,992][166323] Avg episode reward: [(0, '868.068')]
[36m[2025-07-01 22:58:45,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2555904. Throughput: 0: 284.7. Samples: 2565344. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 22:58:45,952][166323] Avg episode reward: [(0, '864.685')]
[36m[2025-07-01 22:58:50,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2555904. Throughput: 0: 288.8. Samples: 2566288. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 22:58:50,956][166323] Avg episode reward: [(0, '906.316')]
[36m[2025-07-01 22:58:55,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2555904. Throughput: 0: 292.3. Samples: 2568160. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 22:58:55,957][166323] Avg episode reward: [(0, '956.048')]
[36m[2025-07-01 22:59:00,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2555904. Throughput: 0: 292.8. Samples: 2569824. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 22:59:00,981][166323] Avg episode reward: [(0, '999.934')]
[36m[2025-07-01 22:59:05,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2555904. Throughput: 0: 292.7. Samples: 2570688. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 22:59:05,956][166323] Avg episode reward: [(0, '962.952')]
[36m[2025-07-01 22:59:10,978][166323] Fps is (10 sec: 1638.9, 60 sec: 545.9, 300 sec: 333.2). Total num frames: 2572288. Throughput: 0: 291.9. Samples: 2572400. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:59:10,978][166323] Avg episode reward: [(0, '1002.631')]
[31m[8741015 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8741016 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[8741016 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:59:15,945][166323] Fps is (10 sec: 1640.1, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 2572288. Throughput: 0: 294.5. Samples: 2574208. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:59:15,945][166323] Avg episode reward: [(0, '1008.903')]
[36m[2025-07-01 22:59:20,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2572288. Throughput: 0: 294.3. Samples: 2575104. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:59:20,973][166323] Avg episode reward: [(0, '920.982')]
[36m[2025-07-01 22:59:25,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2572288. Throughput: 0: 292.8. Samples: 2576880. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:59:25,966][166323] Avg episode reward: [(0, '937.755')]
[36m[2025-07-01 22:59:30,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2572288. Throughput: 0: 296.3. Samples: 2578688. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:59:30,989][166323] Avg episode reward: [(0, '926.656')]
[36m[2025-07-01 22:59:35,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2572288. Throughput: 0: 296.6. Samples: 2579632. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:59:35,949][166323] Avg episode reward: [(0, '888.007')]
[37m[1m[2025-07-01 22:59:35,997][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000005016_2572288.pth...
[36m[2025-07-01 22:59:36,001][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000004888_2506752.pth
[31m[8769148 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8769148 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[8769149 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:59:40,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2572288. Throughput: 0: 296.0. Samples: 2581488. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:59:40,989][166323] Avg episode reward: [(0, '803.507')]
[36m[2025-07-01 22:59:45,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2572288. Throughput: 0: 299.8. Samples: 2583312. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:59:45,976][166323] Avg episode reward: [(0, '870.024')]
[31m[8775251 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8775251 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[8775251 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:59:50,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2572288. Throughput: 0: 300.8. Samples: 2584224. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:59:50,960][166323] Avg episode reward: [(0, '870.431')]
[31m[8780686 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8780686 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[8780687 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 22:59:55,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2572288. Throughput: 0: 302.6. Samples: 2586016. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 22:59:55,977][166323] Avg episode reward: [(0, '833.411')]
[36m[2025-07-01 23:00:00,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2572288. Throughput: 0: 303.5. Samples: 2587872. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-01 23:00:00,960][166323] Avg episode reward: [(0, '874.637')]
[31m[8792225 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8792226 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[8792226 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:00:05,978][166323] Fps is (10 sec: 1638.1, 60 sec: 545.9, 300 sec: 333.3). Total num frames: 2588672. Throughput: 0: 302.2. Samples: 2588704. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:00:05,978][166323] Avg episode reward: [(0, '846.719')]
[36m[2025-07-01 23:00:10,952][166323] Fps is (10 sec: 1639.6, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 2588672. Throughput: 0: 300.2. Samples: 2590384. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:00:10,953][166323] Avg episode reward: [(0, '916.828')]
[36m[2025-07-01 23:00:15,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.3). Total num frames: 2588672. Throughput: 0: 297.4. Samples: 2592064. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:00:15,967][166323] Avg episode reward: [(0, '901.977')]
[36m[2025-07-01 23:00:20,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2588672. Throughput: 0: 295.6. Samples: 2592944. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:00:20,982][166323] Avg episode reward: [(0, '913.402')]
[36m[2025-07-01 23:00:25,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2588672. Throughput: 0: 294.0. Samples: 2594704. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:00:25,947][166323] Avg episode reward: [(0, '906.338')]
[36m[2025-07-01 23:00:30,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2588672. Throughput: 0: 293.7. Samples: 2596528. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:00:30,968][166323] Avg episode reward: [(0, '910.402')]
[31m[8823582 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8823582 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[8823582 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:00:35,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2588672. Throughput: 0: 293.3. Samples: 2597424. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:00:35,962][166323] Avg episode reward: [(0, '930.590')]
[31m[8825227 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8825227 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[8825227 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:00:40,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2588672. Throughput: 0: 290.9. Samples: 2599104. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:00:40,965][166323] Avg episode reward: [(0, '884.087')]
[36m[2025-07-01 23:00:45,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2588672. Throughput: 0: 288.5. Samples: 2600864. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:00:45,986][166323] Avg episode reward: [(0, '872.579')]
[36m[2025-07-01 23:00:50,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2588672. Throughput: 0: 292.4. Samples: 2601856. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:00:50,955][166323] Avg episode reward: [(0, '922.476')]
[36m[2025-07-01 23:00:55,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2588672. Throughput: 0: 294.8. Samples: 2603648. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:00:55,950][166323] Avg episode reward: [(0, '914.003')]
[31m[8845987 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8845987 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[8845988 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:01:00,974][166323] Fps is (10 sec: 1635.3, 60 sec: 546.0, 300 sec: 333.3). Total num frames: 2605056. Throughput: 0: 296.1. Samples: 2605392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:01:00,974][166323] Avg episode reward: [(0, '906.768')]
[36m[2025-07-01 23:01:05,944][166323] Fps is (10 sec: 1639.3, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 2605056. Throughput: 0: 296.8. Samples: 2606288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:01:05,944][166323] Avg episode reward: [(0, '929.498')]
[36m[2025-07-01 23:01:10,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2605056. Throughput: 0: 296.8. Samples: 2608064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:01:10,961][166323] Avg episode reward: [(0, '934.216')]
[36m[2025-07-01 23:01:15,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2605056. Throughput: 0: 295.4. Samples: 2609824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:01:15,981][166323] Avg episode reward: [(0, '929.854')]
[36m[2025-07-01 23:01:21,000][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2605056. Throughput: 0: 292.4. Samples: 2610592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:01:21,000][166323] Avg episode reward: [(0, '895.508')]
[36m[2025-07-01 23:01:25,979][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2605056. Throughput: 0: 293.2. Samples: 2612304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:01:25,980][166323] Avg episode reward: [(0, '877.850')]
[36m[2025-07-01 23:01:30,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2605056. Throughput: 0: 292.3. Samples: 2614016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:01:30,975][166323] Avg episode reward: [(0, '932.556')]
[31m[8880248 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8880248 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[8880248 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:01:35,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2605056. Throughput: 0: 289.3. Samples: 2614880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:01:35,971][166323] Avg episode reward: [(0, '845.845')]
[37m[1m[2025-07-01 23:01:36,020][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000005080_2605056.pth...
[36m[2025-07-01 23:01:36,024][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000004952_2539520.pth
[36m[2025-07-01 23:01:40,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2605056. Throughput: 0: 290.7. Samples: 2616736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:01:40,986][166323] Avg episode reward: [(0, '918.852')]
[36m[2025-07-01 23:01:45,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2605056. Throughput: 0: 291.8. Samples: 2618528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:01:45,994][166323] Avg episode reward: [(0, '939.697')]
[36m[2025-07-01 23:01:50,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2605056. Throughput: 0: 289.4. Samples: 2619312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:01:50,946][166323] Avg episode reward: [(0, '899.625')]
[36m[2025-07-01 23:01:55,977][166323] Fps is (10 sec: 1641.2, 60 sec: 545.9, 300 sec: 333.2). Total num frames: 2621440. Throughput: 0: 290.7. Samples: 2621152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:01:55,977][166323] Avg episode reward: [(0, '973.099')]
[31m[8906817 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8906817 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[8906817 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:02:00,969][166323] Fps is (10 sec: 1634.7, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2621440. Throughput: 0: 292.7. Samples: 2622992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:02:00,969][166323] Avg episode reward: [(0, '970.003')]
[36m[2025-07-01 23:02:05,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 2621440. Throughput: 0: 293.8. Samples: 2623808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:02:05,986][166323] Avg episode reward: [(0, '1028.854')]
[36m[2025-07-01 23:02:10,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2621440. Throughput: 0: 293.7. Samples: 2625520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:02:10,972][166323] Avg episode reward: [(0, '904.654')]
[36m[2025-07-01 23:02:15,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2621440. Throughput: 0: 294.2. Samples: 2627248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:02:15,955][166323] Avg episode reward: [(0, '939.578')]
[31m[8927688 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8927689 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[8927689 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:02:20,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 2621440. Throughput: 0: 295.2. Samples: 2628160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:02:20,959][166323] Avg episode reward: [(0, '854.375')]
[36m[2025-07-01 23:02:25,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2621440. Throughput: 0: 292.5. Samples: 2629904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:02:25,992][166323] Avg episode reward: [(0, '853.711')]
[36m[2025-07-01 23:02:30,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2621440. Throughput: 0: 291.8. Samples: 2631648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:02:30,960][166323] Avg episode reward: [(0, '886.132')]
[36m[2025-07-01 23:02:35,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2621440. Throughput: 0: 293.0. Samples: 2632496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:02:35,949][166323] Avg episode reward: [(0, '868.809')]
[36m[2025-07-01 23:02:40,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2621440. Throughput: 0: 289.9. Samples: 2634192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:02:40,962][166323] Avg episode reward: [(0, '925.861')]
[36m[2025-07-01 23:02:45,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2621440. Throughput: 0: 289.8. Samples: 2636032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:02:45,962][166323] Avg episode reward: [(0, '893.039')]
[36m[2025-07-01 23:02:50,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2621440. Throughput: 0: 290.8. Samples: 2636896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:02:50,986][166323] Avg episode reward: [(0, '971.778')]
[31m[8959827 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8959827 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[8959828 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:02:55,960][166323] Fps is (10 sec: 1638.7, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 2637824. Throughput: 0: 292.0. Samples: 2638656. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 23:02:55,960][166323] Avg episode reward: [(0, '994.826')]
[36m[2025-07-01 23:03:00,956][166323] Fps is (10 sec: 1643.2, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 2637824. Throughput: 0: 293.7. Samples: 2640464. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 23:03:00,957][166323] Avg episode reward: [(0, '975.969')]
[36m[2025-07-01 23:03:05,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 2637824. Throughput: 0: 293.4. Samples: 2641360. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 23:03:05,954][166323] Avg episode reward: [(0, '1001.687')]
[36m[2025-07-01 23:03:10,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2637824. Throughput: 0: 293.4. Samples: 2643104. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 23:03:10,982][166323] Avg episode reward: [(0, '943.603')]
[36m[2025-07-01 23:03:15,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2637824. Throughput: 0: 295.3. Samples: 2644944. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 23:03:15,987][166323] Avg episode reward: [(0, '960.544')]
[36m[2025-07-01 23:03:20,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2637824. Throughput: 0: 295.7. Samples: 2645808. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 23:03:20,975][166323] Avg episode reward: [(0, '949.237')]
[36m[2025-07-01 23:03:25,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2637824. Throughput: 0: 297.6. Samples: 2647584. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 23:03:25,955][166323] Avg episode reward: [(0, '928.431')]
[36m[2025-07-01 23:03:30,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2637824. Throughput: 0: 296.6. Samples: 2649376. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 23:03:30,956][166323] Avg episode reward: [(0, '847.713')]
[36m[2025-07-01 23:03:35,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2637824. Throughput: 0: 299.1. Samples: 2650352. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 23:03:35,981][166323] Avg episode reward: [(0, '829.310')]
[37m[1m[2025-07-01 23:03:36,042][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000005144_2637824.pth...
[36m[2025-07-01 23:03:36,046][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000005016_2572288.pth
[36m[2025-07-01 23:03:40,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2637824. Throughput: 0: 302.0. Samples: 2652240. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 23:03:40,944][166323] Avg episode reward: [(0, '878.640')]
[36m[2025-07-01 23:03:46,003][166323] Fps is (10 sec: 1634.8, 60 sec: 545.8, 300 sec: 333.2). Total num frames: 2654208. Throughput: 0: 304.4. Samples: 2654176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:03:46,004][166323] Avg episode reward: [(0, '918.849')]
[36m[2025-07-01 23:03:50,995][166323] Fps is (10 sec: 1630.0, 60 sec: 546.0, 300 sec: 333.2). Total num frames: 2654208. Throughput: 0: 304.4. Samples: 2655072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:03:50,995][166323] Avg episode reward: [(0, '870.861')]
[31m[9020511 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9020511 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[9020511 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:03:55,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.3). Total num frames: 2654208. Throughput: 0: 305.9. Samples: 2656864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:03:55,965][166323] Avg episode reward: [(0, '911.022')]
[36m[2025-07-01 23:04:00,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2654208. Throughput: 0: 300.9. Samples: 2658480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:04:00,971][166323] Avg episode reward: [(0, '922.258')]
[36m[2025-07-01 23:04:05,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2654208. Throughput: 0: 301.2. Samples: 2659360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:04:05,973][166323] Avg episode reward: [(0, '962.872')]
[36m[2025-07-01 23:04:10,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2654208. Throughput: 0: 302.7. Samples: 2661216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:04:10,985][166323] Avg episode reward: [(0, '961.474')]
[36m[2025-07-01 23:04:15,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2654208. Throughput: 0: 304.3. Samples: 2663072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:04:15,968][166323] Avg episode reward: [(0, '904.357')]
[36m[2025-07-01 23:04:21,024][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 2654208. Throughput: 0: 303.0. Samples: 2664000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:04:21,024][166323] Avg episode reward: [(0, '885.691')]
[36m[2025-07-01 23:04:25,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2654208. Throughput: 0: 301.1. Samples: 2665792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:04:25,950][166323] Avg episode reward: [(0, '879.012')]
[36m[2025-07-01 23:04:30,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2654208. Throughput: 0: 298.2. Samples: 2667584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:04:30,959][166323] Avg episode reward: [(0, '916.207')]
[36m[2025-07-01 23:04:35,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2654208. Throughput: 0: 298.8. Samples: 2668512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:04:35,983][166323] Avg episode reward: [(0, '899.555')]
[36m[2025-07-01 23:04:40,958][166323] Fps is (10 sec: 1638.6, 60 sec: 546.0, 300 sec: 333.3). Total num frames: 2670592. Throughput: 0: 298.7. Samples: 2670304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:04:40,958][166323] Avg episode reward: [(0, '885.742')]
[36m[2025-07-01 23:04:45,955][166323] Fps is (10 sec: 1642.8, 60 sec: 273.3, 300 sec: 333.2). Total num frames: 2670592. Throughput: 0: 302.0. Samples: 2672064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:04:45,956][166323] Avg episode reward: [(0, '868.671')]
[36m[2025-07-01 23:04:50,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 2670592. Throughput: 0: 303.7. Samples: 2673024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:04:50,959][166323] Avg episode reward: [(0, '891.203')]
[36m[2025-07-01 23:04:55,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2670592. Throughput: 0: 302.7. Samples: 2674832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:04:55,965][166323] Avg episode reward: [(0, '854.527')]
[36m[2025-07-01 23:05:00,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2670592. Throughput: 0: 299.7. Samples: 2676560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:05:00,978][166323] Avg episode reward: [(0, '854.120')]
[36m[2025-07-01 23:05:05,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2670592. Throughput: 0: 300.7. Samples: 2677520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:05:05,979][166323] Avg episode reward: [(0, '865.132')]
[36m[2025-07-01 23:05:10,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2670592. Throughput: 0: 297.9. Samples: 2679200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:05:10,958][166323] Avg episode reward: [(0, '842.481')]
[36m[2025-07-01 23:05:15,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2670592. Throughput: 0: 300.5. Samples: 2681104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:05:15,957][166323] Avg episode reward: [(0, '929.131')]
[36m[2025-07-01 23:05:20,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2670592. Throughput: 0: 298.6. Samples: 2681952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:05:20,988][166323] Avg episode reward: [(0, '931.245')]
[36m[2025-07-01 23:05:25,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2670592. Throughput: 0: 295.6. Samples: 2683616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:05:25,992][166323] Avg episode reward: [(0, '903.096')]
[36m[2025-07-01 23:05:30,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2670592. Throughput: 0: 296.2. Samples: 2685392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:05:30,949][166323] Avg episode reward: [(0, '927.475')]
[36m[2025-07-01 23:05:35,970][166323] Fps is (10 sec: 1642.1, 60 sec: 546.3, 300 sec: 333.2). Total num frames: 2686976. Throughput: 0: 295.8. Samples: 2686336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:05:35,970][166323] Avg episode reward: [(0, '904.623')]
[37m[1m[2025-07-01 23:05:36,028][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000005240_2686976.pth...
[36m[2025-07-01 23:05:36,034][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000005080_2605056.pth
[36m[2025-07-01 23:05:40,966][166323] Fps is (10 sec: 1635.7, 60 sec: 273.0, 300 sec: 333.3). Total num frames: 2686976. Throughput: 0: 293.3. Samples: 2688032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:05:40,966][166323] Avg episode reward: [(0, '961.062')]
[31m[9130827 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9130828 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[9130828 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:05:45,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2686976. Throughput: 0: 296.0. Samples: 2689872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:05:45,944][166323] Avg episode reward: [(0, '882.297')]
[36m[2025-07-01 23:05:50,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2686976. Throughput: 0: 294.8. Samples: 2690784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:05:50,979][166323] Avg episode reward: [(0, '901.237')]
[36m[2025-07-01 23:05:55,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2686976. Throughput: 0: 300.1. Samples: 2692704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:05:55,963][166323] Avg episode reward: [(0, '938.952')]
[36m[2025-07-01 23:06:00,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2686976. Throughput: 0: 295.8. Samples: 2694416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:06:00,962][166323] Avg episode reward: [(0, '856.665')]
[36m[2025-07-01 23:06:05,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2686976. Throughput: 0: 296.1. Samples: 2695264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:06:05,952][166323] Avg episode reward: [(0, '906.975')]
[36m[2025-07-01 23:06:10,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2686976. Throughput: 0: 300.5. Samples: 2697136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:06:10,982][166323] Avg episode reward: [(0, '847.934')]
[36m[2025-07-01 23:06:15,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2686976. Throughput: 0: 301.3. Samples: 2698960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:06:15,974][166323] Avg episode reward: [(0, '890.931')]
[33m[9166642 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[9166643 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.8004878163337708
[33mCrash Rate: 0.18000000715255737
[33mTimeout Rate: 0.019512195140123367 (navigation_task.py:265)
[33m[9166643 ms][navigation_task] - WARNING : 
[33mSuccesses: 1641
[33mCrashes : 369
[33mTimeouts: 40 (navigation_task.py:268)
[36m[2025-07-01 23:06:20,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2686976. Throughput: 0: 299.8. Samples: 2699824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:06:20,963][166323] Avg episode reward: [(0, '882.951')]
[31m[9172160 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9172160 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[9172161 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[9173577 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9173578 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[9173578 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:06:25,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2686976. Throughput: 0: 298.7. Samples: 2701472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:06:25,956][166323] Avg episode reward: [(0, '910.311')]
[36m[2025-07-01 23:06:30,998][166323] Fps is (10 sec: 1632.8, 60 sec: 545.7, 300 sec: 333.2). Total num frames: 2703360. Throughput: 0: 297.6. Samples: 2703280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:06:30,998][166323] Avg episode reward: [(0, '957.781')]
[36m[2025-07-01 23:06:35,983][166323] Fps is (10 sec: 1633.9, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2703360. Throughput: 0: 297.2. Samples: 2704160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:06:35,983][166323] Avg episode reward: [(0, '896.859')]
[36m[2025-07-01 23:06:40,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.3). Total num frames: 2703360. Throughput: 0: 294.3. Samples: 2705952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:06:40,971][166323] Avg episode reward: [(0, '946.312')]
[36m[2025-07-01 23:06:45,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 333.2). Total num frames: 2703360. Throughput: 0: 296.0. Samples: 2707744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:06:45,992][166323] Avg episode reward: [(0, '969.001')]
[36m[2025-07-01 23:06:50,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2703360. Throughput: 0: 297.5. Samples: 2708656. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:06:50,974][166323] Avg episode reward: [(0, '944.994')]
[36m[2025-07-01 23:06:55,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2703360. Throughput: 0: 293.2. Samples: 2710320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:06:55,943][166323] Avg episode reward: [(0, '873.124')]
[36m[2025-07-01 23:07:00,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2703360. Throughput: 0: 292.8. Samples: 2712128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:07:00,950][166323] Avg episode reward: [(0, '912.333')]
[36m[2025-07-01 23:07:05,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2703360. Throughput: 0: 291.5. Samples: 2712944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:07:05,975][166323] Avg episode reward: [(0, '907.585')]
[36m[2025-07-01 23:07:10,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2703360. Throughput: 0: 293.9. Samples: 2714704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:07:10,982][166323] Avg episode reward: [(0, '900.495')]
[36m[2025-07-01 23:07:15,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2703360. Throughput: 0: 294.1. Samples: 2716512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:07:15,985][166323] Avg episode reward: [(0, '846.579')]
[36m[2025-07-01 23:07:20,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2703360. Throughput: 0: 296.2. Samples: 2717488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:07:20,984][166323] Avg episode reward: [(0, '921.315')]
[36m[2025-07-01 23:07:26,005][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 2703360. Throughput: 0: 293.8. Samples: 2719184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:07:26,005][166323] Avg episode reward: [(0, '1005.557')]
[36m[2025-07-01 23:07:30,959][166323] Fps is (10 sec: 1642.3, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 2719744. Throughput: 0: 294.6. Samples: 2720992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:07:30,960][166323] Avg episode reward: [(0, '960.872')]
[36m[2025-07-01 23:07:35,951][166323] Fps is (10 sec: 1647.3, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 2719744. Throughput: 0: 296.7. Samples: 2722000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:07:35,951][166323] Avg episode reward: [(0, '1025.830')]
[37m[1m[2025-07-01 23:07:36,002][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000005304_2719744.pth...
[36m[2025-07-01 23:07:36,005][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000005144_2637824.pth
[36m[2025-07-01 23:07:40,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 2719744. Throughput: 0: 300.1. Samples: 2723824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:07:40,947][166323] Avg episode reward: [(0, '1024.629')]
[36m[2025-07-01 23:07:45,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2719744. Throughput: 0: 299.6. Samples: 2725616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:07:45,976][166323] Avg episode reward: [(0, '1057.468')]
[36m[2025-07-01 23:07:50,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2719744. Throughput: 0: 301.3. Samples: 2726496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:07:50,950][166323] Avg episode reward: [(0, '1033.459')]
[36m[2025-07-01 23:07:55,978][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2719744. Throughput: 0: 303.0. Samples: 2728336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:07:55,978][166323] Avg episode reward: [(0, '988.543')]
[36m[2025-07-01 23:08:00,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2719744. Throughput: 0: 300.3. Samples: 2730016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:08:00,951][166323] Avg episode reward: [(0, '1001.132')]
[36m[2025-07-01 23:08:05,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2719744. Throughput: 0: 296.6. Samples: 2730832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:08:05,971][166323] Avg episode reward: [(0, '992.022')]
[31m[9275948 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9275949 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[9275949 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:08:10,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2719744. Throughput: 0: 300.0. Samples: 2732672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:08:10,965][166323] Avg episode reward: [(0, '1005.670')]
[36m[2025-07-01 23:08:15,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2719744. Throughput: 0: 298.9. Samples: 2734448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:08:15,973][166323] Avg episode reward: [(0, '1016.349')]
[36m[2025-07-01 23:08:20,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2719744. Throughput: 0: 294.8. Samples: 2735264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:08:20,949][166323] Avg episode reward: [(0, '928.422')]
[31m[9292929 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9292930 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[9292930 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:08:25,945][166323] Fps is (10 sec: 1643.1, 60 sec: 546.7, 300 sec: 333.2). Total num frames: 2736128. Throughput: 0: 290.5. Samples: 2736896. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 23:08:25,945][166323] Avg episode reward: [(0, '869.986')]
[36m[2025-07-01 23:08:30,950][166323] Fps is (10 sec: 1638.1, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 2736128. Throughput: 0: 287.8. Samples: 2738560. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 23:08:30,951][166323] Avg episode reward: [(0, '863.379')]
[36m[2025-07-01 23:08:36,000][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 333.2). Total num frames: 2736128. Throughput: 0: 288.4. Samples: 2739488. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 23:08:36,000][166323] Avg episode reward: [(0, '854.927')]
[36m[2025-07-01 23:08:41,005][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 2736128. Throughput: 0: 289.2. Samples: 2741360. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 23:08:41,006][166323] Avg episode reward: [(0, '843.194')]
[36m[2025-07-01 23:08:45,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2736128. Throughput: 0: 290.7. Samples: 2743104. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 23:08:45,968][166323] Avg episode reward: [(0, '787.579')]
[36m[2025-07-01 23:08:50,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2736128. Throughput: 0: 291.8. Samples: 2743968. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 23:08:50,995][166323] Avg episode reward: [(0, '891.791')]
[36m[2025-07-01 23:08:55,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2736128. Throughput: 0: 292.7. Samples: 2745840. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 23:08:55,958][166323] Avg episode reward: [(0, '1005.479')]
[36m[2025-07-01 23:09:00,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2736128. Throughput: 0: 292.7. Samples: 2747616. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 23:09:00,962][166323] Avg episode reward: [(0, '966.864')]
[36m[2025-07-01 23:09:05,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2736128. Throughput: 0: 292.5. Samples: 2748432. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 23:09:05,970][166323] Avg episode reward: [(0, '928.331')]
[36m[2025-07-01 23:09:10,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2736128. Throughput: 0: 294.9. Samples: 2750176. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 23:09:10,977][166323] Avg episode reward: [(0, '978.628')]
[36m[2025-07-01 23:09:15,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2736128. Throughput: 0: 297.1. Samples: 2751936. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-01 23:09:15,976][166323] Avg episode reward: [(0, '1001.014')]
[36m[2025-07-01 23:09:20,983][166323] Fps is (10 sec: 1637.3, 60 sec: 545.8, 300 sec: 333.2). Total num frames: 2752512. Throughput: 0: 297.0. Samples: 2752848. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 23:09:20,983][166323] Avg episode reward: [(0, '1042.272')]
[36m[2025-07-01 23:09:25,986][166323] Fps is (10 sec: 1636.8, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 2752512. Throughput: 0: 292.4. Samples: 2754512. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 23:09:25,986][166323] Avg episode reward: [(0, '1031.749')]
[36m[2025-07-01 23:09:30,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.3). Total num frames: 2752512. Throughput: 0: 294.4. Samples: 2756352. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 23:09:30,968][166323] Avg episode reward: [(0, '994.155')]
[36m[2025-07-01 23:09:35,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2752512. Throughput: 0: 295.9. Samples: 2757280. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 23:09:35,987][166323] Avg episode reward: [(0, '1049.734')]
[37m[1m[2025-07-01 23:09:36,037][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000005368_2752512.pth...
[36m[2025-07-01 23:09:36,040][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000005240_2686976.pth
[36m[2025-07-01 23:09:40,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2752512. Throughput: 0: 293.4. Samples: 2759056. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 23:09:40,995][166323] Avg episode reward: [(0, '993.125')]
[36m[2025-07-01 23:09:45,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2752512. Throughput: 0: 294.1. Samples: 2760848. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 23:09:45,948][166323] Avg episode reward: [(0, '960.439')]
[36m[2025-07-01 23:09:50,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2752512. Throughput: 0: 296.1. Samples: 2761760. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 23:09:50,989][166323] Avg episode reward: [(0, '965.981')]
[36m[2025-07-01 23:09:55,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2752512. Throughput: 0: 298.7. Samples: 2763616. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 23:09:55,972][166323] Avg episode reward: [(0, '983.403')]
[36m[2025-07-01 23:10:00,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2752512. Throughput: 0: 295.6. Samples: 2765232. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 23:10:00,950][166323] Avg episode reward: [(0, '1026.048')]
[36m[2025-07-01 23:10:05,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2752512. Throughput: 0: 296.0. Samples: 2766160. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 23:10:05,963][166323] Avg episode reward: [(0, '971.290')]
[36m[2025-07-01 23:10:10,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2752512. Throughput: 0: 297.4. Samples: 2767888. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 23:10:10,968][166323] Avg episode reward: [(0, '1032.923')]
[36m[2025-07-01 23:10:15,974][166323] Fps is (10 sec: 1636.6, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 2768896. Throughput: 0: 296.8. Samples: 2769712. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 23:10:15,974][166323] Avg episode reward: [(0, '964.021')]
[36m[2025-07-01 23:10:20,995][166323] Fps is (10 sec: 1634.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2768896. Throughput: 0: 297.9. Samples: 2770688. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 23:10:20,995][166323] Avg episode reward: [(0, '899.342')]
[36m[2025-07-01 23:10:25,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 2768896. Throughput: 0: 297.1. Samples: 2772416. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 23:10:25,961][166323] Avg episode reward: [(0, '835.839')]
[36m[2025-07-01 23:10:30,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2768896. Throughput: 0: 293.7. Samples: 2774064. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 23:10:30,943][166323] Avg episode reward: [(0, '805.627')]
[36m[2025-07-01 23:10:35,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2768896. Throughput: 0: 295.3. Samples: 2775040. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 23:10:35,964][166323] Avg episode reward: [(0, '810.155')]
[36m[2025-07-01 23:10:40,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 2768896. Throughput: 0: 294.2. Samples: 2776848. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 23:10:40,952][166323] Avg episode reward: [(0, '785.729')]
[36m[2025-07-01 23:10:45,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2768896. Throughput: 0: 300.4. Samples: 2778752. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 23:10:45,962][166323] Avg episode reward: [(0, '816.496')]
[36m[2025-07-01 23:10:50,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 2768896. Throughput: 0: 300.6. Samples: 2779680. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 23:10:50,947][166323] Avg episode reward: [(0, '854.876')]
[36m[2025-07-01 23:10:55,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2768896. Throughput: 0: 301.7. Samples: 2781456. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 23:10:55,945][166323] Avg episode reward: [(0, '866.121')]
[36m[2025-07-01 23:11:00,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2768896. Throughput: 0: 304.8. Samples: 2783424. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 23:11:00,957][166323] Avg episode reward: [(0, '925.432')]
[36m[2025-07-01 23:11:05,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2768896. Throughput: 0: 302.6. Samples: 2784288. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 23:11:05,945][166323] Avg episode reward: [(0, '951.447')]
[36m[2025-07-01 23:11:10,973][166323] Fps is (10 sec: 1635.7, 60 sec: 546.1, 300 sec: 333.2). Total num frames: 2785280. Throughput: 0: 303.9. Samples: 2786096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:11:10,973][166323] Avg episode reward: [(0, '1043.014')]
[36m[2025-07-01 23:11:15,989][166323] Fps is (10 sec: 1631.2, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2785280. Throughput: 0: 306.2. Samples: 2787856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:11:15,989][166323] Avg episode reward: [(0, '1083.521')]
[36m[2025-07-01 23:11:21,009][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2785280. Throughput: 0: 304.4. Samples: 2788752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:11:21,009][166323] Avg episode reward: [(0, '1083.345')]
[36m[2025-07-01 23:11:25,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2785280. Throughput: 0: 300.2. Samples: 2790368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:11:25,984][166323] Avg episode reward: [(0, '1083.233')]
[36m[2025-07-01 23:11:30,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2785280. Throughput: 0: 295.1. Samples: 2792032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:11:30,964][166323] Avg episode reward: [(0, '1003.480')]
[36m[2025-07-01 23:11:35,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2785280. Throughput: 0: 292.7. Samples: 2792864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:11:35,987][166323] Avg episode reward: [(0, '975.040')]
[37m[1m[2025-07-01 23:11:35,990][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000005432_2785280.pth...
[36m[2025-07-01 23:11:35,994][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000005304_2719744.pth
[36m[2025-07-01 23:11:40,983][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2785280. Throughput: 0: 291.3. Samples: 2794576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:11:40,983][166323] Avg episode reward: [(0, '961.613')]
[36m[2025-07-01 23:11:45,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2785280. Throughput: 0: 292.2. Samples: 2796576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:11:45,969][166323] Avg episode reward: [(0, '919.262')]
[36m[2025-07-01 23:11:50,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2785280. Throughput: 0: 293.6. Samples: 2797504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:11:50,953][166323] Avg episode reward: [(0, '896.852')]
[36m[2025-07-01 23:11:55,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2785280. Throughput: 0: 292.9. Samples: 2799280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:11:55,985][166323] Avg episode reward: [(0, '848.936')]
[36m[2025-07-01 23:12:00,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2785280. Throughput: 0: 292.2. Samples: 2800992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:12:00,951][166323] Avg episode reward: [(0, '943.284')]
[36m[2025-07-01 23:12:06,003][166323] Fps is (10 sec: 1635.4, 60 sec: 545.6, 300 sec: 333.2). Total num frames: 2801664. Throughput: 0: 293.4. Samples: 2801952. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 23:12:06,004][166323] Avg episode reward: [(0, '917.251')]
[36m[2025-07-01 23:12:10,969][166323] Fps is (10 sec: 1635.3, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 2801664. Throughput: 0: 294.5. Samples: 2803616. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 23:12:10,970][166323] Avg episode reward: [(0, '913.410')]
[36m[2025-07-01 23:12:15,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 2801664. Throughput: 0: 296.9. Samples: 2805392. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 23:12:15,956][166323] Avg episode reward: [(0, '887.723')]
[36m[2025-07-01 23:12:20,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 2801664. Throughput: 0: 296.5. Samples: 2806208. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 23:12:20,985][166323] Avg episode reward: [(0, '894.034')]
[36m[2025-07-01 23:12:25,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2801664. Throughput: 0: 298.4. Samples: 2808000. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 23:12:25,973][166323] Avg episode reward: [(0, '969.293')]
[36m[2025-07-01 23:12:30,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2801664. Throughput: 0: 291.5. Samples: 2809696. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 23:12:30,971][166323] Avg episode reward: [(0, '924.072')]
[36m[2025-07-01 23:12:35,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2801664. Throughput: 0: 290.0. Samples: 2810560. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 23:12:35,979][166323] Avg episode reward: [(0, '961.066')]
[36m[2025-07-01 23:12:40,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2801664. Throughput: 0: 289.5. Samples: 2812304. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 23:12:40,979][166323] Avg episode reward: [(0, '1000.501')]
[36m[2025-07-01 23:12:45,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2801664. Throughput: 0: 292.5. Samples: 2814160. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 23:12:45,967][166323] Avg episode reward: [(0, '1016.810')]
[36m[2025-07-01 23:12:50,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2801664. Throughput: 0: 292.2. Samples: 2815088. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 23:12:50,954][166323] Avg episode reward: [(0, '1018.222')]
[36m[2025-07-01 23:12:55,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2801664. Throughput: 0: 294.0. Samples: 2816848. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 23:12:55,976][166323] Avg episode reward: [(0, '1058.238')]
[36m[2025-07-01 23:13:00,948][166323] Fps is (10 sec: 1639.3, 60 sec: 546.2, 300 sec: 333.3). Total num frames: 2818048. Throughput: 0: 294.4. Samples: 2818640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:13:00,949][166323] Avg episode reward: [(0, '984.453')]
[36m[2025-07-01 23:13:05,973][166323] Fps is (10 sec: 1638.9, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 2818048. Throughput: 0: 295.5. Samples: 2819504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:13:05,973][166323] Avg episode reward: [(0, '1031.190')]
[36m[2025-07-01 23:13:10,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 2818048. Throughput: 0: 295.7. Samples: 2821312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:13:10,999][166323] Avg episode reward: [(0, '1015.083')]
[36m[2025-07-01 23:13:15,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2818048. Throughput: 0: 300.4. Samples: 2823216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:13:15,978][166323] Avg episode reward: [(0, '981.131')]
[36m[2025-07-01 23:13:20,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2818048. Throughput: 0: 299.9. Samples: 2824048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:13:20,947][166323] Avg episode reward: [(0, '995.623')]
[31m[9594158 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9594158 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[9594158 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:13:25,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2818048. Throughput: 0: 301.9. Samples: 2825888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:13:25,977][166323] Avg episode reward: [(0, '979.604')]
[36m[2025-07-01 23:13:30,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2818048. Throughput: 0: 298.3. Samples: 2827584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:13:30,973][166323] Avg episode reward: [(0, '1037.142')]
[36m[2025-07-01 23:13:35,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2818048. Throughput: 0: 297.4. Samples: 2828480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:13:35,992][166323] Avg episode reward: [(0, '1053.492')]
[37m[1m[2025-07-01 23:13:35,995][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000005496_2818048.pth...
[36m[2025-07-01 23:13:35,998][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000005368_2752512.pth
[36m[2025-07-01 23:13:40,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2818048. Throughput: 0: 299.1. Samples: 2830304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:13:40,966][166323] Avg episode reward: [(0, '1060.817')]
[36m[2025-07-01 23:13:45,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2818048. Throughput: 0: 298.9. Samples: 2832096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:13:45,973][166323] Avg episode reward: [(0, '1076.197')]
[36m[2025-07-01 23:13:51,000][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2818048. Throughput: 0: 301.7. Samples: 2833088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:13:51,000][166323] Avg episode reward: [(0, '1081.867')]
[36m[2025-07-01 23:13:55,949][166323] Fps is (10 sec: 1642.3, 60 sec: 546.4, 300 sec: 333.2). Total num frames: 2834432. Throughput: 0: 298.6. Samples: 2834736. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:13:55,949][166323] Avg episode reward: [(0, '1027.105')]
[36m[2025-07-01 23:14:00,985][166323] Fps is (10 sec: 1640.8, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 2834432. Throughput: 0: 298.6. Samples: 2836656. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:14:00,985][166323] Avg episode reward: [(0, '965.517')]
[36m[2025-07-01 23:14:05,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2834432. Throughput: 0: 298.1. Samples: 2837472. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:14:05,986][166323] Avg episode reward: [(0, '935.909')]
[36m[2025-07-01 23:14:11,010][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2834432. Throughput: 0: 296.3. Samples: 2839232. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:14:11,010][166323] Avg episode reward: [(0, '887.609')]
[36m[2025-07-01 23:14:15,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2834432. Throughput: 0: 296.8. Samples: 2840944. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:14:15,993][166323] Avg episode reward: [(0, '878.022')]
[36m[2025-07-01 23:14:20,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2834432. Throughput: 0: 296.0. Samples: 2841792. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:14:20,958][166323] Avg episode reward: [(0, '881.721')]
[36m[2025-07-01 23:14:25,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2834432. Throughput: 0: 294.9. Samples: 2843568. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:14:25,947][166323] Avg episode reward: [(0, '872.203')]
[36m[2025-07-01 23:14:30,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2834432. Throughput: 0: 293.3. Samples: 2845296. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:14:30,984][166323] Avg episode reward: [(0, '904.587')]
[36m[2025-07-01 23:14:35,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 2834432. Throughput: 0: 290.1. Samples: 2846128. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:14:35,951][166323] Avg episode reward: [(0, '915.201')]
[36m[2025-07-01 23:14:40,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2834432. Throughput: 0: 291.8. Samples: 2847872. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:14:40,971][166323] Avg episode reward: [(0, '955.624')]
[36m[2025-07-01 23:14:45,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2834432. Throughput: 0: 289.2. Samples: 2849664. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:14:45,969][166323] Avg episode reward: [(0, '958.550')]
[36m[2025-07-01 23:14:50,947][166323] Fps is (10 sec: 1642.3, 60 sec: 546.6, 300 sec: 333.3). Total num frames: 2850816. Throughput: 0: 290.0. Samples: 2850512. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 23:14:50,947][166323] Avg episode reward: [(0, '948.375')]
[36m[2025-07-01 23:14:55,961][166323] Fps is (10 sec: 1639.8, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2850816. Throughput: 0: 287.6. Samples: 2852160. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 23:14:55,961][166323] Avg episode reward: [(0, '1028.265')]
[36m[2025-07-01 23:15:00,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2850816. Throughput: 0: 288.8. Samples: 2853936. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 23:15:00,974][166323] Avg episode reward: [(0, '1037.612')]
[36m[2025-07-01 23:15:05,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2850816. Throughput: 0: 290.6. Samples: 2854880. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 23:15:05,993][166323] Avg episode reward: [(0, '1046.535')]
[36m[2025-07-01 23:15:10,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2850816. Throughput: 0: 291.3. Samples: 2856688. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 23:15:10,989][166323] Avg episode reward: [(0, '1035.504')]
[36m[2025-07-01 23:15:15,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2850816. Throughput: 0: 292.2. Samples: 2858448. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 23:15:15,987][166323] Avg episode reward: [(0, '1067.167')]
[31m[9707368 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9707368 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[9707368 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:15:20,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2850816. Throughput: 0: 295.3. Samples: 2859424. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 23:15:20,983][166323] Avg episode reward: [(0, '1055.553')]
[36m[2025-07-01 23:15:25,978][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2850816. Throughput: 0: 296.5. Samples: 2861216. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 23:15:25,979][166323] Avg episode reward: [(0, '1026.981')]
[36m[2025-07-01 23:15:30,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2850816. Throughput: 0: 296.4. Samples: 2863008. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 23:15:30,988][166323] Avg episode reward: [(0, '1039.901')]
[36m[2025-07-01 23:15:35,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2850816. Throughput: 0: 297.4. Samples: 2863904. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 23:15:35,976][166323] Avg episode reward: [(0, '1099.664')]
[37m[1m[2025-07-01 23:15:36,030][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000005560_2850816.pth...
[36m[2025-07-01 23:15:36,035][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000005432_2785280.pth
[36m[2025-07-01 23:15:40,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2850816. Throughput: 0: 301.8. Samples: 2865744. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-01 23:15:40,978][166323] Avg episode reward: [(0, '1100.830')]
[36m[2025-07-01 23:15:45,973][166323] Fps is (10 sec: 1638.8, 60 sec: 546.1, 300 sec: 333.2). Total num frames: 2867200. Throughput: 0: 302.2. Samples: 2867536. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 23:15:45,973][166323] Avg episode reward: [(0, '1066.968')]
[36m[2025-07-01 23:15:50,946][166323] Fps is (10 sec: 1643.6, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2867200. Throughput: 0: 301.5. Samples: 2868432. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 23:15:50,946][166323] Avg episode reward: [(0, '1085.296')]
[36m[2025-07-01 23:15:55,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 2867200. Throughput: 0: 299.0. Samples: 2870144. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 23:15:55,987][166323] Avg episode reward: [(0, '1065.568')]
[36m[2025-07-01 23:16:00,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2867200. Throughput: 0: 300.4. Samples: 2871968. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 23:16:00,987][166323] Avg episode reward: [(0, '1001.780')]
[36m[2025-07-01 23:16:05,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2867200. Throughput: 0: 300.5. Samples: 2872944. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 23:16:05,974][166323] Avg episode reward: [(0, '971.659')]
[36m[2025-07-01 23:16:10,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2867200. Throughput: 0: 302.5. Samples: 2874832. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 23:16:10,996][166323] Avg episode reward: [(0, '968.437')]
[36m[2025-07-01 23:16:15,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2867200. Throughput: 0: 303.7. Samples: 2876672. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 23:16:15,985][166323] Avg episode reward: [(0, '930.806')]
[36m[2025-07-01 23:16:20,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2867200. Throughput: 0: 304.1. Samples: 2877584. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 23:16:20,964][166323] Avg episode reward: [(0, '936.420')]
[36m[2025-07-01 23:16:25,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2867200. Throughput: 0: 301.3. Samples: 2879296. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 23:16:25,960][166323] Avg episode reward: [(0, '919.180')]
[31m[9775377 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9775377 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[9775378 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:16:30,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2867200. Throughput: 0: 301.7. Samples: 2881104. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 23:16:30,949][166323] Avg episode reward: [(0, '935.089')]
[36m[2025-07-01 23:16:35,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2867200. Throughput: 0: 301.2. Samples: 2882000. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 23:16:35,990][166323] Avg episode reward: [(0, '988.049')]
[36m[2025-07-01 23:16:40,960][166323] Fps is (10 sec: 1636.6, 60 sec: 546.3, 300 sec: 333.2). Total num frames: 2883584. Throughput: 0: 302.8. Samples: 2883760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:16:40,960][166323] Avg episode reward: [(0, '902.592')]
[33m[9789689 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[9789689 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.80224609375
[33mCrash Rate: 0.1787109375
[33mTimeout Rate: 0.01904296875 (navigation_task.py:265)
[33m[9789689 ms][navigation_task] - WARNING : 
[33mSuccesses: 1643
[33mCrashes : 366
[33mTimeouts: 39 (navigation_task.py:268)
[36m[2025-07-01 23:16:45,945][166323] Fps is (10 sec: 1645.7, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 2883584. Throughput: 0: 300.0. Samples: 2885456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:16:45,945][166323] Avg episode reward: [(0, '891.686')]
[36m[2025-07-01 23:16:50,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 2883584. Throughput: 0: 297.5. Samples: 2886336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:16:50,988][166323] Avg episode reward: [(0, '904.275')]
[36m[2025-07-01 23:16:55,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 2883584. Throughput: 0: 293.6. Samples: 2888032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:16:55,953][166323] Avg episode reward: [(0, '892.461')]
[36m[2025-07-01 23:17:00,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2883584. Throughput: 0: 291.2. Samples: 2889776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:17:00,985][166323] Avg episode reward: [(0, '899.878')]
[36m[2025-07-01 23:17:05,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2883584. Throughput: 0: 293.1. Samples: 2890768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:17:05,947][166323] Avg episode reward: [(0, '866.007')]
[36m[2025-07-01 23:17:10,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 2883584. Throughput: 0: 295.5. Samples: 2892592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:17:10,948][166323] Avg episode reward: [(0, '919.955')]
[36m[2025-07-01 23:17:15,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2883584. Throughput: 0: 294.4. Samples: 2894352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:17:15,953][166323] Avg episode reward: [(0, '957.502')]
[36m[2025-07-01 23:17:20,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2883584. Throughput: 0: 296.0. Samples: 2895312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:17:20,956][166323] Avg episode reward: [(0, '950.163')]
[36m[2025-07-01 23:17:25,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2883584. Throughput: 0: 295.4. Samples: 2897056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:17:25,976][166323] Avg episode reward: [(0, '950.005')]
[36m[2025-07-01 23:17:31,010][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 2883584. Throughput: 0: 297.9. Samples: 2898880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:17:31,010][166323] Avg episode reward: [(0, '888.819')]
[36m[2025-07-01 23:17:35,988][166323] Fps is (10 sec: 1636.5, 60 sec: 546.1, 300 sec: 333.2). Total num frames: 2899968. Throughput: 0: 298.7. Samples: 2899776. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 23:17:35,988][166323] Avg episode reward: [(0, '908.900')]
[37m[1m[2025-07-01 23:17:35,995][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000005656_2899968.pth...
[36m[2025-07-01 23:17:36,001][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000005496_2818048.pth
[36m[2025-07-01 23:17:40,987][166323] Fps is (10 sec: 1642.1, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 2899968. Throughput: 0: 299.5. Samples: 2901520. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 23:17:40,987][166323] Avg episode reward: [(0, '943.144')]
[36m[2025-07-01 23:17:45,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2899968. Throughput: 0: 302.0. Samples: 2903360. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 23:17:45,959][166323] Avg episode reward: [(0, '966.511')]
[36m[2025-07-01 23:17:50,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 2899968. Throughput: 0: 301.5. Samples: 2904336. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 23:17:50,951][166323] Avg episode reward: [(0, '976.236')]
[36m[2025-07-01 23:17:55,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2899968. Throughput: 0: 298.6. Samples: 2906032. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 23:17:55,960][166323] Avg episode reward: [(0, '928.687')]
[31m[9866715 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9866716 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[9866716 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:18:00,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2899968. Throughput: 0: 303.4. Samples: 2908016. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 23:18:00,987][166323] Avg episode reward: [(0, '926.310')]
[36m[2025-07-01 23:18:05,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2899968. Throughput: 0: 304.2. Samples: 2909008. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 23:18:05,981][166323] Avg episode reward: [(0, '900.951')]
[36m[2025-07-01 23:18:10,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2899968. Throughput: 0: 305.0. Samples: 2910784. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 23:18:10,988][166323] Avg episode reward: [(0, '914.851')]
[31m[9883745 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9883746 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[9883746 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:18:15,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2899968. Throughput: 0: 303.5. Samples: 2912528. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 23:18:15,976][166323] Avg episode reward: [(0, '886.069')]
[36m[2025-07-01 23:18:20,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2899968. Throughput: 0: 302.3. Samples: 2913376. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 23:18:20,973][166323] Avg episode reward: [(0, '880.781')]
[36m[2025-07-01 23:18:25,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2899968. Throughput: 0: 301.3. Samples: 2915072. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 23:18:25,960][166323] Avg episode reward: [(0, '980.648')]
[36m[2025-07-01 23:18:30,984][166323] Fps is (10 sec: 1636.6, 60 sec: 546.4, 300 sec: 333.2). Total num frames: 2916352. Throughput: 0: 301.0. Samples: 2916912. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 23:18:30,984][166323] Avg episode reward: [(0, '1001.535')]
[36m[2025-07-01 23:18:35,964][166323] Fps is (10 sec: 1637.7, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 2916352. Throughput: 0: 298.6. Samples: 2917776. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 23:18:35,964][166323] Avg episode reward: [(0, '968.734')]
[31m[9904963 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9904963 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[9904963 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[9907720 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9907720 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[9907720 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:18:40,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2916352. Throughput: 0: 301.3. Samples: 2919600. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 23:18:40,985][166323] Avg episode reward: [(0, '917.965')]
[36m[2025-07-01 23:18:45,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 2916352. Throughput: 0: 297.1. Samples: 2921376. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 23:18:45,960][166323] Avg episode reward: [(0, '939.518')]
[36m[2025-07-01 23:18:50,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2916352. Throughput: 0: 294.6. Samples: 2922256. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 23:18:50,957][166323] Avg episode reward: [(0, '939.428')]
[36m[2025-07-01 23:18:55,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2916352. Throughput: 0: 299.0. Samples: 2924240. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 23:18:55,999][166323] Avg episode reward: [(0, '837.258')]
[36m[2025-07-01 23:19:00,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2916352. Throughput: 0: 299.4. Samples: 2926000. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 23:19:00,970][166323] Avg episode reward: [(0, '879.398')]
[36m[2025-07-01 23:19:05,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2916352. Throughput: 0: 301.5. Samples: 2926944. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 23:19:05,970][166323] Avg episode reward: [(0, '949.424')]
[36m[2025-07-01 23:19:10,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2916352. Throughput: 0: 303.5. Samples: 2928736. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 23:19:10,987][166323] Avg episode reward: [(0, '973.826')]
[36m[2025-07-01 23:19:15,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2916352. Throughput: 0: 302.2. Samples: 2930512. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 23:19:15,987][166323] Avg episode reward: [(0, '952.789')]
[36m[2025-07-01 23:19:20,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2916352. Throughput: 0: 303.5. Samples: 2931440. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 23:19:20,983][166323] Avg episode reward: [(0, '931.198')]
[31m[9949692 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9949692 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[9949692 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:19:25,947][166323] Fps is (10 sec: 1644.9, 60 sec: 546.2, 300 sec: 333.3). Total num frames: 2932736. Throughput: 0: 303.5. Samples: 2933248. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 23:19:25,947][166323] Avg episode reward: [(0, '1001.271')]
[36m[2025-07-01 23:19:30,993][166323] Fps is (10 sec: 1636.8, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2932736. Throughput: 0: 299.9. Samples: 2934880. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 23:19:30,993][166323] Avg episode reward: [(0, '959.123')]
[36m[2025-07-01 23:19:35,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2932736. Throughput: 0: 300.4. Samples: 2935776. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 23:19:35,957][166323] Avg episode reward: [(0, '993.895')]
[37m[1m[2025-07-01 23:19:36,016][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000005720_2932736.pth...
[36m[2025-07-01 23:19:36,022][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000005560_2850816.pth
[36m[2025-07-01 23:19:40,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2932736. Throughput: 0: 296.6. Samples: 2937584. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 23:19:40,994][166323] Avg episode reward: [(0, '986.852')]
[36m[2025-07-01 23:19:45,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2932736. Throughput: 0: 296.2. Samples: 2939328. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 23:19:45,971][166323] Avg episode reward: [(0, '992.683')]
[36m[2025-07-01 23:19:50,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2932736. Throughput: 0: 294.4. Samples: 2940192. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 23:19:50,973][166323] Avg episode reward: [(0, '997.820')]
[31m[9982563 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9982563 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[9982564 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:19:55,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2932736. Throughput: 0: 293.6. Samples: 2941952. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 23:19:56,000][166323] Avg episode reward: [(0, '979.573')]
[31m[9988659 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9988659 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[9988659 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:20:00,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2932736. Throughput: 0: 294.9. Samples: 2943776. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 23:20:00,972][166323] Avg episode reward: [(0, '1013.056')]
[36m[2025-07-01 23:20:05,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2932736. Throughput: 0: 294.4. Samples: 2944688. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 23:20:05,985][166323] Avg episode reward: [(0, '930.232')]
[36m[2025-07-01 23:20:10,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2932736. Throughput: 0: 297.1. Samples: 2946624. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 23:20:10,972][166323] Avg episode reward: [(0, '915.688')]
[36m[2025-07-01 23:20:15,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2932736. Throughput: 0: 300.2. Samples: 2948384. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 23:20:15,980][166323] Avg episode reward: [(0, '935.729')]
[36m[2025-07-01 23:20:20,978][166323] Fps is (10 sec: 1637.4, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 2949120. Throughput: 0: 298.2. Samples: 2949200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:20:20,979][166323] Avg episode reward: [(0, '955.201')]
[31m[10009962 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[10009962 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[10009963 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:20:25,951][166323] Fps is (10 sec: 1643.1, 60 sec: 273.0, 300 sec: 333.3). Total num frames: 2949120. Throughput: 0: 300.7. Samples: 2951104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:20:25,951][166323] Avg episode reward: [(0, '965.153')]
[31m[10016627 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[10016628 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[10016628 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:20:30,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2949120. Throughput: 0: 301.0. Samples: 2952880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:20:30,989][166323] Avg episode reward: [(0, '982.715')]
[36m[2025-07-01 23:20:35,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2949120. Throughput: 0: 300.7. Samples: 2953728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:20:35,981][166323] Avg episode reward: [(0, '1062.482')]
[36m[2025-07-01 23:20:40,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2949120. Throughput: 0: 301.4. Samples: 2955504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:20:40,966][166323] Avg episode reward: [(0, '1055.255')]
[36m[2025-07-01 23:20:45,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2949120. Throughput: 0: 302.9. Samples: 2957408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:20:45,978][166323] Avg episode reward: [(0, '1020.243')]
[36m[2025-07-01 23:20:50,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2949120. Throughput: 0: 301.7. Samples: 2958256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:20:50,962][166323] Avg episode reward: [(0, '1059.466')]
[36m[2025-07-01 23:20:55,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2949120. Throughput: 0: 296.5. Samples: 2959968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:20:55,974][166323] Avg episode reward: [(0, '998.625')]
[36m[2025-07-01 23:21:00,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2949120. Throughput: 0: 295.6. Samples: 2961680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:21:00,960][166323] Avg episode reward: [(0, '1011.433')]
[36m[2025-07-01 23:21:05,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2949120. Throughput: 0: 295.4. Samples: 2962496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:21:05,993][166323] Avg episode reward: [(0, '980.388')]
[36m[2025-07-01 23:21:10,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2949120. Throughput: 0: 294.4. Samples: 2964352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:21:10,957][166323] Avg episode reward: [(0, '950.437')]
[36m[2025-07-01 23:21:15,983][166323] Fps is (10 sec: 1640.0, 60 sec: 546.1, 300 sec: 333.2). Total num frames: 2965504. Throughput: 0: 293.7. Samples: 2966096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:21:15,983][166323] Avg episode reward: [(0, '965.652')]
[36m[2025-07-01 23:21:20,971][166323] Fps is (10 sec: 1636.1, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2965504. Throughput: 0: 296.6. Samples: 2967072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:21:20,971][166323] Avg episode reward: [(0, '961.671')]
[36m[2025-07-01 23:21:25,977][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 2965504. Throughput: 0: 294.3. Samples: 2968752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:21:25,977][166323] Avg episode reward: [(0, '984.939')]
[36m[2025-07-01 23:21:30,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 2965504. Throughput: 0: 293.5. Samples: 2970608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:21:30,951][166323] Avg episode reward: [(0, '987.121')]
[36m[2025-07-01 23:21:36,000][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2965504. Throughput: 0: 294.9. Samples: 2971536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:21:36,001][166323] Avg episode reward: [(0, '960.366')]
[37m[1m[2025-07-01 23:21:36,067][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000005784_2965504.pth...
[36m[2025-07-01 23:21:36,072][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000005656_2899968.pth
[36m[2025-07-01 23:21:40,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2965504. Throughput: 0: 297.5. Samples: 2973360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:21:40,988][166323] Avg episode reward: [(0, '1039.165')]
[36m[2025-07-01 23:21:45,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2965504. Throughput: 0: 298.2. Samples: 2975104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:21:45,982][166323] Avg episode reward: [(0, '1014.923')]
[36m[2025-07-01 23:21:51,023][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 2965504. Throughput: 0: 300.6. Samples: 2976032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:21:51,023][166323] Avg episode reward: [(0, '997.357')]
[36m[2025-07-01 23:21:55,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2965504. Throughput: 0: 297.6. Samples: 2977744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:21:55,951][166323] Avg episode reward: [(0, '1016.740')]
[36m[2025-07-01 23:22:00,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2965504. Throughput: 0: 299.4. Samples: 2979568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:22:00,978][166323] Avg episode reward: [(0, '1020.277')]
[36m[2025-07-01 23:22:05,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 2965504. Throughput: 0: 298.1. Samples: 2980480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:22:05,946][166323] Avg episode reward: [(0, '1059.863')]
[36m[2025-07-01 23:22:11,011][166323] Fps is (10 sec: 1633.1, 60 sec: 545.6, 300 sec: 333.2). Total num frames: 2981888. Throughput: 0: 300.2. Samples: 2982272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:22:11,011][166323] Avg episode reward: [(0, '1014.471')]
[36m[2025-07-01 23:22:15,962][166323] Fps is (10 sec: 1635.8, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 2981888. Throughput: 0: 298.6. Samples: 2984048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:22:15,962][166323] Avg episode reward: [(0, '1079.485')]
[36m[2025-07-01 23:22:20,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 2981888. Throughput: 0: 298.4. Samples: 2984960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:22:20,987][166323] Avg episode reward: [(0, '1057.476')]
[36m[2025-07-01 23:22:25,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.3). Total num frames: 2981888. Throughput: 0: 295.4. Samples: 2986656. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:22:25,994][166323] Avg episode reward: [(0, '998.607')]
[36m[2025-07-01 23:22:30,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2981888. Throughput: 0: 297.0. Samples: 2988464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:22:30,963][166323] Avg episode reward: [(0, '998.824')]
[31m[10143007 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[10143007 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[10143007 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:22:35,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2981888. Throughput: 0: 296.2. Samples: 2989344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:22:35,964][166323] Avg episode reward: [(0, '979.352')]
[36m[2025-07-01 23:22:40,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2981888. Throughput: 0: 298.1. Samples: 2991168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:22:40,976][166323] Avg episode reward: [(0, '996.786')]
[36m[2025-07-01 23:22:45,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2981888. Throughput: 0: 297.5. Samples: 2992944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:22:45,947][166323] Avg episode reward: [(0, '946.007')]
[36m[2025-07-01 23:22:50,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 2981888. Throughput: 0: 295.7. Samples: 2993792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:22:50,972][166323] Avg episode reward: [(0, '980.186')]
[36m[2025-07-01 23:22:55,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2981888. Throughput: 0: 293.4. Samples: 2995456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:22:55,951][166323] Avg episode reward: [(0, '946.779')]
[31m[10167094 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[10167094 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[10167094 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:23:00,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2981888. Throughput: 0: 292.3. Samples: 2997200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:23:00,954][166323] Avg episode reward: [(0, '983.583')]
[36m[2025-07-01 23:23:05,947][166323] Fps is (10 sec: 1639.0, 60 sec: 546.1, 300 sec: 333.3). Total num frames: 2998272. Throughput: 0: 293.2. Samples: 2998144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:23:05,947][166323] Avg episode reward: [(0, '986.649')]
[36m[2025-07-01 23:23:10,980][166323] Fps is (10 sec: 1634.1, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 2998272. Throughput: 0: 293.4. Samples: 2999856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:23:10,980][166323] Avg episode reward: [(0, '1017.039')]
[36m[2025-07-01 23:23:15,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2998272. Throughput: 0: 291.2. Samples: 3001568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:23:15,962][166323] Avg episode reward: [(0, '1037.879')]
[36m[2025-07-01 23:23:20,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 2998272. Throughput: 0: 291.1. Samples: 3002448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:23:20,977][166323] Avg episode reward: [(0, '994.898')]
[36m[2025-07-01 23:23:25,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2998272. Throughput: 0: 291.5. Samples: 3004288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:23:25,978][166323] Avg episode reward: [(0, '974.709')]
[36m[2025-07-01 23:23:30,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2998272. Throughput: 0: 290.5. Samples: 3006032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:23:30,993][166323] Avg episode reward: [(0, '969.445')]
[36m[2025-07-01 23:23:35,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 2998272. Throughput: 0: 290.9. Samples: 3006880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:23:35,966][166323] Avg episode reward: [(0, '992.453')]
[37m[1m[2025-07-01 23:23:36,014][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000005848_2998272.pth...
[36m[2025-07-01 23:23:36,018][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000005720_2932736.pth
[36m[2025-07-01 23:23:40,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2998272. Throughput: 0: 296.5. Samples: 3008800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:23:40,957][166323] Avg episode reward: [(0, '994.496')]
[36m[2025-07-01 23:23:45,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 2998272. Throughput: 0: 296.7. Samples: 3010560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:23:45,985][166323] Avg episode reward: [(0, '1024.856')]
[36m[2025-07-01 23:23:50,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 2998272. Throughput: 0: 293.7. Samples: 3011360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:23:50,947][166323] Avg episode reward: [(0, '1030.194')]
[36m[2025-07-01 23:23:55,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 2998272. Throughput: 0: 295.6. Samples: 3013152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:23:55,961][166323] Avg episode reward: [(0, '1076.181')]
[36m[2025-07-01 23:24:00,958][166323] Fps is (10 sec: 1636.6, 60 sec: 546.1, 300 sec: 333.2). Total num frames: 3014656. Throughput: 0: 295.5. Samples: 3014864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:24:00,958][166323] Avg episode reward: [(0, '1081.454')]
[36m[2025-07-01 23:24:05,990][166323] Fps is (10 sec: 1633.6, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 3014656. Throughput: 0: 294.7. Samples: 3015712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:24:05,991][166323] Avg episode reward: [(0, '1057.885')]
[36m[2025-07-01 23:24:10,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 3014656. Throughput: 0: 292.6. Samples: 3017456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:24:10,987][166323] Avg episode reward: [(0, '1019.874')]
[36m[2025-07-01 23:24:15,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 3014656. Throughput: 0: 295.1. Samples: 3019312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:24:15,994][166323] Avg episode reward: [(0, '1013.676')]
[36m[2025-07-01 23:24:20,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3014656. Throughput: 0: 295.0. Samples: 3020160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:24:20,986][166323] Avg episode reward: [(0, '960.832')]
[36m[2025-07-01 23:24:25,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3014656. Throughput: 0: 294.1. Samples: 3022048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:24:25,996][166323] Avg episode reward: [(0, '980.774')]
[31m[10257592 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[10257593 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[10257593 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[10257641 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[10257641 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[10257642 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[10257691 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[10257691 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[10257691 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:24:30,997][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3014656. Throughput: 0: 294.7. Samples: 3023824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:24:30,998][166323] Avg episode reward: [(0, '923.973')]
[36m[2025-07-01 23:24:35,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3014656. Throughput: 0: 297.5. Samples: 3024752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:24:35,955][166323] Avg episode reward: [(0, '908.100')]
[36m[2025-07-01 23:24:41,008][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 3014656. Throughput: 0: 298.4. Samples: 3026592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:24:41,008][166323] Avg episode reward: [(0, '923.070')]
[36m[2025-07-01 23:24:45,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3014656. Throughput: 0: 301.1. Samples: 3028416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:24:45,962][166323] Avg episode reward: [(0, '924.108')]
[36m[2025-07-01 23:24:50,975][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3014656. Throughput: 0: 302.7. Samples: 3029328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:24:50,975][166323] Avg episode reward: [(0, '965.511')]
[36m[2025-07-01 23:24:55,974][166323] Fps is (10 sec: 1636.4, 60 sec: 546.0, 300 sec: 333.2). Total num frames: 3031040. Throughput: 0: 301.2. Samples: 3031008. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 23:24:55,974][166323] Avg episode reward: [(0, '949.358')]
[36m[2025-07-01 23:25:00,949][166323] Fps is (10 sec: 1642.7, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 3031040. Throughput: 0: 299.0. Samples: 3032752. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 23:25:00,949][166323] Avg episode reward: [(0, '969.855')]
[36m[2025-07-01 23:25:05,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 3031040. Throughput: 0: 299.8. Samples: 3033648. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 23:25:05,979][166323] Avg episode reward: [(0, '970.825')]
[36m[2025-07-01 23:25:10,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 3031040. Throughput: 0: 296.8. Samples: 3035392. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 23:25:10,963][166323] Avg episode reward: [(0, '966.485')]
[36m[2025-07-01 23:25:15,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3031040. Throughput: 0: 295.2. Samples: 3037104. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 23:25:15,978][166323] Avg episode reward: [(0, '955.095')]
[31m[10306959 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[10306959 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[10306959 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:25:20,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3031040. Throughput: 0: 290.9. Samples: 3037840. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 23:25:20,952][166323] Avg episode reward: [(0, '925.551')]
[36m[2025-07-01 23:25:25,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3031040. Throughput: 0: 288.7. Samples: 3039568. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 23:25:25,954][166323] Avg episode reward: [(0, '892.352')]
[36m[2025-07-01 23:25:30,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3031040. Throughput: 0: 290.3. Samples: 3041488. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 23:25:30,997][166323] Avg episode reward: [(0, '929.502')]
[36m[2025-07-01 23:25:35,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3031040. Throughput: 0: 289.1. Samples: 3042336. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 23:25:35,968][166323] Avg episode reward: [(0, '893.193')]
[37m[1m[2025-07-01 23:25:36,042][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000005912_3031040.pth...
[36m[2025-07-01 23:25:36,061][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000005784_2965504.pth
[36m[2025-07-01 23:25:40,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3031040. Throughput: 0: 291.7. Samples: 3044128. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 23:25:40,954][166323] Avg episode reward: [(0, '917.579')]
[36m[2025-07-01 23:25:45,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3031040. Throughput: 0: 289.6. Samples: 3045792. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 23:25:45,976][166323] Avg episode reward: [(0, '858.628')]
[31m[10337901 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[10337902 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[10337902 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:25:50,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3031040. Throughput: 0: 288.3. Samples: 3046624. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 23:25:50,990][166323] Avg episode reward: [(0, '962.649')]
[36m[2025-07-01 23:25:55,984][166323] Fps is (10 sec: 1637.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 3047424. Throughput: 0: 286.4. Samples: 3048288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:25:55,985][166323] Avg episode reward: [(0, '980.171')]
[36m[2025-07-01 23:26:00,946][166323] Fps is (10 sec: 1645.6, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 3047424. Throughput: 0: 288.9. Samples: 3050096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:26:00,946][166323] Avg episode reward: [(0, '1003.190')]
[36m[2025-07-01 23:26:06,012][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 3047424. Throughput: 0: 290.8. Samples: 3050944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:26:06,012][166323] Avg episode reward: [(0, '1019.116')]
[36m[2025-07-01 23:26:10,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3047424. Throughput: 0: 292.1. Samples: 3052720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:26:10,975][166323] Avg episode reward: [(0, '916.456')]
[36m[2025-07-01 23:26:15,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3047424. Throughput: 0: 290.1. Samples: 3054528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:26:15,953][166323] Avg episode reward: [(0, '953.679')]
[36m[2025-07-01 23:26:20,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3047424. Throughput: 0: 292.1. Samples: 3055488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:26:20,996][166323] Avg episode reward: [(0, '903.958')]
[36m[2025-07-01 23:26:25,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3047424. Throughput: 0: 289.3. Samples: 3057152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:26:25,974][166323] Avg episode reward: [(0, '857.940')]
[36m[2025-07-01 23:26:30,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3047424. Throughput: 0: 289.4. Samples: 3058816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:26:30,974][166323] Avg episode reward: [(0, '861.047')]
[36m[2025-07-01 23:26:35,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3047424. Throughput: 0: 291.1. Samples: 3059712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:26:35,956][166323] Avg episode reward: [(0, '887.473')]
[36m[2025-07-01 23:26:40,980][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3047424. Throughput: 0: 291.6. Samples: 3061408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:26:40,980][166323] Avg episode reward: [(0, '970.422')]
[36m[2025-07-01 23:26:45,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3047424. Throughput: 0: 291.4. Samples: 3063216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:26:45,973][166323] Avg episode reward: [(0, '984.025')]
[36m[2025-07-01 23:26:50,959][166323] Fps is (10 sec: 1641.8, 60 sec: 546.4, 300 sec: 333.2). Total num frames: 3063808. Throughput: 0: 289.8. Samples: 3063968. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 23:26:50,960][166323] Avg episode reward: [(0, '994.877')]
[36m[2025-07-01 23:26:55,989][166323] Fps is (10 sec: 1635.6, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 3063808. Throughput: 0: 291.1. Samples: 3065824. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 23:26:55,990][166323] Avg episode reward: [(0, '1005.130')]
[36m[2025-07-01 23:27:00,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 3063808. Throughput: 0: 290.1. Samples: 3067584. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 23:27:00,957][166323] Avg episode reward: [(0, '968.626')]
[36m[2025-07-01 23:27:05,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3063808. Throughput: 0: 290.7. Samples: 3068560. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 23:27:05,961][166323] Avg episode reward: [(0, '970.781')]
[33m[10415638 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[10415638 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.81103515625
[33mCrash Rate: 0.17333984375
[33mTimeout Rate: 0.015625 (navigation_task.py:265)
[33m[10415638 ms][navigation_task] - WARNING : 
[33mSuccesses: 1661
[33mCrashes : 355
[33mTimeouts: 32 (navigation_task.py:268)
[36m[2025-07-01 23:27:10,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3063808. Throughput: 0: 295.3. Samples: 3070432. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 23:27:10,949][166323] Avg episode reward: [(0, '965.974')]
[36m[2025-07-01 23:27:15,998][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3063808. Throughput: 0: 297.4. Samples: 3072208. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 23:27:15,999][166323] Avg episode reward: [(0, '867.936')]
[36m[2025-07-01 23:27:20,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3063808. Throughput: 0: 298.3. Samples: 3073136. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 23:27:20,965][166323] Avg episode reward: [(0, '880.693')]
[36m[2025-07-01 23:27:25,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3063808. Throughput: 0: 301.6. Samples: 3074976. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 23:27:25,971][166323] Avg episode reward: [(0, '894.182')]
[36m[2025-07-01 23:27:30,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3063808. Throughput: 0: 300.0. Samples: 3076720. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 23:27:30,986][166323] Avg episode reward: [(0, '876.509')]
[36m[2025-07-01 23:27:35,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3063808. Throughput: 0: 304.8. Samples: 3077696. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 23:27:35,992][166323] Avg episode reward: [(0, '875.791')]
[37m[1m[2025-07-01 23:27:36,042][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000005976_3063808.pth...
[36m[2025-07-01 23:27:36,046][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000005848_2998272.pth
[36m[2025-07-01 23:27:40,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3063808. Throughput: 0: 302.3. Samples: 3079424. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 23:27:40,972][166323] Avg episode reward: [(0, '891.887')]
[36m[2025-07-01 23:27:45,966][166323] Fps is (10 sec: 1642.7, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 3080192. Throughput: 0: 304.6. Samples: 3081296. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:27:45,966][166323] Avg episode reward: [(0, '1016.748')]
[36m[2025-07-01 23:27:50,948][166323] Fps is (10 sec: 1642.3, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 3080192. Throughput: 0: 303.7. Samples: 3082224. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:27:50,948][166323] Avg episode reward: [(0, '1008.690')]
[36m[2025-07-01 23:27:55,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 3080192. Throughput: 0: 303.8. Samples: 3084112. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:27:55,974][166323] Avg episode reward: [(0, '1044.629')]
[36m[2025-07-01 23:28:00,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3080192. Throughput: 0: 305.3. Samples: 3085936. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:28:00,965][166323] Avg episode reward: [(0, '1032.644')]
[36m[2025-07-01 23:28:05,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3080192. Throughput: 0: 306.8. Samples: 3086944. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:28:05,971][166323] Avg episode reward: [(0, '1067.252')]
[36m[2025-07-01 23:28:10,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3080192. Throughput: 0: 304.8. Samples: 3088688. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:28:10,964][166323] Avg episode reward: [(0, '1011.631')]
[36m[2025-07-01 23:28:15,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3080192. Throughput: 0: 307.9. Samples: 3090576. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:28:15,990][166323] Avg episode reward: [(0, '925.753')]
[31m[10488140 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[10488140 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[10488140 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:28:21,002][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3080192. Throughput: 0: 305.7. Samples: 3091456. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:28:21,002][166323] Avg episode reward: [(0, '977.854')]
[36m[2025-07-01 23:28:25,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3080192. Throughput: 0: 307.3. Samples: 3093248. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:28:25,958][166323] Avg episode reward: [(0, '956.430')]
[36m[2025-07-01 23:28:30,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3080192. Throughput: 0: 301.4. Samples: 3094864. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:28:30,979][166323] Avg episode reward: [(0, '957.023')]
[36m[2025-07-01 23:28:35,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3080192. Throughput: 0: 300.2. Samples: 3095744. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:28:35,982][166323] Avg episode reward: [(0, '981.552')]
[36m[2025-07-01 23:28:40,948][166323] Fps is (10 sec: 1643.5, 60 sec: 546.4, 300 sec: 333.3). Total num frames: 3096576. Throughput: 0: 295.6. Samples: 3097408. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:28:40,948][166323] Avg episode reward: [(0, '1007.073')]
[36m[2025-07-01 23:28:45,966][166323] Fps is (10 sec: 1641.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 3096576. Throughput: 0: 297.2. Samples: 3099312. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:28:45,967][166323] Avg episode reward: [(0, '1044.573')]
[36m[2025-07-01 23:28:50,977][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 3096576. Throughput: 0: 293.6. Samples: 3100160. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:28:50,977][166323] Avg episode reward: [(0, '1008.545')]
[36m[2025-07-01 23:28:55,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3096576. Throughput: 0: 296.6. Samples: 3102032. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:28:55,955][166323] Avg episode reward: [(0, '981.502')]
[36m[2025-07-01 23:29:00,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3096576. Throughput: 0: 294.4. Samples: 3103824. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:29:00,991][166323] Avg episode reward: [(0, '981.776')]
[36m[2025-07-01 23:29:05,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3096576. Throughput: 0: 295.1. Samples: 3104720. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:29:05,946][166323] Avg episode reward: [(0, '956.675')]
[31m[10535339 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[10535339 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[10535339 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[10538729 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[10538730 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[10538730 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:29:10,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3096576. Throughput: 0: 294.8. Samples: 3106512. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:29:10,953][166323] Avg episode reward: [(0, '858.405')]
[36m[2025-07-01 23:29:15,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3096576. Throughput: 0: 296.5. Samples: 3108208. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:29:15,989][166323] Avg episode reward: [(0, '905.229')]
[36m[2025-07-01 23:29:20,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3096576. Throughput: 0: 297.3. Samples: 3109120. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:29:20,967][166323] Avg episode reward: [(0, '850.826')]
[36m[2025-07-01 23:29:25,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3096576. Throughput: 0: 299.9. Samples: 3110912. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:29:25,970][166323] Avg episode reward: [(0, '916.581')]
[36m[2025-07-01 23:29:30,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3096576. Throughput: 0: 296.8. Samples: 3112672. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:29:30,975][166323] Avg episode reward: [(0, '883.921')]
[36m[2025-07-01 23:29:35,972][166323] Fps is (10 sec: 1638.0, 60 sec: 546.2, 300 sec: 333.3). Total num frames: 3112960. Throughput: 0: 296.9. Samples: 3113520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:29:35,973][166323] Avg episode reward: [(0, '934.761')]
[37m[1m[2025-07-01 23:29:36,026][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000006072_3112960.pth...
[36m[2025-07-01 23:29:36,029][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000005912_3031040.pth
[36m[2025-07-01 23:29:40,969][166323] Fps is (10 sec: 1639.4, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 3112960. Throughput: 0: 297.2. Samples: 3115408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:29:40,969][166323] Avg episode reward: [(0, '1002.010')]
[36m[2025-07-01 23:29:45,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 3112960. Throughput: 0: 298.1. Samples: 3117232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:29:45,964][166323] Avg episode reward: [(0, '1028.334')]
[36m[2025-07-01 23:29:50,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3112960. Throughput: 0: 298.0. Samples: 3118144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:29:50,988][166323] Avg episode reward: [(0, '1051.447')]
[31m[10582385 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[10582385 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[10582385 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:29:55,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3112960. Throughput: 0: 296.0. Samples: 3119840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:29:55,988][166323] Avg episode reward: [(0, '1072.343')]
[36m[2025-07-01 23:30:00,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3112960. Throughput: 0: 296.9. Samples: 3121568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:30:00,982][166323] Avg episode reward: [(0, '1053.075')]
[36m[2025-07-01 23:30:05,972][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3112960. Throughput: 0: 298.6. Samples: 3122560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:30:05,972][166323] Avg episode reward: [(0, '1069.902')]
[36m[2025-07-01 23:30:10,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3112960. Throughput: 0: 298.6. Samples: 3124352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:30:10,976][166323] Avg episode reward: [(0, '1074.690')]
[36m[2025-07-01 23:30:15,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3112960. Throughput: 0: 298.3. Samples: 3126096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:30:15,981][166323] Avg episode reward: [(0, '1042.170')]
[36m[2025-07-01 23:30:20,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3112960. Throughput: 0: 297.3. Samples: 3126896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:30:20,965][166323] Avg episode reward: [(0, '1030.455')]
[36m[2025-07-01 23:30:25,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3112960. Throughput: 0: 282.6. Samples: 3128128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:30:25,987][166323] Avg episode reward: [(0, '977.929')]
[36m[2025-07-01 23:30:31,025][166323] Fps is (10 sec: 1628.6, 60 sec: 545.7, 300 sec: 333.2). Total num frames: 3129344. Throughput: 0: 274.1. Samples: 3129584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:30:31,026][166323] Avg episode reward: [(0, '1002.786')]
[36m[2025-07-01 23:30:35,983][166323] Fps is (10 sec: 1639.1, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 3129344. Throughput: 0: 270.3. Samples: 3130304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:30:35,983][166323] Avg episode reward: [(0, '1003.787')]
[36m[2025-07-01 23:30:41,011][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 3129344. Throughput: 0: 260.5. Samples: 3131568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:30:41,012][166323] Avg episode reward: [(0, '974.891')]
[36m[2025-07-01 23:30:45,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 3129344. Throughput: 0: 250.4. Samples: 3132832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:30:45,966][166323] Avg episode reward: [(0, '995.735')]
[36m[2025-07-01 23:30:50,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3129344. Throughput: 0: 240.0. Samples: 3133360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:30:50,979][166323] Avg episode reward: [(0, '970.477')]
[36m[2025-07-01 23:30:56,015][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 3129344. Throughput: 0: 226.3. Samples: 3134544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:30:56,015][166323] Avg episode reward: [(0, '1026.354')]
[36m[2025-07-01 23:31:00,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3129344. Throughput: 0: 217.6. Samples: 3135888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:31:00,975][166323] Avg episode reward: [(0, '1030.235')]
[31m[10650681 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[10650682 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[10650682 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:31:05,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3129344. Throughput: 0: 213.6. Samples: 3136512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:31:05,989][166323] Avg episode reward: [(0, '1031.644')]
[36m[2025-07-01 23:31:10,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3129344. Throughput: 0: 213.8. Samples: 3137744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:31:10,966][166323] Avg episode reward: [(0, '1045.877')]
[36m[2025-07-01 23:31:15,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3129344. Throughput: 0: 217.2. Samples: 3139344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:31:15,954][166323] Avg episode reward: [(0, '1070.467')]
[36m[2025-07-01 23:31:20,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3129344. Throughput: 0: 220.8. Samples: 3140240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:31:20,992][166323] Avg episode reward: [(0, '1030.031')]
[36m[2025-07-01 23:31:26,002][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3129344. Throughput: 0: 225.8. Samples: 3141728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:31:26,002][166323] Avg episode reward: [(0, '977.607')]
[36m[2025-07-01 23:31:30,944][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 277.7). Total num frames: 3129344. Throughput: 0: 223.8. Samples: 3142896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:31:30,944][166323] Avg episode reward: [(0, '963.064')]
[36m[2025-07-01 23:31:35,984][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 277.7). Total num frames: 3129344. Throughput: 0: 229.7. Samples: 3143696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:31:35,984][166323] Avg episode reward: [(0, '945.502')]
[37m[1m[2025-07-01 23:31:36,036][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000006104_3129344.pth...
[36m[2025-07-01 23:31:36,040][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000005976_3063808.pth
[36m[2025-07-01 23:31:40,968][166323] Fps is (10 sec: 1634.4, 60 sec: 273.3, 300 sec: 333.2). Total num frames: 3145728. Throughput: 0: 243.5. Samples: 3145488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:31:40,968][166323] Avg episode reward: [(0, '968.606')]
[36m[2025-07-01 23:31:45,993][166323] Fps is (10 sec: 1637.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3145728. Throughput: 0: 250.6. Samples: 3147168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:31:45,993][166323] Avg episode reward: [(0, '894.518')]
[36m[2025-07-01 23:31:51,038][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 3145728. Throughput: 0: 255.0. Samples: 3148000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:31:51,039][166323] Avg episode reward: [(0, '912.823')]
[36m[2025-07-01 23:31:55,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3145728. Throughput: 0: 266.9. Samples: 3149760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:31:55,979][166323] Avg episode reward: [(0, '953.904')]
[36m[2025-07-01 23:32:00,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3145728. Throughput: 0: 267.2. Samples: 3151376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:32:00,988][166323] Avg episode reward: [(0, '966.401')]
[36m[2025-07-01 23:32:05,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3145728. Throughput: 0: 267.2. Samples: 3152256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:32:05,969][166323] Avg episode reward: [(0, '1018.964')]
[36m[2025-07-01 23:32:10,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3145728. Throughput: 0: 272.9. Samples: 3154000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:32:10,967][166323] Avg episode reward: [(0, '1004.205')]
[36m[2025-07-01 23:32:15,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3145728. Throughput: 0: 286.2. Samples: 3155776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:32:15,949][166323] Avg episode reward: [(0, '1017.850')]
[36m[2025-07-01 23:32:21,003][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3145728. Throughput: 0: 284.0. Samples: 3156480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:32:21,003][166323] Avg episode reward: [(0, '1062.469')]
[36m[2025-07-01 23:32:25,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3145728. Throughput: 0: 279.9. Samples: 3158080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:32:25,956][166323] Avg episode reward: [(0, '1051.140')]
[36m[2025-07-01 23:32:30,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3145728. Throughput: 0: 277.9. Samples: 3159664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:32:30,960][166323] Avg episode reward: [(0, '1028.658')]
[36m[2025-07-01 23:32:35,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3145728. Throughput: 0: 275.2. Samples: 3160368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:32:35,982][166323] Avg episode reward: [(0, '1047.981')]
[31m[10746164 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[10746164 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[10746164 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:32:41,023][166323] Fps is (10 sec: 1628.1, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 3162112. Throughput: 0: 267.1. Samples: 3161792. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:32:41,023][166323] Avg episode reward: [(0, '959.578')]
[36m[2025-07-01 23:32:45,983][166323] Fps is (10 sec: 1638.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3162112. Throughput: 0: 264.9. Samples: 3163296. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:32:45,983][166323] Avg episode reward: [(0, '1025.233')]
[36m[2025-07-01 23:32:50,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3162112. Throughput: 0: 261.9. Samples: 3164048. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:32:50,987][166323] Avg episode reward: [(0, '985.630')]
[36m[2025-07-01 23:32:55,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3162112. Throughput: 0: 260.3. Samples: 3165712. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:32:55,964][166323] Avg episode reward: [(0, '1006.018')]
[36m[2025-07-01 23:33:00,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3162112. Throughput: 0: 257.9. Samples: 3167392. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:33:00,988][166323] Avg episode reward: [(0, '993.817')]
[36m[2025-07-01 23:33:05,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3162112. Throughput: 0: 260.4. Samples: 3168192. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:33:05,989][166323] Avg episode reward: [(0, '1038.130')]
[36m[2025-07-01 23:33:11,000][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3162112. Throughput: 0: 262.1. Samples: 3169888. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:33:11,000][166323] Avg episode reward: [(0, '998.635')]
[36m[2025-07-01 23:33:15,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3162112. Throughput: 0: 261.4. Samples: 3171424. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:33:15,948][166323] Avg episode reward: [(0, '1042.480')]
[36m[2025-07-01 23:33:20,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3162112. Throughput: 0: 264.1. Samples: 3172256. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:33:20,987][166323] Avg episode reward: [(0, '948.231')]
[31m[10794316 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[10794316 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[10794316 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:33:25,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3162112. Throughput: 0: 270.3. Samples: 3173936. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:33:25,953][166323] Avg episode reward: [(0, '977.302')]
[36m[2025-07-01 23:33:30,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3162112. Throughput: 0: 273.2. Samples: 3175584. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:33:30,968][166323] Avg episode reward: [(0, '1035.251')]
[31m[10803889 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[10803890 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[10803890 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:33:35,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 3162112. Throughput: 0: 272.0. Samples: 3176288. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:33:35,988][166323] Avg episode reward: [(0, '1025.774')]
[37m[1m[2025-07-01 23:33:36,106][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000006168_3162112.pth...
[36m[2025-07-01 23:33:36,113][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000006072_3112960.pth
[36m[2025-07-01 23:33:40,946][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 222.2). Total num frames: 3162112. Throughput: 0: 268.6. Samples: 3177792. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:33:40,946][166323] Avg episode reward: [(0, '968.295')]
[36m[2025-07-01 23:33:45,961][166323] Fps is (10 sec: 1642.8, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3178496. Throughput: 0: 266.1. Samples: 3179360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:33:45,961][166323] Avg episode reward: [(0, '1026.334')]
[36m[2025-07-01 23:33:50,953][166323] Fps is (10 sec: 1637.2, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3178496. Throughput: 0: 264.4. Samples: 3180080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:33:50,953][166323] Avg episode reward: [(0, '1077.942')]
[36m[2025-07-01 23:33:55,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3178496. Throughput: 0: 261.1. Samples: 3181632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:33:55,981][166323] Avg episode reward: [(0, '1111.788')]
[36m[2025-07-01 23:34:00,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3178496. Throughput: 0: 259.5. Samples: 3183104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:34:00,957][166323] Avg episode reward: [(0, '1060.699')]
[36m[2025-07-01 23:34:05,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3178496. Throughput: 0: 257.3. Samples: 3183824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:34:05,952][166323] Avg episode reward: [(0, '1084.834')]
[36m[2025-07-01 23:34:11,031][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3178496. Throughput: 0: 252.0. Samples: 3185296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:34:11,032][166323] Avg episode reward: [(0, '1071.072')]
[36m[2025-07-01 23:34:15,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3178496. Throughput: 0: 248.9. Samples: 3186784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:34:15,967][166323] Avg episode reward: [(0, '1084.652')]
[36m[2025-07-01 23:34:20,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3178496. Throughput: 0: 250.1. Samples: 3187536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:34:20,956][166323] Avg episode reward: [(0, '1081.308')]
[36m[2025-07-01 23:34:25,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3178496. Throughput: 0: 248.1. Samples: 3188960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:34:25,952][166323] Avg episode reward: [(0, '1088.571')]
[36m[2025-07-01 23:34:30,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 3178496. Throughput: 0: 246.1. Samples: 3190432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:34:30,948][166323] Avg episode reward: [(0, '1088.847')]
[36m[2025-07-01 23:34:35,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 3178496. Throughput: 0: 247.5. Samples: 3191216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:34:35,951][166323] Avg episode reward: [(0, '1115.962')]
[36m[2025-07-01 23:34:40,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 3178496. Throughput: 0: 251.0. Samples: 3192928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:34:40,989][166323] Avg episode reward: [(0, '1089.655')]
[36m[2025-07-01 23:34:45,987][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 222.2). Total num frames: 3178496. Throughput: 0: 253.3. Samples: 3194512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:34:45,988][166323] Avg episode reward: [(0, '1084.138')]
[36m[2025-07-01 23:34:50,985][166323] Fps is (10 sec: 1638.9, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3194880. Throughput: 0: 251.5. Samples: 3195152. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 23:34:50,986][166323] Avg episode reward: [(0, '1059.527')]
[36m[2025-07-01 23:34:55,987][166323] Fps is (10 sec: 1638.5, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3194880. Throughput: 0: 250.9. Samples: 3196576. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 23:34:55,987][166323] Avg episode reward: [(0, '1054.957')]
[36m[2025-07-01 23:35:00,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3194880. Throughput: 0: 254.8. Samples: 3198256. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 23:35:00,989][166323] Avg episode reward: [(0, '1058.223')]
[36m[2025-07-01 23:35:05,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3194880. Throughput: 0: 255.3. Samples: 3199024. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 23:35:05,955][166323] Avg episode reward: [(0, '1024.717')]
[36m[2025-07-01 23:35:10,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 3194880. Throughput: 0: 259.5. Samples: 3200640. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 23:35:10,965][166323] Avg episode reward: [(0, '1041.803')]
[36m[2025-07-01 23:35:15,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3194880. Throughput: 0: 261.9. Samples: 3202224. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 23:35:15,976][166323] Avg episode reward: [(0, '1060.299')]
[36m[2025-07-01 23:35:20,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3194880. Throughput: 0: 264.2. Samples: 3203104. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 23:35:20,946][166323] Avg episode reward: [(0, '1093.057')]
[36m[2025-07-01 23:35:25,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.2). Total num frames: 3194880. Throughput: 0: 258.8. Samples: 3204576. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 23:35:25,990][166323] Avg episode reward: [(0, '1065.225')]
[36m[2025-07-01 23:35:31,000][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 222.1). Total num frames: 3194880. Throughput: 0: 260.5. Samples: 3206240. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 23:35:31,000][166323] Avg episode reward: [(0, '1114.960')]
[36m[2025-07-01 23:35:36,008][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 222.2). Total num frames: 3194880. Throughput: 0: 263.0. Samples: 3206992. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 23:35:36,008][166323] Avg episode reward: [(0, '1098.720')]
[37m[1m[2025-07-01 23:35:36,134][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000006232_3194880.pth...
[36m[2025-07-01 23:35:36,138][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000006104_3129344.pth
[31m[10925815 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[10925816 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[10925816 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:35:40,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 3194880. Throughput: 0: 264.0. Samples: 3208448. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 23:35:40,952][166323] Avg episode reward: [(0, '1078.749')]
[36m[2025-07-01 23:35:45,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 3194880. Throughput: 0: 259.9. Samples: 3209952. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-01 23:35:45,986][166323] Avg episode reward: [(0, '1063.693')]
[36m[2025-07-01 23:35:50,987][166323] Fps is (10 sec: 1632.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3211264. Throughput: 0: 260.1. Samples: 3210736. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 23:35:50,988][166323] Avg episode reward: [(0, '1059.316')]
[36m[2025-07-01 23:35:55,956][166323] Fps is (10 sec: 1643.3, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3211264. Throughput: 0: 258.2. Samples: 3212256. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 23:35:55,956][166323] Avg episode reward: [(0, '1034.606')]
[31m[10947614 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[10947614 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[10947615 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:36:00,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3211264. Throughput: 0: 256.0. Samples: 3213744. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 23:36:00,969][166323] Avg episode reward: [(0, '1027.522')]
[36m[2025-07-01 23:36:06,023][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 3211264. Throughput: 0: 249.9. Samples: 3214368. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 23:36:06,024][166323] Avg episode reward: [(0, '1030.116')]
[36m[2025-07-01 23:36:10,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3211264. Throughput: 0: 251.5. Samples: 3215888. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 23:36:10,972][166323] Avg episode reward: [(0, '1048.643')]
[36m[2025-07-01 23:36:15,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3211264. Throughput: 0: 244.6. Samples: 3217248. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 23:36:15,998][166323] Avg episode reward: [(0, '1069.724')]
[36m[2025-07-01 23:36:20,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3211264. Throughput: 0: 244.6. Samples: 3217984. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 23:36:20,947][166323] Avg episode reward: [(0, '1055.508')]
[36m[2025-07-01 23:36:25,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3211264. Throughput: 0: 245.6. Samples: 3219504. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 23:36:25,971][166323] Avg episode reward: [(0, '1089.015')]
[36m[2025-07-01 23:36:30,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3211264. Throughput: 0: 249.6. Samples: 3221184. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 23:36:30,982][166323] Avg episode reward: [(0, '1124.615')]
[36m[2025-07-01 23:36:35,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 3211264. Throughput: 0: 248.6. Samples: 3221920. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 23:36:35,976][166323] Avg episode reward: [(0, '1129.844')]
[36m[2025-07-01 23:36:40,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 3211264. Throughput: 0: 250.7. Samples: 3223536. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 23:36:40,949][166323] Avg episode reward: [(0, '1118.203')]
[36m[2025-07-01 23:36:45,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 3211264. Throughput: 0: 256.0. Samples: 3225264. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 23:36:45,975][166323] Avg episode reward: [(0, '1077.202')]
[36m[2025-07-01 23:36:50,975][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 222.2). Total num frames: 3211264. Throughput: 0: 257.7. Samples: 3225952. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-01 23:36:50,975][166323] Avg episode reward: [(0, '1128.505')]
[36m[2025-07-01 23:36:55,997][166323] Fps is (10 sec: 1634.8, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3227648. Throughput: 0: 259.4. Samples: 3227568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:36:55,997][166323] Avg episode reward: [(0, '1086.309')]
[36m[2025-07-01 23:37:00,998][166323] Fps is (10 sec: 1634.6, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3227648. Throughput: 0: 263.8. Samples: 3229120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:37:00,998][166323] Avg episode reward: [(0, '1039.414')]
[36m[2025-07-01 23:37:05,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 3227648. Throughput: 0: 265.9. Samples: 3229952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:37:05,956][166323] Avg episode reward: [(0, '1037.584')]
[36m[2025-07-01 23:37:10,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3227648. Throughput: 0: 268.0. Samples: 3231568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:37:10,979][166323] Avg episode reward: [(0, '1043.035')]
[36m[2025-07-01 23:37:15,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3227648. Throughput: 0: 267.6. Samples: 3233216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:37:15,945][166323] Avg episode reward: [(0, '1054.517')]
[36m[2025-07-01 23:37:21,011][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 3227648. Throughput: 0: 267.9. Samples: 3233984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:37:21,011][166323] Avg episode reward: [(0, '1103.968')]
[36m[2025-07-01 23:37:25,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3227648. Throughput: 0: 266.8. Samples: 3235552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:37:25,983][166323] Avg episode reward: [(0, '1113.369')]
[36m[2025-07-01 23:37:31,025][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3227648. Throughput: 0: 261.8. Samples: 3237056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:37:31,026][166323] Avg episode reward: [(0, '1123.807')]
[36m[2025-07-01 23:37:36,015][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.2). Total num frames: 3227648. Throughput: 0: 262.2. Samples: 3237760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:37:36,016][166323] Avg episode reward: [(0, '1119.463')]
[37m[1m[2025-07-01 23:37:36,111][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000006296_3227648.pth...
[36m[2025-07-01 23:37:36,116][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000006168_3162112.pth
[36m[2025-07-01 23:37:40,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 3227648. Throughput: 0: 261.9. Samples: 3239344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:37:40,964][166323] Avg episode reward: [(0, '1117.515')]
[36m[2025-07-01 23:37:45,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 3227648. Throughput: 0: 261.8. Samples: 3240896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:37:45,976][166323] Avg episode reward: [(0, '1096.144')]
[36m[2025-07-01 23:37:50,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 3227648. Throughput: 0: 261.2. Samples: 3241712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:37:50,982][166323] Avg episode reward: [(0, '1084.099')]
[36m[2025-07-01 23:37:55,974][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 222.2). Total num frames: 3227648. Throughput: 0: 261.7. Samples: 3243344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:37:55,974][166323] Avg episode reward: [(0, '1025.510')]
[36m[2025-07-01 23:38:00,995][166323] Fps is (10 sec: 1636.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3244032. Throughput: 0: 258.6. Samples: 3244864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:38:00,995][166323] Avg episode reward: [(0, '944.280')]
[36m[2025-07-01 23:38:05,996][166323] Fps is (10 sec: 1634.7, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3244032. Throughput: 0: 260.4. Samples: 3245696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:38:05,996][166323] Avg episode reward: [(0, '971.284')]
[36m[2025-07-01 23:38:10,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3244032. Throughput: 0: 261.7. Samples: 3247328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:38:10,974][166323] Avg episode reward: [(0, '973.691')]
[36m[2025-07-01 23:38:15,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 3244032. Throughput: 0: 262.6. Samples: 3248864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:38:15,994][166323] Avg episode reward: [(0, '982.448')]
[36m[2025-07-01 23:38:20,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3244032. Throughput: 0: 265.2. Samples: 3249680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:38:20,963][166323] Avg episode reward: [(0, '944.569')]
[36m[2025-07-01 23:38:25,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3244032. Throughput: 0: 261.3. Samples: 3251104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:38:25,972][166323] Avg episode reward: [(0, '940.702')]
[36m[2025-07-01 23:38:30,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3244032. Throughput: 0: 262.8. Samples: 3252720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:38:30,967][166323] Avg episode reward: [(0, '941.481')]
[36m[2025-07-01 23:38:35,995][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.6). Total num frames: 3244032. Throughput: 0: 261.3. Samples: 3253472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:38:35,995][166323] Avg episode reward: [(0, '998.145')]
[36m[2025-07-01 23:38:40,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 3244032. Throughput: 0: 255.2. Samples: 3254832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:38:40,985][166323] Avg episode reward: [(0, '1006.879')]
[36m[2025-07-01 23:38:45,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.1). Total num frames: 3244032. Throughput: 0: 254.8. Samples: 3256320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:38:45,963][166323] Avg episode reward: [(0, '1019.300')]
[33m[11117366 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[11117367 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.842362105846405
[33mCrash Rate: 0.15080526471138
[33mTimeout Rate: 0.00683260103687644 (navigation_task.py:265)
[33m[11117367 ms][navigation_task] - WARNING : 
[33mSuccesses: 1726
[33mCrashes : 309
[33mTimeouts: 14 (navigation_task.py:268)
[36m[2025-07-01 23:38:50,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 3244032. Throughput: 0: 255.7. Samples: 3257200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:38:50,991][166323] Avg episode reward: [(0, '1012.232')]
[36m[2025-07-01 23:38:55,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.1). Total num frames: 3244032. Throughput: 0: 254.2. Samples: 3258768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:38:55,976][166323] Avg episode reward: [(0, '1006.533')]
[36m[2025-07-01 23:39:00,990][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 222.1). Total num frames: 3244032. Throughput: 0: 253.2. Samples: 3260256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:39:00,990][166323] Avg episode reward: [(0, '998.277')]
[36m[2025-07-01 23:39:05,955][166323] Fps is (10 sec: 1641.8, 60 sec: 273.3, 300 sec: 277.8). Total num frames: 3260416. Throughput: 0: 248.9. Samples: 3260880. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:39:05,955][166323] Avg episode reward: [(0, '1019.048')]
[36m[2025-07-01 23:39:10,992][166323] Fps is (10 sec: 1638.1, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3260416. Throughput: 0: 252.0. Samples: 3262448. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:39:10,992][166323] Avg episode reward: [(0, '996.529')]
[31m[11142255 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11142255 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[11142255 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:39:15,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3260416. Throughput: 0: 253.8. Samples: 3264144. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:39:15,984][166323] Avg episode reward: [(0, '947.514')]
[31m[11147500 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11147501 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[11147501 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:39:20,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3260416. Throughput: 0: 257.3. Samples: 3265040. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:39:20,963][166323] Avg episode reward: [(0, '959.419')]
[36m[2025-07-01 23:39:25,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3260416. Throughput: 0: 263.9. Samples: 3266704. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:39:25,980][166323] Avg episode reward: [(0, '980.440')]
[36m[2025-07-01 23:39:30,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3260416. Throughput: 0: 268.6. Samples: 3268416. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:39:30,993][166323] Avg episode reward: [(0, '987.688')]
[31m[11163606 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11163606 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[11163607 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:39:35,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3260416. Throughput: 0: 266.7. Samples: 3269200. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:39:35,983][166323] Avg episode reward: [(0, '977.901')]
[37m[1m[2025-07-01 23:39:36,036][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000006360_3260416.pth...
[36m[2025-07-01 23:39:36,040][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000006232_3194880.pth
[36m[2025-07-01 23:39:41,030][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3260416. Throughput: 0: 267.1. Samples: 3270800. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:39:41,031][166323] Avg episode reward: [(0, '1012.193')]
[36m[2025-07-01 23:39:45,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 3260416. Throughput: 0: 272.0. Samples: 3272496. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:39:45,985][166323] Avg episode reward: [(0, '1044.692')]
[36m[2025-07-01 23:39:50,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 3260416. Throughput: 0: 275.8. Samples: 3273296. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:39:50,965][166323] Avg episode reward: [(0, '1045.364')]
[36m[2025-07-01 23:39:55,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 3260416. Throughput: 0: 280.7. Samples: 3275072. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:39:55,969][166323] Avg episode reward: [(0, '1049.348')]
[36m[2025-07-01 23:40:00,979][166323] Fps is (10 sec: 1636.0, 60 sec: 546.2, 300 sec: 277.7). Total num frames: 3276800. Throughput: 0: 280.6. Samples: 3276768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:40:00,980][166323] Avg episode reward: [(0, '955.789')]
[36m[2025-07-01 23:40:05,973][166323] Fps is (10 sec: 1637.7, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3276800. Throughput: 0: 278.0. Samples: 3277552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:40:05,974][166323] Avg episode reward: [(0, '1018.071')]
[36m[2025-07-01 23:40:10,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3276800. Throughput: 0: 277.2. Samples: 3279168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:40:10,945][166323] Avg episode reward: [(0, '993.602')]
[36m[2025-07-01 23:40:15,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3276800. Throughput: 0: 278.1. Samples: 3280928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:40:15,981][166323] Avg episode reward: [(0, '1008.441')]
[36m[2025-07-01 23:40:20,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3276800. Throughput: 0: 279.5. Samples: 3281776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:40:20,984][166323] Avg episode reward: [(0, '987.237')]
[36m[2025-07-01 23:40:25,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3276800. Throughput: 0: 279.2. Samples: 3283344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:40:25,961][166323] Avg episode reward: [(0, '950.506')]
[31m[11214924 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11214924 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[11214924 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:40:30,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3276800. Throughput: 0: 282.0. Samples: 3285184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:40:30,971][166323] Avg episode reward: [(0, '926.213')]
[36m[2025-07-01 23:40:36,425][166323] Fps is (10 sec: 0.0, 60 sec: 271.1, 300 sec: 277.2). Total num frames: 3276800. Throughput: 0: 280.9. Samples: 3286064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:40:36,426][166323] Avg episode reward: [(0, '935.049')]
[36m[2025-07-01 23:40:40,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 3276800. Throughput: 0: 272.8. Samples: 3287344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:40:40,955][166323] Avg episode reward: [(0, '945.464')]
[36m[2025-07-01 23:40:45,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 3276800. Throughput: 0: 268.5. Samples: 3288848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:40:45,975][166323] Avg episode reward: [(0, '958.547')]
[36m[2025-07-01 23:40:51,043][166323] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 222.1). Total num frames: 3276800. Throughput: 0: 264.1. Samples: 3289456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:40:51,043][166323] Avg episode reward: [(0, '970.424')]
[31m[11241649 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11241650 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[11241650 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:40:56,006][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 3276800. Throughput: 0: 256.7. Samples: 3290736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:40:56,007][166323] Avg episode reward: [(0, '935.685')]
[36m[2025-07-01 23:41:00,967][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 222.2). Total num frames: 3276800. Throughput: 0: 250.7. Samples: 3292208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:41:00,967][166323] Avg episode reward: [(0, '958.093')]
[36m[2025-07-01 23:41:05,983][166323] Fps is (10 sec: 1642.2, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3293184. Throughput: 0: 247.8. Samples: 3292928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:41:05,983][166323] Avg episode reward: [(0, '1037.871')]
[36m[2025-07-01 23:41:10,947][166323] Fps is (10 sec: 1641.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3293184. Throughput: 0: 249.3. Samples: 3294560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:41:10,947][166323] Avg episode reward: [(0, '1028.361')]
[36m[2025-07-01 23:41:15,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3293184. Throughput: 0: 243.3. Samples: 3296128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:41:15,951][166323] Avg episode reward: [(0, '1022.628')]
[36m[2025-07-01 23:41:20,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3293184. Throughput: 0: 243.9. Samples: 3296928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:41:20,969][166323] Avg episode reward: [(0, '1015.992')]
[36m[2025-07-01 23:41:25,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3293184. Throughput: 0: 246.4. Samples: 3298432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:41:25,959][166323] Avg episode reward: [(0, '1023.541')]
[36m[2025-07-01 23:41:30,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3293184. Throughput: 0: 248.6. Samples: 3300032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:41:30,963][166323] Avg episode reward: [(0, '1029.943')]
[36m[2025-07-01 23:41:35,981][166323] Fps is (10 sec: 0.0, 60 sec: 275.1, 300 sec: 277.7). Total num frames: 3293184. Throughput: 0: 253.9. Samples: 3300864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:41:35,981][166323] Avg episode reward: [(0, '1024.567')]
[37m[1m[2025-07-01 23:41:36,050][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000006424_3293184.pth...
[36m[2025-07-01 23:41:36,057][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000006296_3227648.pth
[36m[2025-07-01 23:41:40,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3293184. Throughput: 0: 263.3. Samples: 3302576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:41:40,972][166323] Avg episode reward: [(0, '1032.249')]
[36m[2025-07-01 23:41:45,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3293184. Throughput: 0: 267.0. Samples: 3304224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:41:45,977][166323] Avg episode reward: [(0, '1038.096')]
[36m[2025-07-01 23:41:50,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.5, 300 sec: 222.2). Total num frames: 3293184. Throughput: 0: 270.5. Samples: 3305088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:41:50,944][166323] Avg episode reward: [(0, '1021.610')]
[36m[2025-07-01 23:41:55,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 3293184. Throughput: 0: 270.3. Samples: 3306736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:41:55,987][166323] Avg episode reward: [(0, '991.267')]
[36m[2025-07-01 23:42:00,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 3293184. Throughput: 0: 270.9. Samples: 3308320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:42:00,958][166323] Avg episode reward: [(0, '982.894')]
[31m[11311216 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11311216 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[11311216 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:42:05,993][166323] Fps is (10 sec: 1637.4, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3309568. Throughput: 0: 271.1. Samples: 3309136. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-01 23:42:05,993][166323] Avg episode reward: [(0, '952.031')]
[36m[2025-07-01 23:42:10,967][166323] Fps is (10 sec: 1636.9, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3309568. Throughput: 0: 272.3. Samples: 3310688. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-01 23:42:10,967][166323] Avg episode reward: [(0, '987.964')]
[36m[2025-07-01 23:42:15,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3309568. Throughput: 0: 276.3. Samples: 3312464. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-01 23:42:15,960][166323] Avg episode reward: [(0, '954.957')]
[36m[2025-07-01 23:42:20,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3309568. Throughput: 0: 276.2. Samples: 3313296. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-01 23:42:20,990][166323] Avg episode reward: [(0, '886.529')]
[36m[2025-07-01 23:42:25,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3309568. Throughput: 0: 278.0. Samples: 3315088. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-01 23:42:25,976][166323] Avg episode reward: [(0, '917.133')]
[36m[2025-07-01 23:42:30,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 3309568. Throughput: 0: 276.8. Samples: 3316672. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-01 23:42:30,944][166323] Avg episode reward: [(0, '974.520')]
[36m[2025-07-01 23:42:35,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3309568. Throughput: 0: 276.8. Samples: 3317552. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-01 23:42:35,965][166323] Avg episode reward: [(0, '1002.021')]
[36m[2025-07-01 23:42:40,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3309568. Throughput: 0: 280.0. Samples: 3319328. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-01 23:42:40,954][166323] Avg episode reward: [(0, '914.782')]
[36m[2025-07-01 23:42:45,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3309568. Throughput: 0: 284.5. Samples: 3321120. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-01 23:42:45,953][166323] Avg episode reward: [(0, '949.745')]
[36m[2025-07-01 23:42:50,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3309568. Throughput: 0: 283.4. Samples: 3321888. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-01 23:42:50,991][166323] Avg episode reward: [(0, '1021.691')]
[31m[11363789 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11363789 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[11363789 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:42:55,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 3309568. Throughput: 0: 287.7. Samples: 3323632. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-01 23:42:55,952][166323] Avg episode reward: [(0, '1032.120')]
[36m[2025-07-01 23:43:00,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.2). Total num frames: 3309568. Throughput: 0: 284.6. Samples: 3325280. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-01 23:43:00,986][166323] Avg episode reward: [(0, '1038.203')]
[36m[2025-07-01 23:43:05,975][166323] Fps is (10 sec: 1634.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3325952. Throughput: 0: 284.5. Samples: 3326096. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 23:43:05,976][166323] Avg episode reward: [(0, '951.164')]
[36m[2025-07-01 23:43:10,987][166323] Fps is (10 sec: 1638.1, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3325952. Throughput: 0: 284.0. Samples: 3327872. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 23:43:10,988][166323] Avg episode reward: [(0, '985.920')]
[36m[2025-07-01 23:43:15,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3325952. Throughput: 0: 287.5. Samples: 3329616. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 23:43:15,960][166323] Avg episode reward: [(0, '1038.827')]
[36m[2025-07-01 23:43:20,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3325952. Throughput: 0: 287.7. Samples: 3330496. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 23:43:20,960][166323] Avg episode reward: [(0, '992.096')]
[36m[2025-07-01 23:43:25,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3325952. Throughput: 0: 285.2. Samples: 3332160. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 23:43:25,954][166323] Avg episode reward: [(0, '1006.594')]
[36m[2025-07-01 23:43:30,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3325952. Throughput: 0: 282.6. Samples: 3333840. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 23:43:30,962][166323] Avg episode reward: [(0, '939.123')]
[36m[2025-07-01 23:43:35,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3325952. Throughput: 0: 284.5. Samples: 3334688. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 23:43:35,979][166323] Avg episode reward: [(0, '964.052')]
[37m[1m[2025-07-01 23:43:36,030][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000006488_3325952.pth...
[36m[2025-07-01 23:43:36,033][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000006360_3260416.pth
[36m[2025-07-01 23:43:40,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3325952. Throughput: 0: 281.2. Samples: 3336288. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 23:43:40,958][166323] Avg episode reward: [(0, '970.994')]
[36m[2025-07-01 23:43:45,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3325952. Throughput: 0: 279.1. Samples: 3337840. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 23:43:45,987][166323] Avg episode reward: [(0, '947.062')]
[36m[2025-07-01 23:43:51,013][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3325952. Throughput: 0: 278.2. Samples: 3338624. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 23:43:51,013][166323] Avg episode reward: [(0, '954.565')]
[31m[11420192 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11420193 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[11420193 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:43:55,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3325952. Throughput: 0: 276.7. Samples: 3340320. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 23:43:55,974][166323] Avg episode reward: [(0, '975.633')]
[36m[2025-07-01 23:44:00,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 3325952. Throughput: 0: 273.9. Samples: 3341936. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-01 23:44:00,948][166323] Avg episode reward: [(0, '1004.427')]
[36m[2025-07-01 23:44:05,968][166323] Fps is (10 sec: 1639.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3342336. Throughput: 0: 271.6. Samples: 3342720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:44:05,969][166323] Avg episode reward: [(0, '1037.841')]
[36m[2025-07-01 23:44:10,990][166323] Fps is (10 sec: 1631.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3342336. Throughput: 0: 270.4. Samples: 3344336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:44:10,990][166323] Avg episode reward: [(0, '1051.588')]
[36m[2025-07-01 23:44:15,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3342336. Throughput: 0: 269.7. Samples: 3345984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:44:15,987][166323] Avg episode reward: [(0, '1064.689')]
[36m[2025-07-01 23:44:20,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3342336. Throughput: 0: 273.1. Samples: 3346976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:44:20,973][166323] Avg episode reward: [(0, '1082.203')]
[36m[2025-07-01 23:44:26,013][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 3342336. Throughput: 0: 271.7. Samples: 3348528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:44:26,014][166323] Avg episode reward: [(0, '1079.386')]
[36m[2025-07-01 23:44:30,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3342336. Throughput: 0: 275.5. Samples: 3350240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:44:30,989][166323] Avg episode reward: [(0, '1089.815')]
[36m[2025-07-01 23:44:35,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3342336. Throughput: 0: 276.4. Samples: 3351056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:44:35,986][166323] Avg episode reward: [(0, '1014.362')]
[36m[2025-07-01 23:44:40,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3342336. Throughput: 0: 277.8. Samples: 3352816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:44:40,950][166323] Avg episode reward: [(0, '1002.831')]
[36m[2025-07-01 23:44:45,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3342336. Throughput: 0: 278.8. Samples: 3354480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:44:45,947][166323] Avg episode reward: [(0, '976.924')]
[36m[2025-07-01 23:44:50,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3342336. Throughput: 0: 279.5. Samples: 3355296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:44:50,965][166323] Avg episode reward: [(0, '1003.338')]
[36m[2025-07-01 23:44:55,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 3342336. Throughput: 0: 284.9. Samples: 3357152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:44:55,974][166323] Avg episode reward: [(0, '900.199')]
[31m[11485123 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11485123 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[11485123 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[11485618 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11485619 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[11485619 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:45:00,982][166323] Fps is (10 sec: 1635.6, 60 sec: 545.8, 300 sec: 277.7). Total num frames: 3358720. Throughput: 0: 286.6. Samples: 3358880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:45:00,982][166323] Avg episode reward: [(0, '946.040')]
[36m[2025-07-01 23:45:05,955][166323] Fps is (10 sec: 1641.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3358720. Throughput: 0: 284.2. Samples: 3359760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:45:05,955][166323] Avg episode reward: [(0, '1018.897')]
[36m[2025-07-01 23:45:10,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3358720. Throughput: 0: 285.6. Samples: 3361376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:45:10,993][166323] Avg episode reward: [(0, '1016.572')]
[36m[2025-07-01 23:45:15,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3358720. Throughput: 0: 283.7. Samples: 3363008. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:45:15,989][166323] Avg episode reward: [(0, '1026.540')]
[36m[2025-07-01 23:45:20,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3358720. Throughput: 0: 285.2. Samples: 3363888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:45:20,983][166323] Avg episode reward: [(0, '1043.905')]
[36m[2025-07-01 23:45:25,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3358720. Throughput: 0: 281.5. Samples: 3365488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:45:25,964][166323] Avg episode reward: [(0, '1050.160')]
[36m[2025-07-01 23:45:30,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 278.1). Total num frames: 3358720. Throughput: 0: 280.1. Samples: 3367088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:45:30,954][166323] Avg episode reward: [(0, '1031.845')]
[36m[2025-07-01 23:45:35,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3358720. Throughput: 0: 279.1. Samples: 3367856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:45:35,965][166323] Avg episode reward: [(0, '1033.548')]
[37m[1m[2025-07-01 23:45:36,024][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000006552_3358720.pth...
[36m[2025-07-01 23:45:36,035][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000006424_3293184.pth
[36m[2025-07-01 23:45:40,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3358720. Throughput: 0: 271.1. Samples: 3369344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:45:40,953][166323] Avg episode reward: [(0, '1048.195')]
[31m[11534473 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11534473 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[11534473 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:45:45,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.8). Total num frames: 3358720. Throughput: 0: 266.1. Samples: 3370848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:45:45,963][166323] Avg episode reward: [(0, '1070.143')]
[36m[2025-07-01 23:45:50,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3358720. Throughput: 0: 263.4. Samples: 3371616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:45:50,971][166323] Avg episode reward: [(0, '1069.711')]
[31m[11544394 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11544394 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[11544394 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:45:55,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3358720. Throughput: 0: 265.7. Samples: 3373328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:45:55,980][166323] Avg episode reward: [(0, '1039.604')]
[36m[2025-07-01 23:46:00,995][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 222.1). Total num frames: 3358720. Throughput: 0: 265.2. Samples: 3374944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:46:00,995][166323] Avg episode reward: [(0, '1041.591')]
[36m[2025-07-01 23:46:05,950][166323] Fps is (10 sec: 1643.3, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3375104. Throughput: 0: 265.1. Samples: 3375808. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 23:46:05,950][166323] Avg episode reward: [(0, '1118.157')]
[31m[11557087 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11557088 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[11557088 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:46:10,983][166323] Fps is (10 sec: 1640.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3375104. Throughput: 0: 265.1. Samples: 3377424. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 23:46:10,983][166323] Avg episode reward: [(0, '1046.802')]
[36m[2025-07-01 23:46:15,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3375104. Throughput: 0: 266.7. Samples: 3379088. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 23:46:15,950][166323] Avg episode reward: [(0, '994.366')]
[36m[2025-07-01 23:46:20,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3375104. Throughput: 0: 269.2. Samples: 3379968. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 23:46:20,962][166323] Avg episode reward: [(0, '1047.318')]
[36m[2025-07-01 23:46:25,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3375104. Throughput: 0: 273.0. Samples: 3381632. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 23:46:25,964][166323] Avg episode reward: [(0, '983.335')]
[36m[2025-07-01 23:46:30,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3375104. Throughput: 0: 276.3. Samples: 3383280. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 23:46:30,959][166323] Avg episode reward: [(0, '987.901')]
[36m[2025-07-01 23:46:36,014][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 3375104. Throughput: 0: 277.8. Samples: 3384128. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 23:46:36,014][166323] Avg episode reward: [(0, '952.527')]
[36m[2025-07-01 23:46:40,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3375104. Throughput: 0: 275.8. Samples: 3385744. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 23:46:40,991][166323] Avg episode reward: [(0, '999.314')]
[36m[2025-07-01 23:46:45,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3375104. Throughput: 0: 273.8. Samples: 3387264. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 23:46:45,984][166323] Avg episode reward: [(0, '1063.204')]
[31m[11596877 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11596878 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[11596878 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:46:50,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3375104. Throughput: 0: 272.9. Samples: 3388096. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 23:46:50,979][166323] Avg episode reward: [(0, '971.375')]
[36m[2025-07-01 23:46:56,017][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 3375104. Throughput: 0: 272.1. Samples: 3389680. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 23:46:56,018][166323] Avg episode reward: [(0, '931.940')]
[36m[2025-07-01 23:47:00,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 222.2). Total num frames: 3375104. Throughput: 0: 267.8. Samples: 3391136. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 23:47:00,945][166323] Avg episode reward: [(0, '970.446')]
[36m[2025-07-01 23:47:05,990][166323] Fps is (10 sec: 1642.8, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3391488. Throughput: 0: 264.0. Samples: 3391856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:47:05,991][166323] Avg episode reward: [(0, '960.237')]
[36m[2025-07-01 23:47:10,969][166323] Fps is (10 sec: 1634.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3391488. Throughput: 0: 264.1. Samples: 3393520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:47:10,969][166323] Avg episode reward: [(0, '1002.300')]
[36m[2025-07-01 23:47:15,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3391488. Throughput: 0: 260.3. Samples: 3394992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:47:15,952][166323] Avg episode reward: [(0, '1001.159')]
[36m[2025-07-01 23:47:20,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3391488. Throughput: 0: 256.6. Samples: 3395664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:47:20,973][166323] Avg episode reward: [(0, '939.833')]
[36m[2025-07-01 23:47:25,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 3391488. Throughput: 0: 252.1. Samples: 3397088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:47:25,993][166323] Avg episode reward: [(0, '968.038')]
[36m[2025-07-01 23:47:30,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3391488. Throughput: 0: 243.4. Samples: 3398208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:47:30,946][166323] Avg episode reward: [(0, '959.763')]
[36m[2025-07-01 23:47:36,001][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3391488. Throughput: 0: 239.2. Samples: 3398864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:47:36,001][166323] Avg episode reward: [(0, '902.418')]
[37m[1m[2025-07-01 23:47:36,078][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000006616_3391488.pth...
[36m[2025-07-01 23:47:36,084][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000006488_3325952.pth
[36m[2025-07-01 23:47:40,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3391488. Throughput: 0: 234.6. Samples: 3400224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:47:40,959][166323] Avg episode reward: [(0, '894.671')]
[36m[2025-07-01 23:47:46,010][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3391488. Throughput: 0: 225.1. Samples: 3401280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:47:46,010][166323] Avg episode reward: [(0, '846.189')]
[36m[2025-07-01 23:47:51,005][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 3391488. Throughput: 0: 221.4. Samples: 3401824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:47:51,006][166323] Avg episode reward: [(0, '807.860')]
[36m[2025-07-01 23:47:55,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3391488. Throughput: 0: 214.7. Samples: 3403184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:47:55,991][166323] Avg episode reward: [(0, '834.601')]
[36m[2025-07-01 23:48:00,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 3391488. Throughput: 0: 215.5. Samples: 3404688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:48:00,950][166323] Avg episode reward: [(0, '845.640')]
[36m[2025-07-01 23:48:05,966][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 222.2). Total num frames: 3391488. Throughput: 0: 215.9. Samples: 3405376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:48:05,967][166323] Avg episode reward: [(0, '906.721')]
[36m[2025-07-01 23:48:10,985][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 222.1). Total num frames: 3391488. Throughput: 0: 218.4. Samples: 3406912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:48:10,985][166323] Avg episode reward: [(0, '944.595')]
[31m[11682894 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11682894 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[11682894 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:48:15,981][166323] Fps is (10 sec: 1636.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3407872. Throughput: 0: 227.4. Samples: 3408448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:48:15,981][166323] Avg episode reward: [(0, '946.317')]
[36m[2025-07-01 23:48:20,981][166323] Fps is (10 sec: 1639.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3407872. Throughput: 0: 229.4. Samples: 3409184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:48:20,981][166323] Avg episode reward: [(0, '961.193')]
[36m[2025-07-01 23:48:25,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3407872. Throughput: 0: 237.1. Samples: 3410896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:48:25,966][166323] Avg episode reward: [(0, '1028.831')]
[36m[2025-07-01 23:48:30,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3407872. Throughput: 0: 250.6. Samples: 3412544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:48:30,953][166323] Avg episode reward: [(0, '1024.780')]
[36m[2025-07-01 23:48:35,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3407872. Throughput: 0: 253.7. Samples: 3413232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:48:35,965][166323] Avg episode reward: [(0, '1012.578')]
[36m[2025-07-01 23:48:40,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3407872. Throughput: 0: 260.4. Samples: 3414896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:48:40,976][166323] Avg episode reward: [(0, '973.184')]
[36m[2025-07-01 23:48:45,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3407872. Throughput: 0: 264.8. Samples: 3416608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:48:45,961][166323] Avg episode reward: [(0, '973.883')]
[36m[2025-07-01 23:48:50,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3407872. Throughput: 0: 267.3. Samples: 3417408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:48:50,988][166323] Avg episode reward: [(0, '1052.838')]
[36m[2025-07-01 23:48:55,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3407872. Throughput: 0: 270.9. Samples: 3419104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:48:55,990][166323] Avg episode reward: [(0, '999.712')]
[36m[2025-07-01 23:49:01,003][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 222.1). Total num frames: 3407872. Throughput: 0: 271.9. Samples: 3420688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:49:01,003][166323] Avg episode reward: [(0, '1050.703')]
[36m[2025-07-01 23:49:05,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 3407872. Throughput: 0: 276.3. Samples: 3421616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:49:05,976][166323] Avg episode reward: [(0, '1032.517')]
[36m[2025-07-01 23:49:10,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 3407872. Throughput: 0: 274.1. Samples: 3423232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:49:10,965][166323] Avg episode reward: [(0, '1029.527')]
[36m[2025-07-01 23:49:15,951][166323] Fps is (10 sec: 1642.4, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3424256. Throughput: 0: 276.3. Samples: 3424976. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 23:49:15,952][166323] Avg episode reward: [(0, '1086.894')]
[31m[11744822 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11744823 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[11744823 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:49:20,970][166323] Fps is (10 sec: 1637.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3424256. Throughput: 0: 278.7. Samples: 3425776. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 23:49:20,970][166323] Avg episode reward: [(0, '1078.257')]
[36m[2025-07-01 23:49:25,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3424256. Throughput: 0: 279.5. Samples: 3427472. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 23:49:25,974][166323] Avg episode reward: [(0, '1044.102')]
[36m[2025-07-01 23:49:30,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3424256. Throughput: 0: 277.4. Samples: 3429088. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 23:49:30,954][166323] Avg episode reward: [(0, '1080.638')]
[36m[2025-07-01 23:49:35,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3424256. Throughput: 0: 280.2. Samples: 3430016. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 23:49:35,977][166323] Avg episode reward: [(0, '1066.451')]
[37m[1m[2025-07-01 23:49:36,039][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000006680_3424256.pth...
[36m[2025-07-01 23:49:36,044][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000006552_3358720.pth
[36m[2025-07-01 23:49:40,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3424256. Throughput: 0: 280.2. Samples: 3431712. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 23:49:40,985][166323] Avg episode reward: [(0, '1092.287')]
[36m[2025-07-01 23:49:45,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3424256. Throughput: 0: 278.7. Samples: 3433216. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 23:49:45,955][166323] Avg episode reward: [(0, '1037.393')]
[36m[2025-07-01 23:49:50,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3424256. Throughput: 0: 273.7. Samples: 3433936. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 23:49:50,996][166323] Avg episode reward: [(0, '1084.245')]
[36m[2025-07-01 23:49:55,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.1). Total num frames: 3424256. Throughput: 0: 270.4. Samples: 3435408. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 23:49:55,993][166323] Avg episode reward: [(0, '1067.540')]
[31m[11788216 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11788216 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[11788217 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:50:00,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.1). Total num frames: 3424256. Throughput: 0: 266.8. Samples: 3436992. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 23:50:00,986][166323] Avg episode reward: [(0, '1074.732')]
[36m[2025-07-01 23:50:05,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 3424256. Throughput: 0: 266.1. Samples: 3437744. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 23:50:05,946][166323] Avg episode reward: [(0, '1053.764')]
[36m[2025-07-01 23:50:10,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 3424256. Throughput: 0: 264.3. Samples: 3439360. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-01 23:50:10,961][166323] Avg episode reward: [(0, '1019.897')]
[36m[2025-07-01 23:50:15,962][166323] Fps is (10 sec: 1635.7, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3440640. Throughput: 0: 264.1. Samples: 3440976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:50:15,963][166323] Avg episode reward: [(0, '992.499')]
[33m[11807145 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[11807146 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.81494140625
[33mCrash Rate: 0.169921875
[33mTimeout Rate: 0.01513671875 (navigation_task.py:265)
[33m[11807146 ms][navigation_task] - WARNING : 
[33mSuccesses: 1669
[33mCrashes : 348
[33mTimeouts: 31 (navigation_task.py:268)
[36m[2025-07-01 23:50:20,961][166323] Fps is (10 sec: 1638.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3440640. Throughput: 0: 261.8. Samples: 3441792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:50:20,961][166323] Avg episode reward: [(0, '1017.726')]
[36m[2025-07-01 23:50:26,008][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 3440640. Throughput: 0: 258.7. Samples: 3443360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:50:26,008][166323] Avg episode reward: [(0, '1021.365')]
[36m[2025-07-01 23:50:30,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3440640. Throughput: 0: 261.2. Samples: 3444976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:50:30,971][166323] Avg episode reward: [(0, '1043.757')]
[36m[2025-07-01 23:50:35,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3440640. Throughput: 0: 262.5. Samples: 3445744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:50:35,974][166323] Avg episode reward: [(0, '1030.623')]
[36m[2025-07-01 23:50:40,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3440640. Throughput: 0: 265.7. Samples: 3447360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:50:40,985][166323] Avg episode reward: [(0, '983.035')]
[36m[2025-07-01 23:50:45,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3440640. Throughput: 0: 271.1. Samples: 3449184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:50:45,958][166323] Avg episode reward: [(0, '1028.232')]
[36m[2025-07-01 23:50:50,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3440640. Throughput: 0: 272.6. Samples: 3450016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:50:50,973][166323] Avg episode reward: [(0, '996.142')]
[36m[2025-07-01 23:50:55,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3440640. Throughput: 0: 270.0. Samples: 3451520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:50:56,000][166323] Avg episode reward: [(0, '991.964')]
[36m[2025-07-01 23:51:01,002][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 3440640. Throughput: 0: 267.9. Samples: 3453040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:51:01,002][166323] Avg episode reward: [(0, '991.916')]
[36m[2025-07-01 23:51:05,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.2). Total num frames: 3440640. Throughput: 0: 267.6. Samples: 3453840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:51:05,991][166323] Avg episode reward: [(0, '967.301')]
[36m[2025-07-01 23:51:10,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 3440640. Throughput: 0: 270.9. Samples: 3455536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:51:10,957][166323] Avg episode reward: [(0, '1003.723')]
[36m[2025-07-01 23:51:15,968][166323] Fps is (10 sec: 1642.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3457024. Throughput: 0: 271.0. Samples: 3457168. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:51:15,968][166323] Avg episode reward: [(0, '1083.249')]
[36m[2025-07-01 23:51:20,947][166323] Fps is (10 sec: 1640.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3457024. Throughput: 0: 270.0. Samples: 3457888. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:51:20,947][166323] Avg episode reward: [(0, '1049.596')]
[36m[2025-07-01 23:51:25,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3457024. Throughput: 0: 269.2. Samples: 3459472. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:51:25,972][166323] Avg episode reward: [(0, '1073.390')]
[36m[2025-07-01 23:51:30,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3457024. Throughput: 0: 263.7. Samples: 3461056. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:51:30,979][166323] Avg episode reward: [(0, '1041.775')]
[31m[11882428 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11882428 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[11882428 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:51:35,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3457024. Throughput: 0: 263.8. Samples: 3461888. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:51:35,977][166323] Avg episode reward: [(0, '1063.314')]
[37m[1m[2025-07-01 23:51:36,028][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000006744_3457024.pth...
[36m[2025-07-01 23:51:36,032][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000006616_3391488.pth
[36m[2025-07-01 23:51:40,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3457024. Throughput: 0: 271.2. Samples: 3463712. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:51:40,960][166323] Avg episode reward: [(0, '1061.923')]
[31m[11894133 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11894133 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[11894133 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:51:45,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3457024. Throughput: 0: 274.7. Samples: 3465392. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:51:45,966][166323] Avg episode reward: [(0, '1071.924')]
[31m[11899346 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11899346 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[11899346 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:51:50,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3457024. Throughput: 0: 275.9. Samples: 3466256. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:51:50,990][166323] Avg episode reward: [(0, '1045.227')]
[36m[2025-07-01 23:51:56,014][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 3457024. Throughput: 0: 275.9. Samples: 3467968. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:51:56,014][166323] Avg episode reward: [(0, '1060.621')]
[36m[2025-07-01 23:52:00,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 222.2). Total num frames: 3457024. Throughput: 0: 277.8. Samples: 3469664. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:52:00,954][166323] Avg episode reward: [(0, '1077.748')]
[36m[2025-07-01 23:52:05,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 3457024. Throughput: 0: 279.4. Samples: 3470464. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:52:05,954][166323] Avg episode reward: [(0, '1136.920')]
[31m[11916820 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11916821 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[11916821 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:52:10,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 3457024. Throughput: 0: 280.9. Samples: 3472112. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-01 23:52:10,976][166323] Avg episode reward: [(0, '1172.523')]
[36m[2025-07-01 23:52:15,976][166323] Fps is (10 sec: 1634.8, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3473408. Throughput: 0: 282.3. Samples: 3473760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:52:15,976][166323] Avg episode reward: [(0, '1187.542')]
[36m[2025-07-01 23:52:21,017][166323] Fps is (10 sec: 1631.7, 60 sec: 272.7, 300 sec: 277.7). Total num frames: 3473408. Throughput: 0: 282.8. Samples: 3474624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:52:21,017][166323] Avg episode reward: [(0, '1217.921')]
[36m[2025-07-01 23:52:25,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3473408. Throughput: 0: 277.0. Samples: 3476176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:52:25,963][166323] Avg episode reward: [(0, '1160.055')]
[36m[2025-07-01 23:52:30,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3473408. Throughput: 0: 277.7. Samples: 3477888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:52:30,957][166323] Avg episode reward: [(0, '1047.995')]
[36m[2025-07-01 23:52:35,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3473408. Throughput: 0: 279.2. Samples: 3478816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:52:35,981][166323] Avg episode reward: [(0, '1044.688')]
[36m[2025-07-01 23:52:40,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 3473408. Throughput: 0: 279.5. Samples: 3480528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:52:40,949][166323] Avg episode reward: [(0, '1000.112')]
[36m[2025-07-01 23:52:45,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3473408. Throughput: 0: 279.7. Samples: 3482256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:52:45,973][166323] Avg episode reward: [(0, '986.487')]
[36m[2025-07-01 23:52:50,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3473408. Throughput: 0: 279.8. Samples: 3483056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:52:50,957][166323] Avg episode reward: [(0, '943.125')]
[36m[2025-07-01 23:52:55,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3473408. Throughput: 0: 278.1. Samples: 3484624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:52:55,963][166323] Avg episode reward: [(0, '1016.257')]
[36m[2025-07-01 23:53:00,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3473408. Throughput: 0: 278.1. Samples: 3486272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:53:00,962][166323] Avg episode reward: [(0, '1073.139')]
[31m[11971267 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11971268 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[11971268 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:53:05,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3473408. Throughput: 0: 278.4. Samples: 3487136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:53:05,954][166323] Avg episode reward: [(0, '1135.166')]
[36m[2025-07-01 23:53:10,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 3473408. Throughput: 0: 280.9. Samples: 3488816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:53:10,962][166323] Avg episode reward: [(0, '1117.078')]
[36m[2025-07-01 23:53:15,972][166323] Fps is (10 sec: 1635.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3489792. Throughput: 0: 278.3. Samples: 3490416. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 23:53:15,982][166323] Avg episode reward: [(0, '1118.274')]
[36m[2025-07-01 23:53:20,987][166323] Fps is (10 sec: 1634.3, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3489792. Throughput: 0: 275.2. Samples: 3491200. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 23:53:20,988][166323] Avg episode reward: [(0, '1090.093')]
[31m[11993797 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11993798 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[11993798 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:53:25,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3489792. Throughput: 0: 273.9. Samples: 3492864. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 23:53:25,990][166323] Avg episode reward: [(0, '1105.761')]
[36m[2025-07-01 23:53:30,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3489792. Throughput: 0: 273.1. Samples: 3494544. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 23:53:30,960][166323] Avg episode reward: [(0, '1085.572')]
[31m[12000371 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12000372 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[12000373 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:53:35,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3489792. Throughput: 0: 272.6. Samples: 3495328. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 23:53:35,968][166323] Avg episode reward: [(0, '1086.877')]
[37m[1m[2025-07-01 23:53:36,020][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000006808_3489792.pth...
[36m[2025-07-01 23:53:36,024][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000006680_3424256.pth
[36m[2025-07-01 23:53:40,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3489792. Throughput: 0: 275.2. Samples: 3497008. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 23:53:40,956][166323] Avg episode reward: [(0, '1048.117')]
[36m[2025-07-01 23:53:45,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3489792. Throughput: 0: 276.6. Samples: 3498720. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 23:53:45,974][166323] Avg episode reward: [(0, '1012.711')]
[36m[2025-07-01 23:53:50,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3489792. Throughput: 0: 274.5. Samples: 3499488. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 23:53:50,945][166323] Avg episode reward: [(0, '1073.874')]
[36m[2025-07-01 23:53:55,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3489792. Throughput: 0: 274.4. Samples: 3501168. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 23:53:55,985][166323] Avg episode reward: [(0, '1038.731')]
[36m[2025-07-01 23:54:00,997][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3489792. Throughput: 0: 276.5. Samples: 3502864. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 23:54:00,997][166323] Avg episode reward: [(0, '1035.110')]
[36m[2025-07-01 23:54:05,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3489792. Throughput: 0: 277.8. Samples: 3503696. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 23:54:05,969][166323] Avg episode reward: [(0, '989.362')]
[31m[12036966 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12036967 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[12036967 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:54:10,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 3489792. Throughput: 0: 281.7. Samples: 3505536. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 23:54:10,971][166323] Avg episode reward: [(0, '1032.341')]
[36m[2025-07-01 23:54:15,949][166323] Fps is (10 sec: 1641.7, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3506176. Throughput: 0: 281.3. Samples: 3507200. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 23:54:15,949][166323] Avg episode reward: [(0, '1034.254')]
[36m[2025-07-01 23:54:20,965][166323] Fps is (10 sec: 1639.3, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3506176. Throughput: 0: 282.7. Samples: 3508048. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 23:54:20,965][166323] Avg episode reward: [(0, '1041.122')]
[36m[2025-07-01 23:54:26,005][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 3506176. Throughput: 0: 283.8. Samples: 3509792. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 23:54:26,006][166323] Avg episode reward: [(0, '991.825')]
[36m[2025-07-01 23:54:30,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3506176. Throughput: 0: 282.1. Samples: 3511408. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 23:54:30,950][166323] Avg episode reward: [(0, '982.391')]
[36m[2025-07-01 23:54:36,010][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3506176. Throughput: 0: 282.3. Samples: 3512208. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 23:54:36,010][166323] Avg episode reward: [(0, '1011.162')]
[36m[2025-07-01 23:54:40,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3506176. Throughput: 0: 282.0. Samples: 3513856. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 23:54:40,978][166323] Avg episode reward: [(0, '1038.847')]
[36m[2025-07-01 23:54:45,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3506176. Throughput: 0: 282.8. Samples: 3515584. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 23:54:45,968][166323] Avg episode reward: [(0, '1077.975')]
[36m[2025-07-01 23:54:50,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3506176. Throughput: 0: 282.8. Samples: 3516416. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 23:54:50,951][166323] Avg episode reward: [(0, '1113.387')]
[36m[2025-07-01 23:54:56,003][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3506176. Throughput: 0: 281.4. Samples: 3518208. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 23:54:56,003][166323] Avg episode reward: [(0, '1115.054')]
[36m[2025-07-01 23:55:01,001][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 3506176. Throughput: 0: 279.9. Samples: 3519808. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 23:55:01,001][166323] Avg episode reward: [(0, '1170.180')]
[36m[2025-07-01 23:55:05,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3506176. Throughput: 0: 279.9. Samples: 3520640. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 23:55:05,953][166323] Avg episode reward: [(0, '1180.992')]
[31m[12095238 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12095239 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[12095239 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:55:10,990][166323] Fps is (10 sec: 1640.1, 60 sec: 546.0, 300 sec: 277.7). Total num frames: 3522560. Throughput: 0: 278.1. Samples: 3522304. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 23:55:10,990][166323] Avg episode reward: [(0, '1143.267')]
[36m[2025-07-01 23:55:15,955][166323] Fps is (10 sec: 1638.1, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3522560. Throughput: 0: 276.6. Samples: 3523856. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 23:55:15,955][166323] Avg episode reward: [(0, '1116.222')]
[36m[2025-07-01 23:55:20,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3522560. Throughput: 0: 277.3. Samples: 3524672. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 23:55:20,951][166323] Avg episode reward: [(0, '1141.666')]
[36m[2025-07-01 23:55:25,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3522560. Throughput: 0: 277.1. Samples: 3526320. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 23:55:25,957][166323] Avg episode reward: [(0, '1092.728')]
[36m[2025-07-01 23:55:30,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3522560. Throughput: 0: 277.2. Samples: 3528064. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 23:55:30,989][166323] Avg episode reward: [(0, '1079.557')]
[36m[2025-07-01 23:55:35,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3522560. Throughput: 0: 276.9. Samples: 3528880. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 23:55:35,957][166323] Avg episode reward: [(0, '1063.801')]
[37m[1m[2025-07-01 23:55:36,014][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000006872_3522560.pth...
[36m[2025-07-01 23:55:36,018][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000006744_3457024.pth
[36m[2025-07-01 23:55:40,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3522560. Throughput: 0: 272.1. Samples: 3530448. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 23:55:40,981][166323] Avg episode reward: [(0, '1078.430')]
[31m[12130036 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12130036 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[12130036 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:55:45,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3522560. Throughput: 0: 274.4. Samples: 3532144. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 23:55:45,953][166323] Avg episode reward: [(0, '1074.151')]
[36m[2025-07-01 23:55:50,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3522560. Throughput: 0: 273.7. Samples: 3532960. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 23:55:50,965][166323] Avg episode reward: [(0, '1023.672')]
[36m[2025-07-01 23:55:55,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3522560. Throughput: 0: 273.8. Samples: 3534624. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 23:55:55,979][166323] Avg episode reward: [(0, '1058.733')]
[36m[2025-07-01 23:56:00,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3522560. Throughput: 0: 275.2. Samples: 3536240. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 23:56:00,954][166323] Avg episode reward: [(0, '1067.070')]
[36m[2025-07-01 23:56:05,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3522560. Throughput: 0: 275.3. Samples: 3537072. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-01 23:56:05,988][166323] Avg episode reward: [(0, '1085.361')]
[36m[2025-07-01 23:56:10,949][166323] Fps is (10 sec: 1639.2, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3538944. Throughput: 0: 278.8. Samples: 3538864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:56:10,949][166323] Avg episode reward: [(0, '1106.869')]
[36m[2025-07-01 23:56:15,988][166323] Fps is (10 sec: 1638.4, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3538944. Throughput: 0: 277.7. Samples: 3540560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:56:15,988][166323] Avg episode reward: [(0, '1090.685')]
[36m[2025-07-01 23:56:20,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3538944. Throughput: 0: 279.5. Samples: 3541456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:56:20,946][166323] Avg episode reward: [(0, '1138.774')]
[36m[2025-07-01 23:56:25,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3538944. Throughput: 0: 279.2. Samples: 3543008. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:56:25,963][166323] Avg episode reward: [(0, '1096.966')]
[36m[2025-07-01 23:56:31,014][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3538944. Throughput: 0: 278.0. Samples: 3544672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:56:31,015][166323] Avg episode reward: [(0, '1067.613')]
[36m[2025-07-01 23:56:35,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3538944. Throughput: 0: 279.7. Samples: 3545552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:56:35,988][166323] Avg episode reward: [(0, '1068.241')]
[36m[2025-07-01 23:56:40,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3538944. Throughput: 0: 277.9. Samples: 3547136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:56:40,994][166323] Avg episode reward: [(0, '983.931')]
[36m[2025-07-01 23:56:45,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3538944. Throughput: 0: 277.1. Samples: 3548720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:56:45,989][166323] Avg episode reward: [(0, '955.154')]
[36m[2025-07-01 23:56:50,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3538944. Throughput: 0: 278.9. Samples: 3549616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:56:50,962][166323] Avg episode reward: [(0, '1026.617')]
[36m[2025-07-01 23:56:55,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3538944. Throughput: 0: 273.6. Samples: 3551184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:56:55,975][166323] Avg episode reward: [(0, '979.484')]
[36m[2025-07-01 23:57:01,027][166323] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 277.6). Total num frames: 3538944. Throughput: 0: 274.3. Samples: 3552912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:57:01,028][166323] Avg episode reward: [(0, '970.086')]
[36m[2025-07-01 23:57:05,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3538944. Throughput: 0: 272.3. Samples: 3553712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:57:05,961][166323] Avg episode reward: [(0, '991.728')]
[36m[2025-07-01 23:57:10,983][166323] Fps is (10 sec: 1645.7, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3555328. Throughput: 0: 274.4. Samples: 3555360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:57:10,983][166323] Avg episode reward: [(0, '1021.640')]
[36m[2025-07-01 23:57:15,947][166323] Fps is (10 sec: 1640.6, 60 sec: 273.3, 300 sec: 277.8). Total num frames: 3555328. Throughput: 0: 274.5. Samples: 3557008. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:57:15,947][166323] Avg episode reward: [(0, '1121.760')]
[36m[2025-07-01 23:57:20,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3555328. Throughput: 0: 272.3. Samples: 3557808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:57:20,994][166323] Avg episode reward: [(0, '1139.437')]
[36m[2025-07-01 23:57:25,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3555328. Throughput: 0: 273.2. Samples: 3559424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:57:25,974][166323] Avg episode reward: [(0, '1181.529')]
[36m[2025-07-01 23:57:30,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3555328. Throughput: 0: 274.7. Samples: 3561072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:57:30,961][166323] Avg episode reward: [(0, '1208.523')]
[36m[2025-07-01 23:57:35,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3555328. Throughput: 0: 273.7. Samples: 3561936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:57:35,980][166323] Avg episode reward: [(0, '1229.781')]
[37m[1m[2025-07-01 23:57:36,046][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000006936_3555328.pth...
[36m[2025-07-01 23:57:36,050][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000006808_3489792.pth
[31m[12247192 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12247192 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[12247192 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:57:40,995][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3555328. Throughput: 0: 274.0. Samples: 3563520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:57:40,995][166323] Avg episode reward: [(0, '1207.010')]
[36m[2025-07-01 23:57:45,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3555328. Throughput: 0: 270.5. Samples: 3565072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:57:45,985][166323] Avg episode reward: [(0, '1164.460')]
[36m[2025-07-01 23:57:50,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3555328. Throughput: 0: 271.8. Samples: 3565952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:57:50,990][166323] Avg episode reward: [(0, '1165.048')]
[36m[2025-07-01 23:57:55,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3555328. Throughput: 0: 270.4. Samples: 3567520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:57:55,961][166323] Avg episode reward: [(0, '1128.998')]
[36m[2025-07-01 23:58:00,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3555328. Throughput: 0: 272.5. Samples: 3569280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:58:00,987][166323] Avg episode reward: [(0, '1085.446')]
[36m[2025-07-01 23:58:06,003][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3555328. Throughput: 0: 271.6. Samples: 3570032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:58:06,003][166323] Avg episode reward: [(0, '1115.457')]
[36m[2025-07-01 23:58:10,968][166323] Fps is (10 sec: 1641.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3571712. Throughput: 0: 273.1. Samples: 3571712. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:58:10,968][166323] Avg episode reward: [(0, '1154.424')]
[36m[2025-07-01 23:58:15,988][166323] Fps is (10 sec: 1640.9, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3571712. Throughput: 0: 270.8. Samples: 3573264. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:58:15,988][166323] Avg episode reward: [(0, '1115.548')]
[36m[2025-07-01 23:58:20,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3571712. Throughput: 0: 269.8. Samples: 3574080. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:58:20,992][166323] Avg episode reward: [(0, '1148.839')]
[36m[2025-07-01 23:58:25,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3571712. Throughput: 0: 271.5. Samples: 3575728. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:58:25,963][166323] Avg episode reward: [(0, '1149.537')]
[36m[2025-07-01 23:58:30,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3571712. Throughput: 0: 274.3. Samples: 3577408. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:58:30,964][166323] Avg episode reward: [(0, '1114.061')]
[36m[2025-07-01 23:58:35,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3571712. Throughput: 0: 274.3. Samples: 3578288. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:58:35,965][166323] Avg episode reward: [(0, '1135.951')]
[36m[2025-07-01 23:58:40,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3571712. Throughput: 0: 275.9. Samples: 3579936. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:58:40,963][166323] Avg episode reward: [(0, '1095.728')]
[36m[2025-07-01 23:58:45,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3571712. Throughput: 0: 275.4. Samples: 3581664. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:58:45,956][166323] Avg episode reward: [(0, '1063.384')]
[36m[2025-07-01 23:58:50,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3571712. Throughput: 0: 279.2. Samples: 3582592. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:58:50,992][166323] Avg episode reward: [(0, '1138.356')]
[36m[2025-07-01 23:58:55,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3571712. Throughput: 0: 277.5. Samples: 3584208. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:58:55,993][166323] Avg episode reward: [(0, '1118.546')]
[36m[2025-07-01 23:59:00,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3571712. Throughput: 0: 277.5. Samples: 3585744. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:59:00,964][166323] Avg episode reward: [(0, '1107.719')]
[31m[12332588 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12332589 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[12332589 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 23:59:05,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3571712. Throughput: 0: 276.4. Samples: 3586512. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-01 23:59:05,974][166323] Avg episode reward: [(0, '1088.633')]
[36m[2025-07-01 23:59:10,952][166323] Fps is (10 sec: 1640.3, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3588096. Throughput: 0: 275.6. Samples: 3588128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:59:10,953][166323] Avg episode reward: [(0, '1076.235')]
[36m[2025-07-01 23:59:15,965][166323] Fps is (10 sec: 1639.9, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3588096. Throughput: 0: 276.3. Samples: 3589840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:59:15,965][166323] Avg episode reward: [(0, '1072.376')]
[36m[2025-07-01 23:59:20,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3588096. Throughput: 0: 274.5. Samples: 3590640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:59:20,957][166323] Avg episode reward: [(0, '1105.365')]
[36m[2025-07-01 23:59:26,002][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 3588096. Throughput: 0: 272.8. Samples: 3592224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:59:26,003][166323] Avg episode reward: [(0, '1101.024')]
[36m[2025-07-01 23:59:31,006][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3588096. Throughput: 0: 269.6. Samples: 3593808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:59:31,007][166323] Avg episode reward: [(0, '1099.940')]
[36m[2025-07-01 23:59:36,003][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3588096. Throughput: 0: 265.9. Samples: 3594560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:59:36,004][166323] Avg episode reward: [(0, '1101.448')]
[37m[1m[2025-07-01 23:59:36,076][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007000_3588096.pth...
[36m[2025-07-01 23:59:36,083][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000006872_3522560.pth
[36m[2025-07-01 23:59:40,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3588096. Throughput: 0: 267.3. Samples: 3596224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:59:40,945][166323] Avg episode reward: [(0, '1128.266')]
[36m[2025-07-01 23:59:45,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3588096. Throughput: 0: 271.3. Samples: 3597952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:59:45,967][166323] Avg episode reward: [(0, '1109.563')]
[36m[2025-07-01 23:59:50,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3588096. Throughput: 0: 273.6. Samples: 3598816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:59:50,949][166323] Avg episode reward: [(0, '1118.290')]
[36m[2025-07-01 23:59:55,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3588096. Throughput: 0: 272.2. Samples: 3600384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-01 23:59:55,974][166323] Avg episode reward: [(0, '1112.226')]
[31m[12386070 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12386070 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[12386070 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:00:00,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3588096. Throughput: 0: 273.9. Samples: 3602160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:00:00,947][166323] Avg episode reward: [(0, '1081.835')]
[36m[2025-07-02 00:00:05,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 3588096. Throughput: 0: 276.3. Samples: 3603072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:00:05,946][166323] Avg episode reward: [(0, '1041.606')]
[36m[2025-07-02 00:00:10,963][166323] Fps is (10 sec: 1635.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3604480. Throughput: 0: 275.8. Samples: 3604624. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:00:10,964][166323] Avg episode reward: [(0, '1039.567')]
[36m[2025-07-02 00:00:16,024][166323] Fps is (10 sec: 1625.7, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 3604480. Throughput: 0: 278.6. Samples: 3606352. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:00:16,025][166323] Avg episode reward: [(0, '1043.731')]
[36m[2025-07-02 00:00:20,997][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3604480. Throughput: 0: 279.2. Samples: 3607120. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:00:20,997][166323] Avg episode reward: [(0, '1042.848')]
[36m[2025-07-02 00:00:25,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3604480. Throughput: 0: 277.7. Samples: 3608720. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:00:25,945][166323] Avg episode reward: [(0, '1044.871')]
[36m[2025-07-02 00:00:30,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3604480. Throughput: 0: 274.2. Samples: 3610288. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:00:30,950][166323] Avg episode reward: [(0, '972.315')]
[36m[2025-07-02 00:00:36,002][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3604480. Throughput: 0: 272.4. Samples: 3611088. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:00:36,002][166323] Avg episode reward: [(0, '989.944')]
[36m[2025-07-02 00:00:40,998][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 3604480. Throughput: 0: 273.3. Samples: 3612688. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:00:40,998][166323] Avg episode reward: [(0, '987.850')]
[31m[12429644 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12429644 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[12429644 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:00:45,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3604480. Throughput: 0: 270.8. Samples: 3614352. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:00:45,973][166323] Avg episode reward: [(0, '952.369')]
[36m[2025-07-02 00:00:50,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3604480. Throughput: 0: 267.7. Samples: 3615120. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:00:50,945][166323] Avg episode reward: [(0, '949.727')]
[36m[2025-07-02 00:00:56,013][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 3604480. Throughput: 0: 272.1. Samples: 3616880. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:00:56,013][166323] Avg episode reward: [(0, '962.325')]
[36m[2025-07-02 00:01:00,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3604480. Throughput: 0: 271.0. Samples: 3618528. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:01:00,958][166323] Avg episode reward: [(0, '1028.145')]
[36m[2025-07-02 00:01:05,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 3604480. Throughput: 0: 271.9. Samples: 3619344. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:01:05,959][166323] Avg episode reward: [(0, '1005.313')]
[36m[2025-07-02 00:01:10,959][166323] Fps is (10 sec: 1638.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3620864. Throughput: 0: 274.0. Samples: 3621056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:01:10,959][166323] Avg episode reward: [(0, '1036.661')]
[36m[2025-07-02 00:01:15,964][166323] Fps is (10 sec: 1637.5, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3620864. Throughput: 0: 276.9. Samples: 3622752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:01:15,965][166323] Avg episode reward: [(0, '1131.247')]
[36m[2025-07-02 00:01:20,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3620864. Throughput: 0: 279.3. Samples: 3623648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:01:20,971][166323] Avg episode reward: [(0, '1139.973')]
[36m[2025-07-02 00:01:25,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 3620864. Throughput: 0: 282.3. Samples: 3625392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:01:25,993][166323] Avg episode reward: [(0, '1123.534')]
[36m[2025-07-02 00:01:30,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3620864. Throughput: 0: 283.9. Samples: 3627120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:01:30,954][166323] Avg episode reward: [(0, '1118.222')]
[36m[2025-07-02 00:01:35,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3620864. Throughput: 0: 284.8. Samples: 3627936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:01:35,953][166323] Avg episode reward: [(0, '1130.750')]
[37m[1m[2025-07-02 00:01:36,039][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007064_3620864.pth...
[36m[2025-07-02 00:01:36,046][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000006936_3555328.pth
[36m[2025-07-02 00:01:40,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3620864. Throughput: 0: 285.2. Samples: 3629696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:01:40,956][166323] Avg episode reward: [(0, '1109.062')]
[33m[12490892 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[12490892 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.8404099345207214
[33mCrash Rate: 0.14543679356575012
[33mTimeout Rate: 0.014153245836496353 (navigation_task.py:265)
[33m[12490892 ms][navigation_task] - WARNING : 
[33mSuccesses: 1722
[33mCrashes : 298
[33mTimeouts: 29 (navigation_task.py:268)
[36m[2025-07-02 00:01:45,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3620864. Throughput: 0: 285.7. Samples: 3631392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:01:45,980][166323] Avg episode reward: [(0, '1059.739')]
[36m[2025-07-02 00:01:50,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3620864. Throughput: 0: 287.3. Samples: 3632272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:01:50,958][166323] Avg episode reward: [(0, '1035.128')]
[36m[2025-07-02 00:01:55,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.8). Total num frames: 3620864. Throughput: 0: 284.8. Samples: 3633872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:01:55,958][166323] Avg episode reward: [(0, '1022.032')]
[36m[2025-07-02 00:02:00,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3620864. Throughput: 0: 285.1. Samples: 3635584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:02:00,981][166323] Avg episode reward: [(0, '1030.656')]
[36m[2025-07-02 00:02:05,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 3620864. Throughput: 0: 284.3. Samples: 3636448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:02:05,993][166323] Avg episode reward: [(0, '963.512')]
[36m[2025-07-02 00:02:10,953][166323] Fps is (10 sec: 1642.9, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3637248. Throughput: 0: 283.3. Samples: 3638128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:02:10,953][166323] Avg episode reward: [(0, '963.999')]
[36m[2025-07-02 00:02:15,985][166323] Fps is (10 sec: 1639.7, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3637248. Throughput: 0: 283.2. Samples: 3639872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:02:15,985][166323] Avg episode reward: [(0, '963.686')]
[36m[2025-07-02 00:02:20,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3637248. Throughput: 0: 283.6. Samples: 3640704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:02:20,970][166323] Avg episode reward: [(0, '980.007')]
[36m[2025-07-02 00:02:25,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3637248. Throughput: 0: 278.9. Samples: 3642256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:02:25,991][166323] Avg episode reward: [(0, '980.613')]
[36m[2025-07-02 00:02:30,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3637248. Throughput: 0: 280.5. Samples: 3644016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:02:30,992][166323] Avg episode reward: [(0, '960.478')]
[36m[2025-07-02 00:02:36,002][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 3637248. Throughput: 0: 277.8. Samples: 3644784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:02:36,003][166323] Avg episode reward: [(0, '983.363')]
[36m[2025-07-02 00:02:40,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3637248. Throughput: 0: 280.1. Samples: 3646480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:02:40,963][166323] Avg episode reward: [(0, '997.159')]
[31m[12552482 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12552482 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[12552483 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:02:45,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3637248. Throughput: 0: 278.8. Samples: 3648128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:02:45,972][166323] Avg episode reward: [(0, '1020.509')]
[36m[2025-07-02 00:02:51,007][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 3637248. Throughput: 0: 280.1. Samples: 3649056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:02:51,008][166323] Avg episode reward: [(0, '1075.766')]
[31m[12561078 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12561079 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[12561079 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:02:55,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3637248. Throughput: 0: 280.6. Samples: 3650752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:02:55,948][166323] Avg episode reward: [(0, '1033.557')]
[36m[2025-07-02 00:03:00,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3637248. Throughput: 0: 277.4. Samples: 3652352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:03:00,975][166323] Avg episode reward: [(0, '1061.424')]
[36m[2025-07-02 00:03:06,007][166323] Fps is (10 sec: 1628.8, 60 sec: 546.0, 300 sec: 277.7). Total num frames: 3653632. Throughput: 0: 277.5. Samples: 3653200. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 00:03:06,007][166323] Avg episode reward: [(0, '1069.836')]
[36m[2025-07-02 00:03:10,978][166323] Fps is (10 sec: 1638.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3653632. Throughput: 0: 280.6. Samples: 3654880. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 00:03:10,978][166323] Avg episode reward: [(0, '1088.368')]
[36m[2025-07-02 00:03:15,995][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3653632. Throughput: 0: 276.2. Samples: 3656448. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 00:03:15,996][166323] Avg episode reward: [(0, '1097.347')]
[36m[2025-07-02 00:03:20,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3653632. Throughput: 0: 277.6. Samples: 3657264. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 00:03:20,961][166323] Avg episode reward: [(0, '1088.508')]
[36m[2025-07-02 00:03:26,013][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 3653632. Throughput: 0: 276.3. Samples: 3658928. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 00:03:26,014][166323] Avg episode reward: [(0, '1053.686')]
[36m[2025-07-02 00:03:31,002][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3653632. Throughput: 0: 275.7. Samples: 3660544. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 00:03:31,003][166323] Avg episode reward: [(0, '1049.433')]
[36m[2025-07-02 00:03:35,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3653632. Throughput: 0: 275.1. Samples: 3661424. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 00:03:35,970][166323] Avg episode reward: [(0, '956.492')]
[37m[1m[2025-07-02 00:03:36,031][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007128_3653632.pth...
[36m[2025-07-02 00:03:36,035][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007000_3588096.pth
[36m[2025-07-02 00:03:40,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3653632. Throughput: 0: 272.2. Samples: 3663008. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 00:03:40,974][166323] Avg episode reward: [(0, '920.435')]
[36m[2025-07-02 00:03:45,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3653632. Throughput: 0: 275.3. Samples: 3664736. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 00:03:45,958][166323] Avg episode reward: [(0, '901.443')]
[36m[2025-07-02 00:03:50,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3653632. Throughput: 0: 274.9. Samples: 3665568. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 00:03:50,993][166323] Avg episode reward: [(0, '909.231')]
[36m[2025-07-02 00:03:55,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3653632. Throughput: 0: 273.7. Samples: 3667200. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 00:03:55,991][166323] Avg episode reward: [(0, '971.116')]
[36m[2025-07-02 00:04:00,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3653632. Throughput: 0: 276.2. Samples: 3668864. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 00:04:00,956][166323] Avg episode reward: [(0, '930.755')]
[36m[2025-07-02 00:04:05,953][166323] Fps is (10 sec: 1644.5, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3670016. Throughput: 0: 274.5. Samples: 3669616. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 00:04:05,954][166323] Avg episode reward: [(0, '954.672')]
[36m[2025-07-02 00:04:11,000][166323] Fps is (10 sec: 1631.2, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3670016. Throughput: 0: 274.2. Samples: 3671264. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 00:04:11,000][166323] Avg episode reward: [(0, '1051.720')]
[36m[2025-07-02 00:04:15,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3670016. Throughput: 0: 276.0. Samples: 3672960. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 00:04:15,983][166323] Avg episode reward: [(0, '1071.691')]
[36m[2025-07-02 00:04:21,000][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3670016. Throughput: 0: 274.7. Samples: 3673792. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 00:04:21,001][166323] Avg episode reward: [(0, '1083.234')]
[36m[2025-07-02 00:04:25,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.8). Total num frames: 3670016. Throughput: 0: 278.9. Samples: 3675552. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 00:04:25,947][166323] Avg episode reward: [(0, '1052.762')]
[36m[2025-07-02 00:04:30,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3670016. Throughput: 0: 278.8. Samples: 3677280. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 00:04:30,950][166323] Avg episode reward: [(0, '1041.026')]
[36m[2025-07-02 00:04:35,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3670016. Throughput: 0: 278.8. Samples: 3678112. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 00:04:35,990][166323] Avg episode reward: [(0, '1085.094')]
[36m[2025-07-02 00:04:40,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3670016. Throughput: 0: 279.3. Samples: 3679760. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 00:04:40,964][166323] Avg episode reward: [(0, '1059.500')]
[36m[2025-07-02 00:04:45,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3670016. Throughput: 0: 277.6. Samples: 3681360. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 00:04:45,967][166323] Avg episode reward: [(0, '1024.047')]
[36m[2025-07-02 00:04:50,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3670016. Throughput: 0: 278.7. Samples: 3682160. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 00:04:50,966][166323] Avg episode reward: [(0, '1037.653')]
[36m[2025-07-02 00:04:55,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3670016. Throughput: 0: 278.9. Samples: 3683808. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 00:04:55,974][166323] Avg episode reward: [(0, '1107.046')]
[36m[2025-07-02 00:05:00,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3670016. Throughput: 0: 276.0. Samples: 3685376. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 00:05:00,964][166323] Avg episode reward: [(0, '1106.310')]
[36m[2025-07-02 00:05:05,951][166323] Fps is (10 sec: 1642.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3686400. Throughput: 0: 275.9. Samples: 3686192. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 00:05:05,951][166323] Avg episode reward: [(0, '1116.333')]
[36m[2025-07-02 00:05:10,957][166323] Fps is (10 sec: 1639.5, 60 sec: 273.3, 300 sec: 277.8). Total num frames: 3686400. Throughput: 0: 271.2. Samples: 3687760. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 00:05:10,957][166323] Avg episode reward: [(0, '1124.348')]
[36m[2025-07-02 00:05:15,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3686400. Throughput: 0: 268.9. Samples: 3689392. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 00:05:15,987][166323] Avg episode reward: [(0, '1124.090')]
[36m[2025-07-02 00:05:20,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3686400. Throughput: 0: 270.1. Samples: 3690256. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 00:05:20,950][166323] Avg episode reward: [(0, '1152.227')]
[36m[2025-07-02 00:05:25,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3686400. Throughput: 0: 272.0. Samples: 3692000. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 00:05:25,965][166323] Avg episode reward: [(0, '1170.968')]
[36m[2025-07-02 00:05:30,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3686400. Throughput: 0: 273.4. Samples: 3693664. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 00:05:30,975][166323] Avg episode reward: [(0, '1111.399')]
[36m[2025-07-02 00:05:35,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3686400. Throughput: 0: 276.0. Samples: 3694576. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 00:05:35,945][166323] Avg episode reward: [(0, '1125.608')]
[37m[1m[2025-07-02 00:05:35,995][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007192_3686400.pth...
[36m[2025-07-02 00:05:35,999][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007064_3620864.pth
[36m[2025-07-02 00:05:40,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3686400. Throughput: 0: 277.4. Samples: 3696288. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 00:05:40,964][166323] Avg episode reward: [(0, '1132.893')]
[36m[2025-07-02 00:05:45,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3686400. Throughput: 0: 278.8. Samples: 3697920. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 00:05:45,959][166323] Avg episode reward: [(0, '1081.775')]
[36m[2025-07-02 00:05:50,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 3686400. Throughput: 0: 278.1. Samples: 3698704. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 00:05:50,949][166323] Avg episode reward: [(0, '991.141')]
[36m[2025-07-02 00:05:55,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3686400. Throughput: 0: 281.8. Samples: 3700448. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 00:05:55,988][166323] Avg episode reward: [(0, '1031.385')]
[36m[2025-07-02 00:06:00,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3686400. Throughput: 0: 287.5. Samples: 3702320. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 00:06:00,953][166323] Avg episode reward: [(0, '1036.511')]
[36m[2025-07-02 00:06:05,976][166323] Fps is (10 sec: 1640.3, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3702784. Throughput: 0: 286.8. Samples: 3703168. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 00:06:05,976][166323] Avg episode reward: [(0, '1053.518')]
[36m[2025-07-02 00:06:10,964][166323] Fps is (10 sec: 1636.5, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3702784. Throughput: 0: 283.4. Samples: 3704752. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 00:06:10,965][166323] Avg episode reward: [(0, '1032.241')]
[36m[2025-07-02 00:06:15,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3702784. Throughput: 0: 281.0. Samples: 3706304. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 00:06:15,952][166323] Avg episode reward: [(0, '1075.377')]
[36m[2025-07-02 00:06:21,000][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 3702784. Throughput: 0: 279.1. Samples: 3707152. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 00:06:21,000][166323] Avg episode reward: [(0, '1073.472')]
[36m[2025-07-02 00:06:25,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3702784. Throughput: 0: 279.3. Samples: 3708864. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 00:06:25,988][166323] Avg episode reward: [(0, '1126.566')]
[36m[2025-07-02 00:06:30,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3702784. Throughput: 0: 280.4. Samples: 3710544. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 00:06:30,979][166323] Avg episode reward: [(0, '1086.760')]
[36m[2025-07-02 00:06:35,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3702784. Throughput: 0: 280.5. Samples: 3711328. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 00:06:35,949][166323] Avg episode reward: [(0, '1092.838')]
[36m[2025-07-02 00:06:40,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3702784. Throughput: 0: 281.7. Samples: 3713120. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 00:06:40,977][166323] Avg episode reward: [(0, '1067.238')]
[31m[12792874 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12792874 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[12792874 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:06:45,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3702784. Throughput: 0: 276.2. Samples: 3714752. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 00:06:45,963][166323] Avg episode reward: [(0, '1021.435')]
[31m[12794556 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12794556 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[12794557 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:06:50,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3702784. Throughput: 0: 274.3. Samples: 3715504. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 00:06:50,950][166323] Avg episode reward: [(0, '945.099')]
[36m[2025-07-02 00:06:55,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3702784. Throughput: 0: 274.0. Samples: 3717088. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 00:06:55,985][166323] Avg episode reward: [(0, '939.200')]
[36m[2025-07-02 00:07:00,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3702784. Throughput: 0: 278.1. Samples: 3718816. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 00:07:00,950][166323] Avg episode reward: [(0, '891.654')]
[36m[2025-07-02 00:07:05,967][166323] Fps is (10 sec: 1641.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3719168. Throughput: 0: 276.1. Samples: 3719568. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 00:07:05,967][166323] Avg episode reward: [(0, '917.509')]
[36m[2025-07-02 00:07:10,977][166323] Fps is (10 sec: 1634.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3719168. Throughput: 0: 276.0. Samples: 3721280. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 00:07:10,977][166323] Avg episode reward: [(0, '881.923')]
[36m[2025-07-02 00:07:15,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3719168. Throughput: 0: 274.1. Samples: 3722880. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 00:07:15,991][166323] Avg episode reward: [(0, '897.220')]
[36m[2025-07-02 00:07:20,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3719168. Throughput: 0: 275.0. Samples: 3723712. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 00:07:20,980][166323] Avg episode reward: [(0, '974.456')]
[36m[2025-07-02 00:07:25,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3719168. Throughput: 0: 271.0. Samples: 3725312. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 00:07:25,963][166323] Avg episode reward: [(0, '1040.787')]
[36m[2025-07-02 00:07:30,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3719168. Throughput: 0: 271.8. Samples: 3726992. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 00:07:30,991][166323] Avg episode reward: [(0, '1017.875')]
[36m[2025-07-02 00:07:35,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3719168. Throughput: 0: 275.8. Samples: 3727920. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 00:07:35,973][166323] Avg episode reward: [(0, '1052.831')]
[37m[1m[2025-07-02 00:07:36,025][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007256_3719168.pth...
[36m[2025-07-02 00:07:36,032][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007128_3653632.pth
[36m[2025-07-02 00:07:40,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3719168. Throughput: 0: 275.1. Samples: 3729472. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 00:07:40,994][166323] Avg episode reward: [(0, '1047.251')]
[36m[2025-07-02 00:07:45,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3719168. Throughput: 0: 273.2. Samples: 3731120. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 00:07:45,989][166323] Avg episode reward: [(0, '1065.142')]
[36m[2025-07-02 00:07:50,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3719168. Throughput: 0: 276.3. Samples: 3732000. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 00:07:50,968][166323] Avg episode reward: [(0, '1024.229')]
[36m[2025-07-02 00:07:55,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3719168. Throughput: 0: 274.2. Samples: 3733616. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 00:07:55,963][166323] Avg episode reward: [(0, '1026.460')]
[36m[2025-07-02 00:08:00,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 3719168. Throughput: 0: 273.2. Samples: 3735168. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 00:08:00,965][166323] Avg episode reward: [(0, '1058.432')]
[36m[2025-07-02 00:08:05,945][166323] Fps is (10 sec: 1641.3, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3735552. Throughput: 0: 274.3. Samples: 3736048. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 00:08:05,945][166323] Avg episode reward: [(0, '1062.505')]
[36m[2025-07-02 00:08:10,961][166323] Fps is (10 sec: 1639.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3735552. Throughput: 0: 279.1. Samples: 3737872. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 00:08:10,961][166323] Avg episode reward: [(0, '984.530')]
[36m[2025-07-02 00:08:15,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3735552. Throughput: 0: 278.2. Samples: 3739504. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 00:08:15,970][166323] Avg episode reward: [(0, '1006.735')]
[36m[2025-07-02 00:08:21,000][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3735552. Throughput: 0: 276.5. Samples: 3740368. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 00:08:21,000][166323] Avg episode reward: [(0, '1031.884')]
[36m[2025-07-02 00:08:25,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3735552. Throughput: 0: 276.4. Samples: 3741904. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 00:08:25,976][166323] Avg episode reward: [(0, '1010.160')]
[36m[2025-07-02 00:08:30,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3735552. Throughput: 0: 275.3. Samples: 3743504. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 00:08:30,974][166323] Avg episode reward: [(0, '1010.756')]
[36m[2025-07-02 00:08:35,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3735552. Throughput: 0: 274.6. Samples: 3744352. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 00:08:35,953][166323] Avg episode reward: [(0, '1031.852')]
[36m[2025-07-02 00:08:40,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3735552. Throughput: 0: 276.1. Samples: 3746048. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 00:08:40,991][166323] Avg episode reward: [(0, '1049.136')]
[36m[2025-07-02 00:08:45,997][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3735552. Throughput: 0: 276.8. Samples: 3747632. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 00:08:45,997][166323] Avg episode reward: [(0, '1093.750')]
[36m[2025-07-02 00:08:50,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3735552. Throughput: 0: 275.3. Samples: 3748448. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 00:08:50,987][166323] Avg episode reward: [(0, '1060.809')]
[36m[2025-07-02 00:08:56,009][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 3735552. Throughput: 0: 271.4. Samples: 3750096. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 00:08:56,009][166323] Avg episode reward: [(0, '1070.362')]
[36m[2025-07-02 00:09:00,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 3735552. Throughput: 0: 271.2. Samples: 3751712. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 00:09:00,987][166323] Avg episode reward: [(0, '1106.792')]
[36m[2025-07-02 00:09:05,983][166323] Fps is (10 sec: 1642.6, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3751936. Throughput: 0: 270.0. Samples: 3752512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:09:05,983][166323] Avg episode reward: [(0, '1085.558')]
[36m[2025-07-02 00:09:10,982][166323] Fps is (10 sec: 1639.1, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3751936. Throughput: 0: 271.2. Samples: 3754112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:09:10,982][166323] Avg episode reward: [(0, '1079.908')]
[36m[2025-07-02 00:09:15,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3751936. Throughput: 0: 276.3. Samples: 3755936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:09:15,969][166323] Avg episode reward: [(0, '1073.996')]
[36m[2025-07-02 00:09:20,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3751936. Throughput: 0: 276.1. Samples: 3756784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:09:20,977][166323] Avg episode reward: [(0, '1091.226')]
[36m[2025-07-02 00:09:25,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3751936. Throughput: 0: 275.1. Samples: 3758416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:09:25,952][166323] Avg episode reward: [(0, '1004.290')]
[36m[2025-07-02 00:09:30,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3751936. Throughput: 0: 277.1. Samples: 3760096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:09:30,976][166323] Avg episode reward: [(0, '1035.189')]
[36m[2025-07-02 00:09:36,006][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 3751936. Throughput: 0: 276.5. Samples: 3760896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:09:36,006][166323] Avg episode reward: [(0, '1032.373')]
[37m[1m[2025-07-02 00:09:36,077][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007320_3751936.pth...
[36m[2025-07-02 00:09:36,082][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007192_3686400.pth
[36m[2025-07-02 00:09:40,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3751936. Throughput: 0: 279.7. Samples: 3762672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:09:40,975][166323] Avg episode reward: [(0, '1002.726')]
[36m[2025-07-02 00:09:45,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3751936. Throughput: 0: 284.1. Samples: 3764496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:09:45,978][166323] Avg episode reward: [(0, '1011.773')]
[36m[2025-07-02 00:09:50,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3751936. Throughput: 0: 284.5. Samples: 3765312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:09:50,966][166323] Avg episode reward: [(0, '975.352')]
[36m[2025-07-02 00:09:56,011][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3751936. Throughput: 0: 288.5. Samples: 3767104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:09:56,012][166323] Avg episode reward: [(0, '1030.011')]
[36m[2025-07-02 00:10:00,945][166323] Fps is (10 sec: 1641.8, 60 sec: 546.5, 300 sec: 277.7). Total num frames: 3768320. Throughput: 0: 284.6. Samples: 3768736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:10:00,945][166323] Avg episode reward: [(0, '1098.824')]
[36m[2025-07-02 00:10:05,974][166323] Fps is (10 sec: 1644.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3768320. Throughput: 0: 286.2. Samples: 3769664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:10:05,974][166323] Avg episode reward: [(0, '1017.376')]
[36m[2025-07-02 00:10:10,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3768320. Throughput: 0: 285.1. Samples: 3771248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:10:10,960][166323] Avg episode reward: [(0, '1045.794')]
[36m[2025-07-02 00:10:15,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3768320. Throughput: 0: 287.8. Samples: 3773040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:10:15,947][166323] Avg episode reward: [(0, '952.763')]
[36m[2025-07-02 00:10:20,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3768320. Throughput: 0: 291.1. Samples: 3773984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:10:20,968][166323] Avg episode reward: [(0, '1027.912')]
[36m[2025-07-02 00:10:25,998][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3768320. Throughput: 0: 291.4. Samples: 3775792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:10:25,998][166323] Avg episode reward: [(0, '1016.895')]
[36m[2025-07-02 00:10:30,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3768320. Throughput: 0: 286.3. Samples: 3777376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:10:30,969][166323] Avg episode reward: [(0, '1030.017')]
[36m[2025-07-02 00:10:35,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3768320. Throughput: 0: 287.5. Samples: 3778256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:10:35,988][166323] Avg episode reward: [(0, '1034.539')]
[36m[2025-07-02 00:10:41,003][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3768320. Throughput: 0: 287.7. Samples: 3780048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:10:41,003][166323] Avg episode reward: [(0, '970.506')]
[31m[13030791 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13030792 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[13030792 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:10:46,013][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 3768320. Throughput: 0: 289.7. Samples: 3781792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:10:46,014][166323] Avg episode reward: [(0, '1053.388')]
[36m[2025-07-02 00:10:51,005][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3768320. Throughput: 0: 287.8. Samples: 3782624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:10:51,006][166323] Avg episode reward: [(0, '1071.764')]
[31m[13041661 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13041662 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[13041663 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:10:55,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 3768320. Throughput: 0: 288.5. Samples: 3784224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:10:55,944][166323] Avg episode reward: [(0, '1036.236')]
[36m[2025-07-02 00:11:00,949][166323] Fps is (10 sec: 1647.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3784704. Throughput: 0: 284.4. Samples: 3785840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:11:00,949][166323] Avg episode reward: [(0, '1026.058')]
[36m[2025-07-02 00:11:05,991][166323] Fps is (10 sec: 1630.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3784704. Throughput: 0: 282.9. Samples: 3786720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:11:05,992][166323] Avg episode reward: [(0, '1081.758')]
[36m[2025-07-02 00:11:10,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3784704. Throughput: 0: 283.2. Samples: 3788528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:11:10,969][166323] Avg episode reward: [(0, '1069.558')]
[36m[2025-07-02 00:11:15,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3784704. Throughput: 0: 284.0. Samples: 3790160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:11:15,987][166323] Avg episode reward: [(0, '1049.507')]
[36m[2025-07-02 00:11:20,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3784704. Throughput: 0: 282.9. Samples: 3790976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:11:20,947][166323] Avg episode reward: [(0, '1070.059')]
[36m[2025-07-02 00:11:25,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3784704. Throughput: 0: 279.8. Samples: 3792624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:11:25,947][166323] Avg episode reward: [(0, '1063.948')]
[36m[2025-07-02 00:11:30,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3784704. Throughput: 0: 276.3. Samples: 3794208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:11:30,947][166323] Avg episode reward: [(0, '1070.494')]
[36m[2025-07-02 00:11:35,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3784704. Throughput: 0: 278.6. Samples: 3795152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:11:35,967][166323] Avg episode reward: [(0, '1050.971')]
[37m[1m[2025-07-02 00:11:36,048][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007384_3784704.pth...
[36m[2025-07-02 00:11:36,056][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007256_3719168.pth
[36m[2025-07-02 00:11:40,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3784704. Throughput: 0: 279.5. Samples: 3796816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:11:40,993][166323] Avg episode reward: [(0, '1064.973')]
[36m[2025-07-02 00:11:45,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 3784704. Throughput: 0: 279.5. Samples: 3798416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:11:45,948][166323] Avg episode reward: [(0, '1107.972')]
[36m[2025-07-02 00:11:50,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3784704. Throughput: 0: 277.5. Samples: 3799200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:11:50,959][166323] Avg episode reward: [(0, '1107.825')]
[36m[2025-07-02 00:11:55,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3784704. Throughput: 0: 271.4. Samples: 3800736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:11:55,949][166323] Avg episode reward: [(0, '1086.063')]
[36m[2025-07-02 00:12:00,977][166323] Fps is (10 sec: 1635.5, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3801088. Throughput: 0: 272.4. Samples: 3802416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:12:00,977][166323] Avg episode reward: [(0, '1120.226')]
[36m[2025-07-02 00:12:05,973][166323] Fps is (10 sec: 1634.5, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3801088. Throughput: 0: 272.2. Samples: 3803232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:12:05,973][166323] Avg episode reward: [(0, '1111.626')]
[36m[2025-07-02 00:12:10,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3801088. Throughput: 0: 273.2. Samples: 3804928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:12:10,983][166323] Avg episode reward: [(0, '1095.491')]
[36m[2025-07-02 00:12:15,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3801088. Throughput: 0: 276.7. Samples: 3806672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:12:15,986][166323] Avg episode reward: [(0, '1100.293')]
[36m[2025-07-02 00:12:20,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3801088. Throughput: 0: 274.5. Samples: 3807504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:12:20,971][166323] Avg episode reward: [(0, '1016.747')]
[36m[2025-07-02 00:12:25,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3801088. Throughput: 0: 274.9. Samples: 3809184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:12:25,986][166323] Avg episode reward: [(0, '1010.572')]
[36m[2025-07-02 00:12:30,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3801088. Throughput: 0: 274.4. Samples: 3810768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:12:30,964][166323] Avg episode reward: [(0, '987.791')]
[36m[2025-07-02 00:12:35,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3801088. Throughput: 0: 276.3. Samples: 3811632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:12:35,955][166323] Avg episode reward: [(0, '1006.900')]
[36m[2025-07-02 00:12:40,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3801088. Throughput: 0: 278.7. Samples: 3813280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:12:40,964][166323] Avg episode reward: [(0, '1001.355')]
[36m[2025-07-02 00:12:46,021][166323] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 277.6). Total num frames: 3801088. Throughput: 0: 277.8. Samples: 3814928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:12:46,021][166323] Avg episode reward: [(0, '1027.338')]
[36m[2025-07-02 00:12:50,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3801088. Throughput: 0: 279.2. Samples: 3815792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:12:50,952][166323] Avg episode reward: [(0, '971.675')]
[36m[2025-07-02 00:12:55,988][166323] Fps is (10 sec: 1643.8, 60 sec: 545.8, 300 sec: 333.2). Total num frames: 3817472. Throughput: 0: 277.3. Samples: 3817408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:12:55,988][166323] Avg episode reward: [(0, '1003.404')]
[36m[2025-07-02 00:13:00,951][166323] Fps is (10 sec: 1638.4, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3817472. Throughput: 0: 275.4. Samples: 3819056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:13:00,952][166323] Avg episode reward: [(0, '1073.701')]
[33m[13173014 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[13173014 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.8330078125
[33mCrash Rate: 0.140625
[33mTimeout Rate: 0.0263671875 (navigation_task.py:265)
[33m[13173015 ms][navigation_task] - WARNING : 
[33mSuccesses: 1706
[33mCrashes : 288
[33mTimeouts: 54 (navigation_task.py:268)
[36m[2025-07-02 00:13:05,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3817472. Throughput: 0: 275.0. Samples: 3819872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:13:05,944][166323] Avg episode reward: [(0, '1069.116')]
[36m[2025-07-02 00:13:10,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3817472. Throughput: 0: 275.0. Samples: 3821552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:13:10,961][166323] Avg episode reward: [(0, '1090.236')]
[36m[2025-07-02 00:13:16,014][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3817472. Throughput: 0: 276.3. Samples: 3823216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:13:16,015][166323] Avg episode reward: [(0, '1046.741')]
[36m[2025-07-02 00:13:20,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3817472. Throughput: 0: 274.6. Samples: 3824000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:13:20,988][166323] Avg episode reward: [(0, '1045.082')]
[36m[2025-07-02 00:13:25,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3817472. Throughput: 0: 273.4. Samples: 3825584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:13:25,962][166323] Avg episode reward: [(0, '1070.125')]
[36m[2025-07-02 00:13:30,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3817472. Throughput: 0: 272.8. Samples: 3827184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:13:30,950][166323] Avg episode reward: [(0, '1098.167')]
[36m[2025-07-02 00:13:35,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3817472. Throughput: 0: 270.2. Samples: 3827952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:13:35,961][166323] Avg episode reward: [(0, '1087.862')]
[37m[1m[2025-07-02 00:13:36,017][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007448_3817472.pth...
[36m[2025-07-02 00:13:36,022][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007320_3751936.pth
[36m[2025-07-02 00:13:40,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3817472. Throughput: 0: 270.4. Samples: 3829568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:13:40,958][166323] Avg episode reward: [(0, '1119.962')]
[36m[2025-07-02 00:13:45,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3817472. Throughput: 0: 272.5. Samples: 3831328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:13:45,989][166323] Avg episode reward: [(0, '1090.169')]
[36m[2025-07-02 00:13:50,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3817472. Throughput: 0: 271.6. Samples: 3832096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:13:50,958][166323] Avg episode reward: [(0, '1116.618')]
[31m[13223730 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13223731 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[13223731 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:13:56,001][166323] Fps is (10 sec: 1636.5, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 3833856. Throughput: 0: 271.8. Samples: 3833792. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 00:13:56,001][166323] Avg episode reward: [(0, '1111.730')]
[36m[2025-07-02 00:14:00,970][166323] Fps is (10 sec: 1636.4, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3833856. Throughput: 0: 267.6. Samples: 3835248. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 00:14:00,970][166323] Avg episode reward: [(0, '1105.170')]
[36m[2025-07-02 00:14:05,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3833856. Throughput: 0: 268.3. Samples: 3836064. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 00:14:05,955][166323] Avg episode reward: [(0, '1029.420')]
[36m[2025-07-02 00:14:10,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3833856. Throughput: 0: 273.8. Samples: 3837904. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 00:14:10,959][166323] Avg episode reward: [(0, '1008.352')]
[36m[2025-07-02 00:14:15,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3833856. Throughput: 0: 276.0. Samples: 3839616. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 00:14:15,987][166323] Avg episode reward: [(0, '1032.078')]
[36m[2025-07-02 00:14:20,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3833856. Throughput: 0: 276.6. Samples: 3840400. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 00:14:20,971][166323] Avg episode reward: [(0, '1016.625')]
[31m[13250606 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13250606 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[13250606 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:14:25,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3833856. Throughput: 0: 276.5. Samples: 3842016. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 00:14:25,976][166323] Avg episode reward: [(0, '1005.577')]
[36m[2025-07-02 00:14:31,006][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 3833856. Throughput: 0: 274.0. Samples: 3843664. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 00:14:31,007][166323] Avg episode reward: [(0, '1008.309')]
[36m[2025-07-02 00:14:35,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3833856. Throughput: 0: 274.8. Samples: 3844464. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 00:14:35,971][166323] Avg episode reward: [(0, '1046.075')]
[36m[2025-07-02 00:14:40,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3833856. Throughput: 0: 277.6. Samples: 3846272. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 00:14:40,957][166323] Avg episode reward: [(0, '1020.227')]
[36m[2025-07-02 00:14:45,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3833856. Throughput: 0: 283.3. Samples: 3848000. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 00:14:45,989][166323] Avg episode reward: [(0, '1011.575')]
[36m[2025-07-02 00:14:51,007][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 3833856. Throughput: 0: 285.2. Samples: 3848912. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 00:14:51,007][166323] Avg episode reward: [(0, '1044.905')]
[36m[2025-07-02 00:14:55,948][166323] Fps is (10 sec: 1645.2, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3850240. Throughput: 0: 281.0. Samples: 3850544. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 00:14:55,948][166323] Avg episode reward: [(0, '1101.769')]
[36m[2025-07-02 00:15:00,948][166323] Fps is (10 sec: 1648.1, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3850240. Throughput: 0: 278.6. Samples: 3852144. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 00:15:00,948][166323] Avg episode reward: [(0, '1117.403')]
[36m[2025-07-02 00:15:05,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3850240. Throughput: 0: 280.9. Samples: 3853040. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 00:15:05,970][166323] Avg episode reward: [(0, '1176.494')]
[36m[2025-07-02 00:15:10,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3850240. Throughput: 0: 283.3. Samples: 3854768. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 00:15:10,983][166323] Avg episode reward: [(0, '1145.063')]
[36m[2025-07-02 00:15:15,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3850240. Throughput: 0: 284.8. Samples: 3856464. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 00:15:15,955][166323] Avg episode reward: [(0, '1148.718')]
[36m[2025-07-02 00:15:20,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3850240. Throughput: 0: 284.9. Samples: 3857280. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 00:15:20,954][166323] Avg episode reward: [(0, '1122.346')]
[36m[2025-07-02 00:15:26,011][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3850240. Throughput: 0: 280.6. Samples: 3858912. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 00:15:26,011][166323] Avg episode reward: [(0, '1105.409')]
[36m[2025-07-02 00:15:30,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3850240. Throughput: 0: 280.2. Samples: 3860608. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 00:15:30,981][166323] Avg episode reward: [(0, '1071.263')]
[36m[2025-07-02 00:15:35,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3850240. Throughput: 0: 277.5. Samples: 3861392. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 00:15:35,976][166323] Avg episode reward: [(0, '1053.610')]
[37m[1m[2025-07-02 00:15:36,041][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007512_3850240.pth...
[36m[2025-07-02 00:15:36,045][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007384_3784704.pth
[36m[2025-07-02 00:15:40,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3850240. Throughput: 0: 280.6. Samples: 3863184. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 00:15:40,987][166323] Avg episode reward: [(0, '1065.922')]
[36m[2025-07-02 00:15:45,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3850240. Throughput: 0: 284.0. Samples: 3864928. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 00:15:45,958][166323] Avg episode reward: [(0, '1054.827')]
[36m[2025-07-02 00:15:50,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3850240. Throughput: 0: 284.2. Samples: 3865824. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 00:15:50,952][166323] Avg episode reward: [(0, '1080.084')]
[36m[2025-07-02 00:15:55,982][166323] Fps is (10 sec: 1634.4, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3866624. Throughput: 0: 282.3. Samples: 3867472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:15:55,982][166323] Avg episode reward: [(0, '1042.841')]
[36m[2025-07-02 00:16:01,027][166323] Fps is (10 sec: 1626.2, 60 sec: 272.7, 300 sec: 277.7). Total num frames: 3866624. Throughput: 0: 280.1. Samples: 3869088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:16:01,028][166323] Avg episode reward: [(0, '1018.960')]
[36m[2025-07-02 00:16:05,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3866624. Throughput: 0: 281.1. Samples: 3869936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:16:05,982][166323] Avg episode reward: [(0, '998.859')]
[36m[2025-07-02 00:16:10,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3866624. Throughput: 0: 280.7. Samples: 3871536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:16:10,985][166323] Avg episode reward: [(0, '1028.460')]
[36m[2025-07-02 00:16:16,013][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 3866624. Throughput: 0: 280.3. Samples: 3873232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:16:16,013][166323] Avg episode reward: [(0, '1073.994')]
[36m[2025-07-02 00:16:20,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3866624. Throughput: 0: 282.4. Samples: 3874096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:16:20,967][166323] Avg episode reward: [(0, '1090.031')]
[36m[2025-07-02 00:16:25,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3866624. Throughput: 0: 277.3. Samples: 3875664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:16:25,991][166323] Avg episode reward: [(0, '1109.273')]
[36m[2025-07-02 00:16:31,008][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3866624. Throughput: 0: 279.2. Samples: 3877504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:16:31,008][166323] Avg episode reward: [(0, '1136.281')]
[36m[2025-07-02 00:16:35,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3866624. Throughput: 0: 277.4. Samples: 3878304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:16:35,944][166323] Avg episode reward: [(0, '1135.649')]
[31m[13384951 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13384951 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[13384951 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:16:40,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3866624. Throughput: 0: 278.9. Samples: 3880016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:16:40,966][166323] Avg episode reward: [(0, '1071.878')]
[36m[2025-07-02 00:16:45,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3866624. Throughput: 0: 279.0. Samples: 3881632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:16:45,983][166323] Avg episode reward: [(0, '1028.511')]
[36m[2025-07-02 00:16:50,958][166323] Fps is (10 sec: 1639.8, 60 sec: 546.1, 300 sec: 333.2). Total num frames: 3883008. Throughput: 0: 278.5. Samples: 3882464. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 00:16:50,958][166323] Avg episode reward: [(0, '1034.872')]
[36m[2025-07-02 00:16:55,963][166323] Fps is (10 sec: 1641.8, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3883008. Throughput: 0: 279.6. Samples: 3884112. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 00:16:55,963][166323] Avg episode reward: [(0, '990.437')]
[36m[2025-07-02 00:17:00,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 3883008. Throughput: 0: 282.0. Samples: 3885904. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 00:17:00,954][166323] Avg episode reward: [(0, '1059.632')]
[36m[2025-07-02 00:17:05,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3883008. Throughput: 0: 281.2. Samples: 3886752. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 00:17:05,972][166323] Avg episode reward: [(0, '1007.401')]
[36m[2025-07-02 00:17:11,016][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3883008. Throughput: 0: 285.0. Samples: 3888496. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 00:17:11,016][166323] Avg episode reward: [(0, '1044.688')]
[36m[2025-07-02 00:17:15,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3883008. Throughput: 0: 280.1. Samples: 3890096. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 00:17:15,960][166323] Avg episode reward: [(0, '1076.451')]
[36m[2025-07-02 00:17:20,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3883008. Throughput: 0: 281.9. Samples: 3890992. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 00:17:20,955][166323] Avg episode reward: [(0, '1068.686')]
[36m[2025-07-02 00:17:25,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3883008. Throughput: 0: 282.5. Samples: 3892736. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 00:17:25,990][166323] Avg episode reward: [(0, '1058.644')]
[36m[2025-07-02 00:17:30,997][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3883008. Throughput: 0: 285.4. Samples: 3894480. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 00:17:30,998][166323] Avg episode reward: [(0, '1103.056')]
[36m[2025-07-02 00:17:35,974][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3883008. Throughput: 0: 286.1. Samples: 3895344. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 00:17:35,975][166323] Avg episode reward: [(0, '1062.289')]
[37m[1m[2025-07-02 00:17:36,057][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007576_3883008.pth...
[36m[2025-07-02 00:17:36,065][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007448_3817472.pth
[36m[2025-07-02 00:17:41,007][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3883008. Throughput: 0: 285.9. Samples: 3896992. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 00:17:41,007][166323] Avg episode reward: [(0, '1048.938')]
[31m[13450005 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13450005 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[13450005 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:17:46,005][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 3883008. Throughput: 0: 282.3. Samples: 3898624. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 00:17:46,006][166323] Avg episode reward: [(0, '1077.525')]
[36m[2025-07-02 00:17:50,964][166323] Fps is (10 sec: 1645.3, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3899392. Throughput: 0: 281.3. Samples: 3899408. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 00:17:50,965][166323] Avg episode reward: [(0, '1107.141')]
[36m[2025-07-02 00:17:55,968][166323] Fps is (10 sec: 1644.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3899392. Throughput: 0: 280.1. Samples: 3901088. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 00:17:55,968][166323] Avg episode reward: [(0, '1128.220')]
[36m[2025-07-02 00:18:01,001][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 3899392. Throughput: 0: 280.3. Samples: 3902720. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 00:18:01,002][166323] Avg episode reward: [(0, '1151.201')]
[31m[13472940 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13472941 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[13472941 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:18:05,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3899392. Throughput: 0: 278.6. Samples: 3903536. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 00:18:05,986][166323] Avg episode reward: [(0, '1075.296')]
[36m[2025-07-02 00:18:10,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.8). Total num frames: 3899392. Throughput: 0: 280.4. Samples: 3905344. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 00:18:10,956][166323] Avg episode reward: [(0, '1108.784')]
[31m[13482667 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13482667 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[13482667 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:18:15,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3899392. Throughput: 0: 279.8. Samples: 3907056. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 00:18:15,944][166323] Avg episode reward: [(0, '1104.655')]
[36m[2025-07-02 00:18:20,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3899392. Throughput: 0: 278.7. Samples: 3907888. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 00:18:20,980][166323] Avg episode reward: [(0, '1128.273')]
[31m[13490609 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13490610 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[13490610 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:18:25,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3899392. Throughput: 0: 279.8. Samples: 3909568. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 00:18:25,953][166323] Avg episode reward: [(0, '1117.987')]
[36m[2025-07-02 00:18:30,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3899392. Throughput: 0: 279.4. Samples: 3911184. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 00:18:30,954][166323] Avg episode reward: [(0, '1118.117')]
[36m[2025-07-02 00:18:35,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3899392. Throughput: 0: 282.0. Samples: 3912096. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 00:18:35,962][166323] Avg episode reward: [(0, '1139.701')]
[36m[2025-07-02 00:18:40,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 3899392. Throughput: 0: 284.6. Samples: 3913888. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 00:18:40,943][166323] Avg episode reward: [(0, '1187.485')]
[31m[13509508 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13509509 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[13509509 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:18:45,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3899392. Throughput: 0: 286.5. Samples: 3915600. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 00:18:45,954][166323] Avg episode reward: [(0, '1103.474')]
[36m[2025-07-02 00:18:50,963][166323] Fps is (10 sec: 1635.1, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3915776. Throughput: 0: 285.3. Samples: 3916368. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 00:18:50,963][166323] Avg episode reward: [(0, '1102.996')]
[36m[2025-07-02 00:18:55,989][166323] Fps is (10 sec: 1632.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3915776. Throughput: 0: 280.3. Samples: 3917968. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 00:18:55,989][166323] Avg episode reward: [(0, '1072.078')]
[36m[2025-07-02 00:19:00,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3915776. Throughput: 0: 276.1. Samples: 3919488. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 00:19:00,978][166323] Avg episode reward: [(0, '971.497')]
[36m[2025-07-02 00:19:05,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3915776. Throughput: 0: 275.6. Samples: 3920288. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 00:19:05,972][166323] Avg episode reward: [(0, '942.764')]
[36m[2025-07-02 00:19:10,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3915776. Throughput: 0: 277.7. Samples: 3922064. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 00:19:10,956][166323] Avg episode reward: [(0, '918.052')]
[36m[2025-07-02 00:19:15,979][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3915776. Throughput: 0: 278.6. Samples: 3923728. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 00:19:15,979][166323] Avg episode reward: [(0, '966.396')]
[31m[13547122 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13547123 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[13547123 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:19:20,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3915776. Throughput: 0: 275.9. Samples: 3924512. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 00:19:20,969][166323] Avg episode reward: [(0, '872.637')]
[36m[2025-07-02 00:19:25,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3915776. Throughput: 0: 271.9. Samples: 3926128. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 00:19:25,955][166323] Avg episode reward: [(0, '957.799')]
[36m[2025-07-02 00:19:30,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3915776. Throughput: 0: 267.8. Samples: 3927648. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 00:19:30,949][166323] Avg episode reward: [(0, '972.145')]
[31m[13563994 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13563995 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[13563995 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:19:35,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3915776. Throughput: 0: 269.4. Samples: 3928496. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 00:19:35,979][166323] Avg episode reward: [(0, '983.606')]
[37m[1m[2025-07-02 00:19:36,060][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007640_3915776.pth...
[36m[2025-07-02 00:19:36,064][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007512_3850240.pth
[36m[2025-07-02 00:19:40,976][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3915776. Throughput: 0: 269.6. Samples: 3930096. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 00:19:40,977][166323] Avg episode reward: [(0, '1026.084')]
[36m[2025-07-02 00:19:45,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 3915776. Throughput: 0: 275.7. Samples: 3931888. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 00:19:45,947][166323] Avg episode reward: [(0, '1051.299')]
[36m[2025-07-02 00:19:50,951][166323] Fps is (10 sec: 1642.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3932160. Throughput: 0: 275.3. Samples: 3932672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:19:50,952][166323] Avg episode reward: [(0, '1109.286')]
[36m[2025-07-02 00:19:55,986][166323] Fps is (10 sec: 1632.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3932160. Throughput: 0: 271.5. Samples: 3934288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:19:55,986][166323] Avg episode reward: [(0, '1140.121')]
[36m[2025-07-02 00:20:00,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3932160. Throughput: 0: 274.0. Samples: 3936048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:20:00,948][166323] Avg episode reward: [(0, '1144.177')]
[36m[2025-07-02 00:20:06,011][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3932160. Throughput: 0: 274.9. Samples: 3936896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:20:06,011][166323] Avg episode reward: [(0, '1150.502')]
[36m[2025-07-02 00:20:10,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3932160. Throughput: 0: 274.8. Samples: 3938496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:20:10,964][166323] Avg episode reward: [(0, '1167.415')]
[36m[2025-07-02 00:20:15,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3932160. Throughput: 0: 279.0. Samples: 3940208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:20:15,972][166323] Avg episode reward: [(0, '1152.790')]
[36m[2025-07-02 00:20:20,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3932160. Throughput: 0: 279.1. Samples: 3941056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:20:20,976][166323] Avg episode reward: [(0, '1128.516')]
[31m[13614041 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13614042 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[13614042 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:20:25,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3932160. Throughput: 0: 279.6. Samples: 3942672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:20:25,953][166323] Avg episode reward: [(0, '1117.247')]
[36m[2025-07-02 00:20:30,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3932160. Throughput: 0: 276.7. Samples: 3944352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:20:30,993][166323] Avg episode reward: [(0, '1109.008')]
[36m[2025-07-02 00:20:35,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3932160. Throughput: 0: 279.4. Samples: 3945248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:20:35,966][166323] Avg episode reward: [(0, '1111.987')]
[36m[2025-07-02 00:20:40,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3932160. Throughput: 0: 277.9. Samples: 3946784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:20:40,959][166323] Avg episode reward: [(0, '1104.913')]
[36m[2025-07-02 00:20:45,945][166323] Fps is (10 sec: 1641.8, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 3948544. Throughput: 0: 278.4. Samples: 3948576. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 00:20:45,945][166323] Avg episode reward: [(0, '1122.326')]
[36m[2025-07-02 00:20:50,996][166323] Fps is (10 sec: 1632.4, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3948544. Throughput: 0: 278.8. Samples: 3949440. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 00:20:50,996][166323] Avg episode reward: [(0, '1086.301')]
[36m[2025-07-02 00:20:55,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3948544. Throughput: 0: 282.2. Samples: 3951200. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 00:20:55,988][166323] Avg episode reward: [(0, '1065.882')]
[36m[2025-07-02 00:21:00,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3948544. Throughput: 0: 280.9. Samples: 3952848. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 00:21:00,967][166323] Avg episode reward: [(0, '1086.488')]
[36m[2025-07-02 00:21:05,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 3948544. Throughput: 0: 282.9. Samples: 3953776. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 00:21:05,945][166323] Avg episode reward: [(0, '1061.391')]
[36m[2025-07-02 00:21:10,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3948544. Throughput: 0: 285.3. Samples: 3955520. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 00:21:10,982][166323] Avg episode reward: [(0, '1136.122')]
[36m[2025-07-02 00:21:15,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3948544. Throughput: 0: 282.1. Samples: 3957040. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 00:21:15,967][166323] Avg episode reward: [(0, '1123.686')]
[36m[2025-07-02 00:21:20,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3948544. Throughput: 0: 282.2. Samples: 3957952. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 00:21:20,987][166323] Avg episode reward: [(0, '1070.955')]
[36m[2025-07-02 00:21:25,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3948544. Throughput: 0: 285.9. Samples: 3959648. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 00:21:25,959][166323] Avg episode reward: [(0, '1141.559')]
[31m[13676207 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13676207 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[13676207 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:21:30,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3948544. Throughput: 0: 283.4. Samples: 3961328. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 00:21:30,946][166323] Avg episode reward: [(0, '1129.292')]
[36m[2025-07-02 00:21:35,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3948544. Throughput: 0: 282.9. Samples: 3962160. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 00:21:35,959][166323] Avg episode reward: [(0, '1091.797')]
[37m[1m[2025-07-02 00:21:36,046][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007704_3948544.pth...
[36m[2025-07-02 00:21:36,051][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007576_3883008.pth
[36m[2025-07-02 00:21:40,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3948544. Throughput: 0: 282.5. Samples: 3963904. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 00:21:40,952][166323] Avg episode reward: [(0, '1030.610')]
[31m[13690343 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13690344 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[13690344 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:21:45,954][166323] Fps is (10 sec: 1639.1, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3964928. Throughput: 0: 283.1. Samples: 3965584. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 00:21:45,954][166323] Avg episode reward: [(0, '994.971')]
[36m[2025-07-02 00:21:50,999][166323] Fps is (10 sec: 1630.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3964928. Throughput: 0: 279.1. Samples: 3966352. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 00:21:51,000][166323] Avg episode reward: [(0, '1067.158')]
[36m[2025-07-02 00:21:55,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3964928. Throughput: 0: 276.4. Samples: 3967952. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 00:21:55,955][166323] Avg episode reward: [(0, '1056.093')]
[36m[2025-07-02 00:22:00,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3964928. Throughput: 0: 280.3. Samples: 3969648. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 00:22:00,945][166323] Avg episode reward: [(0, '1041.480')]
[36m[2025-07-02 00:22:05,972][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3964928. Throughput: 0: 279.2. Samples: 3970512. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 00:22:05,972][166323] Avg episode reward: [(0, '1075.856')]
[36m[2025-07-02 00:22:10,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3964928. Throughput: 0: 277.5. Samples: 3972144. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 00:22:10,987][166323] Avg episode reward: [(0, '1052.123')]
[36m[2025-07-02 00:22:15,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3964928. Throughput: 0: 279.9. Samples: 3973936. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 00:22:15,993][166323] Avg episode reward: [(0, '1071.248')]
[36m[2025-07-02 00:22:21,022][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3964928. Throughput: 0: 281.2. Samples: 3974832. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 00:22:21,022][166323] Avg episode reward: [(0, '1102.564')]
[36m[2025-07-02 00:22:25,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3964928. Throughput: 0: 277.8. Samples: 3976416. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 00:22:25,987][166323] Avg episode reward: [(0, '1070.110')]
[36m[2025-07-02 00:22:30,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3964928. Throughput: 0: 275.3. Samples: 3977984. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 00:22:30,989][166323] Avg episode reward: [(0, '1060.306')]
[31m[13739555 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13739555 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[13739555 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:22:35,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3964928. Throughput: 0: 277.2. Samples: 3978816. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 00:22:35,960][166323] Avg episode reward: [(0, '1005.798')]
[36m[2025-07-02 00:22:40,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3964928. Throughput: 0: 277.7. Samples: 3980448. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 00:22:40,960][166323] Avg episode reward: [(0, '1056.639')]
[36m[2025-07-02 00:22:45,988][166323] Fps is (10 sec: 1633.8, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3981312. Throughput: 0: 277.4. Samples: 3982144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:22:45,988][166323] Avg episode reward: [(0, '1131.462')]
[31m[13757428 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13757429 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[13757429 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:22:50,974][166323] Fps is (10 sec: 1636.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3981312. Throughput: 0: 275.2. Samples: 3982896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:22:50,975][166323] Avg episode reward: [(0, '1120.654')]
[36m[2025-07-02 00:22:55,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3981312. Throughput: 0: 278.0. Samples: 3984640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:22:55,944][166323] Avg episode reward: [(0, '1069.207')]
[36m[2025-07-02 00:23:00,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3981312. Throughput: 0: 275.8. Samples: 3986336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:23:00,947][166323] Avg episode reward: [(0, '1092.424')]
[36m[2025-07-02 00:23:06,006][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 3981312. Throughput: 0: 276.7. Samples: 3987280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:23:06,006][166323] Avg episode reward: [(0, '1099.912')]
[36m[2025-07-02 00:23:10,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3981312. Throughput: 0: 282.4. Samples: 3989120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:23:10,974][166323] Avg episode reward: [(0, '1075.944')]
[36m[2025-07-02 00:23:15,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3981312. Throughput: 0: 285.7. Samples: 3990832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:23:15,957][166323] Avg episode reward: [(0, '1056.995')]
[36m[2025-07-02 00:23:20,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3981312. Throughput: 0: 286.9. Samples: 3991728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:23:20,971][166323] Avg episode reward: [(0, '1038.045')]
[36m[2025-07-02 00:23:25,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3981312. Throughput: 0: 283.5. Samples: 3993216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:23:25,994][166323] Avg episode reward: [(0, '1060.235')]
[36m[2025-07-02 00:23:30,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 3981312. Throughput: 0: 281.7. Samples: 3994816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:23:30,964][166323] Avg episode reward: [(0, '1057.218')]
[36m[2025-07-02 00:23:35,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3981312. Throughput: 0: 283.4. Samples: 3995648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:23:35,966][166323] Avg episode reward: [(0, '1042.527')]
[37m[1m[2025-07-02 00:23:36,035][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007768_3981312.pth...
[36m[2025-07-02 00:23:36,039][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007640_3915776.pth
[36m[2025-07-02 00:23:40,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3981312. Throughput: 0: 280.1. Samples: 3997248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:23:40,959][166323] Avg episode reward: [(0, '994.128')]
[36m[2025-07-02 00:23:45,972][166323] Fps is (10 sec: 1637.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3997696. Throughput: 0: 281.1. Samples: 3998992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:23:45,972][166323] Avg episode reward: [(0, '1012.998')]
[31m[13818744 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13818745 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[13818745 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:23:50,966][166323] Fps is (10 sec: 1637.3, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3997696. Throughput: 0: 281.5. Samples: 3999936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:23:50,966][166323] Avg episode reward: [(0, '915.570')]
[36m[2025-07-02 00:23:55,970][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3997696. Throughput: 0: 276.3. Samples: 4001552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:23:55,970][166323] Avg episode reward: [(0, '962.552')]
[36m[2025-07-02 00:24:00,973][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3997696. Throughput: 0: 278.3. Samples: 4003360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:24:00,973][166323] Avg episode reward: [(0, '969.038')]
[36m[2025-07-02 00:24:05,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3997696. Throughput: 0: 279.7. Samples: 4004320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:24:05,991][166323] Avg episode reward: [(0, '982.598')]
[36m[2025-07-02 00:24:10,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3997696. Throughput: 0: 282.5. Samples: 4005920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:24:10,963][166323] Avg episode reward: [(0, '958.143')]
[33m[13840360 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[13840361 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.83203125
[33mCrash Rate: 0.15966796875
[33mTimeout Rate: 0.00830078125 (navigation_task.py:265)
[33m[13840361 ms][navigation_task] - WARNING : 
[33mSuccesses: 1704
[33mCrashes : 327
[33mTimeouts: 17 (navigation_task.py:268)
[36m[2025-07-02 00:24:15,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3997696. Throughput: 0: 284.2. Samples: 4007600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:24:15,944][166323] Avg episode reward: [(0, '971.890')]
[36m[2025-07-02 00:24:20,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 3997696. Throughput: 0: 284.4. Samples: 4008448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:24:20,967][166323] Avg episode reward: [(0, '1022.970')]
[36m[2025-07-02 00:24:25,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 3997696. Throughput: 0: 288.8. Samples: 4010240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:24:25,946][166323] Avg episode reward: [(0, '1126.741')]
[31m[13854755 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13854755 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[13854756 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:24:30,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 3997696. Throughput: 0: 284.3. Samples: 4011792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:24:30,992][166323] Avg episode reward: [(0, '1094.207')]
[36m[2025-07-02 00:24:35,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 3997696. Throughput: 0: 283.6. Samples: 4012704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:24:35,981][166323] Avg episode reward: [(0, '1094.257')]
[36m[2025-07-02 00:24:40,961][166323] Fps is (10 sec: 1643.6, 60 sec: 546.1, 300 sec: 333.2). Total num frames: 4014080. Throughput: 0: 283.8. Samples: 4014320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:24:40,961][166323] Avg episode reward: [(0, '1099.148')]
[36m[2025-07-02 00:24:45,993][166323] Fps is (10 sec: 1636.5, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4014080. Throughput: 0: 281.1. Samples: 4016016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:24:46,002][166323] Avg episode reward: [(0, '1155.953')]
[36m[2025-07-02 00:24:50,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4014080. Throughput: 0: 279.3. Samples: 4016880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:24:50,961][166323] Avg episode reward: [(0, '1157.033')]
[31m[13881474 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13881474 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[13881475 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:24:55,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4014080. Throughput: 0: 280.4. Samples: 4018544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:24:55,978][166323] Avg episode reward: [(0, '1173.843')]
[36m[2025-07-02 00:25:01,000][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4014080. Throughput: 0: 280.2. Samples: 4020224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:25:01,001][166323] Avg episode reward: [(0, '1151.078')]
[36m[2025-07-02 00:25:05,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4014080. Throughput: 0: 280.6. Samples: 4021072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:25:05,956][166323] Avg episode reward: [(0, '1099.086')]
[36m[2025-07-02 00:25:10,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4014080. Throughput: 0: 279.7. Samples: 4022832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:25:10,959][166323] Avg episode reward: [(0, '1186.768')]
[36m[2025-07-02 00:25:15,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4014080. Throughput: 0: 285.0. Samples: 4024608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:25:15,967][166323] Avg episode reward: [(0, '1151.177')]
[36m[2025-07-02 00:25:20,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4014080. Throughput: 0: 284.0. Samples: 4025472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:25:20,945][166323] Avg episode reward: [(0, '1127.224')]
[36m[2025-07-02 00:25:25,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4014080. Throughput: 0: 286.5. Samples: 4027216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:25:25,966][166323] Avg episode reward: [(0, '1092.514')]
[36m[2025-07-02 00:25:31,025][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 4014080. Throughput: 0: 282.5. Samples: 4028736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:25:31,025][166323] Avg episode reward: [(0, '1085.591')]
[36m[2025-07-02 00:25:35,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4014080. Throughput: 0: 283.0. Samples: 4029616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:25:35,959][166323] Avg episode reward: [(0, '1130.447')]
[37m[1m[2025-07-02 00:25:36,027][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007832_4014080.pth...
[36m[2025-07-02 00:25:36,031][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007704_3948544.pth
[36m[2025-07-02 00:25:40,958][166323] Fps is (10 sec: 1649.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4030464. Throughput: 0: 280.7. Samples: 4031168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:25:40,959][166323] Avg episode reward: [(0, '1136.256')]
[36m[2025-07-02 00:25:45,966][166323] Fps is (10 sec: 1637.2, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4030464. Throughput: 0: 279.7. Samples: 4032800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:25:45,966][166323] Avg episode reward: [(0, '1084.780')]
[36m[2025-07-02 00:25:50,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4030464. Throughput: 0: 280.3. Samples: 4033696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:25:50,989][166323] Avg episode reward: [(0, '1109.543')]
[31m[13943467 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13943467 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[13943468 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:25:55,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4030464. Throughput: 0: 276.7. Samples: 4035280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:25:55,950][166323] Avg episode reward: [(0, '1122.568')]
[36m[2025-07-02 00:26:00,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4030464. Throughput: 0: 273.4. Samples: 4036912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:26:00,963][166323] Avg episode reward: [(0, '1163.936')]
[36m[2025-07-02 00:26:05,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4030464. Throughput: 0: 272.8. Samples: 4037760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:26:05,995][166323] Avg episode reward: [(0, '1073.575')]
[31m[13956772 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13956773 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[13956773 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:26:10,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4030464. Throughput: 0: 267.7. Samples: 4039264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:26:10,977][166323] Avg episode reward: [(0, '1138.818')]
[36m[2025-07-02 00:26:15,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4030464. Throughput: 0: 271.9. Samples: 4040960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:26:15,989][166323] Avg episode reward: [(0, '1166.054')]
[36m[2025-07-02 00:26:20,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4030464. Throughput: 0: 271.6. Samples: 4041840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:26:20,971][166323] Avg episode reward: [(0, '1179.879')]
[36m[2025-07-02 00:26:25,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4030464. Throughput: 0: 274.9. Samples: 4043536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:26:25,955][166323] Avg episode reward: [(0, '1157.593')]
[36m[2025-07-02 00:26:30,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 4030464. Throughput: 0: 277.1. Samples: 4045264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:26:30,947][166323] Avg episode reward: [(0, '1175.188')]
[36m[2025-07-02 00:26:35,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4030464. Throughput: 0: 275.6. Samples: 4046096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:26:35,978][166323] Avg episode reward: [(0, '1167.264')]
[36m[2025-07-02 00:26:40,956][166323] Fps is (10 sec: 1636.9, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4046848. Throughput: 0: 277.3. Samples: 4047760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:26:40,956][166323] Avg episode reward: [(0, '1220.468')]
[36m[2025-07-02 00:26:45,965][166323] Fps is (10 sec: 1640.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4046848. Throughput: 0: 279.4. Samples: 4049488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:26:45,966][166323] Avg episode reward: [(0, '1208.546')]
[36m[2025-07-02 00:26:50,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 4046848. Throughput: 0: 280.8. Samples: 4050384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:26:50,947][166323] Avg episode reward: [(0, '1188.332')]
[36m[2025-07-02 00:26:55,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4046848. Throughput: 0: 285.2. Samples: 4052096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:26:55,973][166323] Avg episode reward: [(0, '1160.099')]
[36m[2025-07-02 00:27:00,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4046848. Throughput: 0: 287.3. Samples: 4053888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:27:00,985][166323] Avg episode reward: [(0, '1165.551')]
[36m[2025-07-02 00:27:05,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4046848. Throughput: 0: 286.9. Samples: 4054752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:27:05,978][166323] Avg episode reward: [(0, '1178.089')]
[36m[2025-07-02 00:27:11,015][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4046848. Throughput: 0: 285.5. Samples: 4056400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:27:11,016][166323] Avg episode reward: [(0, '1188.772')]
[36m[2025-07-02 00:27:15,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4046848. Throughput: 0: 282.2. Samples: 4057968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:27:15,969][166323] Avg episode reward: [(0, '1119.533')]
[36m[2025-07-02 00:27:20,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4046848. Throughput: 0: 280.1. Samples: 4058704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:27:20,987][166323] Avg episode reward: [(0, '1093.674')]
[36m[2025-07-02 00:27:26,004][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 4046848. Throughput: 0: 278.8. Samples: 4060320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:27:26,004][166323] Avg episode reward: [(0, '1130.120')]
[36m[2025-07-02 00:27:30,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4046848. Throughput: 0: 279.2. Samples: 4062048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:27:30,945][166323] Avg episode reward: [(0, '1116.087')]
[36m[2025-07-02 00:27:35,993][166323] Fps is (10 sec: 1640.1, 60 sec: 546.0, 300 sec: 333.2). Total num frames: 4063232. Throughput: 0: 277.4. Samples: 4062880. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:27:35,993][166323] Avg episode reward: [(0, '1047.752')]
[37m[1m[2025-07-02 00:27:36,000][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007928_4063232.pth...
[36m[2025-07-02 00:27:36,010][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007768_3981312.pth
[36m[2025-07-02 00:27:40,943][166323] Fps is (10 sec: 1638.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4063232. Throughput: 0: 273.2. Samples: 4064384. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:27:40,943][166323] Avg episode reward: [(0, '1102.422')]
[36m[2025-07-02 00:27:45,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4063232. Throughput: 0: 269.0. Samples: 4065984. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:27:45,953][166323] Avg episode reward: [(0, '1046.593')]
[36m[2025-07-02 00:27:50,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4063232. Throughput: 0: 270.3. Samples: 4066912. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:27:50,964][166323] Avg episode reward: [(0, '1103.919')]
[36m[2025-07-02 00:27:56,001][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 4063232. Throughput: 0: 270.7. Samples: 4068576. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:27:56,001][166323] Avg episode reward: [(0, '1057.023')]
[36m[2025-07-02 00:28:01,001][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4063232. Throughput: 0: 272.2. Samples: 4070224. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:28:01,001][166323] Avg episode reward: [(0, '1014.512')]
[36m[2025-07-02 00:28:05,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4063232. Throughput: 0: 274.9. Samples: 4071072. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:28:05,977][166323] Avg episode reward: [(0, '1078.741')]
[36m[2025-07-02 00:28:10,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4063232. Throughput: 0: 280.0. Samples: 4072912. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:28:10,982][166323] Avg episode reward: [(0, '1098.162')]
[36m[2025-07-02 00:28:15,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4063232. Throughput: 0: 275.5. Samples: 4074448. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:28:15,959][166323] Avg episode reward: [(0, '1100.416')]
[36m[2025-07-02 00:28:20,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4063232. Throughput: 0: 277.1. Samples: 4075344. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:28:20,969][166323] Avg episode reward: [(0, '1063.722')]
[36m[2025-07-02 00:28:25,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 4063232. Throughput: 0: 281.5. Samples: 4077056. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:28:25,960][166323] Avg episode reward: [(0, '1078.577')]
[36m[2025-07-02 00:28:30,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4063232. Throughput: 0: 282.6. Samples: 4078704. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:28:30,963][166323] Avg episode reward: [(0, '1062.802')]
[36m[2025-07-02 00:28:35,971][166323] Fps is (10 sec: 1636.5, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 4079616. Throughput: 0: 280.1. Samples: 4079520. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 00:28:35,971][166323] Avg episode reward: [(0, '1069.707')]
[31m[14108137 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14108137 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[14108137 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:28:40,994][166323] Fps is (10 sec: 1633.3, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 4079616. Throughput: 0: 281.3. Samples: 4081232. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 00:28:40,995][166323] Avg episode reward: [(0, '1045.061')]
[36m[2025-07-02 00:28:45,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4079616. Throughput: 0: 282.6. Samples: 4082928. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 00:28:45,954][166323] Avg episode reward: [(0, '1028.687')]
[36m[2025-07-02 00:28:50,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4079616. Throughput: 0: 283.2. Samples: 4083808. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 00:28:50,944][166323] Avg episode reward: [(0, '1032.212')]
[36m[2025-07-02 00:28:55,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 4079616. Throughput: 0: 280.7. Samples: 4085536. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 00:28:55,958][166323] Avg episode reward: [(0, '1041.045')]
[36m[2025-07-02 00:29:00,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4079616. Throughput: 0: 283.0. Samples: 4087184. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 00:29:00,966][166323] Avg episode reward: [(0, '1047.170')]
[36m[2025-07-02 00:29:05,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4079616. Throughput: 0: 281.5. Samples: 4088016. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 00:29:05,989][166323] Avg episode reward: [(0, '1074.666')]
[31m[14138751 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14138751 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[14138751 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:29:10,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4079616. Throughput: 0: 280.9. Samples: 4089696. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 00:29:10,953][166323] Avg episode reward: [(0, '1064.693')]
[36m[2025-07-02 00:29:15,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4079616. Throughput: 0: 280.7. Samples: 4091344. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 00:29:15,989][166323] Avg episode reward: [(0, '1086.592')]
[36m[2025-07-02 00:29:20,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4079616. Throughput: 0: 282.6. Samples: 4092240. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 00:29:20,979][166323] Avg episode reward: [(0, '1095.860')]
[36m[2025-07-02 00:29:25,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4079616. Throughput: 0: 281.1. Samples: 4093872. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 00:29:25,967][166323] Avg episode reward: [(0, '1133.281')]
[36m[2025-07-02 00:29:30,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4079616. Throughput: 0: 283.5. Samples: 4095696. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 00:29:30,984][166323] Avg episode reward: [(0, '1165.629')]
[31m[14161845 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14161846 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[14161846 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:29:35,952][166323] Fps is (10 sec: 1640.8, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4096000. Throughput: 0: 281.5. Samples: 4096480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:29:35,952][166323] Avg episode reward: [(0, '1149.345')]
[37m[1m[2025-07-02 00:29:36,001][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007992_4096000.pth...
[36m[2025-07-02 00:29:36,005][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007832_4014080.pth
[36m[2025-07-02 00:29:40,982][166323] Fps is (10 sec: 1638.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4096000. Throughput: 0: 281.8. Samples: 4098224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:29:40,983][166323] Avg episode reward: [(0, '1152.991')]
[36m[2025-07-02 00:29:45,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4096000. Throughput: 0: 284.5. Samples: 4099984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:29:45,956][166323] Avg episode reward: [(0, '1171.852')]
[36m[2025-07-02 00:29:50,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4096000. Throughput: 0: 283.0. Samples: 4100752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:29:50,989][166323] Avg episode reward: [(0, '1184.122')]
[36m[2025-07-02 00:29:55,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4096000. Throughput: 0: 281.0. Samples: 4102352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:29:55,990][166323] Avg episode reward: [(0, '1147.552')]
[36m[2025-07-02 00:30:00,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4096000. Throughput: 0: 282.1. Samples: 4104032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:30:00,973][166323] Avg episode reward: [(0, '1115.282')]
[36m[2025-07-02 00:30:05,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4096000. Throughput: 0: 280.9. Samples: 4104880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:30:05,982][166323] Avg episode reward: [(0, '1106.026')]
[31m[14195720 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14195720 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[14195721 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:30:11,028][166323] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 277.6). Total num frames: 4096000. Throughput: 0: 276.2. Samples: 4106320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:30:11,028][166323] Avg episode reward: [(0, '1090.655')]
[36m[2025-07-02 00:30:15,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4096000. Throughput: 0: 272.7. Samples: 4107968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:30:15,990][166323] Avg episode reward: [(0, '1069.417')]
[36m[2025-07-02 00:30:20,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4096000. Throughput: 0: 273.6. Samples: 4108800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:30:20,980][166323] Avg episode reward: [(0, '1095.739')]
[36m[2025-07-02 00:30:25,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4096000. Throughput: 0: 269.6. Samples: 4110352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:30:25,970][166323] Avg episode reward: [(0, '1096.852')]
[36m[2025-07-02 00:30:30,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4096000. Throughput: 0: 268.3. Samples: 4112064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:30:30,988][166323] Avg episode reward: [(0, '1135.258')]
[36m[2025-07-02 00:30:35,945][166323] Fps is (10 sec: 1642.3, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4112384. Throughput: 0: 269.4. Samples: 4112864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:30:35,946][166323] Avg episode reward: [(0, '1175.052')]
[36m[2025-07-02 00:30:40,975][166323] Fps is (10 sec: 1640.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4112384. Throughput: 0: 272.1. Samples: 4114592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:30:40,976][166323] Avg episode reward: [(0, '1172.968')]
[36m[2025-07-02 00:30:45,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4112384. Throughput: 0: 275.4. Samples: 4116416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:30:45,945][166323] Avg episode reward: [(0, '1192.320')]
[36m[2025-07-02 00:30:50,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 4112384. Throughput: 0: 277.2. Samples: 4117360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:30:50,999][166323] Avg episode reward: [(0, '1197.469')]
[36m[2025-07-02 00:30:56,026][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 4112384. Throughput: 0: 283.4. Samples: 4119072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:30:56,027][166323] Avg episode reward: [(0, '1149.227')]
[31m[14248453 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14248454 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[14248454 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:31:00,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4112384. Throughput: 0: 284.1. Samples: 4120752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:31:00,984][166323] Avg episode reward: [(0, '1150.054')]
[36m[2025-07-02 00:31:05,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4112384. Throughput: 0: 282.2. Samples: 4121504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:31:05,991][166323] Avg episode reward: [(0, '1048.017')]
[36m[2025-07-02 00:31:10,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 4112384. Throughput: 0: 284.8. Samples: 4123168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:31:10,962][166323] Avg episode reward: [(0, '1052.680')]
[36m[2025-07-02 00:31:16,004][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4112384. Throughput: 0: 285.4. Samples: 4124912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:31:16,004][166323] Avg episode reward: [(0, '1075.878')]
[36m[2025-07-02 00:31:20,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4112384. Throughput: 0: 284.6. Samples: 4125680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:31:20,978][166323] Avg episode reward: [(0, '1043.957')]
[36m[2025-07-02 00:31:25,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4112384. Throughput: 0: 282.8. Samples: 4127312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:31:25,956][166323] Avg episode reward: [(0, '1037.724')]
[31m[14275709 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14275709 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[14275709 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:31:30,950][166323] Fps is (10 sec: 1643.0, 60 sec: 546.5, 300 sec: 333.3). Total num frames: 4128768. Throughput: 0: 279.1. Samples: 4128976. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 00:31:30,951][166323] Avg episode reward: [(0, '1063.015')]
[36m[2025-07-02 00:31:36,000][166323] Fps is (10 sec: 1631.2, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 4128768. Throughput: 0: 275.2. Samples: 4129744. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 00:31:36,000][166323] Avg episode reward: [(0, '1087.756')]
[37m[1m[2025-07-02 00:31:36,065][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000008056_4128768.pth...
[36m[2025-07-02 00:31:36,069][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007928_4063232.pth
[36m[2025-07-02 00:31:40,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4128768. Throughput: 0: 274.4. Samples: 4131408. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 00:31:40,975][166323] Avg episode reward: [(0, '1065.748')]
[36m[2025-07-02 00:31:45,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4128768. Throughput: 0: 276.1. Samples: 4133168. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 00:31:45,946][166323] Avg episode reward: [(0, '1070.886')]
[36m[2025-07-02 00:31:50,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4128768. Throughput: 0: 277.0. Samples: 4133968. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 00:31:50,980][166323] Avg episode reward: [(0, '1076.635')]
[31m[14302239 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14302240 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[14302240 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:31:55,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 4128768. Throughput: 0: 274.0. Samples: 4135504. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 00:31:55,985][166323] Avg episode reward: [(0, '1109.307')]
[36m[2025-07-02 00:32:00,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4128768. Throughput: 0: 270.8. Samples: 4137088. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 00:32:00,963][166323] Avg episode reward: [(0, '1123.582')]
[36m[2025-07-02 00:32:05,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4128768. Throughput: 0: 272.0. Samples: 4137920. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 00:32:05,972][166323] Avg episode reward: [(0, '1133.173')]
[36m[2025-07-02 00:32:10,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4128768. Throughput: 0: 269.5. Samples: 4139440. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 00:32:10,958][166323] Avg episode reward: [(0, '1112.682')]
[36m[2025-07-02 00:32:15,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 4128768. Throughput: 0: 267.3. Samples: 4141008. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 00:32:15,960][166323] Avg episode reward: [(0, '1113.342')]
[31m[14326396 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14326397 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[14326397 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:32:20,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4128768. Throughput: 0: 268.0. Samples: 4141792. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 00:32:20,957][166323] Avg episode reward: [(0, '1127.167')]
[36m[2025-07-02 00:32:25,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 4128768. Throughput: 0: 267.6. Samples: 4143456. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 00:32:25,994][166323] Avg episode reward: [(0, '1144.994')]
[36m[2025-07-02 00:32:31,046][166323] Fps is (10 sec: 1623.9, 60 sec: 272.6, 300 sec: 277.6). Total num frames: 4145152. Throughput: 0: 263.9. Samples: 4145072. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 00:32:31,047][166323] Avg episode reward: [(0, '1199.456')]
[36m[2025-07-02 00:32:35,980][166323] Fps is (10 sec: 1640.7, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4145152. Throughput: 0: 264.5. Samples: 4145872. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 00:32:35,980][166323] Avg episode reward: [(0, '1128.474')]
[36m[2025-07-02 00:32:40,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4145152. Throughput: 0: 268.1. Samples: 4147568. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 00:32:40,975][166323] Avg episode reward: [(0, '1102.336')]
[36m[2025-07-02 00:32:45,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 4145152. Throughput: 0: 266.5. Samples: 4149088. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 00:32:45,996][166323] Avg episode reward: [(0, '1142.634')]
[36m[2025-07-02 00:32:50,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4145152. Throughput: 0: 265.3. Samples: 4149856. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 00:32:50,961][166323] Avg episode reward: [(0, '1142.375')]
[36m[2025-07-02 00:32:55,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4145152. Throughput: 0: 269.4. Samples: 4151568. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 00:32:55,968][166323] Avg episode reward: [(0, '1104.781')]
[36m[2025-07-02 00:33:00,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4145152. Throughput: 0: 271.6. Samples: 4153232. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 00:33:00,960][166323] Avg episode reward: [(0, '1027.229')]
[36m[2025-07-02 00:33:05,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4145152. Throughput: 0: 275.4. Samples: 4154192. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 00:33:05,975][166323] Avg episode reward: [(0, '1079.502')]
[36m[2025-07-02 00:33:10,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4145152. Throughput: 0: 275.9. Samples: 4155872. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 00:33:10,995][166323] Avg episode reward: [(0, '1078.272')]
[36m[2025-07-02 00:33:15,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4145152. Throughput: 0: 277.9. Samples: 4157552. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 00:33:15,958][166323] Avg episode reward: [(0, '1095.253')]
[31m[14388471 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14388472 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[14388472 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:33:21,004][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4145152. Throughput: 0: 279.7. Samples: 4158464. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 00:33:21,005][166323] Avg episode reward: [(0, '1106.159')]
[36m[2025-07-02 00:33:25,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4145152. Throughput: 0: 275.9. Samples: 4159984. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 00:33:25,970][166323] Avg episode reward: [(0, '1125.495')]
[31m[14395571 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14395571 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[14395572 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[14395622 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14395622 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[14395622 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:33:30,989][166323] Fps is (10 sec: 1640.9, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 4161536. Throughput: 0: 281.3. Samples: 4161744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:33:30,989][166323] Avg episode reward: [(0, '1140.775')]
[36m[2025-07-02 00:33:36,006][166323] Fps is (10 sec: 1632.5, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4161536. Throughput: 0: 284.5. Samples: 4162672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:33:36,006][166323] Avg episode reward: [(0, '1151.595')]
[37m[1m[2025-07-02 00:33:36,073][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000008120_4161536.pth...
[36m[2025-07-02 00:33:36,077][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000007992_4096000.pth
[31m[14404690 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14404690 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[14404691 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:33:40,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4161536. Throughput: 0: 287.7. Samples: 4164512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:33:40,966][166323] Avg episode reward: [(0, '1137.470')]
[36m[2025-07-02 00:33:45,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4161536. Throughput: 0: 286.6. Samples: 4166128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:33:45,964][166323] Avg episode reward: [(0, '1133.775')]
[36m[2025-07-02 00:33:50,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4161536. Throughput: 0: 281.9. Samples: 4166880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:33:50,976][166323] Avg episode reward: [(0, '1181.286')]
[36m[2025-07-02 00:33:56,007][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4161536. Throughput: 0: 280.5. Samples: 4168496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:33:56,007][166323] Avg episode reward: [(0, '1183.876')]
[31m[14428166 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14428166 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[14428166 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:34:00,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4161536. Throughput: 0: 280.7. Samples: 4170192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:34:00,986][166323] Avg episode reward: [(0, '1183.912')]
[36m[2025-07-02 00:34:05,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4161536. Throughput: 0: 278.0. Samples: 4170960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:34:05,961][166323] Avg episode reward: [(0, '1187.862')]
[36m[2025-07-02 00:34:10,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4161536. Throughput: 0: 283.0. Samples: 4172720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:34:10,969][166323] Avg episode reward: [(0, '1194.154')]
[36m[2025-07-02 00:34:15,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4161536. Throughput: 0: 279.9. Samples: 4174336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:34:15,975][166323] Avg episode reward: [(0, '1197.941')]
[36m[2025-07-02 00:34:20,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4161536. Throughput: 0: 280.3. Samples: 4175280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:34:20,992][166323] Avg episode reward: [(0, '1126.000')]
[36m[2025-07-02 00:34:25,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4161536. Throughput: 0: 276.7. Samples: 4176960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:34:25,956][166323] Avg episode reward: [(0, '1145.219')]
[36m[2025-07-02 00:34:30,949][166323] Fps is (10 sec: 1645.5, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4177920. Throughput: 0: 275.3. Samples: 4178512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:34:30,950][166323] Avg episode reward: [(0, '1158.560')]
[36m[2025-07-02 00:34:35,988][166323] Fps is (10 sec: 1633.1, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4177920. Throughput: 0: 276.5. Samples: 4179328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:34:35,988][166323] Avg episode reward: [(0, '1120.964')]
[36m[2025-07-02 00:34:40,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4177920. Throughput: 0: 280.5. Samples: 4181104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:34:40,951][166323] Avg episode reward: [(0, '1137.146')]
[36m[2025-07-02 00:34:45,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4177920. Throughput: 0: 279.7. Samples: 4182768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:34:45,950][166323] Avg episode reward: [(0, '1144.942')]
[36m[2025-07-02 00:34:50,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4177920. Throughput: 0: 279.3. Samples: 4183536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:34:50,982][166323] Avg episode reward: [(0, '1124.071')]
[36m[2025-07-02 00:34:55,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 4177920. Throughput: 0: 276.3. Samples: 4185152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:34:55,964][166323] Avg episode reward: [(0, '1123.789')]
[36m[2025-07-02 00:35:00,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4177920. Throughput: 0: 275.4. Samples: 4186720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:35:00,946][166323] Avg episode reward: [(0, '1098.867')]
[31m[14491233 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14491234 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[14491234 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:35:05,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 4177920. Throughput: 0: 272.9. Samples: 4187552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:35:05,958][166323] Avg episode reward: [(0, '1084.262')]
[36m[2025-07-02 00:35:10,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4177920. Throughput: 0: 269.7. Samples: 4189104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:35:10,976][166323] Avg episode reward: [(0, '1124.150')]
[36m[2025-07-02 00:35:15,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4177920. Throughput: 0: 271.8. Samples: 4190752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:35:15,979][166323] Avg episode reward: [(0, '1176.121')]
[36m[2025-07-02 00:35:20,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 4177920. Throughput: 0: 272.2. Samples: 4191568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:35:20,951][166323] Avg episode reward: [(0, '1169.622')]
[36m[2025-07-02 00:35:25,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4177920. Throughput: 0: 269.6. Samples: 4193248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:35:25,992][166323] Avg episode reward: [(0, '1181.762')]
[36m[2025-07-02 00:35:30,947][166323] Fps is (10 sec: 1639.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4194304. Throughput: 0: 268.1. Samples: 4194832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:35:30,947][166323] Avg episode reward: [(0, '1193.556')]
[33m[14523889 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[14523889 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.84912109375
[33mCrash Rate: 0.13720703125
[33mTimeout Rate: 0.013671875 (navigation_task.py:265)
[33m[14523889 ms][navigation_task] - WARNING : 
[33mSuccesses: 1739
[33mCrashes : 281
[33mTimeouts: 28 (navigation_task.py:268)
[36m[2025-07-02 00:35:36,000][166323] Fps is (10 sec: 1637.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4194304. Throughput: 0: 269.0. Samples: 4195648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:35:36,001][166323] Avg episode reward: [(0, '1138.483')]
[37m[1m[2025-07-02 00:35:36,067][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000008184_4194304.pth...
[36m[2025-07-02 00:35:36,073][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000008056_4128768.pth
[36m[2025-07-02 00:35:40,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4194304. Throughput: 0: 269.8. Samples: 4197296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:35:40,969][166323] Avg episode reward: [(0, '1129.631')]
[36m[2025-07-02 00:35:45,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4194304. Throughput: 0: 269.1. Samples: 4198832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:35:45,958][166323] Avg episode reward: [(0, '1039.071')]
[36m[2025-07-02 00:35:50,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4194304. Throughput: 0: 271.8. Samples: 4199792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:35:50,989][166323] Avg episode reward: [(0, '1028.235')]
[36m[2025-07-02 00:35:55,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4194304. Throughput: 0: 270.2. Samples: 4201264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:35:55,977][166323] Avg episode reward: [(0, '1033.216')]
[36m[2025-07-02 00:36:00,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4194304. Throughput: 0: 270.5. Samples: 4202928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:36:00,984][166323] Avg episode reward: [(0, '1059.743')]
[36m[2025-07-02 00:36:05,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4194304. Throughput: 0: 271.6. Samples: 4203792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:36:05,960][166323] Avg episode reward: [(0, '1088.422')]
[36m[2025-07-02 00:36:10,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4194304. Throughput: 0: 269.5. Samples: 4205376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:36:10,989][166323] Avg episode reward: [(0, '1118.906')]
[36m[2025-07-02 00:36:15,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4194304. Throughput: 0: 274.6. Samples: 4207200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:36:15,984][166323] Avg episode reward: [(0, '1094.468')]
[36m[2025-07-02 00:36:20,980][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4194304. Throughput: 0: 276.7. Samples: 4208096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:36:20,980][166323] Avg episode reward: [(0, '1208.188')]
[31m[14573707 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14573707 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[14573708 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[14574288 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14574288 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[14574288 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:36:25,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 222.2). Total num frames: 4194304. Throughput: 0: 279.6. Samples: 4209872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:36:25,946][166323] Avg episode reward: [(0, '1109.956')]
[36m[2025-07-02 00:36:30,968][166323] Fps is (10 sec: 1640.3, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4210688. Throughput: 0: 278.7. Samples: 4211376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:36:30,969][166323] Avg episode reward: [(0, '1078.650')]
[36m[2025-07-02 00:36:35,997][166323] Fps is (10 sec: 1630.1, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4210688. Throughput: 0: 278.0. Samples: 4212304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:36:35,997][166323] Avg episode reward: [(0, '1056.499')]
[31m[14588540 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14588540 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[14588540 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:36:40,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4210688. Throughput: 0: 280.8. Samples: 4213904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:36:40,986][166323] Avg episode reward: [(0, '1073.334')]
[36m[2025-07-02 00:36:45,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4210688. Throughput: 0: 279.9. Samples: 4215520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:36:45,977][166323] Avg episode reward: [(0, '1097.301')]
[31m[14598455 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14598455 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[14598455 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:36:50,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4210688. Throughput: 0: 277.9. Samples: 4216304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:36:50,985][166323] Avg episode reward: [(0, '1096.584')]
[36m[2025-07-02 00:36:55,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4210688. Throughput: 0: 279.9. Samples: 4217968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:36:55,983][166323] Avg episode reward: [(0, '1100.847')]
[36m[2025-07-02 00:37:00,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4210688. Throughput: 0: 277.7. Samples: 4219696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:37:00,979][166323] Avg episode reward: [(0, '1121.428')]
[36m[2025-07-02 00:37:05,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4210688. Throughput: 0: 274.7. Samples: 4220448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:37:05,945][166323] Avg episode reward: [(0, '1158.696')]
[36m[2025-07-02 00:37:10,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4210688. Throughput: 0: 271.1. Samples: 4222080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:37:10,974][166323] Avg episode reward: [(0, '1162.657')]
[36m[2025-07-02 00:37:15,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4210688. Throughput: 0: 274.2. Samples: 4223712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:37:15,965][166323] Avg episode reward: [(0, '1159.018')]
[36m[2025-07-02 00:37:20,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4210688. Throughput: 0: 274.4. Samples: 4224640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:37:20,947][166323] Avg episode reward: [(0, '1113.661')]
[36m[2025-07-02 00:37:25,977][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.2). Total num frames: 4210688. Throughput: 0: 278.5. Samples: 4226432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:37:25,977][166323] Avg episode reward: [(0, '1141.698')]
[36m[2025-07-02 00:37:30,968][166323] Fps is (10 sec: 1635.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4227072. Throughput: 0: 280.9. Samples: 4228160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:37:30,968][166323] Avg episode reward: [(0, '1196.356')]
[36m[2025-07-02 00:37:35,991][166323] Fps is (10 sec: 1636.1, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4227072. Throughput: 0: 281.2. Samples: 4228960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:37:35,991][166323] Avg episode reward: [(0, '1145.387')]
[37m[1m[2025-07-02 00:37:36,045][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000008248_4227072.pth...
[36m[2025-07-02 00:37:36,050][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000008120_4161536.pth
[36m[2025-07-02 00:37:40,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4227072. Throughput: 0: 282.0. Samples: 4230656. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:37:40,974][166323] Avg episode reward: [(0, '1102.960')]
[36m[2025-07-02 00:37:45,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4227072. Throughput: 0: 280.1. Samples: 4232304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:37:45,988][166323] Avg episode reward: [(0, '1116.828')]
[36m[2025-07-02 00:37:50,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4227072. Throughput: 0: 285.1. Samples: 4233280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:37:50,951][166323] Avg episode reward: [(0, '1107.418')]
[36m[2025-07-02 00:37:55,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4227072. Throughput: 0: 285.4. Samples: 4234928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:37:55,990][166323] Avg episode reward: [(0, '1107.217')]
[36m[2025-07-02 00:38:00,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4227072. Throughput: 0: 288.4. Samples: 4236688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:38:00,957][166323] Avg episode reward: [(0, '1024.664')]
[36m[2025-07-02 00:38:05,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4227072. Throughput: 0: 286.9. Samples: 4237552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:38:05,951][166323] Avg episode reward: [(0, '1028.316')]
[36m[2025-07-02 00:38:10,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4227072. Throughput: 0: 282.2. Samples: 4239136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:38:10,988][166323] Avg episode reward: [(0, '1049.214')]
[36m[2025-07-02 00:38:15,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 4227072. Throughput: 0: 282.1. Samples: 4240848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:38:15,944][166323] Avg episode reward: [(0, '1026.227')]
[36m[2025-07-02 00:38:20,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4227072. Throughput: 0: 281.7. Samples: 4241632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:38:20,969][166323] Avg episode reward: [(0, '1130.682')]
[36m[2025-07-02 00:38:25,974][166323] Fps is (10 sec: 1633.4, 60 sec: 546.2, 300 sec: 277.7). Total num frames: 4243456. Throughput: 0: 283.0. Samples: 4243392. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 00:38:25,975][166323] Avg episode reward: [(0, '1150.436')]
[36m[2025-07-02 00:38:30,945][166323] Fps is (10 sec: 1642.2, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 4243456. Throughput: 0: 281.9. Samples: 4244976. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 00:38:30,946][166323] Avg episode reward: [(0, '1129.026')]
[36m[2025-07-02 00:38:35,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4243456. Throughput: 0: 278.2. Samples: 4245808. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 00:38:35,979][166323] Avg episode reward: [(0, '1175.767')]
[36m[2025-07-02 00:38:40,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4243456. Throughput: 0: 279.9. Samples: 4247520. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 00:38:40,976][166323] Avg episode reward: [(0, '1167.637')]
[36m[2025-07-02 00:38:45,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4243456. Throughput: 0: 277.7. Samples: 4249184. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 00:38:45,948][166323] Avg episode reward: [(0, '1172.204')]
[36m[2025-07-02 00:38:50,977][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4243456. Throughput: 0: 276.5. Samples: 4250000. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 00:38:50,978][166323] Avg episode reward: [(0, '1161.221')]
[36m[2025-07-02 00:38:55,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4243456. Throughput: 0: 281.0. Samples: 4251776. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 00:38:55,974][166323] Avg episode reward: [(0, '1102.473')]
[36m[2025-07-02 00:39:00,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4243456. Throughput: 0: 279.7. Samples: 4253440. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 00:39:00,964][166323] Avg episode reward: [(0, '1103.342')]
[36m[2025-07-02 00:39:05,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4243456. Throughput: 0: 281.9. Samples: 4254320. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 00:39:05,972][166323] Avg episode reward: [(0, '1108.611')]
[36m[2025-07-02 00:39:11,001][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4243456. Throughput: 0: 278.2. Samples: 4255920. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 00:39:11,002][166323] Avg episode reward: [(0, '1094.174')]
[36m[2025-07-02 00:39:15,976][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4243456. Throughput: 0: 281.4. Samples: 4257648. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 00:39:15,976][166323] Avg episode reward: [(0, '1104.527')]
[36m[2025-07-02 00:39:20,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4243456. Throughput: 0: 280.6. Samples: 4258432. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 00:39:20,967][166323] Avg episode reward: [(0, '1033.381')]
[36m[2025-07-02 00:39:25,959][166323] Fps is (10 sec: 1641.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4259840. Throughput: 0: 279.2. Samples: 4260080. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 00:39:25,959][166323] Avg episode reward: [(0, '1035.759')]
[36m[2025-07-02 00:39:30,974][166323] Fps is (10 sec: 1637.2, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4259840. Throughput: 0: 275.8. Samples: 4261600. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 00:39:30,974][166323] Avg episode reward: [(0, '1066.909')]
[36m[2025-07-02 00:39:35,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4259840. Throughput: 0: 275.5. Samples: 4262400. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 00:39:35,981][166323] Avg episode reward: [(0, '1024.421')]
[37m[1m[2025-07-02 00:39:36,070][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000008312_4259840.pth...
[36m[2025-07-02 00:39:36,074][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000008184_4194304.pth
[36m[2025-07-02 00:39:41,001][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 4259840. Throughput: 0: 269.7. Samples: 4263920. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 00:39:41,001][166323] Avg episode reward: [(0, '1051.818')]
[36m[2025-07-02 00:39:45,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4259840. Throughput: 0: 266.3. Samples: 4265424. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 00:39:45,969][166323] Avg episode reward: [(0, '1078.087')]
[36m[2025-07-02 00:39:50,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4259840. Throughput: 0: 264.8. Samples: 4266240. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 00:39:50,979][166323] Avg episode reward: [(0, '1105.448')]
[36m[2025-07-02 00:39:55,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4259840. Throughput: 0: 269.0. Samples: 4268016. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 00:39:55,962][166323] Avg episode reward: [(0, '1143.011')]
[36m[2025-07-02 00:40:00,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4259840. Throughput: 0: 266.4. Samples: 4269632. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 00:40:00,963][166323] Avg episode reward: [(0, '1184.419')]
[36m[2025-07-02 00:40:05,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4259840. Throughput: 0: 267.9. Samples: 4270480. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 00:40:05,944][166323] Avg episode reward: [(0, '1186.355')]
[36m[2025-07-02 00:40:10,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4259840. Throughput: 0: 266.2. Samples: 4272064. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 00:40:10,984][166323] Avg episode reward: [(0, '1208.006')]
[36m[2025-07-02 00:40:15,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4259840. Throughput: 0: 271.1. Samples: 4273792. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 00:40:15,949][166323] Avg episode reward: [(0, '1183.745')]
[36m[2025-07-02 00:40:20,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4259840. Throughput: 0: 273.4. Samples: 4274704. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 00:40:20,985][166323] Avg episode reward: [(0, '1158.596')]
[36m[2025-07-02 00:40:25,951][166323] Fps is (10 sec: 1638.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4276224. Throughput: 0: 277.6. Samples: 4276400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:40:25,951][166323] Avg episode reward: [(0, '1162.729')]
[36m[2025-07-02 00:40:30,975][166323] Fps is (10 sec: 1640.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4276224. Throughput: 0: 280.1. Samples: 4278032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:40:30,975][166323] Avg episode reward: [(0, '1160.772')]
[36m[2025-07-02 00:40:35,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4276224. Throughput: 0: 281.1. Samples: 4278880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:40:35,946][166323] Avg episode reward: [(0, '1138.299')]
[36m[2025-07-02 00:40:40,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4276224. Throughput: 0: 279.4. Samples: 4280592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:40:40,978][166323] Avg episode reward: [(0, '1169.252')]
[36m[2025-07-02 00:40:45,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4276224. Throughput: 0: 279.4. Samples: 4282208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:40:45,976][166323] Avg episode reward: [(0, '1194.232')]
[36m[2025-07-02 00:40:50,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4276224. Throughput: 0: 280.8. Samples: 4283120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:40:50,964][166323] Avg episode reward: [(0, '1158.904')]
[36m[2025-07-02 00:40:55,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4276224. Throughput: 0: 284.2. Samples: 4284848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:40:55,972][166323] Avg episode reward: [(0, '1175.820')]
[31m[14847799 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14847800 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[14847800 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:41:00,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4276224. Throughput: 0: 283.5. Samples: 4286560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:41:00,979][166323] Avg episode reward: [(0, '1198.274')]
[36m[2025-07-02 00:41:05,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4276224. Throughput: 0: 280.4. Samples: 4287312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:41:05,948][166323] Avg episode reward: [(0, '1160.301')]
[36m[2025-07-02 00:41:10,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4276224. Throughput: 0: 279.1. Samples: 4288960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:41:10,947][166323] Avg episode reward: [(0, '1144.165')]
[36m[2025-07-02 00:41:15,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4276224. Throughput: 0: 278.6. Samples: 4290560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:41:15,945][166323] Avg episode reward: [(0, '1059.922')]
[36m[2025-07-02 00:41:20,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4276224. Throughput: 0: 278.2. Samples: 4291408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:41:20,974][166323] Avg episode reward: [(0, '1082.126')]
[36m[2025-07-02 00:41:25,959][166323] Fps is (10 sec: 1636.1, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4292608. Throughput: 0: 278.9. Samples: 4293136. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 00:41:25,959][166323] Avg episode reward: [(0, '1062.830')]
[36m[2025-07-02 00:41:30,947][166323] Fps is (10 sec: 1642.8, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4292608. Throughput: 0: 279.3. Samples: 4294768. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 00:41:30,947][166323] Avg episode reward: [(0, '1070.095')]
[31m[14882138 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14882139 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[14882139 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:41:35,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4292608. Throughput: 0: 277.4. Samples: 4295600. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 00:41:35,961][166323] Avg episode reward: [(0, '1079.383')]
[37m[1m[2025-07-02 00:41:36,010][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000008376_4292608.pth...
[36m[2025-07-02 00:41:36,014][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000008248_4227072.pth
[36m[2025-07-02 00:41:40,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4292608. Throughput: 0: 277.8. Samples: 4297344. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 00:41:40,953][166323] Avg episode reward: [(0, '1078.477')]
[36m[2025-07-02 00:41:45,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4292608. Throughput: 0: 279.3. Samples: 4299136. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 00:41:45,999][166323] Avg episode reward: [(0, '1085.982')]
[36m[2025-07-02 00:41:50,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4292608. Throughput: 0: 283.0. Samples: 4300048. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 00:41:50,951][166323] Avg episode reward: [(0, '1131.018')]
[36m[2025-07-02 00:41:55,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4292608. Throughput: 0: 283.1. Samples: 4301712. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 00:41:55,990][166323] Avg episode reward: [(0, '1120.836')]
[36m[2025-07-02 00:42:00,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4292608. Throughput: 0: 282.6. Samples: 4303280. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 00:42:00,953][166323] Avg episode reward: [(0, '1134.385')]
[36m[2025-07-02 00:42:05,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4292608. Throughput: 0: 283.6. Samples: 4304160. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 00:42:05,946][166323] Avg episode reward: [(0, '1094.959')]
[36m[2025-07-02 00:42:10,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4292608. Throughput: 0: 283.3. Samples: 4305888. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 00:42:10,967][166323] Avg episode reward: [(0, '1148.756')]
[36m[2025-07-02 00:42:15,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4292608. Throughput: 0: 284.5. Samples: 4307584. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 00:42:15,991][166323] Avg episode reward: [(0, '1172.821')]
[36m[2025-07-02 00:42:21,010][166323] Fps is (10 sec: 1631.3, 60 sec: 545.8, 300 sec: 333.2). Total num frames: 4308992. Throughput: 0: 284.1. Samples: 4308400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:42:21,010][166323] Avg episode reward: [(0, '1124.190')]
[36m[2025-07-02 00:42:25,958][166323] Fps is (10 sec: 1643.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4308992. Throughput: 0: 283.0. Samples: 4310080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:42:25,958][166323] Avg episode reward: [(0, '1109.493')]
[36m[2025-07-02 00:42:30,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4308992. Throughput: 0: 283.0. Samples: 4311856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:42:30,947][166323] Avg episode reward: [(0, '1119.685')]
[36m[2025-07-02 00:42:35,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4308992. Throughput: 0: 279.6. Samples: 4312640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:42:35,994][166323] Avg episode reward: [(0, '1126.284')]
[31m[14945726 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14945727 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[14945727 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[14947011 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14947011 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[14947012 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:42:40,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4308992. Throughput: 0: 279.5. Samples: 4314288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:42:40,978][166323] Avg episode reward: [(0, '1093.342')]
[36m[2025-07-02 00:42:45,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4308992. Throughput: 0: 282.8. Samples: 4316016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:42:45,987][166323] Avg episode reward: [(0, '1041.880')]
[36m[2025-07-02 00:42:51,019][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 4308992. Throughput: 0: 280.8. Samples: 4316816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:42:51,019][166323] Avg episode reward: [(0, '1091.442')]
[36m[2025-07-02 00:42:55,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4308992. Throughput: 0: 281.8. Samples: 4318576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:42:55,987][166323] Avg episode reward: [(0, '1137.976')]
[36m[2025-07-02 00:43:00,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4308992. Throughput: 0: 281.9. Samples: 4320256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:43:00,945][166323] Avg episode reward: [(0, '1117.869')]
[31m[14970033 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14970033 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[14970033 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:43:05,972][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4308992. Throughput: 0: 282.9. Samples: 4321120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:43:05,973][166323] Avg episode reward: [(0, '1125.216')]
[36m[2025-07-02 00:43:10,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4308992. Throughput: 0: 281.6. Samples: 4322752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:43:10,961][166323] Avg episode reward: [(0, '1150.630')]
[36m[2025-07-02 00:43:15,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 4308992. Throughput: 0: 279.1. Samples: 4324416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:43:15,950][166323] Avg episode reward: [(0, '1139.612')]
[36m[2025-07-02 00:43:20,977][166323] Fps is (10 sec: 1635.7, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4325376. Throughput: 0: 278.9. Samples: 4325184. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 00:43:20,977][166323] Avg episode reward: [(0, '1148.774')]
[36m[2025-07-02 00:43:25,977][166323] Fps is (10 sec: 1633.9, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4325376. Throughput: 0: 275.2. Samples: 4326672. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 00:43:25,977][166323] Avg episode reward: [(0, '1150.050')]
[36m[2025-07-02 00:43:30,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4325376. Throughput: 0: 274.4. Samples: 4328352. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 00:43:30,951][166323] Avg episode reward: [(0, '1115.766')]
[31m[15003063 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15003063 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[15003063 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:43:35,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4325376. Throughput: 0: 277.0. Samples: 4329264. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 00:43:35,958][166323] Avg episode reward: [(0, '1109.662')]
[37m[1m[2025-07-02 00:43:36,010][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000008440_4325376.pth...
[36m[2025-07-02 00:43:36,014][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000008312_4259840.pth
[31m[15006357 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15006357 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[15006357 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:43:40,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4325376. Throughput: 0: 274.7. Samples: 4330928. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 00:43:40,959][166323] Avg episode reward: [(0, '1127.296')]
[36m[2025-07-02 00:43:45,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4325376. Throughput: 0: 271.9. Samples: 4332496. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 00:43:45,959][166323] Avg episode reward: [(0, '1116.094')]
[36m[2025-07-02 00:43:50,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4325376. Throughput: 0: 271.2. Samples: 4333328. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 00:43:50,985][166323] Avg episode reward: [(0, '1191.918')]
[36m[2025-07-02 00:43:55,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4325376. Throughput: 0: 271.9. Samples: 4334992. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 00:43:55,973][166323] Avg episode reward: [(0, '1200.015')]
[31m[15025264 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15025264 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[15025265 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:44:00,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4325376. Throughput: 0: 274.8. Samples: 4336784. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 00:44:00,949][166323] Avg episode reward: [(0, '1174.651')]
[36m[2025-07-02 00:44:05,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4325376. Throughput: 0: 279.6. Samples: 4337760. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 00:44:05,961][166323] Avg episode reward: [(0, '1191.594')]
[36m[2025-07-02 00:44:10,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4325376. Throughput: 0: 287.5. Samples: 4339600. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 00:44:10,952][166323] Avg episode reward: [(0, '1194.245')]
[36m[2025-07-02 00:44:15,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4325376. Throughput: 0: 288.7. Samples: 4341344. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 00:44:15,946][166323] Avg episode reward: [(0, '1201.728')]
[36m[2025-07-02 00:44:20,991][166323] Fps is (10 sec: 1631.9, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4341760. Throughput: 0: 286.0. Samples: 4342144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:44:20,991][166323] Avg episode reward: [(0, '1233.092')]
[36m[2025-07-02 00:44:25,951][166323] Fps is (10 sec: 1637.5, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4341760. Throughput: 0: 284.5. Samples: 4343728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:44:25,952][166323] Avg episode reward: [(0, '1178.679')]
[31m[15056559 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15056560 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[15056561 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:44:30,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4341760. Throughput: 0: 288.4. Samples: 4345472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:44:30,945][166323] Avg episode reward: [(0, '1178.643')]
[36m[2025-07-02 00:44:35,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4341760. Throughput: 0: 289.4. Samples: 4346352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:44:35,982][166323] Avg episode reward: [(0, '1203.428')]
[36m[2025-07-02 00:44:40,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4341760. Throughput: 0: 292.4. Samples: 4348144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:44:40,956][166323] Avg episode reward: [(0, '1217.302')]
[36m[2025-07-02 00:44:45,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4341760. Throughput: 0: 293.2. Samples: 4349984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:44:45,968][166323] Avg episode reward: [(0, '1174.347')]
[36m[2025-07-02 00:44:50,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4341760. Throughput: 0: 289.4. Samples: 4350784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:44:50,963][166323] Avg episode reward: [(0, '1172.874')]
[36m[2025-07-02 00:44:55,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4341760. Throughput: 0: 289.1. Samples: 4352624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:44:55,998][166323] Avg episode reward: [(0, '1188.980')]
[36m[2025-07-02 00:45:00,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4341760. Throughput: 0: 287.7. Samples: 4354304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:45:00,988][166323] Avg episode reward: [(0, '1243.126')]
[36m[2025-07-02 00:45:05,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4341760. Throughput: 0: 290.7. Samples: 4355216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:45:05,956][166323] Avg episode reward: [(0, '1191.768')]
[36m[2025-07-02 00:45:10,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4341760. Throughput: 0: 294.5. Samples: 4356992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:45:10,988][166323] Avg episode reward: [(0, '1205.998')]
[36m[2025-07-02 00:45:15,972][166323] Fps is (10 sec: 1635.8, 60 sec: 545.9, 300 sec: 333.2). Total num frames: 4358144. Throughput: 0: 291.4. Samples: 4358592. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 00:45:15,972][166323] Avg episode reward: [(0, '1173.056')]
[36m[2025-07-02 00:45:20,947][166323] Fps is (10 sec: 1645.1, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 4358144. Throughput: 0: 289.3. Samples: 4359360. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 00:45:20,947][166323] Avg episode reward: [(0, '1150.311')]
[36m[2025-07-02 00:45:25,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4358144. Throughput: 0: 285.6. Samples: 4361008. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 00:45:25,991][166323] Avg episode reward: [(0, '1161.288')]
[36m[2025-07-02 00:45:30,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4358144. Throughput: 0: 278.7. Samples: 4362528. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 00:45:30,985][166323] Avg episode reward: [(0, '1130.810')]
[31m[15122523 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15122524 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[15122524 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[15123577 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15123578 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[15123578 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:45:35,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4358144. Throughput: 0: 276.2. Samples: 4363216. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 00:45:35,978][166323] Avg episode reward: [(0, '1065.112')]
[37m[1m[2025-07-02 00:45:36,040][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000008504_4358144.pth...
[36m[2025-07-02 00:45:36,045][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000008376_4292608.pth
[31m[15128549 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15128550 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[15128550 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:45:40,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4358144. Throughput: 0: 271.3. Samples: 4364832. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 00:45:40,995][166323] Avg episode reward: [(0, '1035.521')]
[36m[2025-07-02 00:45:46,001][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4358144. Throughput: 0: 269.4. Samples: 4366432. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 00:45:46,002][166323] Avg episode reward: [(0, '1055.945')]
[36m[2025-07-02 00:45:50,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4358144. Throughput: 0: 269.8. Samples: 4367360. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 00:45:50,959][166323] Avg episode reward: [(0, '1065.756')]
[36m[2025-07-02 00:45:55,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 4358144. Throughput: 0: 269.7. Samples: 4369120. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 00:45:55,954][166323] Avg episode reward: [(0, '1049.281')]
[36m[2025-07-02 00:46:00,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4358144. Throughput: 0: 271.9. Samples: 4370832. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 00:46:00,982][166323] Avg episode reward: [(0, '1025.045')]
[31m[15149655 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15149655 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[15149656 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[15150697 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15150697 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[15150698 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:46:06,002][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 4358144. Throughput: 0: 274.2. Samples: 4371712. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 00:46:06,003][166323] Avg episode reward: [(0, '1053.867')]
[36m[2025-07-02 00:46:10,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 4358144. Throughput: 0: 271.6. Samples: 4373216. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 00:46:10,945][166323] Avg episode reward: [(0, '1077.073')]
[31m[15161821 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15161822 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[15161822 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:46:15,951][166323] Fps is (10 sec: 1646.7, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 4374528. Throughput: 0: 272.9. Samples: 4374800. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 00:46:15,952][166323] Avg episode reward: [(0, '999.234')]
[36m[2025-07-02 00:46:20,954][166323] Fps is (10 sec: 1637.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4374528. Throughput: 0: 278.5. Samples: 4375744. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 00:46:20,954][166323] Avg episode reward: [(0, '1055.686')]
[36m[2025-07-02 00:46:25,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4374528. Throughput: 0: 283.8. Samples: 4377600. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 00:46:25,983][166323] Avg episode reward: [(0, '1085.087')]
[36m[2025-07-02 00:46:30,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4374528. Throughput: 0: 289.3. Samples: 4379440. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 00:46:30,969][166323] Avg episode reward: [(0, '1099.849')]
[36m[2025-07-02 00:46:35,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4374528. Throughput: 0: 287.2. Samples: 4380288. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 00:46:35,967][166323] Avg episode reward: [(0, '1187.313')]
[36m[2025-07-02 00:46:40,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4374528. Throughput: 0: 287.2. Samples: 4382048. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 00:46:40,967][166323] Avg episode reward: [(0, '1215.257')]
[36m[2025-07-02 00:46:45,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4374528. Throughput: 0: 287.7. Samples: 4383776. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 00:46:45,979][166323] Avg episode reward: [(0, '1245.719')]
[31m[15197412 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15197412 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[15197412 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:46:50,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4374528. Throughput: 0: 288.1. Samples: 4384672. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 00:46:50,987][166323] Avg episode reward: [(0, '1179.218')]
[33m[15202203 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[15202204 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.8584675192832947
[33mCrash Rate: 0.13372376561164856
[33mTimeout Rate: 0.007808687165379524 (navigation_task.py:265)
[33m[15202204 ms][navigation_task] - WARNING : 
[33mSuccesses: 1759
[33mCrashes : 274
[33mTimeouts: 16 (navigation_task.py:268)
[36m[2025-07-02 00:46:55,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4374528. Throughput: 0: 290.6. Samples: 4386304. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 00:46:55,977][166323] Avg episode reward: [(0, '1215.252')]
[36m[2025-07-02 00:47:01,003][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 4374528. Throughput: 0: 292.6. Samples: 4387984. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 00:47:01,003][166323] Avg episode reward: [(0, '1229.032')]
[36m[2025-07-02 00:47:05,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4374528. Throughput: 0: 289.6. Samples: 4388784. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 00:47:05,975][166323] Avg episode reward: [(0, '1225.022')]
[36m[2025-07-02 00:47:10,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4374528. Throughput: 0: 285.8. Samples: 4390464. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 00:47:10,988][166323] Avg episode reward: [(0, '1180.960')]
[36m[2025-07-02 00:47:15,953][166323] Fps is (10 sec: 1641.9, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4390912. Throughput: 0: 284.2. Samples: 4392224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:47:15,953][166323] Avg episode reward: [(0, '1207.023')]
[36m[2025-07-02 00:47:20,988][166323] Fps is (10 sec: 1638.3, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4390912. Throughput: 0: 284.0. Samples: 4393072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:47:20,988][166323] Avg episode reward: [(0, '1185.058')]
[36m[2025-07-02 00:47:25,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4390912. Throughput: 0: 280.4. Samples: 4394672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:47:25,983][166323] Avg episode reward: [(0, '1190.408')]
[36m[2025-07-02 00:47:30,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4390912. Throughput: 0: 283.3. Samples: 4396528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:47:30,984][166323] Avg episode reward: [(0, '1174.969')]
[36m[2025-07-02 00:47:35,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4390912. Throughput: 0: 285.0. Samples: 4397488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:47:35,952][166323] Avg episode reward: [(0, '1163.696')]
[37m[1m[2025-07-02 00:47:36,011][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000008568_4390912.pth...
[36m[2025-07-02 00:47:36,016][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000008440_4325376.pth
[31m[15248427 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15248428 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[15248428 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:47:40,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4390912. Throughput: 0: 283.4. Samples: 4399056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:47:40,975][166323] Avg episode reward: [(0, '1112.098')]
[36m[2025-07-02 00:47:45,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 4390912. Throughput: 0: 282.7. Samples: 4400688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:47:45,948][166323] Avg episode reward: [(0, '1085.358')]
[36m[2025-07-02 00:47:51,006][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4390912. Throughput: 0: 282.5. Samples: 4401504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:47:51,006][166323] Avg episode reward: [(0, '1060.812')]
[36m[2025-07-02 00:47:55,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4390912. Throughput: 0: 285.0. Samples: 4403280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:47:55,958][166323] Avg episode reward: [(0, '1144.161')]
[36m[2025-07-02 00:48:00,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4390912. Throughput: 0: 281.7. Samples: 4404912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:48:00,987][166323] Avg episode reward: [(0, '1181.825')]
[36m[2025-07-02 00:48:06,022][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 4390912. Throughput: 0: 282.8. Samples: 4405808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:48:06,023][166323] Avg episode reward: [(0, '1111.637')]
[36m[2025-07-02 00:48:10,964][166323] Fps is (10 sec: 1642.2, 60 sec: 546.3, 300 sec: 333.2). Total num frames: 4407296. Throughput: 0: 284.9. Samples: 4407488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:48:10,964][166323] Avg episode reward: [(0, '1122.898')]
[31m[15283409 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15283409 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[15283409 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:48:15,965][166323] Fps is (10 sec: 1647.8, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4407296. Throughput: 0: 285.6. Samples: 4409376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:48:15,966][166323] Avg episode reward: [(0, '1171.574')]
[36m[2025-07-02 00:48:20,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4407296. Throughput: 0: 283.6. Samples: 4410256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:48:20,971][166323] Avg episode reward: [(0, '1186.983')]
[36m[2025-07-02 00:48:25,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4407296. Throughput: 0: 286.2. Samples: 4411936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:48:25,975][166323] Avg episode reward: [(0, '1175.535')]
[36m[2025-07-02 00:48:30,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 4407296. Throughput: 0: 289.4. Samples: 4413712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:48:30,944][166323] Avg episode reward: [(0, '1135.997')]
[36m[2025-07-02 00:48:35,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4407296. Throughput: 0: 290.4. Samples: 4414560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:48:35,968][166323] Avg episode reward: [(0, '1108.892')]
[36m[2025-07-02 00:48:40,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4407296. Throughput: 0: 291.0. Samples: 4416384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:48:40,987][166323] Avg episode reward: [(0, '1076.835')]
[36m[2025-07-02 00:48:45,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4407296. Throughput: 0: 295.7. Samples: 4418208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:48:45,949][166323] Avg episode reward: [(0, '1064.805')]
[36m[2025-07-02 00:48:50,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 4407296. Throughput: 0: 294.9. Samples: 4419056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:48:50,948][166323] Avg episode reward: [(0, '1046.774')]
[36m[2025-07-02 00:48:55,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4407296. Throughput: 0: 292.3. Samples: 4420640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:48:55,952][166323] Avg episode reward: [(0, '1006.903')]
[36m[2025-07-02 00:49:00,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4407296. Throughput: 0: 286.1. Samples: 4422256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:49:00,989][166323] Avg episode reward: [(0, '975.457')]
[36m[2025-07-02 00:49:06,155][166323] Fps is (10 sec: 1605.8, 60 sec: 544.9, 300 sec: 333.0). Total num frames: 4423680. Throughput: 0: 284.7. Samples: 4423120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:49:06,155][166323] Avg episode reward: [(0, '1021.646')]
[36m[2025-07-02 00:49:10,976][166323] Fps is (10 sec: 1640.5, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 4423680. Throughput: 0: 283.0. Samples: 4424672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:49:10,977][166323] Avg episode reward: [(0, '1063.639')]
[36m[2025-07-02 00:49:15,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4423680. Throughput: 0: 283.7. Samples: 4426480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:49:15,956][166323] Avg episode reward: [(0, '1071.542')]
[36m[2025-07-02 00:49:20,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4423680. Throughput: 0: 283.2. Samples: 4427312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:49:20,989][166323] Avg episode reward: [(0, '1100.631')]
[36m[2025-07-02 00:49:25,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4423680. Throughput: 0: 279.9. Samples: 4428976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:49:25,976][166323] Avg episode reward: [(0, '1100.302')]
[36m[2025-07-02 00:49:30,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4423680. Throughput: 0: 276.2. Samples: 4430640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:49:30,954][166323] Avg episode reward: [(0, '1143.191')]
[36m[2025-07-02 00:49:35,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4423680. Throughput: 0: 276.6. Samples: 4431504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:49:35,947][166323] Avg episode reward: [(0, '1104.086')]
[37m[1m[2025-07-02 00:49:36,002][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000008632_4423680.pth...
[36m[2025-07-02 00:49:36,006][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000008504_4358144.pth
[36m[2025-07-02 00:49:40,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 4423680. Throughput: 0: 279.5. Samples: 4433216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:49:40,945][166323] Avg episode reward: [(0, '1128.400')]
[36m[2025-07-02 00:49:45,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4423680. Throughput: 0: 282.7. Samples: 4434976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:49:45,989][166323] Avg episode reward: [(0, '1133.348')]
[36m[2025-07-02 00:49:50,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4423680. Throughput: 0: 283.9. Samples: 4435840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:49:50,958][166323] Avg episode reward: [(0, '1128.395')]
[36m[2025-07-02 00:49:55,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4423680. Throughput: 0: 282.2. Samples: 4437360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:49:55,944][166323] Avg episode reward: [(0, '1083.076')]
[36m[2025-07-02 00:50:00,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4423680. Throughput: 0: 279.7. Samples: 4439072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:50:00,978][166323] Avg episode reward: [(0, '1127.525')]
[36m[2025-07-02 00:50:05,968][166323] Fps is (10 sec: 1634.4, 60 sec: 273.9, 300 sec: 333.3). Total num frames: 4440064. Throughput: 0: 280.3. Samples: 4439920. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:50:05,968][166323] Avg episode reward: [(0, '1153.523')]
[36m[2025-07-02 00:50:10,954][166323] Fps is (10 sec: 1642.3, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4440064. Throughput: 0: 278.9. Samples: 4441520. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:50:10,954][166323] Avg episode reward: [(0, '1122.398')]
[36m[2025-07-02 00:50:15,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4440064. Throughput: 0: 281.8. Samples: 4443328. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:50:15,972][166323] Avg episode reward: [(0, '1078.576')]
[36m[2025-07-02 00:50:20,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4440064. Throughput: 0: 280.5. Samples: 4444128. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:50:20,956][166323] Avg episode reward: [(0, '1121.665')]
[36m[2025-07-02 00:50:25,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4440064. Throughput: 0: 281.4. Samples: 4445888. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:50:25,970][166323] Avg episode reward: [(0, '1078.288')]
[36m[2025-07-02 00:50:31,023][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 4440064. Throughput: 0: 280.3. Samples: 4447600. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:50:31,023][166323] Avg episode reward: [(0, '1089.780')]
[36m[2025-07-02 00:50:35,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4440064. Throughput: 0: 278.0. Samples: 4448352. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:50:35,969][166323] Avg episode reward: [(0, '1068.550')]
[31m[15425089 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15425089 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[15425089 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:50:40,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4440064. Throughput: 0: 276.8. Samples: 4449824. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:50:40,965][166323] Avg episode reward: [(0, '1093.098')]
[36m[2025-07-02 00:50:45,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4440064. Throughput: 0: 276.1. Samples: 4451488. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:50:45,952][166323] Avg episode reward: [(0, '1130.776')]
[36m[2025-07-02 00:50:50,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4440064. Throughput: 0: 272.8. Samples: 4452192. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:50:50,951][166323] Avg episode reward: [(0, '1069.043')]
[36m[2025-07-02 00:50:55,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 4440064. Throughput: 0: 277.4. Samples: 4454016. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:50:55,995][166323] Avg episode reward: [(0, '1100.403')]
[31m[15446797 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15446797 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[15446797 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:51:00,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4440064. Throughput: 0: 278.9. Samples: 4455872. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 00:51:00,952][166323] Avg episode reward: [(0, '1162.494')]
[36m[2025-07-02 00:51:05,983][166323] Fps is (10 sec: 1640.2, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 4456448. Throughput: 0: 279.7. Samples: 4456720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:51:05,984][166323] Avg episode reward: [(0, '1171.608')]
[36m[2025-07-02 00:51:10,972][166323] Fps is (10 sec: 1635.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4456448. Throughput: 0: 278.7. Samples: 4458432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:51:10,972][166323] Avg episode reward: [(0, '1169.051')]
[36m[2025-07-02 00:51:15,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4456448. Throughput: 0: 278.1. Samples: 4460096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:51:15,949][166323] Avg episode reward: [(0, '1184.972')]
[36m[2025-07-02 00:51:20,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4456448. Throughput: 0: 280.6. Samples: 4460976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:51:20,953][166323] Avg episode reward: [(0, '1157.784')]
[36m[2025-07-02 00:51:25,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4456448. Throughput: 0: 286.7. Samples: 4462720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:51:25,943][166323] Avg episode reward: [(0, '1145.784')]
[36m[2025-07-02 00:51:30,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 4456448. Throughput: 0: 288.3. Samples: 4464464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:51:30,954][166323] Avg episode reward: [(0, '1112.561')]
[36m[2025-07-02 00:51:35,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4456448. Throughput: 0: 291.1. Samples: 4465296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:51:35,960][166323] Avg episode reward: [(0, '1070.684')]
[37m[1m[2025-07-02 00:51:36,017][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000008696_4456448.pth...
[36m[2025-07-02 00:51:36,021][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000008568_4390912.pth
[36m[2025-07-02 00:51:40,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4456448. Throughput: 0: 286.4. Samples: 4466896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:51:40,962][166323] Avg episode reward: [(0, '1082.646')]
[36m[2025-07-02 00:51:45,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4456448. Throughput: 0: 279.7. Samples: 4468464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:51:45,964][166323] Avg episode reward: [(0, '1085.953')]
[36m[2025-07-02 00:51:50,980][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4456448. Throughput: 0: 278.1. Samples: 4469232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:51:50,980][166323] Avg episode reward: [(0, '1074.229')]
[36m[2025-07-02 00:51:55,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4456448. Throughput: 0: 278.7. Samples: 4470976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:51:55,979][166323] Avg episode reward: [(0, '1103.028')]
[36m[2025-07-02 00:52:00,961][166323] Fps is (10 sec: 1641.4, 60 sec: 546.0, 300 sec: 333.2). Total num frames: 4472832. Throughput: 0: 282.6. Samples: 4472816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:52:00,961][166323] Avg episode reward: [(0, '1126.390')]
[36m[2025-07-02 00:52:05,984][166323] Fps is (10 sec: 1637.5, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 4472832. Throughput: 0: 280.3. Samples: 4473600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:52:05,985][166323] Avg episode reward: [(0, '1159.831')]
[36m[2025-07-02 00:52:10,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4472832. Throughput: 0: 280.1. Samples: 4475328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:52:10,960][166323] Avg episode reward: [(0, '1220.373')]
[36m[2025-07-02 00:52:15,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4472832. Throughput: 0: 276.8. Samples: 4476928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:52:15,986][166323] Avg episode reward: [(0, '1157.357')]
[36m[2025-07-02 00:52:20,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4472832. Throughput: 0: 277.5. Samples: 4477792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:52:20,990][166323] Avg episode reward: [(0, '1096.305')]
[36m[2025-07-02 00:52:25,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4472832. Throughput: 0: 280.6. Samples: 4479520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:52:25,945][166323] Avg episode reward: [(0, '1129.139')]
[36m[2025-07-02 00:52:30,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4472832. Throughput: 0: 284.8. Samples: 4481280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:52:30,964][166323] Avg episode reward: [(0, '1169.096')]
[36m[2025-07-02 00:52:35,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4472832. Throughput: 0: 286.4. Samples: 4482112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:52:35,947][166323] Avg episode reward: [(0, '1161.255')]
[36m[2025-07-02 00:52:40,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4472832. Throughput: 0: 286.2. Samples: 4483856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:52:40,986][166323] Avg episode reward: [(0, '1165.635')]
[36m[2025-07-02 00:52:45,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4472832. Throughput: 0: 286.1. Samples: 4485696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:52:45,981][166323] Avg episode reward: [(0, '1157.808')]
[36m[2025-07-02 00:52:51,004][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4472832. Throughput: 0: 287.2. Samples: 4486528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:52:51,005][166323] Avg episode reward: [(0, '1270.395')]
[36m[2025-07-02 00:52:55,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4472832. Throughput: 0: 283.0. Samples: 4488064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:52:55,963][166323] Avg episode reward: [(0, '1269.256')]
[36m[2025-07-02 00:53:00,979][166323] Fps is (10 sec: 1642.6, 60 sec: 273.0, 300 sec: 333.3). Total num frames: 4489216. Throughput: 0: 283.4. Samples: 4489680. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 00:53:00,979][166323] Avg episode reward: [(0, '1191.954')]
[36m[2025-07-02 00:53:05,965][166323] Fps is (10 sec: 1638.1, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4489216. Throughput: 0: 282.8. Samples: 4490512. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 00:53:05,965][166323] Avg episode reward: [(0, '1169.016')]
[31m[15574828 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15574829 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[15574829 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:53:10,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4489216. Throughput: 0: 282.0. Samples: 4492224. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 00:53:10,988][166323] Avg episode reward: [(0, '1162.330')]
[31m[15584397 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15584397 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[15584397 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:53:15,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4489216. Throughput: 0: 278.1. Samples: 4493792. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 00:53:15,952][166323] Avg episode reward: [(0, '1117.952')]
[36m[2025-07-02 00:53:20,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 4489216. Throughput: 0: 278.4. Samples: 4494640. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 00:53:20,948][166323] Avg episode reward: [(0, '1065.121')]
[36m[2025-07-02 00:53:25,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4489216. Throughput: 0: 275.4. Samples: 4496240. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 00:53:25,945][166323] Avg episode reward: [(0, '961.568')]
[36m[2025-07-02 00:53:30,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4489216. Throughput: 0: 275.3. Samples: 4498080. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 00:53:30,965][166323] Avg episode reward: [(0, '971.200')]
[36m[2025-07-02 00:53:35,979][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4489216. Throughput: 0: 276.1. Samples: 4498944. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 00:53:35,979][166323] Avg episode reward: [(0, '1015.788')]
[37m[1m[2025-07-02 00:53:36,033][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000008760_4489216.pth...
[36m[2025-07-02 00:53:36,037][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000008632_4423680.pth
[36m[2025-07-02 00:53:40,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4489216. Throughput: 0: 282.9. Samples: 4500800. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 00:53:40,981][166323] Avg episode reward: [(0, '1062.558')]
[36m[2025-07-02 00:53:45,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4489216. Throughput: 0: 282.5. Samples: 4502384. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 00:53:45,956][166323] Avg episode reward: [(0, '1040.936')]
[31m[15616378 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15616378 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[15616378 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:53:50,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4489216. Throughput: 0: 284.4. Samples: 4503312. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 00:53:50,973][166323] Avg episode reward: [(0, '1081.925')]
[36m[2025-07-02 00:53:55,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4489216. Throughput: 0: 284.5. Samples: 4505024. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 00:53:55,984][166323] Avg episode reward: [(0, '1088.406')]
[36m[2025-07-02 00:54:01,011][166323] Fps is (10 sec: 1632.1, 60 sec: 272.9, 300 sec: 277.8). Total num frames: 4505600. Throughput: 0: 288.7. Samples: 4506800. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 00:54:01,012][166323] Avg episode reward: [(0, '1143.833')]
[36m[2025-07-02 00:54:05,977][166323] Fps is (10 sec: 1639.4, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4505600. Throughput: 0: 288.2. Samples: 4507616. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 00:54:05,978][166323] Avg episode reward: [(0, '1197.741')]
[36m[2025-07-02 00:54:10,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4505600. Throughput: 0: 290.6. Samples: 4509328. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 00:54:10,981][166323] Avg episode reward: [(0, '1202.116')]
[36m[2025-07-02 00:54:15,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4505600. Throughput: 0: 286.5. Samples: 4510976. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 00:54:15,981][166323] Avg episode reward: [(0, '1106.473')]
[36m[2025-07-02 00:54:20,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4505600. Throughput: 0: 286.0. Samples: 4511808. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 00:54:20,959][166323] Avg episode reward: [(0, '1117.656')]
[36m[2025-07-02 00:54:26,012][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 4505600. Throughput: 0: 283.2. Samples: 4513552. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 00:54:26,012][166323] Avg episode reward: [(0, '1066.834')]
[36m[2025-07-02 00:54:30,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4505600. Throughput: 0: 285.2. Samples: 4515216. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 00:54:30,951][166323] Avg episode reward: [(0, '1036.600')]
[36m[2025-07-02 00:54:35,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4505600. Throughput: 0: 285.3. Samples: 4516144. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 00:54:35,957][166323] Avg episode reward: [(0, '1023.803')]
[36m[2025-07-02 00:54:40,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4505600. Throughput: 0: 286.7. Samples: 4517920. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 00:54:40,972][166323] Avg episode reward: [(0, '973.242')]
[36m[2025-07-02 00:54:45,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4505600. Throughput: 0: 284.3. Samples: 4519584. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 00:54:45,985][166323] Avg episode reward: [(0, '1063.637')]
[36m[2025-07-02 00:54:50,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4505600. Throughput: 0: 285.4. Samples: 4520464. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 00:54:50,990][166323] Avg episode reward: [(0, '1078.585')]
[36m[2025-07-02 00:54:55,951][166323] Fps is (10 sec: 1644.0, 60 sec: 546.4, 300 sec: 333.3). Total num frames: 4521984. Throughput: 0: 284.6. Samples: 4522128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:54:55,951][166323] Avg episode reward: [(0, '1157.347')]
[31m[15686190 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15686190 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[15686190 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:55:00,985][166323] Fps is (10 sec: 1639.3, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4521984. Throughput: 0: 283.7. Samples: 4523744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:55:00,985][166323] Avg episode reward: [(0, '1118.146')]
[36m[2025-07-02 00:55:05,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4521984. Throughput: 0: 285.1. Samples: 4524640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:55:05,961][166323] Avg episode reward: [(0, '1168.061')]
[31m[15697309 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15697310 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[15697310 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:55:10,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4521984. Throughput: 0: 285.3. Samples: 4526384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:55:10,996][166323] Avg episode reward: [(0, '1169.715')]
[36m[2025-07-02 00:55:15,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4521984. Throughput: 0: 281.8. Samples: 4527904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:55:15,974][166323] Avg episode reward: [(0, '1207.074')]
[36m[2025-07-02 00:55:20,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4521984. Throughput: 0: 281.6. Samples: 4528816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:55:20,959][166323] Avg episode reward: [(0, '1137.669')]
[36m[2025-07-02 00:55:25,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4521984. Throughput: 0: 279.4. Samples: 4530496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:55:25,981][166323] Avg episode reward: [(0, '1144.364')]
[36m[2025-07-02 00:55:30,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4521984. Throughput: 0: 278.9. Samples: 4532128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:55:30,960][166323] Avg episode reward: [(0, '1148.827')]
[36m[2025-07-02 00:55:35,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4521984. Throughput: 0: 278.3. Samples: 4532976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:55:35,955][166323] Avg episode reward: [(0, '1195.169')]
[37m[1m[2025-07-02 00:55:36,005][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000008824_4521984.pth...
[36m[2025-07-02 00:55:36,009][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000008696_4456448.pth
[36m[2025-07-02 00:55:40,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4521984. Throughput: 0: 281.0. Samples: 4534784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:55:40,997][166323] Avg episode reward: [(0, '1184.888')]
[36m[2025-07-02 00:55:46,001][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 4521984. Throughput: 0: 282.6. Samples: 4536464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:55:46,001][166323] Avg episode reward: [(0, '1126.396')]
[36m[2025-07-02 00:55:50,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4521984. Throughput: 0: 281.1. Samples: 4537296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:55:50,989][166323] Avg episode reward: [(0, '1142.619')]
[36m[2025-07-02 00:55:55,963][166323] Fps is (10 sec: 1644.5, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 4538368. Throughput: 0: 279.7. Samples: 4538960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:55:55,964][166323] Avg episode reward: [(0, '1189.172')]
[31m[15747871 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15747871 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[15747871 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:56:00,975][166323] Fps is (10 sec: 1640.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4538368. Throughput: 0: 280.9. Samples: 4540544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:56:00,975][166323] Avg episode reward: [(0, '1082.366')]
[36m[2025-07-02 00:56:05,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4538368. Throughput: 0: 281.5. Samples: 4541488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:56:05,972][166323] Avg episode reward: [(0, '1088.752')]
[36m[2025-07-02 00:56:10,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 4538368. Throughput: 0: 281.8. Samples: 4543168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:56:10,956][166323] Avg episode reward: [(0, '1084.116')]
[36m[2025-07-02 00:56:15,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4538368. Throughput: 0: 282.8. Samples: 4544864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:56:15,998][166323] Avg episode reward: [(0, '1098.630')]
[36m[2025-07-02 00:56:20,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4538368. Throughput: 0: 283.6. Samples: 4545744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:56:20,970][166323] Avg episode reward: [(0, '1144.993')]
[36m[2025-07-02 00:56:25,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4538368. Throughput: 0: 282.9. Samples: 4547504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:56:25,961][166323] Avg episode reward: [(0, '1081.044')]
[36m[2025-07-02 00:56:30,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4538368. Throughput: 0: 284.1. Samples: 4549248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:56:30,991][166323] Avg episode reward: [(0, '1118.009')]
[36m[2025-07-02 00:56:35,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4538368. Throughput: 0: 282.7. Samples: 4550016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:56:35,976][166323] Avg episode reward: [(0, '1133.455')]
[36m[2025-07-02 00:56:40,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4538368. Throughput: 0: 283.2. Samples: 4551712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:56:40,986][166323] Avg episode reward: [(0, '1139.534')]
[36m[2025-07-02 00:56:45,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 4538368. Throughput: 0: 283.9. Samples: 4553312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:56:45,950][166323] Avg episode reward: [(0, '1074.556')]
[31m[15797205 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15797206 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[15797206 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:56:50,957][166323] Fps is (10 sec: 1643.1, 60 sec: 546.4, 300 sec: 333.3). Total num frames: 4554752. Throughput: 0: 283.1. Samples: 4554224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:56:50,957][166323] Avg episode reward: [(0, '1073.501')]
[36m[2025-07-02 00:56:55,983][166323] Fps is (10 sec: 1633.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4554752. Throughput: 0: 278.6. Samples: 4555712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:56:55,983][166323] Avg episode reward: [(0, '1085.402')]
[31m[15807146 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15807146 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[15807147 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:57:00,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4554752. Throughput: 0: 278.9. Samples: 4557408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:57:00,969][166323] Avg episode reward: [(0, '1096.386')]
[36m[2025-07-02 00:57:06,003][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4554752. Throughput: 0: 275.0. Samples: 4558128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:57:06,004][166323] Avg episode reward: [(0, '1081.829')]
[36m[2025-07-02 00:57:10,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4554752. Throughput: 0: 274.6. Samples: 4559856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:57:10,947][166323] Avg episode reward: [(0, '1091.291')]
[36m[2025-07-02 00:57:15,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 4554752. Throughput: 0: 274.7. Samples: 4561600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:57:15,951][166323] Avg episode reward: [(0, '1133.442')]
[31m[15828108 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15828109 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[15828109 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:57:21,011][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 4554752. Throughput: 0: 278.5. Samples: 4562560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:57:21,012][166323] Avg episode reward: [(0, '1174.825')]
[36m[2025-07-02 00:57:25,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4554752. Throughput: 0: 281.6. Samples: 4564384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:57:25,979][166323] Avg episode reward: [(0, '1197.519')]
[36m[2025-07-02 00:57:31,006][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 4554752. Throughput: 0: 281.2. Samples: 4565984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:57:31,007][166323] Avg episode reward: [(0, '1230.353')]
[36m[2025-07-02 00:57:35,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4554752. Throughput: 0: 278.3. Samples: 4566752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:57:35,975][166323] Avg episode reward: [(0, '1218.634')]
[37m[1m[2025-07-02 00:57:36,043][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000008888_4554752.pth...
[36m[2025-07-02 00:57:36,048][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000008760_4489216.pth
[36m[2025-07-02 00:57:40,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4554752. Throughput: 0: 281.2. Samples: 4568368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:57:40,985][166323] Avg episode reward: [(0, '1208.098')]
[36m[2025-07-02 00:57:46,011][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 4554752. Throughput: 0: 282.0. Samples: 4570112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:57:46,012][166323] Avg episode reward: [(0, '1176.670')]
[36m[2025-07-02 00:57:50,968][166323] Fps is (10 sec: 1641.2, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 4571136. Throughput: 0: 282.9. Samples: 4570848. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 00:57:50,968][166323] Avg episode reward: [(0, '1139.707')]
[36m[2025-07-02 00:57:56,006][166323] Fps is (10 sec: 1639.3, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4571136. Throughput: 0: 280.5. Samples: 4572496. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 00:57:56,006][166323] Avg episode reward: [(0, '1127.333')]
[36m[2025-07-02 00:58:00,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4571136. Throughput: 0: 276.6. Samples: 4574048. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 00:58:00,962][166323] Avg episode reward: [(0, '1073.790')]
[33m[15871213 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[15871214 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.84912109375
[33mCrash Rate: 0.140625
[33mTimeout Rate: 0.01025390625 (navigation_task.py:265)
[33m[15871214 ms][navigation_task] - WARNING : 
[33mSuccesses: 1739
[33mCrashes : 288
[33mTimeouts: 21 (navigation_task.py:268)
[36m[2025-07-02 00:58:05,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4571136. Throughput: 0: 275.8. Samples: 4574960. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 00:58:05,972][166323] Avg episode reward: [(0, '1070.914')]
[36m[2025-07-02 00:58:11,001][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 4571136. Throughput: 0: 271.5. Samples: 4576608. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 00:58:11,002][166323] Avg episode reward: [(0, '1108.805')]
[36m[2025-07-02 00:58:15,980][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4571136. Throughput: 0: 269.7. Samples: 4578112. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 00:58:15,981][166323] Avg episode reward: [(0, '1125.928')]
[36m[2025-07-02 00:58:20,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4571136. Throughput: 0: 273.1. Samples: 4579040. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 00:58:20,976][166323] Avg episode reward: [(0, '1071.158')]
[36m[2025-07-02 00:58:25,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4571136. Throughput: 0: 273.3. Samples: 4580656. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 00:58:25,954][166323] Avg episode reward: [(0, '1106.327')]
[36m[2025-07-02 00:58:30,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4571136. Throughput: 0: 273.7. Samples: 4582416. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 00:58:30,971][166323] Avg episode reward: [(0, '1103.799')]
[36m[2025-07-02 00:58:35,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4571136. Throughput: 0: 277.6. Samples: 4583344. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 00:58:35,976][166323] Avg episode reward: [(0, '1187.987')]
[36m[2025-07-02 00:58:40,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4571136. Throughput: 0: 275.3. Samples: 4584880. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 00:58:40,991][166323] Avg episode reward: [(0, '1174.689')]
[36m[2025-07-02 00:58:45,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4571136. Throughput: 0: 277.9. Samples: 4586560. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 00:58:45,986][166323] Avg episode reward: [(0, '1155.076')]
[31m[15918712 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15918712 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[15918713 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:58:50,967][166323] Fps is (10 sec: 1642.3, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 4587520. Throughput: 0: 277.4. Samples: 4587440. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 00:58:50,967][166323] Avg episode reward: [(0, '1152.507')]
[36m[2025-07-02 00:58:55,974][166323] Fps is (10 sec: 1640.3, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4587520. Throughput: 0: 275.7. Samples: 4589008. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 00:58:55,974][166323] Avg episode reward: [(0, '1188.175')]
[36m[2025-07-02 00:59:00,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4587520. Throughput: 0: 281.6. Samples: 4590784. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 00:59:00,980][166323] Avg episode reward: [(0, '1167.937')]
[31m[15931305 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15931306 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[15931306 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:59:05,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4587520. Throughput: 0: 279.5. Samples: 4591616. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 00:59:05,974][166323] Avg episode reward: [(0, '1141.228')]
[36m[2025-07-02 00:59:10,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4587520. Throughput: 0: 278.2. Samples: 4593184. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 00:59:10,991][166323] Avg episode reward: [(0, '1141.684')]
[36m[2025-07-02 00:59:16,013][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 4587520. Throughput: 0: 275.7. Samples: 4594832. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 00:59:16,013][166323] Avg episode reward: [(0, '1131.629')]
[36m[2025-07-02 00:59:20,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4587520. Throughput: 0: 272.4. Samples: 4595600. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 00:59:20,976][166323] Avg episode reward: [(0, '1145.159')]
[36m[2025-07-02 00:59:25,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4587520. Throughput: 0: 275.5. Samples: 4597264. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 00:59:25,944][166323] Avg episode reward: [(0, '1144.112')]
[36m[2025-07-02 00:59:30,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4587520. Throughput: 0: 274.8. Samples: 4598928. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 00:59:30,988][166323] Avg episode reward: [(0, '1112.830')]
[36m[2025-07-02 00:59:35,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4587520. Throughput: 0: 273.0. Samples: 4599728. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 00:59:35,977][166323] Avg episode reward: [(0, '1112.878')]
[37m[1m[2025-07-02 00:59:36,065][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000008952_4587520.pth...
[36m[2025-07-02 00:59:36,069][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000008824_4521984.pth
[31m[15967011 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15967012 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[15967012 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:59:40,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4587520. Throughput: 0: 274.4. Samples: 4601360. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 00:59:40,993][166323] Avg episode reward: [(0, '1107.950')]
[31m[15971879 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15971879 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[15971879 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 00:59:45,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4587520. Throughput: 0: 271.7. Samples: 4603008. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 00:59:45,970][166323] Avg episode reward: [(0, '1092.253')]
[36m[2025-07-02 00:59:50,958][166323] Fps is (10 sec: 1644.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4603904. Throughput: 0: 272.5. Samples: 4603872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:59:50,958][166323] Avg episode reward: [(0, '1080.139')]
[36m[2025-07-02 00:59:55,960][166323] Fps is (10 sec: 1640.1, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4603904. Throughput: 0: 271.1. Samples: 4605376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 00:59:55,960][166323] Avg episode reward: [(0, '1093.851')]
[36m[2025-07-02 01:00:00,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4603904. Throughput: 0: 271.2. Samples: 4607024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:00:00,966][166323] Avg episode reward: [(0, '1116.786')]
[36m[2025-07-02 01:00:05,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4603904. Throughput: 0: 272.2. Samples: 4607840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:00:05,948][166323] Avg episode reward: [(0, '1088.441')]
[36m[2025-07-02 01:00:10,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4603904. Throughput: 0: 271.9. Samples: 4609504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:00:10,955][166323] Avg episode reward: [(0, '1144.823')]
[36m[2025-07-02 01:00:16,005][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4603904. Throughput: 0: 271.5. Samples: 4611152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:00:16,005][166323] Avg episode reward: [(0, '1145.527')]
[31m[16005030 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16005031 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[16005031 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:00:20,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4603904. Throughput: 0: 272.2. Samples: 4611968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:00:20,945][166323] Avg episode reward: [(0, '1124.539')]
[36m[2025-07-02 01:00:25,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4603904. Throughput: 0: 273.3. Samples: 4613648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:00:25,951][166323] Avg episode reward: [(0, '1163.260')]
[36m[2025-07-02 01:00:30,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4603904. Throughput: 0: 275.1. Samples: 4615392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:00:30,980][166323] Avg episode reward: [(0, '1143.198')]
[36m[2025-07-02 01:00:35,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4603904. Throughput: 0: 274.5. Samples: 4616224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:00:35,952][166323] Avg episode reward: [(0, '1124.290')]
[36m[2025-07-02 01:00:40,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 4603904. Throughput: 0: 277.0. Samples: 4617840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:00:40,949][166323] Avg episode reward: [(0, '1128.379')]
[36m[2025-07-02 01:00:45,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4603904. Throughput: 0: 278.7. Samples: 4619568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:00:45,973][166323] Avg episode reward: [(0, '1100.880')]
[36m[2025-07-02 01:00:50,953][166323] Fps is (10 sec: 1637.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4620288. Throughput: 0: 277.3. Samples: 4620320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:00:50,953][166323] Avg episode reward: [(0, '1120.076')]
[36m[2025-07-02 01:00:55,985][166323] Fps is (10 sec: 1636.4, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4620288. Throughput: 0: 275.4. Samples: 4621904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:00:55,985][166323] Avg episode reward: [(0, '1111.135')]
[36m[2025-07-02 01:01:00,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4620288. Throughput: 0: 275.1. Samples: 4623520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:01:00,959][166323] Avg episode reward: [(0, '1062.464')]
[36m[2025-07-02 01:01:05,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4620288. Throughput: 0: 273.3. Samples: 4624272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:01:05,957][166323] Avg episode reward: [(0, '1099.289')]
[36m[2025-07-02 01:01:10,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4620288. Throughput: 0: 273.0. Samples: 4625936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:01:10,954][166323] Avg episode reward: [(0, '1141.898')]
[36m[2025-07-02 01:01:15,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4620288. Throughput: 0: 274.1. Samples: 4627728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:01:15,988][166323] Avg episode reward: [(0, '1146.526')]
[36m[2025-07-02 01:01:20,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4620288. Throughput: 0: 274.8. Samples: 4628592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:01:20,964][166323] Avg episode reward: [(0, '1188.887')]
[36m[2025-07-02 01:01:25,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4620288. Throughput: 0: 277.3. Samples: 4630320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:01:25,950][166323] Avg episode reward: [(0, '1217.146')]
[36m[2025-07-02 01:01:30,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4620288. Throughput: 0: 275.4. Samples: 4631952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:01:30,944][166323] Avg episode reward: [(0, '1252.861')]
[31m[16079577 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16079577 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[16079577 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:01:35,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4620288. Throughput: 0: 277.0. Samples: 4632784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:01:35,952][166323] Avg episode reward: [(0, '1236.892')]
[37m[1m[2025-07-02 01:01:36,003][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000009016_4620288.pth...
[36m[2025-07-02 01:01:36,006][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000008888_4554752.pth
[36m[2025-07-02 01:01:40,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4620288. Throughput: 0: 278.4. Samples: 4634432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:01:40,989][166323] Avg episode reward: [(0, '1224.820')]
[36m[2025-07-02 01:01:45,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.1). Total num frames: 4620288. Throughput: 0: 279.8. Samples: 4636112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:01:45,968][166323] Avg episode reward: [(0, '1231.852')]
[36m[2025-07-02 01:01:50,981][166323] Fps is (10 sec: 1639.7, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4636672. Throughput: 0: 279.7. Samples: 4636864. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 01:01:50,981][166323] Avg episode reward: [(0, '1155.994')]
[36m[2025-07-02 01:01:55,953][166323] Fps is (10 sec: 1640.8, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4636672. Throughput: 0: 278.0. Samples: 4638448. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 01:01:55,954][166323] Avg episode reward: [(0, '1159.564')]
[36m[2025-07-02 01:02:00,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4636672. Throughput: 0: 277.6. Samples: 4640208. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 01:02:00,951][166323] Avg episode reward: [(0, '1105.042')]
[36m[2025-07-02 01:02:05,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4636672. Throughput: 0: 274.6. Samples: 4640944. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 01:02:05,964][166323] Avg episode reward: [(0, '1100.685')]
[36m[2025-07-02 01:02:10,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4636672. Throughput: 0: 269.5. Samples: 4642448. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 01:02:10,955][166323] Avg episode reward: [(0, '1057.271')]
[36m[2025-07-02 01:02:15,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4636672. Throughput: 0: 272.7. Samples: 4644240. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 01:02:15,998][166323] Avg episode reward: [(0, '1025.475')]
[36m[2025-07-02 01:02:20,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4636672. Throughput: 0: 273.1. Samples: 4645072. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 01:02:20,949][166323] Avg episode reward: [(0, '1068.778')]
[36m[2025-07-02 01:02:25,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4636672. Throughput: 0: 273.9. Samples: 4646752. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 01:02:25,965][166323] Avg episode reward: [(0, '1044.866')]
[36m[2025-07-02 01:02:30,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4636672. Throughput: 0: 274.9. Samples: 4648480. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 01:02:30,956][166323] Avg episode reward: [(0, '1090.648')]
[36m[2025-07-02 01:02:36,003][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 4636672. Throughput: 0: 275.1. Samples: 4649248. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 01:02:36,003][166323] Avg episode reward: [(0, '1135.271')]
[36m[2025-07-02 01:02:40,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4636672. Throughput: 0: 275.7. Samples: 4650864. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 01:02:40,989][166323] Avg episode reward: [(0, '1120.366')]
[36m[2025-07-02 01:02:46,012][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 4636672. Throughput: 0: 274.1. Samples: 4652560. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 01:02:46,012][166323] Avg episode reward: [(0, '1181.868')]
[31m[16154638 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16154638 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[16154638 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:02:50,970][166323] Fps is (10 sec: 1641.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4653056. Throughput: 0: 274.0. Samples: 4653280. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 01:02:50,970][166323] Avg episode reward: [(0, '1169.408')]
[36m[2025-07-02 01:02:55,970][166323] Fps is (10 sec: 1645.3, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4653056. Throughput: 0: 276.5. Samples: 4654896. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 01:02:55,970][166323] Avg episode reward: [(0, '1212.562')]
[36m[2025-07-02 01:03:00,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4653056. Throughput: 0: 275.0. Samples: 4656608. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 01:03:00,971][166323] Avg episode reward: [(0, '1270.993')]
[36m[2025-07-02 01:03:05,983][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4653056. Throughput: 0: 273.6. Samples: 4657392. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 01:03:05,983][166323] Avg episode reward: [(0, '1264.950')]
[36m[2025-07-02 01:03:11,018][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 4653056. Throughput: 0: 272.4. Samples: 4659024. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 01:03:11,018][166323] Avg episode reward: [(0, '1280.966')]
[36m[2025-07-02 01:03:15,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4653056. Throughput: 0: 269.0. Samples: 4660592. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 01:03:15,978][166323] Avg episode reward: [(0, '1253.064')]
[36m[2025-07-02 01:03:20,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4653056. Throughput: 0: 270.0. Samples: 4661392. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 01:03:20,985][166323] Avg episode reward: [(0, '1247.681')]
[36m[2025-07-02 01:03:25,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4653056. Throughput: 0: 272.2. Samples: 4663104. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 01:03:25,957][166323] Avg episode reward: [(0, '1260.702')]
[36m[2025-07-02 01:03:30,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4653056. Throughput: 0: 272.7. Samples: 4664816. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 01:03:30,956][166323] Avg episode reward: [(0, '1249.719')]
[36m[2025-07-02 01:03:35,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 4653056. Throughput: 0: 275.0. Samples: 4665648. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 01:03:35,943][166323] Avg episode reward: [(0, '1186.909')]
[37m[1m[2025-07-02 01:03:35,992][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000009080_4653056.pth...
[36m[2025-07-02 01:03:35,996][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000008952_4587520.pth
[36m[2025-07-02 01:03:40,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4653056. Throughput: 0: 280.5. Samples: 4667520. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 01:03:40,978][166323] Avg episode reward: [(0, '1174.938')]
[36m[2025-07-02 01:03:45,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.1). Total num frames: 4653056. Throughput: 0: 281.8. Samples: 4669296. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 01:03:45,988][166323] Avg episode reward: [(0, '1246.337')]
[36m[2025-07-02 01:03:51,008][166323] Fps is (10 sec: 1633.4, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4669440. Throughput: 0: 281.1. Samples: 4670048. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 01:03:51,008][166323] Avg episode reward: [(0, '1231.853')]
[31m[16223006 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16223006 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[16223006 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:03:55,983][166323] Fps is (10 sec: 1639.2, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4669440. Throughput: 0: 281.5. Samples: 4671680. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 01:03:55,983][166323] Avg episode reward: [(0, '1166.506')]
[36m[2025-07-02 01:04:00,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4669440. Throughput: 0: 285.9. Samples: 4673456. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 01:04:00,976][166323] Avg episode reward: [(0, '1170.903')]
[36m[2025-07-02 01:04:05,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4669440. Throughput: 0: 285.9. Samples: 4674256. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 01:04:05,987][166323] Avg episode reward: [(0, '1187.892')]
[36m[2025-07-02 01:04:10,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4669440. Throughput: 0: 282.5. Samples: 4675824. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 01:04:10,988][166323] Avg episode reward: [(0, '1179.262')]
[36m[2025-07-02 01:04:15,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4669440. Throughput: 0: 280.8. Samples: 4677456. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 01:04:15,965][166323] Avg episode reward: [(0, '1142.786')]
[36m[2025-07-02 01:04:20,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4669440. Throughput: 0: 279.3. Samples: 4678224. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 01:04:20,971][166323] Avg episode reward: [(0, '1097.245')]
[36m[2025-07-02 01:04:25,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4669440. Throughput: 0: 273.7. Samples: 4679840. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 01:04:25,991][166323] Avg episode reward: [(0, '1100.848')]
[36m[2025-07-02 01:04:30,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4669440. Throughput: 0: 270.8. Samples: 4681472. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 01:04:30,952][166323] Avg episode reward: [(0, '1115.981')]
[31m[16259833 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16259834 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[16259834 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:04:35,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4669440. Throughput: 0: 270.9. Samples: 4682224. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 01:04:35,955][166323] Avg episode reward: [(0, '1099.875')]
[36m[2025-07-02 01:04:40,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4669440. Throughput: 0: 272.7. Samples: 4683952. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 01:04:40,986][166323] Avg episode reward: [(0, '1086.428')]
[36m[2025-07-02 01:04:46,194][166323] Fps is (10 sec: 1600.1, 60 sec: 544.3, 300 sec: 277.5). Total num frames: 4685824. Throughput: 0: 271.7. Samples: 4685744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:04:46,194][166323] Avg episode reward: [(0, '1132.658')]
[31m[16277967 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16277968 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[16277968 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:04:50,950][166323] Fps is (10 sec: 1644.2, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 4685824. Throughput: 0: 273.3. Samples: 4686544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:04:50,950][166323] Avg episode reward: [(0, '1150.188')]
[36m[2025-07-02 01:04:55,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4685824. Throughput: 0: 275.9. Samples: 4688240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:04:55,992][166323] Avg episode reward: [(0, '1188.987')]
[36m[2025-07-02 01:05:00,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4685824. Throughput: 0: 275.9. Samples: 4689872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:05:00,965][166323] Avg episode reward: [(0, '1210.345')]
[36m[2025-07-02 01:05:05,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4685824. Throughput: 0: 276.3. Samples: 4690656. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:05:05,964][166323] Avg episode reward: [(0, '1200.810')]
[36m[2025-07-02 01:05:10,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4685824. Throughput: 0: 277.1. Samples: 4692304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:05:10,972][166323] Avg episode reward: [(0, '1218.630')]
[36m[2025-07-02 01:05:15,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4685824. Throughput: 0: 278.6. Samples: 4694016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:05:15,972][166323] Avg episode reward: [(0, '1226.412')]
[36m[2025-07-02 01:05:20,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4685824. Throughput: 0: 280.2. Samples: 4694832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:05:20,952][166323] Avg episode reward: [(0, '1150.999')]
[36m[2025-07-02 01:05:25,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4685824. Throughput: 0: 279.2. Samples: 4696512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:05:25,971][166323] Avg episode reward: [(0, '1178.562')]
[31m[16315337 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16315338 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[16315338 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:05:30,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4685824. Throughput: 0: 278.6. Samples: 4698224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:05:30,989][166323] Avg episode reward: [(0, '1158.741')]
[36m[2025-07-02 01:05:35,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4685824. Throughput: 0: 278.8. Samples: 4699088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:05:35,945][166323] Avg episode reward: [(0, '1152.168')]
[37m[1m[2025-07-02 01:05:36,021][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000009144_4685824.pth...
[36m[2025-07-02 01:05:36,028][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000009016_4620288.pth
[36m[2025-07-02 01:05:40,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4685824. Throughput: 0: 280.6. Samples: 4700864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:05:40,978][166323] Avg episode reward: [(0, '1128.779')]
[36m[2025-07-02 01:05:45,951][166323] Fps is (10 sec: 1637.4, 60 sec: 274.2, 300 sec: 277.7). Total num frames: 4702208. Throughput: 0: 281.0. Samples: 4702512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:05:45,952][166323] Avg episode reward: [(0, '1141.073')]
[36m[2025-07-02 01:05:51,007][166323] Fps is (10 sec: 1633.6, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 4702208. Throughput: 0: 283.5. Samples: 4703424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:05:51,013][166323] Avg episode reward: [(0, '1133.265')]
[31m[16340618 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16340618 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[16340618 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:05:55,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 4702208. Throughput: 0: 285.0. Samples: 4705120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:05:55,948][166323] Avg episode reward: [(0, '1118.653')]
[36m[2025-07-02 01:06:00,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4702208. Throughput: 0: 285.6. Samples: 4706864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:06:00,952][166323] Avg episode reward: [(0, '1163.591')]
[36m[2025-07-02 01:06:05,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4702208. Throughput: 0: 285.8. Samples: 4707696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:06:05,956][166323] Avg episode reward: [(0, '1138.599')]
[36m[2025-07-02 01:06:10,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4702208. Throughput: 0: 284.2. Samples: 4709296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:06:10,956][166323] Avg episode reward: [(0, '1207.861')]
[31m[16361556 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16361556 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[16361557 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:06:15,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4702208. Throughput: 0: 283.8. Samples: 4710992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:06:15,979][166323] Avg episode reward: [(0, '1194.589')]
[36m[2025-07-02 01:06:20,980][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4702208. Throughput: 0: 280.0. Samples: 4711696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:06:20,981][166323] Avg episode reward: [(0, '1201.285')]
[36m[2025-07-02 01:06:25,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4702208. Throughput: 0: 279.6. Samples: 4713440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:06:25,960][166323] Avg episode reward: [(0, '1164.134')]
[36m[2025-07-02 01:06:30,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4702208. Throughput: 0: 280.3. Samples: 4715136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:06:30,982][166323] Avg episode reward: [(0, '1202.221')]
[36m[2025-07-02 01:06:35,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4702208. Throughput: 0: 278.2. Samples: 4715936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:06:35,986][166323] Avg episode reward: [(0, '1133.407')]
[36m[2025-07-02 01:06:40,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4702208. Throughput: 0: 275.9. Samples: 4717536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:06:40,955][166323] Avg episode reward: [(0, '1152.541')]
[36m[2025-07-02 01:06:45,947][166323] Fps is (10 sec: 1644.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4718592. Throughput: 0: 274.2. Samples: 4719200. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 01:06:45,947][166323] Avg episode reward: [(0, '1145.488')]
[36m[2025-07-02 01:06:50,980][166323] Fps is (10 sec: 1634.1, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4718592. Throughput: 0: 273.3. Samples: 4720000. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 01:06:50,980][166323] Avg episode reward: [(0, '1159.251')]
[36m[2025-07-02 01:06:56,006][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 4718592. Throughput: 0: 275.6. Samples: 4721712. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 01:06:56,006][166323] Avg episode reward: [(0, '1149.440')]
[36m[2025-07-02 01:07:00,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4718592. Throughput: 0: 277.3. Samples: 4723472. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 01:07:00,983][166323] Avg episode reward: [(0, '1187.731')]
[36m[2025-07-02 01:07:05,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4718592. Throughput: 0: 281.6. Samples: 4724368. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 01:07:05,979][166323] Avg episode reward: [(0, '1201.009')]
[36m[2025-07-02 01:07:10,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4718592. Throughput: 0: 280.5. Samples: 4726064. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 01:07:10,969][166323] Avg episode reward: [(0, '1220.600')]
[36m[2025-07-02 01:07:15,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4718592. Throughput: 0: 281.5. Samples: 4727792. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 01:07:15,944][166323] Avg episode reward: [(0, '1183.685')]
[36m[2025-07-02 01:07:20,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4718592. Throughput: 0: 283.9. Samples: 4728704. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 01:07:20,957][166323] Avg episode reward: [(0, '1141.887')]
[36m[2025-07-02 01:07:25,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4718592. Throughput: 0: 288.0. Samples: 4730496. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 01:07:25,957][166323] Avg episode reward: [(0, '1119.851')]
[36m[2025-07-02 01:07:31,003][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4718592. Throughput: 0: 287.3. Samples: 4732144. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 01:07:31,003][166323] Avg episode reward: [(0, '1117.499')]
[36m[2025-07-02 01:07:35,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4718592. Throughput: 0: 287.4. Samples: 4732928. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 01:07:35,968][166323] Avg episode reward: [(0, '1127.420')]
[37m[1m[2025-07-02 01:07:36,018][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000009208_4718592.pth...
[36m[2025-07-02 01:07:36,023][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000009080_4653056.pth
[36m[2025-07-02 01:07:40,981][166323] Fps is (10 sec: 1641.9, 60 sec: 545.9, 300 sec: 333.3). Total num frames: 4734976. Throughput: 0: 290.6. Samples: 4734784. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 01:07:40,982][166323] Avg episode reward: [(0, '1113.988')]
[36m[2025-07-02 01:07:45,980][166323] Fps is (10 sec: 1636.3, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4734976. Throughput: 0: 286.2. Samples: 4736352. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 01:07:45,980][166323] Avg episode reward: [(0, '1085.872')]
[36m[2025-07-02 01:07:51,006][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4734976. Throughput: 0: 284.6. Samples: 4737184. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 01:07:51,007][166323] Avg episode reward: [(0, '1118.452')]
[36m[2025-07-02 01:07:55,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4734976. Throughput: 0: 281.6. Samples: 4738736. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 01:07:55,968][166323] Avg episode reward: [(0, '1063.322')]
[36m[2025-07-02 01:08:00,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4734976. Throughput: 0: 278.7. Samples: 4740336. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 01:08:00,946][166323] Avg episode reward: [(0, '1054.945')]
[36m[2025-07-02 01:08:05,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 4734976. Throughput: 0: 276.7. Samples: 4741152. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 01:08:05,944][166323] Avg episode reward: [(0, '1023.524')]
[36m[2025-07-02 01:08:10,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4734976. Throughput: 0: 276.5. Samples: 4742944. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 01:08:10,977][166323] Avg episode reward: [(0, '1077.729')]
[36m[2025-07-02 01:08:15,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4734976. Throughput: 0: 275.8. Samples: 4744544. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 01:08:15,959][166323] Avg episode reward: [(0, '1121.926')]
[36m[2025-07-02 01:08:20,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4734976. Throughput: 0: 276.7. Samples: 4745376. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 01:08:20,952][166323] Avg episode reward: [(0, '1170.192')]
[36m[2025-07-02 01:08:25,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4734976. Throughput: 0: 270.6. Samples: 4746960. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 01:08:25,974][166323] Avg episode reward: [(0, '1198.541')]
[36m[2025-07-02 01:08:30,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4734976. Throughput: 0: 272.1. Samples: 4748592. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 01:08:30,971][166323] Avg episode reward: [(0, '1269.131')]
[36m[2025-07-02 01:08:35,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4734976. Throughput: 0: 271.8. Samples: 4749408. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 01:08:35,980][166323] Avg episode reward: [(0, '1344.864')]
[36m[2025-07-02 01:08:40,986][166323] Fps is (10 sec: 1635.9, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 4751360. Throughput: 0: 276.2. Samples: 4751168. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 01:08:40,986][166323] Avg episode reward: [(0, '1314.176')]
[31m[16509776 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16509776 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[16509776 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:08:46,003][166323] Fps is (10 sec: 1634.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4751360. Throughput: 0: 276.6. Samples: 4752800. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 01:08:46,003][166323] Avg episode reward: [(0, '1257.133')]
[36m[2025-07-02 01:08:50,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4751360. Throughput: 0: 277.7. Samples: 4753664. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 01:08:50,999][166323] Avg episode reward: [(0, '1190.853')]
[36m[2025-07-02 01:08:55,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4751360. Throughput: 0: 276.2. Samples: 4755376. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 01:08:55,993][166323] Avg episode reward: [(0, '1142.189')]
[36m[2025-07-02 01:09:01,021][166323] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 277.7). Total num frames: 4751360. Throughput: 0: 277.0. Samples: 4757024. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 01:09:01,022][166323] Avg episode reward: [(0, '1119.033')]
[36m[2025-07-02 01:09:06,006][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 4751360. Throughput: 0: 276.6. Samples: 4757840. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 01:09:06,007][166323] Avg episode reward: [(0, '1104.108')]
[36m[2025-07-02 01:09:10,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4751360. Throughput: 0: 281.7. Samples: 4759632. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 01:09:10,959][166323] Avg episode reward: [(0, '1103.954')]
[36m[2025-07-02 01:09:15,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4751360. Throughput: 0: 281.3. Samples: 4761248. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 01:09:15,962][166323] Avg episode reward: [(0, '1140.867')]
[36m[2025-07-02 01:09:20,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4751360. Throughput: 0: 284.2. Samples: 4762192. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 01:09:20,965][166323] Avg episode reward: [(0, '1106.078')]
[36m[2025-07-02 01:09:25,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4751360. Throughput: 0: 281.6. Samples: 4763840. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 01:09:25,988][166323] Avg episode reward: [(0, '1128.838')]
[33m[16556822 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[16556822 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.8623046875
[33mCrash Rate: 0.12451171875
[33mTimeout Rate: 0.01318359375 (navigation_task.py:265)
[33m[16556822 ms][navigation_task] - WARNING : 
[33mSuccesses: 1766
[33mCrashes : 255
[33mTimeouts: 27 (navigation_task.py:268)
[36m[2025-07-02 01:09:30,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4751360. Throughput: 0: 281.2. Samples: 4765440. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 01:09:30,952][166323] Avg episode reward: [(0, '1136.277')]
[36m[2025-07-02 01:09:35,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4751360. Throughput: 0: 280.5. Samples: 4766272. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 01:09:35,950][166323] Avg episode reward: [(0, '1132.864')]
[37m[1m[2025-07-02 01:09:36,014][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000009272_4751360.pth...
[36m[2025-07-02 01:09:36,020][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000009144_4685824.pth
[36m[2025-07-02 01:09:40,944][166323] Fps is (10 sec: 1639.7, 60 sec: 273.3, 300 sec: 277.9). Total num frames: 4767744. Throughput: 0: 281.2. Samples: 4768016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:09:40,944][166323] Avg episode reward: [(0, '1164.593')]
[36m[2025-07-02 01:09:45,993][166323] Fps is (10 sec: 1631.3, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4767744. Throughput: 0: 283.2. Samples: 4769760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:09:45,994][166323] Avg episode reward: [(0, '1198.234')]
[36m[2025-07-02 01:09:50,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4767744. Throughput: 0: 284.7. Samples: 4770640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:09:50,973][166323] Avg episode reward: [(0, '1228.458')]
[36m[2025-07-02 01:09:55,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4767744. Throughput: 0: 284.1. Samples: 4772416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:09:55,959][166323] Avg episode reward: [(0, '1269.996')]
[36m[2025-07-02 01:10:00,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4767744. Throughput: 0: 286.4. Samples: 4774144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:10:00,984][166323] Avg episode reward: [(0, '1303.067')]
[36m[2025-07-02 01:10:05,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4767744. Throughput: 0: 283.3. Samples: 4774944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:10:05,972][166323] Avg episode reward: [(0, '1340.699')]
[36m[2025-07-02 01:10:10,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4767744. Throughput: 0: 282.8. Samples: 4776560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:10:10,967][166323] Avg episode reward: [(0, '1265.655')]
[36m[2025-07-02 01:10:15,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4767744. Throughput: 0: 284.3. Samples: 4778240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:10:15,974][166323] Avg episode reward: [(0, '1221.686')]
[36m[2025-07-02 01:10:21,002][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4767744. Throughput: 0: 284.5. Samples: 4779088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:10:21,003][166323] Avg episode reward: [(0, '1191.252')]
[36m[2025-07-02 01:10:25,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4767744. Throughput: 0: 284.3. Samples: 4780816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:10:25,962][166323] Avg episode reward: [(0, '1080.360')]
[36m[2025-07-02 01:10:30,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4767744. Throughput: 0: 285.3. Samples: 4782592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:10:30,976][166323] Avg episode reward: [(0, '1048.115')]
[36m[2025-07-02 01:10:36,040][166323] Fps is (10 sec: 1625.7, 60 sec: 545.3, 300 sec: 333.2). Total num frames: 4784128. Throughput: 0: 284.4. Samples: 4783456. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 01:10:36,040][166323] Avg episode reward: [(0, '1003.971')]
[36m[2025-07-02 01:10:40,966][166323] Fps is (10 sec: 1639.9, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4784128. Throughput: 0: 281.2. Samples: 4785072. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 01:10:40,966][166323] Avg episode reward: [(0, '1026.688')]
[36m[2025-07-02 01:10:45,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4784128. Throughput: 0: 280.5. Samples: 4786768. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 01:10:45,983][166323] Avg episode reward: [(0, '1010.994')]
[36m[2025-07-02 01:10:50,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4784128. Throughput: 0: 281.6. Samples: 4787616. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 01:10:50,970][166323] Avg episode reward: [(0, '1022.169')]
[36m[2025-07-02 01:10:55,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4784128. Throughput: 0: 285.5. Samples: 4789408. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 01:10:55,963][166323] Avg episode reward: [(0, '1091.622')]
[36m[2025-07-02 01:11:00,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4784128. Throughput: 0: 285.2. Samples: 4791072. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 01:11:00,966][166323] Avg episode reward: [(0, '1112.835')]
[36m[2025-07-02 01:11:05,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4784128. Throughput: 0: 285.2. Samples: 4791904. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 01:11:05,946][166323] Avg episode reward: [(0, '1070.904')]
[36m[2025-07-02 01:11:10,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4784128. Throughput: 0: 286.5. Samples: 4793712. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 01:11:10,974][166323] Avg episode reward: [(0, '1058.061')]
[36m[2025-07-02 01:11:15,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4784128. Throughput: 0: 286.9. Samples: 4795504. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 01:11:15,986][166323] Avg episode reward: [(0, '1138.891')]
[36m[2025-07-02 01:11:20,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4784128. Throughput: 0: 286.3. Samples: 4796320. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 01:11:20,975][166323] Avg episode reward: [(0, '1036.594')]
[36m[2025-07-02 01:11:25,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4784128. Throughput: 0: 287.5. Samples: 4798016. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 01:11:25,990][166323] Avg episode reward: [(0, '1102.233')]
[36m[2025-07-02 01:11:31,000][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4784128. Throughput: 0: 286.1. Samples: 4799648. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 01:11:31,001][166323] Avg episode reward: [(0, '983.333')]
[36m[2025-07-02 01:11:35,946][166323] Fps is (10 sec: 1645.6, 60 sec: 273.5, 300 sec: 333.2). Total num frames: 4800512. Throughput: 0: 287.8. Samples: 4800560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:11:35,947][166323] Avg episode reward: [(0, '980.214')]
[37m[1m[2025-07-02 01:11:36,023][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000009368_4800512.pth...
[36m[2025-07-02 01:11:36,030][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000009208_4718592.pth
[36m[2025-07-02 01:11:40,965][166323] Fps is (10 sec: 1644.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4800512. Throughput: 0: 285.9. Samples: 4802272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:11:40,965][166323] Avg episode reward: [(0, '1005.634')]
[31m[16693670 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16693670 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[16693670 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:11:45,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4800512. Throughput: 0: 288.0. Samples: 4804032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:11:45,965][166323] Avg episode reward: [(0, '960.490')]
[31m[16698722 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16698723 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[16698723 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:11:50,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4800512. Throughput: 0: 289.4. Samples: 4804928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:11:50,952][166323] Avg episode reward: [(0, '997.039')]
[36m[2025-07-02 01:11:55,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4800512. Throughput: 0: 289.5. Samples: 4806736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:11:55,966][166323] Avg episode reward: [(0, '1007.418')]
[36m[2025-07-02 01:12:00,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4800512. Throughput: 0: 288.5. Samples: 4808480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:12:00,959][166323] Avg episode reward: [(0, '1059.356')]
[36m[2025-07-02 01:12:05,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4800512. Throughput: 0: 289.4. Samples: 4809344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:12:05,986][166323] Avg episode reward: [(0, '1091.109')]
[36m[2025-07-02 01:12:10,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4800512. Throughput: 0: 287.8. Samples: 4810960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:12:10,970][166323] Avg episode reward: [(0, '1080.883')]
[36m[2025-07-02 01:12:15,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4800512. Throughput: 0: 290.7. Samples: 4812720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:12:15,962][166323] Avg episode reward: [(0, '1140.957')]
[36m[2025-07-02 01:12:20,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4800512. Throughput: 0: 289.7. Samples: 4813600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:12:20,966][166323] Avg episode reward: [(0, '1133.659')]
[36m[2025-07-02 01:12:25,997][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4800512. Throughput: 0: 291.7. Samples: 4815408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:12:25,997][166323] Avg episode reward: [(0, '1129.972')]
[36m[2025-07-02 01:12:30,950][166323] Fps is (10 sec: 1641.0, 60 sec: 546.6, 300 sec: 333.3). Total num frames: 4816896. Throughput: 0: 290.6. Samples: 4817104. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 01:12:30,950][166323] Avg episode reward: [(0, '1147.699')]
[31m[16739697 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16739697 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[16739697 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:12:36,012][166323] Fps is (10 sec: 1635.9, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 4816896. Throughput: 0: 290.8. Samples: 4818032. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 01:12:36,012][166323] Avg episode reward: [(0, '1093.744')]
[36m[2025-07-02 01:12:40,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4816896. Throughput: 0: 288.1. Samples: 4819696. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 01:12:40,956][166323] Avg episode reward: [(0, '1069.127')]
[36m[2025-07-02 01:12:45,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4816896. Throughput: 0: 284.4. Samples: 4821280. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 01:12:45,972][166323] Avg episode reward: [(0, '1145.393')]
[36m[2025-07-02 01:12:50,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4816896. Throughput: 0: 285.3. Samples: 4822176. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 01:12:50,969][166323] Avg episode reward: [(0, '1119.821')]
[36m[2025-07-02 01:12:55,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4816896. Throughput: 0: 287.1. Samples: 4823872. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 01:12:55,949][166323] Avg episode reward: [(0, '1163.849')]
[36m[2025-07-02 01:13:00,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4816896. Throughput: 0: 287.5. Samples: 4825664. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 01:13:00,981][166323] Avg episode reward: [(0, '1168.344')]
[36m[2025-07-02 01:13:05,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4816896. Throughput: 0: 287.5. Samples: 4826544. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 01:13:05,983][166323] Avg episode reward: [(0, '1178.453')]
[36m[2025-07-02 01:13:10,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4816896. Throughput: 0: 287.3. Samples: 4828336. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 01:13:10,996][166323] Avg episode reward: [(0, '1259.985')]
[36m[2025-07-02 01:13:15,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4816896. Throughput: 0: 290.4. Samples: 4830176. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 01:13:15,971][166323] Avg episode reward: [(0, '1302.752')]
[36m[2025-07-02 01:13:20,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4816896. Throughput: 0: 288.0. Samples: 4830976. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 01:13:20,964][166323] Avg episode reward: [(0, '1238.962')]
[36m[2025-07-02 01:13:25,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4816896. Throughput: 0: 284.7. Samples: 4832512. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 01:13:25,970][166323] Avg episode reward: [(0, '1240.477')]
[36m[2025-07-02 01:13:30,960][166323] Fps is (10 sec: 1639.0, 60 sec: 273.0, 300 sec: 333.3). Total num frames: 4833280. Throughput: 0: 283.1. Samples: 4834016. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 01:13:30,960][166323] Avg episode reward: [(0, '1237.867')]
[36m[2025-07-02 01:13:35,963][166323] Fps is (10 sec: 1639.4, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 4833280. Throughput: 0: 284.5. Samples: 4834976. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 01:13:35,963][166323] Avg episode reward: [(0, '1193.395')]
[37m[1m[2025-07-02 01:13:36,020][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000009432_4833280.pth...
[36m[2025-07-02 01:13:36,027][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000009272_4751360.pth
[36m[2025-07-02 01:13:40,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4833280. Throughput: 0: 281.8. Samples: 4836560. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 01:13:40,967][166323] Avg episode reward: [(0, '1203.531')]
[36m[2025-07-02 01:13:45,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4833280. Throughput: 0: 277.4. Samples: 4838144. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 01:13:45,970][166323] Avg episode reward: [(0, '1135.917')]
[36m[2025-07-02 01:13:50,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4833280. Throughput: 0: 279.5. Samples: 4839120. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 01:13:50,985][166323] Avg episode reward: [(0, '1121.873')]
[36m[2025-07-02 01:13:55,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4833280. Throughput: 0: 274.5. Samples: 4840688. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 01:13:55,988][166323] Avg episode reward: [(0, '1113.604')]
[36m[2025-07-02 01:14:00,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4833280. Throughput: 0: 270.1. Samples: 4842336. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 01:14:00,994][166323] Avg episode reward: [(0, '1107.998')]
[36m[2025-07-02 01:14:05,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4833280. Throughput: 0: 270.1. Samples: 4843136. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 01:14:05,992][166323] Avg episode reward: [(0, '1154.733')]
[36m[2025-07-02 01:14:10,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4833280. Throughput: 0: 271.6. Samples: 4844736. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 01:14:10,983][166323] Avg episode reward: [(0, '1152.842')]
[36m[2025-07-02 01:14:15,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4833280. Throughput: 0: 275.2. Samples: 4846400. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 01:14:15,968][166323] Avg episode reward: [(0, '1136.416')]
[36m[2025-07-02 01:14:20,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4833280. Throughput: 0: 272.5. Samples: 4847232. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 01:14:20,946][166323] Avg episode reward: [(0, '1117.288')]
[36m[2025-07-02 01:14:25,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4833280. Throughput: 0: 277.0. Samples: 4849024. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 01:14:25,962][166323] Avg episode reward: [(0, '1153.066')]
[36m[2025-07-02 01:14:30,993][166323] Fps is (10 sec: 1630.6, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 4849664. Throughput: 0: 278.6. Samples: 4850688. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 01:14:30,994][166323] Avg episode reward: [(0, '1184.676')]
[36m[2025-07-02 01:14:35,981][166323] Fps is (10 sec: 1635.2, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4849664. Throughput: 0: 276.6. Samples: 4851568. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 01:14:35,981][166323] Avg episode reward: [(0, '1161.404')]
[31m[16867614 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16867614 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[16867615 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:14:40,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4849664. Throughput: 0: 279.5. Samples: 4853264. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 01:14:40,986][166323] Avg episode reward: [(0, '1154.229')]
[36m[2025-07-02 01:14:46,000][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4849664. Throughput: 0: 280.5. Samples: 4854960. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 01:14:46,001][166323] Avg episode reward: [(0, '1165.060')]
[31m[16877593 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16877593 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[16877594 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:14:50,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4849664. Throughput: 0: 280.1. Samples: 4855728. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 01:14:50,947][166323] Avg episode reward: [(0, '1111.625')]
[36m[2025-07-02 01:14:55,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4849664. Throughput: 0: 279.7. Samples: 4857312. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 01:14:55,952][166323] Avg episode reward: [(0, '1102.510')]
[36m[2025-07-02 01:15:00,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4849664. Throughput: 0: 280.1. Samples: 4859008. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 01:15:00,974][166323] Avg episode reward: [(0, '1062.584')]
[36m[2025-07-02 01:15:05,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4849664. Throughput: 0: 278.3. Samples: 4859760. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 01:15:05,961][166323] Avg episode reward: [(0, '1075.307')]
[36m[2025-07-02 01:15:10,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4849664. Throughput: 0: 275.0. Samples: 4861408. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 01:15:10,987][166323] Avg episode reward: [(0, '1070.453')]
[36m[2025-07-02 01:15:15,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4849664. Throughput: 0: 277.4. Samples: 4863168. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 01:15:15,979][166323] Avg episode reward: [(0, '1097.932')]
[36m[2025-07-02 01:15:20,977][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4849664. Throughput: 0: 276.7. Samples: 4864016. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 01:15:20,977][166323] Avg episode reward: [(0, '1113.063')]
[36m[2025-07-02 01:15:25,953][166323] Fps is (10 sec: 1642.7, 60 sec: 546.2, 300 sec: 333.3). Total num frames: 4866048. Throughput: 0: 280.7. Samples: 4865888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:15:25,953][166323] Avg episode reward: [(0, '1209.893')]
[36m[2025-07-02 01:15:30,990][166323] Fps is (10 sec: 1636.3, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4866048. Throughput: 0: 283.8. Samples: 4867728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:15:30,990][166323] Avg episode reward: [(0, '1225.832')]
[36m[2025-07-02 01:15:35,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4866048. Throughput: 0: 286.4. Samples: 4868624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:15:35,980][166323] Avg episode reward: [(0, '1271.617')]
[37m[1m[2025-07-02 01:15:36,054][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000009496_4866048.pth...
[36m[2025-07-02 01:15:36,061][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000009368_4800512.pth
[36m[2025-07-02 01:15:40,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4866048. Throughput: 0: 287.2. Samples: 4870240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:15:40,960][166323] Avg episode reward: [(0, '1286.661')]
[36m[2025-07-02 01:15:45,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4866048. Throughput: 0: 286.5. Samples: 4871904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:15:45,982][166323] Avg episode reward: [(0, '1241.833')]
[36m[2025-07-02 01:15:50,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4866048. Throughput: 0: 292.0. Samples: 4872896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:15:50,954][166323] Avg episode reward: [(0, '1254.755')]
[36m[2025-07-02 01:15:55,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4866048. Throughput: 0: 288.9. Samples: 4874400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:15:55,962][166323] Avg episode reward: [(0, '1221.991')]
[36m[2025-07-02 01:16:00,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4866048. Throughput: 0: 285.0. Samples: 4875984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:16:00,954][166323] Avg episode reward: [(0, '1167.276')]
[36m[2025-07-02 01:16:06,019][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 4866048. Throughput: 0: 282.4. Samples: 4876736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:16:06,019][166323] Avg episode reward: [(0, '1150.220')]
[36m[2025-07-02 01:16:10,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 4866048. Throughput: 0: 280.6. Samples: 4878512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:16:10,945][166323] Avg episode reward: [(0, '1147.159')]
[36m[2025-07-02 01:16:15,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4866048. Throughput: 0: 274.3. Samples: 4880064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:16:15,956][166323] Avg episode reward: [(0, '1124.917')]
[36m[2025-07-02 01:16:20,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4866048. Throughput: 0: 271.6. Samples: 4880848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:16:20,981][166323] Avg episode reward: [(0, '1162.510')]
[36m[2025-07-02 01:16:25,961][166323] Fps is (10 sec: 1637.5, 60 sec: 273.0, 300 sec: 333.3). Total num frames: 4882432. Throughput: 0: 271.3. Samples: 4882448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:16:25,962][166323] Avg episode reward: [(0, '1124.304')]
[36m[2025-07-02 01:16:30,956][166323] Fps is (10 sec: 1642.4, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4882432. Throughput: 0: 272.5. Samples: 4884160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:16:30,956][166323] Avg episode reward: [(0, '1120.835')]
[36m[2025-07-02 01:16:35,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4882432. Throughput: 0: 270.6. Samples: 4885072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:16:35,945][166323] Avg episode reward: [(0, '1109.619')]
[36m[2025-07-02 01:16:40,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4882432. Throughput: 0: 274.1. Samples: 4886736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:16:40,971][166323] Avg episode reward: [(0, '1105.428')]
[31m[16991610 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16991610 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[16991610 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:16:45,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4882432. Throughput: 0: 276.7. Samples: 4888432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:16:45,944][166323] Avg episode reward: [(0, '1049.336')]
[36m[2025-07-02 01:16:50,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4882432. Throughput: 0: 276.8. Samples: 4889184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:16:50,995][166323] Avg episode reward: [(0, '1053.254')]
[36m[2025-07-02 01:16:55,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4882432. Throughput: 0: 274.5. Samples: 4890864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:16:55,948][166323] Avg episode reward: [(0, '1046.256')]
[36m[2025-07-02 01:17:00,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4882432. Throughput: 0: 279.8. Samples: 4892656. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:17:00,958][166323] Avg episode reward: [(0, '1111.115')]
[36m[2025-07-02 01:17:05,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4882432. Throughput: 0: 282.3. Samples: 4893552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:17:05,981][166323] Avg episode reward: [(0, '1084.928')]
[36m[2025-07-02 01:17:10,978][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4882432. Throughput: 0: 283.3. Samples: 4895200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:17:10,978][166323] Avg episode reward: [(0, '1098.016')]
[36m[2025-07-02 01:17:15,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4882432. Throughput: 0: 283.3. Samples: 4896912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:17:15,965][166323] Avg episode reward: [(0, '1047.197')]
[36m[2025-07-02 01:17:20,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4882432. Throughput: 0: 278.6. Samples: 4897616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:17:20,963][166323] Avg episode reward: [(0, '1102.847')]
[31m[17031784 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[17031785 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[17031785 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:17:25,943][166323] Fps is (10 sec: 1641.9, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4898816. Throughput: 0: 277.1. Samples: 4899200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:17:25,943][166323] Avg episode reward: [(0, '1082.012')]
[36m[2025-07-02 01:17:30,965][166323] Fps is (10 sec: 1638.1, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4898816. Throughput: 0: 280.0. Samples: 4901040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:17:30,965][166323] Avg episode reward: [(0, '1099.709')]
[36m[2025-07-02 01:17:35,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4898816. Throughput: 0: 283.2. Samples: 4901920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:17:35,965][166323] Avg episode reward: [(0, '1044.385')]
[37m[1m[2025-07-02 01:17:36,021][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000009560_4898816.pth...
[36m[2025-07-02 01:17:36,025][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000009432_4833280.pth
[36m[2025-07-02 01:17:40,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4898816. Throughput: 0: 285.3. Samples: 4903712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:17:40,986][166323] Avg episode reward: [(0, '1117.999')]
[36m[2025-07-02 01:17:45,983][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4898816. Throughput: 0: 286.4. Samples: 4905552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:17:45,983][166323] Avg episode reward: [(0, '1176.993')]
[36m[2025-07-02 01:17:50,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 4898816. Throughput: 0: 285.7. Samples: 4906400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:17:50,954][166323] Avg episode reward: [(0, '1159.762')]
[36m[2025-07-02 01:17:55,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4898816. Throughput: 0: 285.6. Samples: 4908048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:17:55,964][166323] Avg episode reward: [(0, '1186.338')]
[36m[2025-07-02 01:18:00,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4898816. Throughput: 0: 284.0. Samples: 4909696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:18:00,979][166323] Avg episode reward: [(0, '1133.471')]
[36m[2025-07-02 01:18:05,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4898816. Throughput: 0: 287.7. Samples: 4910560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:18:05,960][166323] Avg episode reward: [(0, '1102.589')]
[36m[2025-07-02 01:18:10,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4898816. Throughput: 0: 289.5. Samples: 4912240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:18:10,989][166323] Avg episode reward: [(0, '1057.545')]
[36m[2025-07-02 01:18:15,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4898816. Throughput: 0: 287.2. Samples: 4913968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:18:15,977][166323] Avg episode reward: [(0, '1028.537')]
[36m[2025-07-02 01:18:20,980][166323] Fps is (10 sec: 1639.8, 60 sec: 546.0, 300 sec: 333.2). Total num frames: 4915200. Throughput: 0: 285.8. Samples: 4914784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:18:20,980][166323] Avg episode reward: [(0, '1042.102')]
[36m[2025-07-02 01:18:25,980][166323] Fps is (10 sec: 1637.8, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4915200. Throughput: 0: 278.4. Samples: 4916240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:18:25,981][166323] Avg episode reward: [(0, '1079.295')]
[36m[2025-07-02 01:18:30,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4915200. Throughput: 0: 273.7. Samples: 4917872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:18:30,997][166323] Avg episode reward: [(0, '1083.270')]
[31m[17100843 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[17100844 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[17100844 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:18:35,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4915200. Throughput: 0: 273.5. Samples: 4918720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:18:35,996][166323] Avg episode reward: [(0, '1161.512')]
[36m[2025-07-02 01:18:40,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4915200. Throughput: 0: 273.0. Samples: 4920336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:18:40,971][166323] Avg episode reward: [(0, '1156.711')]
[36m[2025-07-02 01:18:45,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4915200. Throughput: 0: 272.9. Samples: 4921968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:18:45,950][166323] Avg episode reward: [(0, '1186.039')]
[36m[2025-07-02 01:18:50,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4915200. Throughput: 0: 270.6. Samples: 4922736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:18:50,957][166323] Avg episode reward: [(0, '1158.886')]
[36m[2025-07-02 01:18:55,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4915200. Throughput: 0: 269.0. Samples: 4924336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:18:55,960][166323] Avg episode reward: [(0, '1120.441')]
[36m[2025-07-02 01:19:00,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4915200. Throughput: 0: 266.1. Samples: 4925936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:19:00,946][166323] Avg episode reward: [(0, '1107.992')]
[36m[2025-07-02 01:19:05,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4915200. Throughput: 0: 267.1. Samples: 4926800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:19:05,966][166323] Avg episode reward: [(0, '1088.917')]
[36m[2025-07-02 01:19:10,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4915200. Throughput: 0: 271.1. Samples: 4928432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:19:10,955][166323] Avg episode reward: [(0, '1033.386')]
[36m[2025-07-02 01:19:15,995][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 4915200. Throughput: 0: 274.1. Samples: 4930208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:19:15,995][166323] Avg episode reward: [(0, '989.327')]
[36m[2025-07-02 01:19:20,963][166323] Fps is (10 sec: 1637.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 4931584. Throughput: 0: 276.1. Samples: 4931136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:19:20,963][166323] Avg episode reward: [(0, '1030.054')]
[36m[2025-07-02 01:19:25,979][166323] Fps is (10 sec: 1641.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4931584. Throughput: 0: 277.3. Samples: 4932816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:19:25,979][166323] Avg episode reward: [(0, '1047.084')]
[36m[2025-07-02 01:19:30,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4931584. Throughput: 0: 279.6. Samples: 4934560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:19:30,978][166323] Avg episode reward: [(0, '1078.552')]
[36m[2025-07-02 01:19:35,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 4931584. Throughput: 0: 283.0. Samples: 4935472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:19:35,955][166323] Avg episode reward: [(0, '1102.909')]
[37m[1m[2025-07-02 01:19:36,007][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000009624_4931584.pth...
[36m[2025-07-02 01:19:36,011][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000009496_4866048.pth
[36m[2025-07-02 01:19:40,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4931584. Throughput: 0: 286.6. Samples: 4937232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:19:40,957][166323] Avg episode reward: [(0, '1109.134')]
[36m[2025-07-02 01:19:45,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4931584. Throughput: 0: 287.2. Samples: 4938864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:19:45,959][166323] Avg episode reward: [(0, '1166.210')]
[36m[2025-07-02 01:19:50,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4931584. Throughput: 0: 285.2. Samples: 4939632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:19:50,964][166323] Avg episode reward: [(0, '1204.068')]
[36m[2025-07-02 01:19:55,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4931584. Throughput: 0: 284.7. Samples: 4941248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:19:55,976][166323] Avg episode reward: [(0, '1258.652')]
[36m[2025-07-02 01:20:00,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 4931584. Throughput: 0: 284.4. Samples: 4943008. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:20:00,997][166323] Avg episode reward: [(0, '1275.682')]
[36m[2025-07-02 01:20:05,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4931584. Throughput: 0: 280.2. Samples: 4943744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:20:05,964][166323] Avg episode reward: [(0, '1255.828')]
[36m[2025-07-02 01:20:10,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4931584. Throughput: 0: 278.7. Samples: 4945360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:20:10,994][166323] Avg episode reward: [(0, '1284.682')]
[36m[2025-07-02 01:20:15,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4931584. Throughput: 0: 274.8. Samples: 4946928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:20:15,990][166323] Avg episode reward: [(0, '1267.579')]
[36m[2025-07-02 01:20:20,949][166323] Fps is (10 sec: 1645.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4947968. Throughput: 0: 275.2. Samples: 4947856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:20:20,949][166323] Avg episode reward: [(0, '1294.072')]
[36m[2025-07-02 01:20:25,968][166323] Fps is (10 sec: 1641.9, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4947968. Throughput: 0: 267.3. Samples: 4949264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:20:25,969][166323] Avg episode reward: [(0, '1298.099')]
[36m[2025-07-02 01:20:30,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4947968. Throughput: 0: 268.2. Samples: 4950944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:20:30,993][166323] Avg episode reward: [(0, '1219.616')]
[36m[2025-07-02 01:20:35,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4947968. Throughput: 0: 270.6. Samples: 4951808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:20:35,952][166323] Avg episode reward: [(0, '1183.320')]
[36m[2025-07-02 01:20:40,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4947968. Throughput: 0: 271.6. Samples: 4953472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:20:40,988][166323] Avg episode reward: [(0, '1222.506')]
[33m[17232011 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[17232011 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.85205078125
[33mCrash Rate: 0.134765625
[33mTimeout Rate: 0.01318359375 (navigation_task.py:265)
[33m[17232011 ms][navigation_task] - WARNING : 
[33mSuccesses: 1745
[33mCrashes : 276
[33mTimeouts: 27 (navigation_task.py:268)
[36m[2025-07-02 01:20:45,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4947968. Throughput: 0: 269.3. Samples: 4955120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:20:45,965][166323] Avg episode reward: [(0, '1215.360')]
[36m[2025-07-02 01:20:50,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4947968. Throughput: 0: 270.7. Samples: 4955920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:20:50,948][166323] Avg episode reward: [(0, '1249.924')]
[36m[2025-07-02 01:20:55,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4947968. Throughput: 0: 272.9. Samples: 4957632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:20:55,971][166323] Avg episode reward: [(0, '1253.304')]
[36m[2025-07-02 01:21:00,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4947968. Throughput: 0: 277.0. Samples: 4959392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:21:00,994][166323] Avg episode reward: [(0, '1253.417')]
[36m[2025-07-02 01:21:05,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4947968. Throughput: 0: 273.6. Samples: 4960176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:21:05,979][166323] Avg episode reward: [(0, '1300.645')]
[36m[2025-07-02 01:21:10,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4947968. Throughput: 0: 279.0. Samples: 4961824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:21:10,988][166323] Avg episode reward: [(0, '1308.450')]
[36m[2025-07-02 01:21:15,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4947968. Throughput: 0: 276.5. Samples: 4963376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:21:15,955][166323] Avg episode reward: [(0, '1303.236')]
[36m[2025-07-02 01:21:20,975][166323] Fps is (10 sec: 1640.5, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4964352. Throughput: 0: 275.8. Samples: 4964224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:21:20,975][166323] Avg episode reward: [(0, '1265.200')]
[36m[2025-07-02 01:21:25,972][166323] Fps is (10 sec: 1635.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4964352. Throughput: 0: 273.5. Samples: 4965776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:21:25,972][166323] Avg episode reward: [(0, '1248.738')]
[36m[2025-07-02 01:21:30,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 4964352. Throughput: 0: 276.4. Samples: 4967568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:21:30,998][166323] Avg episode reward: [(0, '1222.991')]
[36m[2025-07-02 01:21:35,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4964352. Throughput: 0: 278.8. Samples: 4968480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:21:35,993][166323] Avg episode reward: [(0, '1181.434')]
[37m[1m[2025-07-02 01:21:36,057][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000009688_4964352.pth...
[36m[2025-07-02 01:21:36,062][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000009560_4898816.pth
[36m[2025-07-02 01:21:40,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4964352. Throughput: 0: 277.1. Samples: 4970096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:21:40,948][166323] Avg episode reward: [(0, '1153.308')]
[36m[2025-07-02 01:21:45,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4964352. Throughput: 0: 273.0. Samples: 4971664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:21:45,944][166323] Avg episode reward: [(0, '1114.447')]
[36m[2025-07-02 01:21:50,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4964352. Throughput: 0: 273.8. Samples: 4972496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:21:50,972][166323] Avg episode reward: [(0, '1125.228')]
[31m[17300596 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[17300596 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[17300597 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:21:55,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4964352. Throughput: 0: 272.1. Samples: 4974064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:21:55,967][166323] Avg episode reward: [(0, '1112.655')]
[36m[2025-07-02 01:22:00,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4964352. Throughput: 0: 274.4. Samples: 4975728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:22:00,976][166323] Avg episode reward: [(0, '1093.239')]
[36m[2025-07-02 01:22:05,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4964352. Throughput: 0: 272.1. Samples: 4976464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:22:05,960][166323] Avg episode reward: [(0, '1100.986')]
[36m[2025-07-02 01:22:10,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4964352. Throughput: 0: 275.1. Samples: 4978160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:22:10,984][166323] Avg episode reward: [(0, '1140.402')]
[36m[2025-07-02 01:22:15,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4964352. Throughput: 0: 273.9. Samples: 4979888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:22:15,971][166323] Avg episode reward: [(0, '1186.772')]
[36m[2025-07-02 01:22:21,018][166323] Fps is (10 sec: 1632.8, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 4980736. Throughput: 0: 271.5. Samples: 4980704. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 01:22:21,018][166323] Avg episode reward: [(0, '1215.792')]
[36m[2025-07-02 01:22:25,985][166323] Fps is (10 sec: 1636.1, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4980736. Throughput: 0: 270.7. Samples: 4982288. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 01:22:25,985][166323] Avg episode reward: [(0, '1193.407')]
[31m[17339439 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[17339439 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[17339440 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:22:30,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4980736. Throughput: 0: 272.1. Samples: 4983920. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 01:22:30,978][166323] Avg episode reward: [(0, '1185.766')]
[36m[2025-07-02 01:22:35,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4980736. Throughput: 0: 271.8. Samples: 4984720. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 01:22:35,953][166323] Avg episode reward: [(0, '1136.789')]
[36m[2025-07-02 01:22:40,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4980736. Throughput: 0: 274.9. Samples: 4986432. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 01:22:40,950][166323] Avg episode reward: [(0, '1136.353')]
[36m[2025-07-02 01:22:45,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4980736. Throughput: 0: 272.4. Samples: 4987984. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 01:22:45,967][166323] Avg episode reward: [(0, '1168.082')]
[36m[2025-07-02 01:22:50,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4980736. Throughput: 0: 274.4. Samples: 4988816. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 01:22:50,971][166323] Avg episode reward: [(0, '1170.101')]
[36m[2025-07-02 01:22:56,001][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4980736. Throughput: 0: 269.8. Samples: 4990304. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 01:22:56,002][166323] Avg episode reward: [(0, '1176.407')]
[36m[2025-07-02 01:23:01,004][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 4980736. Throughput: 0: 268.2. Samples: 4991968. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 01:23:01,005][166323] Avg episode reward: [(0, '1196.104')]
[36m[2025-07-02 01:23:05,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4980736. Throughput: 0: 267.6. Samples: 4992736. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 01:23:05,972][166323] Avg episode reward: [(0, '1209.250')]
[36m[2025-07-02 01:23:10,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4980736. Throughput: 0: 271.0. Samples: 4994480. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 01:23:10,972][166323] Avg episode reward: [(0, '1231.415')]
[36m[2025-07-02 01:23:15,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 4980736. Throughput: 0: 269.4. Samples: 4996048. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 01:23:15,993][166323] Avg episode reward: [(0, '1251.790')]
[36m[2025-07-02 01:23:20,970][166323] Fps is (10 sec: 1638.7, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 4997120. Throughput: 0: 270.5. Samples: 4996896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:23:20,970][166323] Avg episode reward: [(0, '1161.845')]
[36m[2025-07-02 01:23:25,977][166323] Fps is (10 sec: 1640.9, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4997120. Throughput: 0: 270.4. Samples: 4998608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:23:25,977][166323] Avg episode reward: [(0, '1145.723')]
[36m[2025-07-02 01:23:30,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4997120. Throughput: 0: 274.5. Samples: 5000336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:23:30,965][166323] Avg episode reward: [(0, '1181.171')]
[36m[2025-07-02 01:23:35,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4997120. Throughput: 0: 274.5. Samples: 5001168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:23:35,968][166323] Avg episode reward: [(0, '1115.669')]
[37m[1m[2025-07-02 01:23:36,023][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000009752_4997120.pth...
[36m[2025-07-02 01:23:36,027][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000009624_4931584.pth
[36m[2025-07-02 01:23:40,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4997120. Throughput: 0: 279.8. Samples: 5002880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:23:40,948][166323] Avg episode reward: [(0, '1167.203')]
[36m[2025-07-02 01:23:45,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 4997120. Throughput: 0: 277.8. Samples: 5004464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:23:45,985][166323] Avg episode reward: [(0, '1109.571')]
[31m[17418749 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[17418749 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[17418749 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:23:50,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4997120. Throughput: 0: 277.1. Samples: 5005200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:23:50,947][166323] Avg episode reward: [(0, '1101.817')]
[36m[2025-07-02 01:23:55,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4997120. Throughput: 0: 274.1. Samples: 5006816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:23:55,980][166323] Avg episode reward: [(0, '1132.928')]
[36m[2025-07-02 01:24:00,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 4997120. Throughput: 0: 276.7. Samples: 5008496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:24:00,977][166323] Avg episode reward: [(0, '1078.172')]
[36m[2025-07-02 01:24:05,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4997120. Throughput: 0: 276.3. Samples: 5009328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:24:05,965][166323] Avg episode reward: [(0, '1072.238')]
[36m[2025-07-02 01:24:10,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 4997120. Throughput: 0: 274.3. Samples: 5010944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:24:10,957][166323] Avg episode reward: [(0, '1016.393')]
[36m[2025-07-02 01:24:15,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.1). Total num frames: 4997120. Throughput: 0: 272.3. Samples: 5012592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:24:15,976][166323] Avg episode reward: [(0, '1079.790')]
[31m[17445976 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[17445977 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[17445977 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[17446120 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[17446120 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[17446121 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:24:20,986][166323] Fps is (10 sec: 1633.5, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5013504. Throughput: 0: 270.8. Samples: 5013360. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 01:24:20,987][166323] Avg episode reward: [(0, '1085.281')]
[37m[1m[2025-07-02 01:24:21,035][166323] Saving new best policy, reward=1085.281!
[36m[2025-07-02 01:24:25,968][166323] Fps is (10 sec: 1639.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5013504. Throughput: 0: 270.1. Samples: 5015040. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 01:24:25,968][166323] Avg episode reward: [(0, '1065.410')]
[36m[2025-07-02 01:24:30,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5013504. Throughput: 0: 271.1. Samples: 5016656. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 01:24:30,964][166323] Avg episode reward: [(0, '1049.639')]
[36m[2025-07-02 01:24:35,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5013504. Throughput: 0: 275.3. Samples: 5017600. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 01:24:35,995][166323] Avg episode reward: [(0, '1120.802')]
[37m[1m[2025-07-02 01:24:36,071][166323] Saving new best policy, reward=1120.802!
[31m[17467757 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[17467757 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[17467758 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:24:40,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5013504. Throughput: 0: 275.1. Samples: 5019184. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 01:24:40,943][166323] Avg episode reward: [(0, '1134.294')]
[37m[1m[2025-07-02 01:24:40,994][166323] Saving new best policy, reward=1134.294!
[36m[2025-07-02 01:24:45,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5013504. Throughput: 0: 276.6. Samples: 5020944. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 01:24:45,982][166323] Avg episode reward: [(0, '1173.093')]
[37m[1m[2025-07-02 01:24:46,034][166323] Saving new best policy, reward=1173.093!
[36m[2025-07-02 01:24:50,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5013504. Throughput: 0: 275.1. Samples: 5021712. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 01:24:50,982][166323] Avg episode reward: [(0, '1179.186')]
[37m[1m[2025-07-02 01:24:51,039][166323] Saving new best policy, reward=1179.186!
[36m[2025-07-02 01:24:55,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5013504. Throughput: 0: 273.0. Samples: 5023232. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 01:24:55,961][166323] Avg episode reward: [(0, '1211.272')]
[37m[1m[2025-07-02 01:24:56,032][166323] Saving new best policy, reward=1211.272!
[31m[17487782 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[17487783 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[17487783 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:25:01,002][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5013504. Throughput: 0: 272.9. Samples: 5024880. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 01:25:01,003][166323] Avg episode reward: [(0, '1202.942')]
[36m[2025-07-02 01:25:05,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5013504. Throughput: 0: 274.5. Samples: 5025712. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 01:25:05,988][166323] Avg episode reward: [(0, '1226.009')]
[37m[1m[2025-07-02 01:25:06,038][166323] Saving new best policy, reward=1226.009!
[36m[2025-07-02 01:25:10,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5013504. Throughput: 0: 275.5. Samples: 5027440. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 01:25:10,979][166323] Avg episode reward: [(0, '1153.141')]
[36m[2025-07-02 01:25:15,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 5013504. Throughput: 0: 278.2. Samples: 5029184. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 01:25:15,990][166323] Avg episode reward: [(0, '1192.406')]
[36m[2025-07-02 01:25:20,953][166323] Fps is (10 sec: 1642.6, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5029888. Throughput: 0: 274.0. Samples: 5029920. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 01:25:20,953][166323] Avg episode reward: [(0, '1176.083')]
[36m[2025-07-02 01:25:25,976][166323] Fps is (10 sec: 1640.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5029888. Throughput: 0: 276.1. Samples: 5031616. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 01:25:25,976][166323] Avg episode reward: [(0, '1102.900')]
[36m[2025-07-02 01:25:30,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5029888. Throughput: 0: 272.5. Samples: 5033200. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 01:25:30,963][166323] Avg episode reward: [(0, '1138.797')]
[36m[2025-07-02 01:25:35,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5029888. Throughput: 0: 274.8. Samples: 5034080. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 01:25:35,990][166323] Avg episode reward: [(0, '1124.952')]
[37m[1m[2025-07-02 01:25:36,041][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000009816_5029888.pth...
[36m[2025-07-02 01:25:36,045][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000009688_4964352.pth
[36m[2025-07-02 01:25:40,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5029888. Throughput: 0: 276.5. Samples: 5035680. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 01:25:40,985][166323] Avg episode reward: [(0, '1103.516')]
[36m[2025-07-02 01:25:45,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5029888. Throughput: 0: 282.2. Samples: 5037568. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 01:25:45,968][166323] Avg episode reward: [(0, '1167.233')]
[36m[2025-07-02 01:25:50,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5029888. Throughput: 0: 283.1. Samples: 5038448. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 01:25:50,975][166323] Avg episode reward: [(0, '1134.148')]
[36m[2025-07-02 01:25:55,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5029888. Throughput: 0: 284.1. Samples: 5040224. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 01:25:55,981][166323] Avg episode reward: [(0, '1107.423')]
[36m[2025-07-02 01:26:00,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5029888. Throughput: 0: 281.8. Samples: 5041856. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 01:26:00,953][166323] Avg episode reward: [(0, '1147.156')]
[36m[2025-07-02 01:26:05,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5029888. Throughput: 0: 286.2. Samples: 5042800. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 01:26:05,960][166323] Avg episode reward: [(0, '1104.903')]
[36m[2025-07-02 01:26:11,019][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 5029888. Throughput: 0: 287.0. Samples: 5044544. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 01:26:11,019][166323] Avg episode reward: [(0, '1137.825')]
[36m[2025-07-02 01:26:15,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 5029888. Throughput: 0: 288.0. Samples: 5046160. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 01:26:15,962][166323] Avg episode reward: [(0, '1156.401')]
[36m[2025-07-02 01:26:20,947][166323] Fps is (10 sec: 1650.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5046272. Throughput: 0: 286.1. Samples: 5046944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:26:20,948][166323] Avg episode reward: [(0, '1142.455')]
[36m[2025-07-02 01:26:25,957][166323] Fps is (10 sec: 1639.2, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5046272. Throughput: 0: 287.1. Samples: 5048592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:26:25,957][166323] Avg episode reward: [(0, '1176.347')]
[36m[2025-07-02 01:26:30,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5046272. Throughput: 0: 284.0. Samples: 5050352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:26:30,983][166323] Avg episode reward: [(0, '1240.693')]
[37m[1m[2025-07-02 01:26:31,039][166323] Saving new best policy, reward=1240.693!
[36m[2025-07-02 01:26:35,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 5046272. Throughput: 0: 285.0. Samples: 5051280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:26:35,999][166323] Avg episode reward: [(0, '1237.736')]
[36m[2025-07-02 01:26:41,007][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 5046272. Throughput: 0: 280.7. Samples: 5052864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:26:41,007][166323] Avg episode reward: [(0, '1270.391')]
[37m[1m[2025-07-02 01:26:41,063][166323] Saving new best policy, reward=1270.391!
[36m[2025-07-02 01:26:45,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5046272. Throughput: 0: 282.2. Samples: 5054560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:26:45,969][166323] Avg episode reward: [(0, '1264.387')]
[36m[2025-07-02 01:26:50,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5046272. Throughput: 0: 279.8. Samples: 5055392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:26:50,960][166323] Avg episode reward: [(0, '1251.527')]
[36m[2025-07-02 01:26:55,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5046272. Throughput: 0: 278.8. Samples: 5057072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:26:55,951][166323] Avg episode reward: [(0, '1248.738')]
[36m[2025-07-02 01:27:00,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5046272. Throughput: 0: 283.0. Samples: 5058896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:27:00,960][166323] Avg episode reward: [(0, '1286.466')]
[37m[1m[2025-07-02 01:27:01,009][166323] Saving new best policy, reward=1286.466!
[36m[2025-07-02 01:27:05,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5046272. Throughput: 0: 285.3. Samples: 5059792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:27:05,978][166323] Avg episode reward: [(0, '1289.111')]
[37m[1m[2025-07-02 01:27:06,066][166323] Saving new best policy, reward=1289.111!
[36m[2025-07-02 01:27:10,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5046272. Throughput: 0: 284.0. Samples: 5061376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:27:10,965][166323] Avg episode reward: [(0, '1267.705')]
[36m[2025-07-02 01:27:15,961][166323] Fps is (10 sec: 1641.0, 60 sec: 546.1, 300 sec: 277.7). Total num frames: 5062656. Throughput: 0: 284.9. Samples: 5063168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:27:15,962][166323] Avg episode reward: [(0, '1272.906')]
[36m[2025-07-02 01:27:21,005][166323] Fps is (10 sec: 1631.7, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 5062656. Throughput: 0: 280.1. Samples: 5063888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:27:21,006][166323] Avg episode reward: [(0, '1285.427')]
[36m[2025-07-02 01:27:25,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5062656. Throughput: 0: 278.9. Samples: 5065408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:27:25,988][166323] Avg episode reward: [(0, '1276.876')]
[36m[2025-07-02 01:27:30,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5062656. Throughput: 0: 277.6. Samples: 5067056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:27:30,976][166323] Avg episode reward: [(0, '1281.798')]
[36m[2025-07-02 01:27:35,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5062656. Throughput: 0: 276.1. Samples: 5067824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:27:35,990][166323] Avg episode reward: [(0, '1247.733')]
[37m[1m[2025-07-02 01:27:36,044][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000009880_5062656.pth...
[36m[2025-07-02 01:27:36,048][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000009752_4997120.pth
[36m[2025-07-02 01:27:40,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5062656. Throughput: 0: 275.9. Samples: 5069488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:27:40,947][166323] Avg episode reward: [(0, '1188.983')]
[36m[2025-07-02 01:27:45,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5062656. Throughput: 0: 272.0. Samples: 5071136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:27:45,954][166323] Avg episode reward: [(0, '1196.615')]
[36m[2025-07-02 01:27:50,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5062656. Throughput: 0: 271.2. Samples: 5072000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:27:50,996][166323] Avg episode reward: [(0, '1172.539')]
[36m[2025-07-02 01:27:55,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5062656. Throughput: 0: 275.2. Samples: 5073760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:27:55,971][166323] Avg episode reward: [(0, '1152.779')]
[36m[2025-07-02 01:28:00,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5062656. Throughput: 0: 270.1. Samples: 5075328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:28:00,987][166323] Avg episode reward: [(0, '1166.683')]
[36m[2025-07-02 01:28:05,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5062656. Throughput: 0: 271.4. Samples: 5076096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:28:05,988][166323] Avg episode reward: [(0, '1120.119')]
[36m[2025-07-02 01:28:10,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5062656. Throughput: 0: 275.8. Samples: 5077808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:28:10,948][166323] Avg episode reward: [(0, '1174.765')]
[36m[2025-07-02 01:28:15,960][166323] Fps is (10 sec: 1643.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5079040. Throughput: 0: 274.2. Samples: 5079392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:28:15,960][166323] Avg episode reward: [(0, '1176.360')]
[36m[2025-07-02 01:28:20,986][166323] Fps is (10 sec: 1632.1, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5079040. Throughput: 0: 273.8. Samples: 5080144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:28:20,987][166323] Avg episode reward: [(0, '1153.780')]
[31m[17692264 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[17692264 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[17692264 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:28:25,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5079040. Throughput: 0: 274.1. Samples: 5081824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:28:25,948][166323] Avg episode reward: [(0, '1100.344')]
[36m[2025-07-02 01:28:30,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5079040. Throughput: 0: 272.0. Samples: 5083376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:28:30,948][166323] Avg episode reward: [(0, '1082.242')]
[36m[2025-07-02 01:28:35,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 5079040. Throughput: 0: 269.5. Samples: 5084128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:28:35,998][166323] Avg episode reward: [(0, '1065.943')]
[36m[2025-07-02 01:28:40,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5079040. Throughput: 0: 267.3. Samples: 5085792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:28:40,985][166323] Avg episode reward: [(0, '1058.196')]
[36m[2025-07-02 01:28:45,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5079040. Throughput: 0: 265.8. Samples: 5087280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:28:45,955][166323] Avg episode reward: [(0, '1079.976')]
[36m[2025-07-02 01:28:51,011][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5079040. Throughput: 0: 266.2. Samples: 5088080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:28:51,011][166323] Avg episode reward: [(0, '1097.886')]
[36m[2025-07-02 01:28:55,997][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5079040. Throughput: 0: 265.0. Samples: 5089744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:28:55,998][166323] Avg episode reward: [(0, '1147.688')]
[36m[2025-07-02 01:29:00,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5079040. Throughput: 0: 267.6. Samples: 5091440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:29:00,991][166323] Avg episode reward: [(0, '1164.833')]
[36m[2025-07-02 01:29:05,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5079040. Throughput: 0: 270.3. Samples: 5092304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:29:05,970][166323] Avg episode reward: [(0, '1087.170')]
[36m[2025-07-02 01:29:10,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5079040. Throughput: 0: 269.1. Samples: 5093936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:29:10,955][166323] Avg episode reward: [(0, '1109.648')]
[36m[2025-07-02 01:29:15,981][166323] Fps is (10 sec: 1636.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5095424. Throughput: 0: 268.6. Samples: 5095472. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 01:29:15,981][166323] Avg episode reward: [(0, '1134.060')]
[36m[2025-07-02 01:29:21,002][166323] Fps is (10 sec: 1630.7, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5095424. Throughput: 0: 272.7. Samples: 5096400. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 01:29:21,002][166323] Avg episode reward: [(0, '1104.244')]
[31m[17749676 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[17749677 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[17749677 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:29:25,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5095424. Throughput: 0: 272.2. Samples: 5098032. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 01:29:25,949][166323] Avg episode reward: [(0, '1086.896')]
[36m[2025-07-02 01:29:30,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5095424. Throughput: 0: 275.5. Samples: 5099680. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 01:29:30,970][166323] Avg episode reward: [(0, '1086.222')]
[36m[2025-07-02 01:29:35,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5095424. Throughput: 0: 278.0. Samples: 5100576. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 01:29:35,962][166323] Avg episode reward: [(0, '1166.045')]
[37m[1m[2025-07-02 01:29:36,040][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000009944_5095424.pth...
[36m[2025-07-02 01:29:36,047][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000009816_5029888.pth
[36m[2025-07-02 01:29:41,007][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5095424. Throughput: 0: 278.0. Samples: 5102256. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 01:29:41,008][166323] Avg episode reward: [(0, '1198.812')]
[36m[2025-07-02 01:29:45,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5095424. Throughput: 0: 274.3. Samples: 5103776. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 01:29:45,957][166323] Avg episode reward: [(0, '1164.869')]
[36m[2025-07-02 01:29:50,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5095424. Throughput: 0: 275.7. Samples: 5104720. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 01:29:50,997][166323] Avg episode reward: [(0, '1137.674')]
[36m[2025-07-02 01:29:55,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5095424. Throughput: 0: 274.5. Samples: 5106288. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 01:29:55,945][166323] Avg episode reward: [(0, '1140.916')]
[36m[2025-07-02 01:30:00,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5095424. Throughput: 0: 272.8. Samples: 5107744. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 01:30:00,972][166323] Avg episode reward: [(0, '1159.161')]
[36m[2025-07-02 01:30:05,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5095424. Throughput: 0: 270.0. Samples: 5108544. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 01:30:05,972][166323] Avg episode reward: [(0, '1200.565')]
[36m[2025-07-02 01:30:10,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5095424. Throughput: 0: 271.8. Samples: 5110272. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 01:30:10,979][166323] Avg episode reward: [(0, '1149.183')]
[36m[2025-07-02 01:30:15,974][166323] Fps is (10 sec: 1638.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5111808. Throughput: 0: 268.8. Samples: 5111776. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 01:30:15,975][166323] Avg episode reward: [(0, '1113.250')]
[36m[2025-07-02 01:30:20,973][166323] Fps is (10 sec: 1639.3, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5111808. Throughput: 0: 265.2. Samples: 5112512. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 01:30:20,973][166323] Avg episode reward: [(0, '1067.463')]
[36m[2025-07-02 01:30:25,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5111808. Throughput: 0: 267.3. Samples: 5114272. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 01:30:25,967][166323] Avg episode reward: [(0, '1145.031')]
[36m[2025-07-02 01:30:30,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5111808. Throughput: 0: 275.3. Samples: 5116160. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 01:30:30,948][166323] Avg episode reward: [(0, '1118.135')]
[36m[2025-07-02 01:30:35,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5111808. Throughput: 0: 275.0. Samples: 5117088. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 01:30:35,967][166323] Avg episode reward: [(0, '1123.805')]
[36m[2025-07-02 01:30:40,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5111808. Throughput: 0: 278.4. Samples: 5118816. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 01:30:40,947][166323] Avg episode reward: [(0, '1194.931')]
[36m[2025-07-02 01:30:45,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5111808. Throughput: 0: 283.3. Samples: 5120496. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 01:30:45,978][166323] Avg episode reward: [(0, '1259.361')]
[36m[2025-07-02 01:30:50,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5111808. Throughput: 0: 286.4. Samples: 5121424. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 01:30:50,951][166323] Avg episode reward: [(0, '1279.746')]
[36m[2025-07-02 01:30:55,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5111808. Throughput: 0: 282.9. Samples: 5122992. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 01:30:55,947][166323] Avg episode reward: [(0, '1247.745')]
[36m[2025-07-02 01:31:00,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5111808. Throughput: 0: 288.7. Samples: 5124768. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 01:31:00,972][166323] Avg episode reward: [(0, '1257.832')]
[36m[2025-07-02 01:31:05,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5111808. Throughput: 0: 291.3. Samples: 5125616. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 01:31:05,962][166323] Avg episode reward: [(0, '1263.246')]
[36m[2025-07-02 01:31:10,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5111808. Throughput: 0: 290.9. Samples: 5127360. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 01:31:10,961][166323] Avg episode reward: [(0, '1229.568')]
[36m[2025-07-02 01:31:15,946][166323] Fps is (10 sec: 1640.9, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5128192. Throughput: 0: 283.0. Samples: 5128896. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:31:15,946][166323] Avg episode reward: [(0, '1183.750')]
[36m[2025-07-02 01:31:20,987][166323] Fps is (10 sec: 1634.2, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5128192. Throughput: 0: 281.8. Samples: 5129776. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:31:20,987][166323] Avg episode reward: [(0, '1117.269')]
[36m[2025-07-02 01:31:25,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5128192. Throughput: 0: 282.3. Samples: 5131520. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:31:25,952][166323] Avg episode reward: [(0, '1135.915')]
[36m[2025-07-02 01:31:31,010][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 5128192. Throughput: 0: 284.2. Samples: 5133296. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:31:31,011][166323] Avg episode reward: [(0, '1114.681')]
[31m[17880302 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[17880302 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[17880302 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:31:35,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5128192. Throughput: 0: 281.6. Samples: 5134096. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:31:35,957][166323] Avg episode reward: [(0, '1107.827')]
[37m[1m[2025-07-02 01:31:36,026][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010008_5128192.pth...
[36m[2025-07-02 01:31:36,033][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000009880_5062656.pth
[36m[2025-07-02 01:31:41,023][166323] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 277.6). Total num frames: 5128192. Throughput: 0: 279.0. Samples: 5135568. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:31:41,023][166323] Avg episode reward: [(0, '1065.693')]
[36m[2025-07-02 01:31:45,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5128192. Throughput: 0: 275.4. Samples: 5137168. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:31:45,994][166323] Avg episode reward: [(0, '1073.033')]
[36m[2025-07-02 01:31:50,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5128192. Throughput: 0: 275.9. Samples: 5138032. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:31:50,969][166323] Avg episode reward: [(0, '1105.613')]
[36m[2025-07-02 01:31:55,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5128192. Throughput: 0: 274.2. Samples: 5139696. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:31:55,958][166323] Avg episode reward: [(0, '1101.741')]
[36m[2025-07-02 01:32:00,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5128192. Throughput: 0: 275.6. Samples: 5141312. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:32:00,991][166323] Avg episode reward: [(0, '1148.812')]
[36m[2025-07-02 01:32:05,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5128192. Throughput: 0: 275.6. Samples: 5142176. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:32:05,987][166323] Avg episode reward: [(0, '1128.183')]
[33m[17915072 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[17915072 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.8564453125
[33mCrash Rate: 0.13671875
[33mTimeout Rate: 0.0068359375 (navigation_task.py:265)
[33m[17915072 ms][navigation_task] - WARNING : 
[33mSuccesses: 1754
[33mCrashes : 280
[33mTimeouts: 14 (navigation_task.py:268)
[36m[2025-07-02 01:32:10,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 5128192. Throughput: 0: 272.8. Samples: 5143792. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:32:10,944][166323] Avg episode reward: [(0, '1127.574')]
[36m[2025-07-02 01:32:15,948][166323] Fps is (10 sec: 1644.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5144576. Throughput: 0: 269.5. Samples: 5145408. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 01:32:15,948][166323] Avg episode reward: [(0, '1176.529')]
[36m[2025-07-02 01:32:20,984][166323] Fps is (10 sec: 1632.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5144576. Throughput: 0: 272.5. Samples: 5146368. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 01:32:20,984][166323] Avg episode reward: [(0, '1196.012')]
[36m[2025-07-02 01:32:25,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5144576. Throughput: 0: 281.7. Samples: 5148224. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 01:32:25,950][166323] Avg episode reward: [(0, '1140.509')]
[36m[2025-07-02 01:32:30,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5144576. Throughput: 0: 280.7. Samples: 5149792. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 01:32:30,963][166323] Avg episode reward: [(0, '1119.074')]
[36m[2025-07-02 01:32:35,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5144576. Throughput: 0: 280.5. Samples: 5150656. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 01:32:35,981][166323] Avg episode reward: [(0, '1142.222')]
[36m[2025-07-02 01:32:40,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 5144576. Throughput: 0: 279.9. Samples: 5152288. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 01:32:40,946][166323] Avg episode reward: [(0, '1161.542')]
[31m[17953499 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[17953499 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[17953499 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:32:45,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5144576. Throughput: 0: 282.8. Samples: 5154032. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 01:32:45,964][166323] Avg episode reward: [(0, '1151.065')]
[36m[2025-07-02 01:32:50,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5144576. Throughput: 0: 280.9. Samples: 5154816. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 01:32:50,979][166323] Avg episode reward: [(0, '1152.901')]
[36m[2025-07-02 01:32:55,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5144576. Throughput: 0: 282.5. Samples: 5156512. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 01:32:55,970][166323] Avg episode reward: [(0, '1142.549')]
[36m[2025-07-02 01:33:00,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5144576. Throughput: 0: 285.2. Samples: 5158256. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 01:33:00,997][166323] Avg episode reward: [(0, '1211.423')]
[36m[2025-07-02 01:33:05,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5144576. Throughput: 0: 281.6. Samples: 5159040. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 01:33:05,976][166323] Avg episode reward: [(0, '1186.812')]
[36m[2025-07-02 01:33:10,991][166323] Fps is (10 sec: 1639.3, 60 sec: 545.7, 300 sec: 277.7). Total num frames: 5160960. Throughput: 0: 279.9. Samples: 5160832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:33:10,991][166323] Avg episode reward: [(0, '1196.845')]
[36m[2025-07-02 01:33:15,978][166323] Fps is (10 sec: 1638.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5160960. Throughput: 0: 282.6. Samples: 5162512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:33:15,979][166323] Avg episode reward: [(0, '1155.818')]
[36m[2025-07-02 01:33:20,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5160960. Throughput: 0: 282.0. Samples: 5163344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:33:20,979][166323] Avg episode reward: [(0, '1122.586')]
[36m[2025-07-02 01:33:25,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5160960. Throughput: 0: 282.0. Samples: 5164992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:33:25,995][166323] Avg episode reward: [(0, '1099.631')]
[36m[2025-07-02 01:33:30,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5160960. Throughput: 0: 279.8. Samples: 5166624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:33:30,970][166323] Avg episode reward: [(0, '1101.618')]
[36m[2025-07-02 01:33:35,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5160960. Throughput: 0: 283.1. Samples: 5167552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:33:35,974][166323] Avg episode reward: [(0, '1122.037')]
[37m[1m[2025-07-02 01:33:36,048][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010072_5160960.pth...
[36m[2025-07-02 01:33:36,052][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000009944_5095424.pth
[36m[2025-07-02 01:33:40,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5160960. Throughput: 0: 282.4. Samples: 5169216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:33:40,951][166323] Avg episode reward: [(0, '1089.296')]
[36m[2025-07-02 01:33:45,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5160960. Throughput: 0: 280.3. Samples: 5170864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:33:45,976][166323] Avg episode reward: [(0, '1115.252')]
[36m[2025-07-02 01:33:50,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5160960. Throughput: 0: 281.4. Samples: 5171696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:33:50,952][166323] Avg episode reward: [(0, '1118.080')]
[36m[2025-07-02 01:33:55,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5160960. Throughput: 0: 281.8. Samples: 5173504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:33:55,952][166323] Avg episode reward: [(0, '1159.633')]
[31m[18026839 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18026840 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[18026840 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:34:00,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5160960. Throughput: 0: 281.1. Samples: 5175152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:34:00,947][166323] Avg episode reward: [(0, '1169.477')]
[36m[2025-07-02 01:34:05,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5160960. Throughput: 0: 280.0. Samples: 5175936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:34:05,945][166323] Avg episode reward: [(0, '1149.248')]
[36m[2025-07-02 01:34:10,981][166323] Fps is (10 sec: 1632.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5177344. Throughput: 0: 281.0. Samples: 5177632. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 01:34:10,981][166323] Avg episode reward: [(0, '1141.250')]
[36m[2025-07-02 01:34:15,948][166323] Fps is (10 sec: 1637.8, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5177344. Throughput: 0: 282.8. Samples: 5179344. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 01:34:15,948][166323] Avg episode reward: [(0, '1095.507')]
[36m[2025-07-02 01:34:20,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5177344. Throughput: 0: 282.5. Samples: 5180272. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 01:34:20,994][166323] Avg episode reward: [(0, '1069.388')]
[36m[2025-07-02 01:34:25,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5177344. Throughput: 0: 282.6. Samples: 5181936. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 01:34:25,962][166323] Avg episode reward: [(0, '1075.442')]
[36m[2025-07-02 01:34:30,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5177344. Throughput: 0: 280.2. Samples: 5183472. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 01:34:30,974][166323] Avg episode reward: [(0, '1089.016')]
[36m[2025-07-02 01:34:35,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5177344. Throughput: 0: 281.1. Samples: 5184352. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 01:34:35,982][166323] Avg episode reward: [(0, '1076.737')]
[36m[2025-07-02 01:34:40,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5177344. Throughput: 0: 279.8. Samples: 5186096. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 01:34:40,949][166323] Avg episode reward: [(0, '1114.965')]
[36m[2025-07-02 01:34:45,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5177344. Throughput: 0: 279.3. Samples: 5187728. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 01:34:45,981][166323] Avg episode reward: [(0, '1117.428')]
[36m[2025-07-02 01:34:50,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5177344. Throughput: 0: 280.0. Samples: 5188544. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 01:34:50,973][166323] Avg episode reward: [(0, '1162.077')]
[36m[2025-07-02 01:34:55,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5177344. Throughput: 0: 280.3. Samples: 5190240. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 01:34:55,964][166323] Avg episode reward: [(0, '1249.331')]
[36m[2025-07-02 01:35:00,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5177344. Throughput: 0: 281.2. Samples: 5192000. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 01:35:00,951][166323] Avg episode reward: [(0, '1204.331')]
[36m[2025-07-02 01:35:05,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5177344. Throughput: 0: 280.1. Samples: 5192864. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 01:35:05,948][166323] Avg episode reward: [(0, '1217.628')]
[36m[2025-07-02 01:35:10,976][166323] Fps is (10 sec: 1634.3, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5193728. Throughput: 0: 281.5. Samples: 5194608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:35:10,976][166323] Avg episode reward: [(0, '1185.185')]
[36m[2025-07-02 01:35:15,985][166323] Fps is (10 sec: 1632.5, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5193728. Throughput: 0: 284.7. Samples: 5196288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:35:15,985][166323] Avg episode reward: [(0, '1203.505')]
[36m[2025-07-02 01:35:20,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5193728. Throughput: 0: 283.7. Samples: 5197120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:35:20,982][166323] Avg episode reward: [(0, '1206.466')]
[36m[2025-07-02 01:35:25,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5193728. Throughput: 0: 278.9. Samples: 5198656. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:35:25,991][166323] Avg episode reward: [(0, '1185.302')]
[36m[2025-07-02 01:35:30,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5193728. Throughput: 0: 280.9. Samples: 5200368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:35:30,978][166323] Avg episode reward: [(0, '1098.151')]
[36m[2025-07-02 01:35:35,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 5193728. Throughput: 0: 279.0. Samples: 5201104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:35:35,998][166323] Avg episode reward: [(0, '1154.932')]
[37m[1m[2025-07-02 01:35:36,066][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010136_5193728.pth...
[36m[2025-07-02 01:35:36,071][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010008_5128192.pth
[31m[18127922 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18127922 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[18127922 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:35:40,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5193728. Throughput: 0: 281.0. Samples: 5202880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:35:40,949][166323] Avg episode reward: [(0, '1148.428')]
[31m[18134311 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18134311 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[18134312 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:35:45,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5193728. Throughput: 0: 275.2. Samples: 5204384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:35:45,945][166323] Avg episode reward: [(0, '1147.007')]
[36m[2025-07-02 01:35:50,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5193728. Throughput: 0: 275.5. Samples: 5205264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:35:50,963][166323] Avg episode reward: [(0, '1116.401')]
[36m[2025-07-02 01:35:56,002][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5193728. Throughput: 0: 272.9. Samples: 5206896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:35:56,002][166323] Avg episode reward: [(0, '1154.955')]
[36m[2025-07-02 01:36:00,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5193728. Throughput: 0: 275.1. Samples: 5208656. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:36:00,951][166323] Avg episode reward: [(0, '1196.526')]
[36m[2025-07-02 01:36:05,983][166323] Fps is (10 sec: 1641.4, 60 sec: 545.8, 300 sec: 333.2). Total num frames: 5210112. Throughput: 0: 276.3. Samples: 5209552. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 01:36:05,984][166323] Avg episode reward: [(0, '1210.104')]
[36m[2025-07-02 01:36:10,970][166323] Fps is (10 sec: 1635.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5210112. Throughput: 0: 279.2. Samples: 5211216. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 01:36:10,970][166323] Avg episode reward: [(0, '1194.815')]
[31m[18164455 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18164456 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[18164456 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:36:15,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5210112. Throughput: 0: 280.4. Samples: 5212976. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 01:36:15,948][166323] Avg episode reward: [(0, '1155.434')]
[36m[2025-07-02 01:36:20,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5210112. Throughput: 0: 282.3. Samples: 5213808. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 01:36:20,999][166323] Avg episode reward: [(0, '1185.360')]
[36m[2025-07-02 01:36:25,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.8). Total num frames: 5210112. Throughput: 0: 276.3. Samples: 5215312. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 01:36:25,947][166323] Avg episode reward: [(0, '1180.152')]
[36m[2025-07-02 01:36:30,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5210112. Throughput: 0: 279.3. Samples: 5216960. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 01:36:30,969][166323] Avg episode reward: [(0, '1139.002')]
[36m[2025-07-02 01:36:35,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5210112. Throughput: 0: 279.0. Samples: 5217824. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 01:36:35,989][166323] Avg episode reward: [(0, '1136.117')]
[36m[2025-07-02 01:36:40,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5210112. Throughput: 0: 280.6. Samples: 5219520. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 01:36:40,986][166323] Avg episode reward: [(0, '1221.434')]
[36m[2025-07-02 01:36:45,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5210112. Throughput: 0: 279.3. Samples: 5221232. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 01:36:45,985][166323] Avg episode reward: [(0, '1145.956')]
[36m[2025-07-02 01:36:50,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5210112. Throughput: 0: 277.2. Samples: 5222016. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 01:36:50,948][166323] Avg episode reward: [(0, '1201.592')]
[36m[2025-07-02 01:36:55,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5210112. Throughput: 0: 277.3. Samples: 5223696. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 01:36:55,980][166323] Avg episode reward: [(0, '1221.150')]
[31m[18205813 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18205813 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[18205814 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:37:00,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5210112. Throughput: 0: 270.9. Samples: 5225168. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 01:37:00,953][166323] Avg episode reward: [(0, '1204.044')]
[36m[2025-07-02 01:37:05,988][166323] Fps is (10 sec: 1637.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 5226496. Throughput: 0: 271.0. Samples: 5226000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:37:05,989][166323] Avg episode reward: [(0, '1240.856')]
[36m[2025-07-02 01:37:10,943][166323] Fps is (10 sec: 1640.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5226496. Throughput: 0: 272.4. Samples: 5227568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:37:10,943][166323] Avg episode reward: [(0, '1207.561')]
[36m[2025-07-02 01:37:15,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5226496. Throughput: 0: 273.6. Samples: 5229264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:37:15,945][166323] Avg episode reward: [(0, '1180.120')]
[36m[2025-07-02 01:37:20,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5226496. Throughput: 0: 271.8. Samples: 5230048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:37:20,965][166323] Avg episode reward: [(0, '1200.945')]
[31m[18232534 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18232534 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[18232534 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:37:25,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5226496. Throughput: 0: 273.4. Samples: 5231824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:37:25,989][166323] Avg episode reward: [(0, '1120.064')]
[36m[2025-07-02 01:37:30,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5226496. Throughput: 0: 274.0. Samples: 5233552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:37:30,945][166323] Avg episode reward: [(0, '1138.594')]
[36m[2025-07-02 01:37:35,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5226496. Throughput: 0: 276.2. Samples: 5234448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:37:35,959][166323] Avg episode reward: [(0, '1135.234')]
[37m[1m[2025-07-02 01:37:36,024][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010200_5226496.pth...
[36m[2025-07-02 01:37:36,030][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010072_5160960.pth
[36m[2025-07-02 01:37:40,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5226496. Throughput: 0: 277.9. Samples: 5236192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:37:40,953][166323] Avg episode reward: [(0, '1168.825')]
[36m[2025-07-02 01:37:45,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5226496. Throughput: 0: 281.6. Samples: 5237840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:37:45,957][166323] Avg episode reward: [(0, '1169.435')]
[36m[2025-07-02 01:37:50,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5226496. Throughput: 0: 278.9. Samples: 5238544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:37:50,968][166323] Avg episode reward: [(0, '1177.373')]
[36m[2025-07-02 01:37:55,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5226496. Throughput: 0: 281.3. Samples: 5240240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:37:55,985][166323] Avg episode reward: [(0, '1211.479')]
[36m[2025-07-02 01:38:00,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5226496. Throughput: 0: 282.3. Samples: 5241968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:38:00,944][166323] Avg episode reward: [(0, '1282.189')]
[36m[2025-07-02 01:38:05,985][166323] Fps is (10 sec: 1638.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5242880. Throughput: 0: 284.0. Samples: 5242832. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 01:38:05,985][166323] Avg episode reward: [(0, '1226.371')]
[36m[2025-07-02 01:38:10,963][166323] Fps is (10 sec: 1635.2, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5242880. Throughput: 0: 281.4. Samples: 5244480. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 01:38:10,963][166323] Avg episode reward: [(0, '1225.029')]
[36m[2025-07-02 01:38:15,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5242880. Throughput: 0: 277.6. Samples: 5246048. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 01:38:15,959][166323] Avg episode reward: [(0, '1157.287')]
[36m[2025-07-02 01:38:20,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5242880. Throughput: 0: 275.8. Samples: 5246864. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 01:38:20,978][166323] Avg episode reward: [(0, '1121.190')]
[36m[2025-07-02 01:38:25,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5242880. Throughput: 0: 276.3. Samples: 5248624. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 01:38:25,955][166323] Avg episode reward: [(0, '1119.274')]
[31m[18299395 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18299395 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[18299395 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:38:30,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5242880. Throughput: 0: 278.3. Samples: 5250368. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 01:38:30,981][166323] Avg episode reward: [(0, '1065.072')]
[36m[2025-07-02 01:38:35,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5242880. Throughput: 0: 282.7. Samples: 5251264. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 01:38:35,968][166323] Avg episode reward: [(0, '1061.886')]
[36m[2025-07-02 01:38:40,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5242880. Throughput: 0: 280.0. Samples: 5252832. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 01:38:40,963][166323] Avg episode reward: [(0, '1066.729')]
[36m[2025-07-02 01:38:45,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5242880. Throughput: 0: 280.4. Samples: 5254592. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 01:38:45,968][166323] Avg episode reward: [(0, '1052.797')]
[36m[2025-07-02 01:38:50,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5242880. Throughput: 0: 279.8. Samples: 5255424. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 01:38:50,990][166323] Avg episode reward: [(0, '1086.697')]
[31m[18323017 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18323018 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[18323018 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:38:55,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5242880. Throughput: 0: 280.1. Samples: 5257088. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 01:38:55,977][166323] Avg episode reward: [(0, '1061.936')]
[36m[2025-07-02 01:39:00,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5242880. Throughput: 0: 282.3. Samples: 5258752. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 01:39:00,968][166323] Avg episode reward: [(0, '1041.828')]
[36m[2025-07-02 01:39:05,961][166323] Fps is (10 sec: 1640.9, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5259264. Throughput: 0: 283.5. Samples: 5259616. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 01:39:05,961][166323] Avg episode reward: [(0, '1125.220')]
[36m[2025-07-02 01:39:10,971][166323] Fps is (10 sec: 1637.9, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5259264. Throughput: 0: 279.0. Samples: 5261184. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 01:39:10,972][166323] Avg episode reward: [(0, '1127.114')]
[36m[2025-07-02 01:39:15,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5259264. Throughput: 0: 278.7. Samples: 5262912. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 01:39:15,988][166323] Avg episode reward: [(0, '1161.758')]
[36m[2025-07-02 01:39:20,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5259264. Throughput: 0: 275.3. Samples: 5263648. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 01:39:20,944][166323] Avg episode reward: [(0, '1182.237')]
[36m[2025-07-02 01:39:25,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5259264. Throughput: 0: 278.1. Samples: 5265344. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 01:39:25,961][166323] Avg episode reward: [(0, '1161.822')]
[36m[2025-07-02 01:39:30,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5259264. Throughput: 0: 277.7. Samples: 5267088. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 01:39:30,959][166323] Avg episode reward: [(0, '1198.679')]
[36m[2025-07-02 01:39:35,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5259264. Throughput: 0: 276.3. Samples: 5267856. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 01:39:35,984][166323] Avg episode reward: [(0, '1187.165')]
[37m[1m[2025-07-02 01:39:36,033][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010264_5259264.pth...
[36m[2025-07-02 01:39:36,037][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010136_5193728.pth
[36m[2025-07-02 01:39:41,014][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 5259264. Throughput: 0: 276.7. Samples: 5269552. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 01:39:41,014][166323] Avg episode reward: [(0, '1138.470')]
[36m[2025-07-02 01:39:45,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5259264. Throughput: 0: 273.2. Samples: 5271040. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 01:39:45,947][166323] Avg episode reward: [(0, '1126.849')]
[36m[2025-07-02 01:39:50,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5259264. Throughput: 0: 272.6. Samples: 5271888. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 01:39:50,983][166323] Avg episode reward: [(0, '1129.556')]
[36m[2025-07-02 01:39:55,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5259264. Throughput: 0: 275.9. Samples: 5273600. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 01:39:55,966][166323] Avg episode reward: [(0, '1141.452')]
[31m[18385664 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18385664 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[18385664 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:40:00,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5259264. Throughput: 0: 276.9. Samples: 5275360. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 01:40:00,944][166323] Avg episode reward: [(0, '1169.921')]
[36m[2025-07-02 01:40:05,992][166323] Fps is (10 sec: 1634.1, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5275648. Throughput: 0: 276.3. Samples: 5276096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:40:05,993][166323] Avg episode reward: [(0, '1202.144')]
[36m[2025-07-02 01:40:10,970][166323] Fps is (10 sec: 1634.1, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5275648. Throughput: 0: 275.5. Samples: 5277744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:40:10,971][166323] Avg episode reward: [(0, '1177.531')]
[36m[2025-07-02 01:40:15,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5275648. Throughput: 0: 275.0. Samples: 5279472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:40:15,996][166323] Avg episode reward: [(0, '1244.184')]
[36m[2025-07-02 01:40:20,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5275648. Throughput: 0: 277.3. Samples: 5280336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:40:20,986][166323] Avg episode reward: [(0, '1243.080')]
[36m[2025-07-02 01:40:25,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5275648. Throughput: 0: 278.4. Samples: 5282064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:40:25,953][166323] Avg episode reward: [(0, '1241.564')]
[36m[2025-07-02 01:40:30,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5275648. Throughput: 0: 283.2. Samples: 5283792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:40:30,980][166323] Avg episode reward: [(0, '1176.064')]
[36m[2025-07-02 01:40:35,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5275648. Throughput: 0: 283.5. Samples: 5284640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:40:35,956][166323] Avg episode reward: [(0, '1144.246')]
[36m[2025-07-02 01:40:40,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5275648. Throughput: 0: 283.0. Samples: 5286336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:40:40,966][166323] Avg episode reward: [(0, '1170.261')]
[36m[2025-07-02 01:40:45,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5275648. Throughput: 0: 279.8. Samples: 5287952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:40:45,954][166323] Avg episode reward: [(0, '1148.749')]
[36m[2025-07-02 01:40:50,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5275648. Throughput: 0: 279.9. Samples: 5288688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:40:50,973][166323] Avg episode reward: [(0, '1135.231')]
[36m[2025-07-02 01:40:55,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5275648. Throughput: 0: 279.4. Samples: 5290320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:40:55,977][166323] Avg episode reward: [(0, '1187.706')]
[36m[2025-07-02 01:41:00,977][166323] Fps is (10 sec: 1637.6, 60 sec: 545.8, 300 sec: 277.7). Total num frames: 5292032. Throughput: 0: 278.2. Samples: 5291984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:41:00,978][166323] Avg episode reward: [(0, '1146.260')]
[36m[2025-07-02 01:41:05,993][166323] Fps is (10 sec: 1635.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5292032. Throughput: 0: 277.3. Samples: 5292816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:41:05,993][166323] Avg episode reward: [(0, '1228.336')]
[36m[2025-07-02 01:41:10,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5292032. Throughput: 0: 275.1. Samples: 5294448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:41:10,966][166323] Avg episode reward: [(0, '1165.481')]
[36m[2025-07-02 01:41:15,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5292032. Throughput: 0: 271.4. Samples: 5296000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:41:15,959][166323] Avg episode reward: [(0, '1156.795')]
[31m[18469238 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18469239 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[18469239 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:41:20,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5292032. Throughput: 0: 270.9. Samples: 5296832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:41:20,959][166323] Avg episode reward: [(0, '1188.880')]
[36m[2025-07-02 01:41:25,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5292032. Throughput: 0: 270.0. Samples: 5298480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:41:25,946][166323] Avg episode reward: [(0, '1178.827')]
[36m[2025-07-02 01:41:30,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5292032. Throughput: 0: 275.5. Samples: 5300352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:41:30,961][166323] Avg episode reward: [(0, '1164.575')]
[36m[2025-07-02 01:41:35,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5292032. Throughput: 0: 278.4. Samples: 5301216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:41:35,968][166323] Avg episode reward: [(0, '1182.546')]
[37m[1m[2025-07-02 01:41:36,020][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010328_5292032.pth...
[36m[2025-07-02 01:41:36,024][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010200_5226496.pth
[36m[2025-07-02 01:41:40,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5292032. Throughput: 0: 282.0. Samples: 5303008. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:41:40,966][166323] Avg episode reward: [(0, '1164.827')]
[36m[2025-07-02 01:41:45,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5292032. Throughput: 0: 281.3. Samples: 5304640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:41:45,972][166323] Avg episode reward: [(0, '1228.220')]
[36m[2025-07-02 01:41:51,000][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5292032. Throughput: 0: 280.1. Samples: 5305424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:41:51,001][166323] Avg episode reward: [(0, '1191.554')]
[36m[2025-07-02 01:41:55,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5292032. Throughput: 0: 281.0. Samples: 5307088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:41:55,955][166323] Avg episode reward: [(0, '1217.300')]
[36m[2025-07-02 01:42:00,960][166323] Fps is (10 sec: 1645.1, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5308416. Throughput: 0: 283.4. Samples: 5308752. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 01:42:00,960][166323] Avg episode reward: [(0, '1158.735')]
[36m[2025-07-02 01:42:05,981][166323] Fps is (10 sec: 1634.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5308416. Throughput: 0: 282.5. Samples: 5309552. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 01:42:05,981][166323] Avg episode reward: [(0, '1214.608')]
[36m[2025-07-02 01:42:10,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5308416. Throughput: 0: 284.6. Samples: 5311296. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 01:42:10,983][166323] Avg episode reward: [(0, '1247.444')]
[36m[2025-07-02 01:42:15,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5308416. Throughput: 0: 277.4. Samples: 5312832. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 01:42:15,953][166323] Avg episode reward: [(0, '1242.254')]
[36m[2025-07-02 01:42:20,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5308416. Throughput: 0: 277.0. Samples: 5313680. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 01:42:20,956][166323] Avg episode reward: [(0, '1207.219')]
[36m[2025-07-02 01:42:25,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5308416. Throughput: 0: 276.0. Samples: 5315424. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 01:42:25,945][166323] Avg episode reward: [(0, '1206.507')]
[36m[2025-07-02 01:42:30,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5308416. Throughput: 0: 275.1. Samples: 5317024. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 01:42:30,985][166323] Avg episode reward: [(0, '1238.696')]
[36m[2025-07-02 01:42:35,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5308416. Throughput: 0: 275.3. Samples: 5317808. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 01:42:35,978][166323] Avg episode reward: [(0, '1202.001')]
[36m[2025-07-02 01:42:40,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5308416. Throughput: 0: 274.8. Samples: 5319456. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 01:42:40,956][166323] Avg episode reward: [(0, '1184.889')]
[36m[2025-07-02 01:42:45,998][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5308416. Throughput: 0: 277.8. Samples: 5321264. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 01:42:45,999][166323] Avg episode reward: [(0, '1196.851')]
[36m[2025-07-02 01:42:50,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5308416. Throughput: 0: 280.3. Samples: 5322160. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 01:42:50,960][166323] Avg episode reward: [(0, '1199.388')]
[36m[2025-07-02 01:42:55,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5308416. Throughput: 0: 278.5. Samples: 5323824. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 01:42:55,961][166323] Avg episode reward: [(0, '1227.725')]
[36m[2025-07-02 01:43:00,988][166323] Fps is (10 sec: 1633.8, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5324800. Throughput: 0: 278.9. Samples: 5325392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:43:00,988][166323] Avg episode reward: [(0, '1194.093')]
[36m[2025-07-02 01:43:05,951][166323] Fps is (10 sec: 1640.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5324800. Throughput: 0: 277.4. Samples: 5326160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:43:05,951][166323] Avg episode reward: [(0, '1223.162')]
[36m[2025-07-02 01:43:10,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5324800. Throughput: 0: 271.2. Samples: 5327632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:43:10,964][166323] Avg episode reward: [(0, '1198.797')]
[31m[18584098 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18584098 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[18584098 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:43:15,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5324800. Throughput: 0: 276.3. Samples: 5329456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:43:15,977][166323] Avg episode reward: [(0, '1134.073')]
[36m[2025-07-02 01:43:20,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5324800. Throughput: 0: 278.8. Samples: 5330352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:43:20,975][166323] Avg episode reward: [(0, '1152.516')]
[33m[18590571 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[18590571 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.8466796875
[33mCrash Rate: 0.140625
[33mTimeout Rate: 0.0126953125 (navigation_task.py:265)
[33m[18590571 ms][navigation_task] - WARNING : 
[33mSuccesses: 1734
[33mCrashes : 288
[33mTimeouts: 26 (navigation_task.py:268)
[36m[2025-07-02 01:43:25,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5324800. Throughput: 0: 276.8. Samples: 5331920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:43:25,985][166323] Avg episode reward: [(0, '1098.475')]
[36m[2025-07-02 01:43:30,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5324800. Throughput: 0: 273.6. Samples: 5333568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:43:30,975][166323] Avg episode reward: [(0, '1095.969')]
[36m[2025-07-02 01:43:35,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5324800. Throughput: 0: 269.6. Samples: 5334288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:43:35,946][166323] Avg episode reward: [(0, '1045.350')]
[37m[1m[2025-07-02 01:43:35,997][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010392_5324800.pth...
[36m[2025-07-02 01:43:36,001][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010264_5259264.pth
[31m[18607007 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18607007 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[18607008 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[18607060 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18607060 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[18607061 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:43:40,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5324800. Throughput: 0: 268.7. Samples: 5335920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:43:40,970][166323] Avg episode reward: [(0, '1024.615')]
[31m[18614121 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18614122 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[18614122 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:43:45,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5324800. Throughput: 0: 274.3. Samples: 5337728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:43:45,969][166323] Avg episode reward: [(0, '1011.659')]
[36m[2025-07-02 01:43:50,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5324800. Throughput: 0: 274.9. Samples: 5338528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:43:50,947][166323] Avg episode reward: [(0, '990.986')]
[36m[2025-07-02 01:43:55,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5324800. Throughput: 0: 278.0. Samples: 5340144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:43:55,977][166323] Avg episode reward: [(0, '1062.208')]
[36m[2025-07-02 01:44:00,944][166323] Fps is (10 sec: 1638.8, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5341184. Throughput: 0: 271.5. Samples: 5341664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:44:00,947][166323] Avg episode reward: [(0, '1097.573')]
[36m[2025-07-02 01:44:05,975][166323] Fps is (10 sec: 1638.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5341184. Throughput: 0: 269.2. Samples: 5342464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:44:05,975][166323] Avg episode reward: [(0, '1148.464')]
[36m[2025-07-02 01:44:10,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5341184. Throughput: 0: 270.3. Samples: 5344080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:44:10,966][166323] Avg episode reward: [(0, '1174.780')]
[36m[2025-07-02 01:44:15,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5341184. Throughput: 0: 270.3. Samples: 5345728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:44:15,958][166323] Avg episode reward: [(0, '1158.613')]
[36m[2025-07-02 01:44:20,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5341184. Throughput: 0: 273.6. Samples: 5346608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:44:20,972][166323] Avg episode reward: [(0, '1189.068')]
[36m[2025-07-02 01:44:25,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5341184. Throughput: 0: 276.9. Samples: 5348384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:44:25,980][166323] Avg episode reward: [(0, '1229.913')]
[36m[2025-07-02 01:44:30,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5341184. Throughput: 0: 274.7. Samples: 5350096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:44:30,986][166323] Avg episode reward: [(0, '1160.160')]
[36m[2025-07-02 01:44:35,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5341184. Throughput: 0: 274.6. Samples: 5350896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:44:35,990][166323] Avg episode reward: [(0, '1135.653')]
[36m[2025-07-02 01:44:40,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 5341184. Throughput: 0: 276.9. Samples: 5352608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:44:40,995][166323] Avg episode reward: [(0, '1113.271')]
[36m[2025-07-02 01:44:45,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5341184. Throughput: 0: 281.1. Samples: 5354320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:44:45,974][166323] Avg episode reward: [(0, '1158.445')]
[36m[2025-07-02 01:44:50,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5341184. Throughput: 0: 283.2. Samples: 5355200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:44:50,945][166323] Avg episode reward: [(0, '1134.970')]
[36m[2025-07-02 01:44:56,001][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 5341184. Throughput: 0: 284.6. Samples: 5356896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:44:56,001][166323] Avg episode reward: [(0, '1143.989')]
[36m[2025-07-02 01:45:00,964][166323] Fps is (10 sec: 1635.2, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5357568. Throughput: 0: 286.5. Samples: 5358624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:45:00,965][166323] Avg episode reward: [(0, '1162.846')]
[36m[2025-07-02 01:45:05,989][166323] Fps is (10 sec: 1640.4, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5357568. Throughput: 0: 285.8. Samples: 5359472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:45:05,989][166323] Avg episode reward: [(0, '1213.449')]
[36m[2025-07-02 01:45:10,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5357568. Throughput: 0: 286.1. Samples: 5361264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:45:10,994][166323] Avg episode reward: [(0, '1240.213')]
[36m[2025-07-02 01:45:15,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5357568. Throughput: 0: 286.4. Samples: 5362976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:45:15,957][166323] Avg episode reward: [(0, '1198.319')]
[36m[2025-07-02 01:45:20,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5357568. Throughput: 0: 286.2. Samples: 5363760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:45:20,944][166323] Avg episode reward: [(0, '1203.583')]
[36m[2025-07-02 01:45:25,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5357568. Throughput: 0: 284.0. Samples: 5365376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:45:25,954][166323] Avg episode reward: [(0, '1211.613')]
[36m[2025-07-02 01:45:30,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5357568. Throughput: 0: 284.7. Samples: 5367136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:45:30,984][166323] Avg episode reward: [(0, '1241.305')]
[36m[2025-07-02 01:45:35,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5357568. Throughput: 0: 284.0. Samples: 5367984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:45:35,955][166323] Avg episode reward: [(0, '1171.709')]
[37m[1m[2025-07-02 01:45:36,005][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010456_5357568.pth...
[36m[2025-07-02 01:45:36,009][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010328_5292032.pth
[36m[2025-07-02 01:45:40,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5357568. Throughput: 0: 282.6. Samples: 5369600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:45:40,961][166323] Avg episode reward: [(0, '1202.569')]
[36m[2025-07-02 01:45:45,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5357568. Throughput: 0: 281.3. Samples: 5371280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:45:45,960][166323] Avg episode reward: [(0, '1244.382')]
[36m[2025-07-02 01:45:50,973][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5357568. Throughput: 0: 281.7. Samples: 5372144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:45:50,973][166323] Avg episode reward: [(0, '1228.895')]
[36m[2025-07-02 01:45:55,952][166323] Fps is (10 sec: 1639.5, 60 sec: 546.6, 300 sec: 277.7). Total num frames: 5373952. Throughput: 0: 277.9. Samples: 5373760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:45:55,953][166323] Avg episode reward: [(0, '1217.224')]
[36m[2025-07-02 01:46:01,013][166323] Fps is (10 sec: 1631.8, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 5373952. Throughput: 0: 278.1. Samples: 5375504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:46:01,014][166323] Avg episode reward: [(0, '1197.902')]
[36m[2025-07-02 01:46:05,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5373952. Throughput: 0: 279.0. Samples: 5376320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:46:05,969][166323] Avg episode reward: [(0, '1211.474')]
[31m[18758670 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18758670 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[18758670 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:46:10,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5373952. Throughput: 0: 277.7. Samples: 5377872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:46:10,949][166323] Avg episode reward: [(0, '1234.637')]
[31m[18760008 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18760008 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[18760008 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:46:15,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5373952. Throughput: 0: 274.9. Samples: 5379504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:46:15,969][166323] Avg episode reward: [(0, '1167.032')]
[36m[2025-07-02 01:46:20,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5373952. Throughput: 0: 274.8. Samples: 5380352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:46:20,966][166323] Avg episode reward: [(0, '1185.134')]
[36m[2025-07-02 01:46:25,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5373952. Throughput: 0: 277.0. Samples: 5382064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:46:25,953][166323] Avg episode reward: [(0, '1202.508')]
[36m[2025-07-02 01:46:30,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5373952. Throughput: 0: 280.4. Samples: 5383904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:46:30,986][166323] Avg episode reward: [(0, '1270.763')]
[36m[2025-07-02 01:46:35,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5373952. Throughput: 0: 279.2. Samples: 5384704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:46:35,957][166323] Avg episode reward: [(0, '1260.250')]
[36m[2025-07-02 01:46:40,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5373952. Throughput: 0: 280.2. Samples: 5386368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:46:40,945][166323] Avg episode reward: [(0, '1201.370')]
[31m[18793864 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18793864 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[18793864 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:46:45,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5373952. Throughput: 0: 280.6. Samples: 5388112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:46:45,947][166323] Avg episode reward: [(0, '1246.005')]
[36m[2025-07-02 01:46:50,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5373952. Throughput: 0: 280.4. Samples: 5388944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:46:50,991][166323] Avg episode reward: [(0, '1213.217')]
[36m[2025-07-02 01:46:55,978][166323] Fps is (10 sec: 1633.3, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5390336. Throughput: 0: 279.6. Samples: 5390464. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 01:46:55,978][166323] Avg episode reward: [(0, '1177.734')]
[36m[2025-07-02 01:47:01,005][166323] Fps is (10 sec: 1636.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5390336. Throughput: 0: 277.5. Samples: 5392000. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 01:47:01,006][166323] Avg episode reward: [(0, '1186.734')]
[36m[2025-07-02 01:47:05,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5390336. Throughput: 0: 277.1. Samples: 5392816. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 01:47:05,948][166323] Avg episode reward: [(0, '1152.457')]
[36m[2025-07-02 01:47:10,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5390336. Throughput: 0: 277.2. Samples: 5394544. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 01:47:10,972][166323] Avg episode reward: [(0, '1159.116')]
[36m[2025-07-02 01:47:16,005][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 5390336. Throughput: 0: 272.9. Samples: 5396192. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 01:47:16,006][166323] Avg episode reward: [(0, '1162.872')]
[36m[2025-07-02 01:47:20,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5390336. Throughput: 0: 274.3. Samples: 5397056. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 01:47:20,983][166323] Avg episode reward: [(0, '1137.347')]
[36m[2025-07-02 01:47:25,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5390336. Throughput: 0: 272.4. Samples: 5398640. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 01:47:25,989][166323] Avg episode reward: [(0, '1176.573')]
[36m[2025-07-02 01:47:30,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5390336. Throughput: 0: 271.6. Samples: 5400336. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 01:47:30,946][166323] Avg episode reward: [(0, '1184.758')]
[36m[2025-07-02 01:47:35,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5390336. Throughput: 0: 273.2. Samples: 5401232. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 01:47:35,970][166323] Avg episode reward: [(0, '1203.869')]
[37m[1m[2025-07-02 01:47:36,038][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010520_5390336.pth...
[36m[2025-07-02 01:47:36,042][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010392_5324800.pth
[36m[2025-07-02 01:47:40,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5390336. Throughput: 0: 275.8. Samples: 5402880. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 01:47:40,991][166323] Avg episode reward: [(0, '1184.305')]
[36m[2025-07-02 01:47:45,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5390336. Throughput: 0: 278.2. Samples: 5404512. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 01:47:45,988][166323] Avg episode reward: [(0, '1161.651')]
[31m[18856298 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18856299 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[18856299 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:47:50,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5390336. Throughput: 0: 275.7. Samples: 5405232. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 01:47:50,980][166323] Avg episode reward: [(0, '1171.717')]
[36m[2025-07-02 01:47:55,986][166323] Fps is (10 sec: 1638.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5406720. Throughput: 0: 274.0. Samples: 5406880. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 01:47:55,986][166323] Avg episode reward: [(0, '1194.914')]
[36m[2025-07-02 01:48:00,998][166323] Fps is (10 sec: 1635.3, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5406720. Throughput: 0: 274.2. Samples: 5408528. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 01:48:00,999][166323] Avg episode reward: [(0, '1151.976')]
[36m[2025-07-02 01:48:06,007][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 5406720. Throughput: 0: 273.6. Samples: 5409376. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 01:48:06,007][166323] Avg episode reward: [(0, '1190.884')]
[36m[2025-07-02 01:48:11,007][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5406720. Throughput: 0: 274.0. Samples: 5410976. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 01:48:11,007][166323] Avg episode reward: [(0, '1154.310')]
[31m[18884431 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18884432 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[18884433 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:48:16,007][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5406720. Throughput: 0: 270.6. Samples: 5412528. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 01:48:16,007][166323] Avg episode reward: [(0, '1157.549')]
[36m[2025-07-02 01:48:20,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5406720. Throughput: 0: 269.4. Samples: 5413360. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 01:48:20,985][166323] Avg episode reward: [(0, '1102.870')]
[36m[2025-07-02 01:48:25,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5406720. Throughput: 0: 267.6. Samples: 5414912. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 01:48:25,948][166323] Avg episode reward: [(0, '1106.478')]
[36m[2025-07-02 01:48:30,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5406720. Throughput: 0: 267.4. Samples: 5416544. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 01:48:30,984][166323] Avg episode reward: [(0, '1143.302')]
[36m[2025-07-02 01:48:36,017][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5406720. Throughput: 0: 270.0. Samples: 5417392. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 01:48:36,017][166323] Avg episode reward: [(0, '1136.020')]
[36m[2025-07-02 01:48:40,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5406720. Throughput: 0: 267.3. Samples: 5418912. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 01:48:40,992][166323] Avg episode reward: [(0, '1131.460')]
[36m[2025-07-02 01:48:46,006][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 5406720. Throughput: 0: 263.4. Samples: 5420384. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 01:48:46,006][166323] Avg episode reward: [(0, '1113.501')]
[36m[2025-07-02 01:48:50,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5406720. Throughput: 0: 261.9. Samples: 5421152. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 01:48:50,979][166323] Avg episode reward: [(0, '1139.507')]
[31m[18920402 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18920403 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[18920403 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:48:55,966][166323] Fps is (10 sec: 1645.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5423104. Throughput: 0: 265.5. Samples: 5422912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:48:55,966][166323] Avg episode reward: [(0, '1236.490')]
[36m[2025-07-02 01:49:00,959][166323] Fps is (10 sec: 1641.7, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5423104. Throughput: 0: 268.0. Samples: 5424576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:49:00,959][166323] Avg episode reward: [(0, '1238.392')]
[36m[2025-07-02 01:49:05,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5423104. Throughput: 0: 267.9. Samples: 5425408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:49:05,957][166323] Avg episode reward: [(0, '1177.927')]
[36m[2025-07-02 01:49:10,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5423104. Throughput: 0: 269.6. Samples: 5427056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:49:10,995][166323] Avg episode reward: [(0, '1220.660')]
[36m[2025-07-02 01:49:15,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5423104. Throughput: 0: 271.4. Samples: 5428752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:49:15,973][166323] Avg episode reward: [(0, '1193.321')]
[36m[2025-07-02 01:49:20,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5423104. Throughput: 0: 273.0. Samples: 5429664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:49:20,974][166323] Avg episode reward: [(0, '1232.696')]
[36m[2025-07-02 01:49:25,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5423104. Throughput: 0: 275.1. Samples: 5431280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:49:25,943][166323] Avg episode reward: [(0, '1183.718')]
[36m[2025-07-02 01:49:30,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5423104. Throughput: 0: 277.6. Samples: 5432864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:49:30,965][166323] Avg episode reward: [(0, '1227.546')]
[31m[18959675 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18959675 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[18959675 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:49:35,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5423104. Throughput: 0: 280.2. Samples: 5433760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:49:35,979][166323] Avg episode reward: [(0, '1221.162')]
[37m[1m[2025-07-02 01:49:36,033][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010584_5423104.pth...
[36m[2025-07-02 01:49:36,037][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010456_5357568.pth
[36m[2025-07-02 01:49:40,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5423104. Throughput: 0: 281.4. Samples: 5435568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:49:40,944][166323] Avg episode reward: [(0, '1257.483')]
[36m[2025-07-02 01:49:46,002][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.6). Total num frames: 5423104. Throughput: 0: 279.5. Samples: 5437168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:49:46,003][166323] Avg episode reward: [(0, '1224.427')]
[36m[2025-07-02 01:49:50,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5423104. Throughput: 0: 279.9. Samples: 5438000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:49:50,952][166323] Avg episode reward: [(0, '1178.829')]
[36m[2025-07-02 01:49:55,960][166323] Fps is (10 sec: 1645.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5439488. Throughput: 0: 281.1. Samples: 5439696. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:49:55,960][166323] Avg episode reward: [(0, '1199.833')]
[36m[2025-07-02 01:50:00,964][166323] Fps is (10 sec: 1636.3, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5439488. Throughput: 0: 280.6. Samples: 5441376. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:50:00,965][166323] Avg episode reward: [(0, '1234.569')]
[36m[2025-07-02 01:50:05,998][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5439488. Throughput: 0: 278.6. Samples: 5442208. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:50:05,998][166323] Avg episode reward: [(0, '1172.932')]
[36m[2025-07-02 01:50:11,021][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 5439488. Throughput: 0: 279.3. Samples: 5443872. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:50:11,021][166323] Avg episode reward: [(0, '1219.564')]
[36m[2025-07-02 01:50:15,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5439488. Throughput: 0: 280.7. Samples: 5445488. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:50:15,944][166323] Avg episode reward: [(0, '1195.239')]
[31m[19004921 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19004921 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[19004921 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[19004972 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19004973 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[19004973 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:50:20,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5439488. Throughput: 0: 281.4. Samples: 5446416. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:50:20,949][166323] Avg episode reward: [(0, '1112.617')]
[36m[2025-07-02 01:50:25,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5439488. Throughput: 0: 276.3. Samples: 5448000. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:50:25,944][166323] Avg episode reward: [(0, '1135.397')]
[36m[2025-07-02 01:50:30,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5439488. Throughput: 0: 277.7. Samples: 5449648. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:50:30,945][166323] Avg episode reward: [(0, '1155.200')]
[36m[2025-07-02 01:50:36,002][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5439488. Throughput: 0: 279.2. Samples: 5450576. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:50:36,003][166323] Avg episode reward: [(0, '1162.282')]
[36m[2025-07-02 01:50:40,977][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5439488. Throughput: 0: 278.6. Samples: 5452240. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:50:40,978][166323] Avg episode reward: [(0, '1192.116')]
[36m[2025-07-02 01:50:45,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5439488. Throughput: 0: 277.4. Samples: 5453856. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:50:45,950][166323] Avg episode reward: [(0, '1201.663')]
[36m[2025-07-02 01:50:50,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 5439488. Throughput: 0: 280.7. Samples: 5454832. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:50:50,967][166323] Avg episode reward: [(0, '1224.543')]
[36m[2025-07-02 01:50:55,947][166323] Fps is (10 sec: 1638.9, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 5455872. Throughput: 0: 281.3. Samples: 5456512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:50:55,947][166323] Avg episode reward: [(0, '1266.848')]
[36m[2025-07-02 01:51:00,968][166323] Fps is (10 sec: 1638.3, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5455872. Throughput: 0: 280.0. Samples: 5458096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:51:00,968][166323] Avg episode reward: [(0, '1255.669')]
[36m[2025-07-02 01:51:05,995][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5455872. Throughput: 0: 277.4. Samples: 5458912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:51:05,995][166323] Avg episode reward: [(0, '1218.262')]
[31m[19054958 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19054958 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[19054958 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:51:10,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5455872. Throughput: 0: 277.1. Samples: 5460480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:51:10,975][166323] Avg episode reward: [(0, '1223.671')]
[36m[2025-07-02 01:51:15,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5455872. Throughput: 0: 278.4. Samples: 5462176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:51:15,943][166323] Avg episode reward: [(0, '1225.512')]
[36m[2025-07-02 01:51:20,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5455872. Throughput: 0: 276.2. Samples: 5462992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:51:20,955][166323] Avg episode reward: [(0, '1158.454')]
[36m[2025-07-02 01:51:26,001][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 5455872. Throughput: 0: 277.9. Samples: 5464752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:51:26,001][166323] Avg episode reward: [(0, '1194.460')]
[36m[2025-07-02 01:51:30,979][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5455872. Throughput: 0: 278.2. Samples: 5466384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:51:30,980][166323] Avg episode reward: [(0, '1228.366')]
[36m[2025-07-02 01:51:35,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5455872. Throughput: 0: 274.8. Samples: 5467200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:51:35,973][166323] Avg episode reward: [(0, '1214.852')]
[37m[1m[2025-07-02 01:51:36,024][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010648_5455872.pth...
[36m[2025-07-02 01:51:36,028][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010520_5390336.pth
[36m[2025-07-02 01:51:40,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5455872. Throughput: 0: 275.1. Samples: 5468896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:51:40,964][166323] Avg episode reward: [(0, '1213.867')]
[36m[2025-07-02 01:51:45,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5455872. Throughput: 0: 274.5. Samples: 5470448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:51:45,960][166323] Avg episode reward: [(0, '1144.557')]
[36m[2025-07-02 01:51:50,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 5455872. Throughput: 0: 275.4. Samples: 5471296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:51:50,970][166323] Avg episode reward: [(0, '1162.247')]
[31m[19101255 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19101255 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[19101256 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:51:55,997][166323] Fps is (10 sec: 1632.2, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 5472256. Throughput: 0: 275.4. Samples: 5472880. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 01:51:55,998][166323] Avg episode reward: [(0, '1117.941')]
[36m[2025-07-02 01:52:00,988][166323] Fps is (10 sec: 1635.4, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5472256. Throughput: 0: 274.6. Samples: 5474544. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 01:52:00,988][166323] Avg episode reward: [(0, '1118.576')]
[31m[19111743 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19111743 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[19111743 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:52:05,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5472256. Throughput: 0: 274.6. Samples: 5475360. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 01:52:05,990][166323] Avg episode reward: [(0, '1126.312')]
[36m[2025-07-02 01:52:10,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5472256. Throughput: 0: 271.2. Samples: 5476944. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 01:52:10,956][166323] Avg episode reward: [(0, '1086.916')]
[36m[2025-07-02 01:52:15,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5472256. Throughput: 0: 270.6. Samples: 5478560. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 01:52:15,981][166323] Avg episode reward: [(0, '1098.447')]
[36m[2025-07-02 01:52:20,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5472256. Throughput: 0: 270.1. Samples: 5479360. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 01:52:20,992][166323] Avg episode reward: [(0, '1121.142')]
[31m[19132924 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19132924 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[19132924 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:52:25,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5472256. Throughput: 0: 268.5. Samples: 5480976. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 01:52:25,954][166323] Avg episode reward: [(0, '1130.290')]
[36m[2025-07-02 01:52:30,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5472256. Throughput: 0: 269.6. Samples: 5482576. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 01:52:30,953][166323] Avg episode reward: [(0, '1184.842')]
[31m[19142669 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19142670 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[19142670 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:52:35,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5472256. Throughput: 0: 271.0. Samples: 5483488. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 01:52:35,964][166323] Avg episode reward: [(0, '1158.760')]
[36m[2025-07-02 01:52:40,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5472256. Throughput: 0: 272.2. Samples: 5485120. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 01:52:40,968][166323] Avg episode reward: [(0, '1186.555')]
[36m[2025-07-02 01:52:45,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5472256. Throughput: 0: 273.6. Samples: 5486848. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 01:52:45,961][166323] Avg episode reward: [(0, '1201.979')]
[36m[2025-07-02 01:52:50,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 5472256. Throughput: 0: 272.2. Samples: 5487600. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 01:52:50,954][166323] Avg episode reward: [(0, '1206.359')]
[36m[2025-07-02 01:52:55,966][166323] Fps is (10 sec: 1637.6, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5488640. Throughput: 0: 271.6. Samples: 5489168. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 01:52:55,966][166323] Avg episode reward: [(0, '1181.831')]
[36m[2025-07-02 01:53:00,969][166323] Fps is (10 sec: 1635.9, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5488640. Throughput: 0: 272.4. Samples: 5490816. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 01:53:00,969][166323] Avg episode reward: [(0, '1178.502')]
[36m[2025-07-02 01:53:05,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5488640. Throughput: 0: 272.8. Samples: 5491632. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 01:53:05,972][166323] Avg episode reward: [(0, '1198.802')]
[36m[2025-07-02 01:53:10,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5488640. Throughput: 0: 274.3. Samples: 5493328. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 01:53:10,987][166323] Avg episode reward: [(0, '1150.846')]
[36m[2025-07-02 01:53:15,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5488640. Throughput: 0: 278.0. Samples: 5495088. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 01:53:15,958][166323] Avg episode reward: [(0, '1119.038')]
[36m[2025-07-02 01:53:20,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5488640. Throughput: 0: 277.1. Samples: 5495952. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 01:53:20,945][166323] Avg episode reward: [(0, '1102.268')]
[36m[2025-07-02 01:53:25,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5488640. Throughput: 0: 278.4. Samples: 5497648. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 01:53:25,976][166323] Avg episode reward: [(0, '1153.683')]
[36m[2025-07-02 01:53:30,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5488640. Throughput: 0: 273.1. Samples: 5499136. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 01:53:30,963][166323] Avg episode reward: [(0, '1177.699')]
[36m[2025-07-02 01:53:35,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5488640. Throughput: 0: 276.6. Samples: 5500048. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 01:53:35,950][166323] Avg episode reward: [(0, '1158.873')]
[37m[1m[2025-07-02 01:53:36,039][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010712_5488640.pth...
[36m[2025-07-02 01:53:36,044][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010584_5423104.pth
[36m[2025-07-02 01:53:40,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5488640. Throughput: 0: 278.0. Samples: 5501680. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 01:53:40,973][166323] Avg episode reward: [(0, '1185.850')]
[31m[19212666 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19212667 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[19212667 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:53:45,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5488640. Throughput: 0: 278.7. Samples: 5503360. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 01:53:45,982][166323] Avg episode reward: [(0, '1226.433')]
[36m[2025-07-02 01:53:50,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 5488640. Throughput: 0: 277.7. Samples: 5504128. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 01:53:50,962][166323] Avg episode reward: [(0, '1225.808')]
[36m[2025-07-02 01:53:55,947][166323] Fps is (10 sec: 1644.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5505024. Throughput: 0: 277.2. Samples: 5505792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:53:55,948][166323] Avg episode reward: [(0, '1143.276')]
[36m[2025-07-02 01:54:00,945][166323] Fps is (10 sec: 1641.1, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5505024. Throughput: 0: 274.9. Samples: 5507456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:54:00,946][166323] Avg episode reward: [(0, '1116.969')]
[36m[2025-07-02 01:54:05,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5505024. Throughput: 0: 272.8. Samples: 5508240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:54:05,992][166323] Avg episode reward: [(0, '1130.126')]
[31m[19235583 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19235583 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[19235584 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:54:10,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5505024. Throughput: 0: 267.4. Samples: 5509680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:54:10,964][166323] Avg episode reward: [(0, '1141.494')]
[31m[19239581 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19239581 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[19239582 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:54:15,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5505024. Throughput: 0: 267.7. Samples: 5511184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:54:15,967][166323] Avg episode reward: [(0, '1135.303')]
[36m[2025-07-02 01:54:20,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5505024. Throughput: 0: 265.6. Samples: 5512000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:54:20,948][166323] Avg episode reward: [(0, '1196.703')]
[36m[2025-07-02 01:54:25,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5505024. Throughput: 0: 264.8. Samples: 5513600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:54:25,990][166323] Avg episode reward: [(0, '1204.383')]
[36m[2025-07-02 01:54:31,006][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5505024. Throughput: 0: 264.7. Samples: 5515280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:54:31,006][166323] Avg episode reward: [(0, '1258.912')]
[36m[2025-07-02 01:54:35,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5505024. Throughput: 0: 264.9. Samples: 5516048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:54:35,962][166323] Avg episode reward: [(0, '1251.740')]
[36m[2025-07-02 01:54:41,003][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5505024. Throughput: 0: 266.3. Samples: 5517792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:54:41,004][166323] Avg episode reward: [(0, '1200.199')]
[36m[2025-07-02 01:54:45,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5505024. Throughput: 0: 266.4. Samples: 5519456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:54:45,992][166323] Avg episode reward: [(0, '1237.248')]
[33m[19278457 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[19278457 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.8544921875
[33mCrash Rate: 0.13427734375
[33mTimeout Rate: 0.01123046875 (navigation_task.py:265)
[33m[19278457 ms][navigation_task] - WARNING : 
[33mSuccesses: 1750
[33mCrashes : 275
[33mTimeouts: 23 (navigation_task.py:268)
[36m[2025-07-02 01:54:51,013][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 222.1). Total num frames: 5505024. Throughput: 0: 268.7. Samples: 5520336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:54:51,013][166323] Avg episode reward: [(0, '1191.521')]
[36m[2025-07-02 01:54:56,003][166323] Fps is (10 sec: 1636.6, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 5521408. Throughput: 0: 273.5. Samples: 5522000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:54:56,003][166323] Avg episode reward: [(0, '1161.155')]
[36m[2025-07-02 01:55:00,983][166323] Fps is (10 sec: 1643.3, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5521408. Throughput: 0: 279.0. Samples: 5523744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:55:00,984][166323] Avg episode reward: [(0, '1127.764')]
[36m[2025-07-02 01:55:05,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5521408. Throughput: 0: 281.7. Samples: 5524688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:55:05,984][166323] Avg episode reward: [(0, '1149.010')]
[36m[2025-07-02 01:55:10,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5521408. Throughput: 0: 285.3. Samples: 5526432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:55:10,967][166323] Avg episode reward: [(0, '1172.614')]
[36m[2025-07-02 01:55:15,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5521408. Throughput: 0: 287.6. Samples: 5528208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:55:15,950][166323] Avg episode reward: [(0, '1150.621')]
[31m[19305856 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19305857 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[19305857 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:55:20,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 5521408. Throughput: 0: 288.5. Samples: 5529040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:55:20,995][166323] Avg episode reward: [(0, '1183.225')]
[36m[2025-07-02 01:55:25,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5521408. Throughput: 0: 288.8. Samples: 5530784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:55:25,982][166323] Avg episode reward: [(0, '1223.292')]
[36m[2025-07-02 01:55:30,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5521408. Throughput: 0: 287.6. Samples: 5532384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:55:30,949][166323] Avg episode reward: [(0, '1124.180')]
[36m[2025-07-02 01:55:35,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5521408. Throughput: 0: 286.0. Samples: 5533200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:55:36,000][166323] Avg episode reward: [(0, '1098.481')]
[37m[1m[2025-07-02 01:55:36,061][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010776_5521408.pth...
[36m[2025-07-02 01:55:36,069][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010648_5455872.pth
[36m[2025-07-02 01:55:40,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5521408. Throughput: 0: 284.8. Samples: 5534800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:55:40,944][166323] Avg episode reward: [(0, '1048.433')]
[36m[2025-07-02 01:55:45,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5521408. Throughput: 0: 284.5. Samples: 5536544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:55:45,980][166323] Avg episode reward: [(0, '1094.230')]
[36m[2025-07-02 01:55:50,973][166323] Fps is (10 sec: 1633.5, 60 sec: 546.5, 300 sec: 277.7). Total num frames: 5537792. Throughput: 0: 281.0. Samples: 5537328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:55:50,974][166323] Avg episode reward: [(0, '1083.709')]
[36m[2025-07-02 01:55:55,960][166323] Fps is (10 sec: 1641.5, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5537792. Throughput: 0: 279.9. Samples: 5539024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:55:55,961][166323] Avg episode reward: [(0, '1063.649')]
[36m[2025-07-02 01:56:00,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5537792. Throughput: 0: 280.1. Samples: 5540816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:56:00,970][166323] Avg episode reward: [(0, '1121.068')]
[36m[2025-07-02 01:56:05,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5537792. Throughput: 0: 281.2. Samples: 5541680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:56:05,948][166323] Avg episode reward: [(0, '1193.888')]
[36m[2025-07-02 01:56:10,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5537792. Throughput: 0: 279.7. Samples: 5543360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:56:10,948][166323] Avg episode reward: [(0, '1182.532')]
[36m[2025-07-02 01:56:15,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5537792. Throughput: 0: 277.0. Samples: 5544848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:56:15,948][166323] Avg episode reward: [(0, '1221.526')]
[36m[2025-07-02 01:56:20,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5537792. Throughput: 0: 279.0. Samples: 5545744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:56:20,964][166323] Avg episode reward: [(0, '1242.748')]
[36m[2025-07-02 01:56:26,018][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5537792. Throughput: 0: 279.7. Samples: 5547408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:56:26,018][166323] Avg episode reward: [(0, '1222.406')]
[36m[2025-07-02 01:56:30,978][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5537792. Throughput: 0: 279.5. Samples: 5549120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:56:30,978][166323] Avg episode reward: [(0, '1215.590')]
[36m[2025-07-02 01:56:35,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5537792. Throughput: 0: 280.9. Samples: 5549968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:56:35,971][166323] Avg episode reward: [(0, '1205.966')]
[36m[2025-07-02 01:56:40,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5537792. Throughput: 0: 283.4. Samples: 5551776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:56:40,965][166323] Avg episode reward: [(0, '1210.353')]
[36m[2025-07-02 01:56:45,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5537792. Throughput: 0: 280.0. Samples: 5553408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:56:45,946][166323] Avg episode reward: [(0, '1196.998')]
[36m[2025-07-02 01:56:50,971][166323] Fps is (10 sec: 1637.3, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5554176. Throughput: 0: 277.9. Samples: 5554192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:56:50,971][166323] Avg episode reward: [(0, '1134.919')]
[36m[2025-07-02 01:56:56,001][166323] Fps is (10 sec: 1629.4, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5554176. Throughput: 0: 278.4. Samples: 5555904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:56:56,001][166323] Avg episode reward: [(0, '1185.845')]
[31m[19406499 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19406500 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[19406500 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:57:00,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5554176. Throughput: 0: 284.0. Samples: 5557632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:57:00,955][166323] Avg episode reward: [(0, '1183.170')]
[36m[2025-07-02 01:57:05,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5554176. Throughput: 0: 282.7. Samples: 5558464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:57:05,957][166323] Avg episode reward: [(0, '1190.966')]
[36m[2025-07-02 01:57:10,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5554176. Throughput: 0: 281.7. Samples: 5560064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:57:10,944][166323] Avg episode reward: [(0, '1146.355')]
[36m[2025-07-02 01:57:15,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5554176. Throughput: 0: 281.0. Samples: 5561760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:57:15,955][166323] Avg episode reward: [(0, '1210.216')]
[36m[2025-07-02 01:57:20,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5554176. Throughput: 0: 281.1. Samples: 5562624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:57:20,988][166323] Avg episode reward: [(0, '1200.014')]
[36m[2025-07-02 01:57:26,004][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.6). Total num frames: 5554176. Throughput: 0: 277.1. Samples: 5564256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:57:26,005][166323] Avg episode reward: [(0, '1173.360')]
[36m[2025-07-02 01:57:30,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5554176. Throughput: 0: 277.8. Samples: 5565920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:57:30,979][166323] Avg episode reward: [(0, '1154.594')]
[36m[2025-07-02 01:57:35,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5554176. Throughput: 0: 281.0. Samples: 5566832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:57:35,952][166323] Avg episode reward: [(0, '1160.718')]
[37m[1m[2025-07-02 01:57:36,023][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010840_5554176.pth...
[36m[2025-07-02 01:57:36,028][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010712_5488640.pth
[36m[2025-07-02 01:57:40,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5554176. Throughput: 0: 279.0. Samples: 5568448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:57:40,954][166323] Avg episode reward: [(0, '1131.998')]
[36m[2025-07-02 01:57:45,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5554176. Throughput: 0: 275.0. Samples: 5570016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:57:45,987][166323] Avg episode reward: [(0, '1060.311')]
[36m[2025-07-02 01:57:50,989][166323] Fps is (10 sec: 1632.7, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5570560. Throughput: 0: 272.2. Samples: 5570720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:57:50,989][166323] Avg episode reward: [(0, '1067.983')]
[36m[2025-07-02 01:57:55,946][166323] Fps is (10 sec: 1645.2, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5570560. Throughput: 0: 272.7. Samples: 5572336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:57:55,946][166323] Avg episode reward: [(0, '1042.927')]
[36m[2025-07-02 01:58:01,017][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 5570560. Throughput: 0: 271.3. Samples: 5573984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:58:01,018][166323] Avg episode reward: [(0, '1079.384')]
[36m[2025-07-02 01:58:05,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5570560. Throughput: 0: 270.0. Samples: 5574768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:58:05,973][166323] Avg episode reward: [(0, '1083.310')]
[36m[2025-07-02 01:58:10,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5570560. Throughput: 0: 273.3. Samples: 5576544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:58:10,967][166323] Avg episode reward: [(0, '1098.310')]
[36m[2025-07-02 01:58:15,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5570560. Throughput: 0: 273.2. Samples: 5578208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:58:15,956][166323] Avg episode reward: [(0, '1142.782')]
[36m[2025-07-02 01:58:20,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5570560. Throughput: 0: 271.9. Samples: 5579072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:58:20,971][166323] Avg episode reward: [(0, '1156.931')]
[36m[2025-07-02 01:58:25,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5570560. Throughput: 0: 270.5. Samples: 5580624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:58:25,972][166323] Avg episode reward: [(0, '1183.241')]
[36m[2025-07-02 01:58:31,011][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 5570560. Throughput: 0: 270.8. Samples: 5582208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:58:31,012][166323] Avg episode reward: [(0, '1165.400')]
[36m[2025-07-02 01:58:35,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5570560. Throughput: 0: 273.2. Samples: 5583008. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:58:35,971][166323] Avg episode reward: [(0, '1178.926')]
[36m[2025-07-02 01:58:40,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5570560. Throughput: 0: 272.9. Samples: 5584624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:58:40,970][166323] Avg episode reward: [(0, '1193.254')]
[36m[2025-07-02 01:58:45,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5570560. Throughput: 0: 277.2. Samples: 5586448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 01:58:45,988][166323] Avg episode reward: [(0, '1190.480')]
[36m[2025-07-02 01:58:50,944][166323] Fps is (10 sec: 1642.7, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5586944. Throughput: 0: 278.9. Samples: 5587312. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:58:50,944][166323] Avg episode reward: [(0, '1197.785')]
[36m[2025-07-02 01:58:55,984][166323] Fps is (10 sec: 1639.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5586944. Throughput: 0: 275.1. Samples: 5588928. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:58:55,984][166323] Avg episode reward: [(0, '1168.542')]
[31m[19527534 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19527534 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[19527534 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[19528948 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19528948 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[19528949 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:59:00,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 5586944. Throughput: 0: 277.4. Samples: 5590688. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:59:00,950][166323] Avg episode reward: [(0, '1177.935')]
[31m[19533447 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19533447 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[19533448 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:59:05,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5586944. Throughput: 0: 277.6. Samples: 5591568. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:59:05,990][166323] Avg episode reward: [(0, '1191.394')]
[36m[2025-07-02 01:59:10,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5586944. Throughput: 0: 279.2. Samples: 5593184. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:59:10,964][166323] Avg episode reward: [(0, '1153.672')]
[36m[2025-07-02 01:59:15,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5586944. Throughput: 0: 278.7. Samples: 5594736. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:59:15,970][166323] Avg episode reward: [(0, '1209.595')]
[36m[2025-07-02 01:59:20,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5586944. Throughput: 0: 280.7. Samples: 5595632. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:59:20,944][166323] Avg episode reward: [(0, '1187.037')]
[36m[2025-07-02 01:59:25,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5586944. Throughput: 0: 279.5. Samples: 5597200. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:59:25,959][166323] Avg episode reward: [(0, '1182.405')]
[36m[2025-07-02 01:59:30,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5586944. Throughput: 0: 279.3. Samples: 5599008. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:59:30,954][166323] Avg episode reward: [(0, '1124.321')]
[36m[2025-07-02 01:59:35,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5586944. Throughput: 0: 278.5. Samples: 5599856. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:59:35,982][166323] Avg episode reward: [(0, '1162.781')]
[37m[1m[2025-07-02 01:59:36,051][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010904_5586944.pth...
[36m[2025-07-02 01:59:36,055][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010776_5521408.pth
[36m[2025-07-02 01:59:40,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5586944. Throughput: 0: 276.6. Samples: 5601376. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:59:40,989][166323] Avg episode reward: [(0, '1175.628')]
[36m[2025-07-02 01:59:45,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 5586944. Throughput: 0: 277.0. Samples: 5603152. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 01:59:45,949][166323] Avg episode reward: [(0, '1183.458')]
[31m[19578640 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19578641 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[19578641 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 01:59:50,975][166323] Fps is (10 sec: 1640.7, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5603328. Throughput: 0: 275.3. Samples: 5603952. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 01:59:50,975][166323] Avg episode reward: [(0, '1168.486')]
[36m[2025-07-02 01:59:56,004][166323] Fps is (10 sec: 1629.4, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5603328. Throughput: 0: 276.0. Samples: 5605616. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 01:59:56,004][166323] Avg episode reward: [(0, '1192.988')]
[36m[2025-07-02 02:00:00,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5603328. Throughput: 0: 278.4. Samples: 5607264. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 02:00:00,964][166323] Avg episode reward: [(0, '1197.749')]
[31m[19589618 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19589618 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[19589618 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:00:06,004][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5603328. Throughput: 0: 275.5. Samples: 5608048. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 02:00:06,004][166323] Avg episode reward: [(0, '1223.056')]
[36m[2025-07-02 02:00:10,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5603328. Throughput: 0: 278.6. Samples: 5609744. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 02:00:10,982][166323] Avg episode reward: [(0, '1177.916')]
[36m[2025-07-02 02:00:15,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5603328. Throughput: 0: 275.2. Samples: 5611392. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 02:00:15,952][166323] Avg episode reward: [(0, '1187.270')]
[36m[2025-07-02 02:00:20,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5603328. Throughput: 0: 275.7. Samples: 5612256. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 02:00:20,965][166323] Avg episode reward: [(0, '1207.612')]
[36m[2025-07-02 02:00:25,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5603328. Throughput: 0: 278.5. Samples: 5613904. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 02:00:25,965][166323] Avg episode reward: [(0, '1252.485')]
[36m[2025-07-02 02:00:30,997][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5603328. Throughput: 0: 272.4. Samples: 5615424. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 02:00:30,997][166323] Avg episode reward: [(0, '1240.295')]
[31m[19620017 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19620017 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[19620017 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:00:36,004][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 5603328. Throughput: 0: 273.6. Samples: 5616272. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 02:00:36,004][166323] Avg episode reward: [(0, '1238.697')]
[31m[19626824 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19626824 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[19626825 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:00:40,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5603328. Throughput: 0: 272.3. Samples: 5617856. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 02:00:40,952][166323] Avg episode reward: [(0, '1228.155')]
[36m[2025-07-02 02:00:46,006][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 222.1). Total num frames: 5603328. Throughput: 0: 271.0. Samples: 5619472. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 02:00:46,006][166323] Avg episode reward: [(0, '1219.899')]
[36m[2025-07-02 02:00:50,983][166323] Fps is (10 sec: 1633.3, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5619712. Throughput: 0: 270.0. Samples: 5620192. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:00:50,983][166323] Avg episode reward: [(0, '1205.354')]
[36m[2025-07-02 02:00:55,954][166323] Fps is (10 sec: 1646.8, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5619712. Throughput: 0: 270.4. Samples: 5621904. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:00:55,955][166323] Avg episode reward: [(0, '1117.696')]
[36m[2025-07-02 02:01:01,007][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 5619712. Throughput: 0: 271.7. Samples: 5623632. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:01:01,007][166323] Avg episode reward: [(0, '1079.352')]
[36m[2025-07-02 02:01:05,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.6). Total num frames: 5619712. Throughput: 0: 270.0. Samples: 5624416. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:01:05,997][166323] Avg episode reward: [(0, '1137.091')]
[36m[2025-07-02 02:01:11,006][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 5619712. Throughput: 0: 270.0. Samples: 5626064. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:01:11,006][166323] Avg episode reward: [(0, '1104.252')]
[36m[2025-07-02 02:01:15,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5619712. Throughput: 0: 272.7. Samples: 5627680. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:01:15,944][166323] Avg episode reward: [(0, '1203.252')]
[36m[2025-07-02 02:01:20,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5619712. Throughput: 0: 272.9. Samples: 5628544. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:01:20,980][166323] Avg episode reward: [(0, '1197.257')]
[36m[2025-07-02 02:01:25,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5619712. Throughput: 0: 270.4. Samples: 5630032. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:01:25,989][166323] Avg episode reward: [(0, '1157.531')]
[36m[2025-07-02 02:01:30,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5619712. Throughput: 0: 269.1. Samples: 5631568. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:01:30,956][166323] Avg episode reward: [(0, '1214.372')]
[36m[2025-07-02 02:01:35,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5619712. Throughput: 0: 270.6. Samples: 5632368. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:01:35,980][166323] Avg episode reward: [(0, '1238.431')]
[37m[1m[2025-07-02 02:01:36,047][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010968_5619712.pth...
[36m[2025-07-02 02:01:36,052][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010840_5554176.pth
[36m[2025-07-02 02:01:40,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5619712. Throughput: 0: 269.6. Samples: 5634032. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:01:40,948][166323] Avg episode reward: [(0, '1220.469')]
[36m[2025-07-02 02:01:45,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 222.2). Total num frames: 5619712. Throughput: 0: 266.6. Samples: 5635616. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:01:45,962][166323] Avg episode reward: [(0, '1265.790')]
[36m[2025-07-02 02:01:50,951][166323] Fps is (10 sec: 1637.8, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5636096. Throughput: 0: 268.4. Samples: 5636480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:01:50,952][166323] Avg episode reward: [(0, '1230.645')]
[36m[2025-07-02 02:01:55,989][166323] Fps is (10 sec: 1634.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5636096. Throughput: 0: 268.5. Samples: 5638144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:01:55,989][166323] Avg episode reward: [(0, '1217.246')]
[36m[2025-07-02 02:02:00,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5636096. Throughput: 0: 269.8. Samples: 5639824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:02:00,950][166323] Avg episode reward: [(0, '1202.327')]
[36m[2025-07-02 02:02:05,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5636096. Throughput: 0: 271.8. Samples: 5640768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:02:05,950][166323] Avg episode reward: [(0, '1221.126')]
[36m[2025-07-02 02:02:10,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5636096. Throughput: 0: 276.8. Samples: 5642480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:02:10,953][166323] Avg episode reward: [(0, '1197.159')]
[36m[2025-07-02 02:02:15,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5636096. Throughput: 0: 279.7. Samples: 5644160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:02:15,970][166323] Avg episode reward: [(0, '1249.942')]
[36m[2025-07-02 02:02:20,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5636096. Throughput: 0: 278.8. Samples: 5644912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:02:20,977][166323] Avg episode reward: [(0, '1239.631')]
[36m[2025-07-02 02:02:25,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5636096. Throughput: 0: 280.4. Samples: 5646656. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:02:25,970][166323] Avg episode reward: [(0, '1235.529')]
[36m[2025-07-02 02:02:30,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5636096. Throughput: 0: 282.9. Samples: 5648352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:02:30,987][166323] Avg episode reward: [(0, '1201.179')]
[36m[2025-07-02 02:02:35,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5636096. Throughput: 0: 282.8. Samples: 5649216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:02:35,993][166323] Avg episode reward: [(0, '1280.144')]
[36m[2025-07-02 02:02:40,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5636096. Throughput: 0: 281.6. Samples: 5650816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:02:40,987][166323] Avg episode reward: [(0, '1235.747')]
[36m[2025-07-02 02:02:45,946][166323] Fps is (10 sec: 1646.0, 60 sec: 546.3, 300 sec: 277.7). Total num frames: 5652480. Throughput: 0: 280.6. Samples: 5652448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:02:45,947][166323] Avg episode reward: [(0, '1295.872')]
[37m[1m[2025-07-02 02:02:45,999][166323] Saving new best policy, reward=1295.872!
[36m[2025-07-02 02:02:50,952][166323] Fps is (10 sec: 1644.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5652480. Throughput: 0: 279.8. Samples: 5653360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:02:50,952][166323] Avg episode reward: [(0, '1254.908')]
[36m[2025-07-02 02:02:55,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5652480. Throughput: 0: 280.7. Samples: 5655120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:02:55,976][166323] Avg episode reward: [(0, '1227.548')]
[31m[19764960 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19764960 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[19764960 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:03:00,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5652480. Throughput: 0: 279.5. Samples: 5656736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:03:00,967][166323] Avg episode reward: [(0, '1204.092')]
[36m[2025-07-02 02:03:05,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5652480. Throughput: 0: 279.3. Samples: 5657472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:03:05,951][166323] Avg episode reward: [(0, '1251.286')]
[36m[2025-07-02 02:03:11,008][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 5652480. Throughput: 0: 277.1. Samples: 5659136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:03:11,008][166323] Avg episode reward: [(0, '1261.757')]
[31m[19783673 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19783674 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[19783674 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:03:15,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5652480. Throughput: 0: 274.0. Samples: 5660672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:03:15,952][166323] Avg episode reward: [(0, '1288.130')]
[36m[2025-07-02 02:03:20,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5652480. Throughput: 0: 273.2. Samples: 5661504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:03:20,968][166323] Avg episode reward: [(0, '1281.876')]
[36m[2025-07-02 02:03:25,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5652480. Throughput: 0: 274.3. Samples: 5663152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:03:25,958][166323] Avg episode reward: [(0, '1266.531')]
[31m[19799141 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19799141 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[19799142 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:03:31,020][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 5652480. Throughput: 0: 273.3. Samples: 5664768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:03:31,020][166323] Avg episode reward: [(0, '1235.864')]
[36m[2025-07-02 02:03:35,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5652480. Throughput: 0: 273.0. Samples: 5665648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:03:35,964][166323] Avg episode reward: [(0, '1278.528')]
[37m[1m[2025-07-02 02:03:36,027][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000011032_5652480.pth...
[36m[2025-07-02 02:03:36,030][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010904_5586944.pth
[36m[2025-07-02 02:03:40,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5652480. Throughput: 0: 270.2. Samples: 5667280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:03:40,975][166323] Avg episode reward: [(0, '1213.887')]
[36m[2025-07-02 02:03:45,954][166323] Fps is (10 sec: 1639.9, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5668864. Throughput: 0: 270.7. Samples: 5668912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:03:45,954][166323] Avg episode reward: [(0, '1159.137')]
[36m[2025-07-02 02:03:50,956][166323] Fps is (10 sec: 1641.5, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5668864. Throughput: 0: 273.7. Samples: 5669792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:03:50,957][166323] Avg episode reward: [(0, '1147.917')]
[36m[2025-07-02 02:03:55,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5668864. Throughput: 0: 272.8. Samples: 5671408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:03:55,989][166323] Avg episode reward: [(0, '1137.771')]
[36m[2025-07-02 02:04:00,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5668864. Throughput: 0: 276.0. Samples: 5673104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:04:00,990][166323] Avg episode reward: [(0, '1158.566')]
[36m[2025-07-02 02:04:05,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5668864. Throughput: 0: 275.9. Samples: 5673920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:04:05,965][166323] Avg episode reward: [(0, '1174.773')]
[36m[2025-07-02 02:04:10,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5668864. Throughput: 0: 276.2. Samples: 5675584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:04:10,968][166323] Avg episode reward: [(0, '1179.905')]
[36m[2025-07-02 02:04:15,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5668864. Throughput: 0: 278.1. Samples: 5677264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:04:15,955][166323] Avg episode reward: [(0, '1193.289')]
[36m[2025-07-02 02:04:20,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5668864. Throughput: 0: 277.6. Samples: 5678144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:04:20,975][166323] Avg episode reward: [(0, '1182.530')]
[36m[2025-07-02 02:04:25,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5668864. Throughput: 0: 277.4. Samples: 5679760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:04:25,964][166323] Avg episode reward: [(0, '1247.517')]
[36m[2025-07-02 02:04:30,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 5668864. Throughput: 0: 277.0. Samples: 5681376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:04:30,944][166323] Avg episode reward: [(0, '1239.121')]
[36m[2025-07-02 02:04:35,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5668864. Throughput: 0: 276.2. Samples: 5682224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:04:35,971][166323] Avg episode reward: [(0, '1269.775')]
[36m[2025-07-02 02:04:40,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5668864. Throughput: 0: 277.2. Samples: 5683872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:04:40,949][166323] Avg episode reward: [(0, '1198.929')]
[36m[2025-07-02 02:04:45,988][166323] Fps is (10 sec: 1635.5, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5685248. Throughput: 0: 279.5. Samples: 5685680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:04:45,988][166323] Avg episode reward: [(0, '1230.647')]
[36m[2025-07-02 02:04:50,986][166323] Fps is (10 sec: 1632.4, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5685248. Throughput: 0: 280.0. Samples: 5686528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:04:50,986][166323] Avg episode reward: [(0, '1278.402')]
[36m[2025-07-02 02:04:55,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5685248. Throughput: 0: 283.3. Samples: 5688336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:04:55,973][166323] Avg episode reward: [(0, '1257.250')]
[36m[2025-07-02 02:05:00,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5685248. Throughput: 0: 280.9. Samples: 5689904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:05:00,953][166323] Avg episode reward: [(0, '1268.497')]
[36m[2025-07-02 02:05:05,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5685248. Throughput: 0: 281.7. Samples: 5690816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:05:05,960][166323] Avg episode reward: [(0, '1238.631')]
[36m[2025-07-02 02:05:10,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5685248. Throughput: 0: 285.4. Samples: 5692608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:05:10,981][166323] Avg episode reward: [(0, '1216.354')]
[36m[2025-07-02 02:05:15,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5685248. Throughput: 0: 289.6. Samples: 5694416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:05:15,971][166323] Avg episode reward: [(0, '1229.255')]
[36m[2025-07-02 02:05:20,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5685248. Throughput: 0: 290.0. Samples: 5695264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:05:20,944][166323] Avg episode reward: [(0, '1131.781')]
[36m[2025-07-02 02:05:25,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5685248. Throughput: 0: 289.7. Samples: 5696912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:05:25,969][166323] Avg episode reward: [(0, '1141.501')]
[36m[2025-07-02 02:05:30,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5685248. Throughput: 0: 286.6. Samples: 5698576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:05:30,992][166323] Avg episode reward: [(0, '1147.818')]
[36m[2025-07-02 02:05:35,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5685248. Throughput: 0: 288.3. Samples: 5699488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:05:35,944][166323] Avg episode reward: [(0, '1117.112')]
[37m[1m[2025-07-02 02:05:36,000][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000011096_5685248.pth...
[36m[2025-07-02 02:05:36,005][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000010968_5619712.pth
[36m[2025-07-02 02:05:40,975][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5685248. Throughput: 0: 285.5. Samples: 5701184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:05:40,975][166323] Avg episode reward: [(0, '1159.179')]
[36m[2025-07-02 02:05:45,953][166323] Fps is (10 sec: 1636.9, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5701632. Throughput: 0: 289.1. Samples: 5702912. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 02:05:45,953][166323] Avg episode reward: [(0, '1129.849')]
[36m[2025-07-02 02:05:50,952][166323] Fps is (10 sec: 1642.2, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5701632. Throughput: 0: 289.5. Samples: 5703840. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 02:05:50,952][166323] Avg episode reward: [(0, '1154.204')]
[36m[2025-07-02 02:05:55,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5701632. Throughput: 0: 288.1. Samples: 5705568. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 02:05:55,964][166323] Avg episode reward: [(0, '1182.315')]
[36m[2025-07-02 02:06:00,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5701632. Throughput: 0: 287.0. Samples: 5707328. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 02:06:00,965][166323] Avg episode reward: [(0, '1137.178')]
[36m[2025-07-02 02:06:05,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5701632. Throughput: 0: 286.1. Samples: 5708144. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 02:06:05,968][166323] Avg episode reward: [(0, '1120.846')]
[36m[2025-07-02 02:06:10,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5701632. Throughput: 0: 285.8. Samples: 5709776. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 02:06:10,973][166323] Avg episode reward: [(0, '1048.454')]
[33m[19961227 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[19961227 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.85986328125
[33mCrash Rate: 0.1279296875
[33mTimeout Rate: 0.01220703125 (navigation_task.py:265)
[33m[19961227 ms][navigation_task] - WARNING : 
[33mSuccesses: 1761
[33mCrashes : 262
[33mTimeouts: 25 (navigation_task.py:268)
[36m[2025-07-02 02:06:15,995][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5701632. Throughput: 0: 282.6. Samples: 5711296. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 02:06:15,995][166323] Avg episode reward: [(0, '1082.098')]
[36m[2025-07-02 02:06:20,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5701632. Throughput: 0: 282.3. Samples: 5712192. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 02:06:20,948][166323] Avg episode reward: [(0, '1155.384')]
[31m[19971020 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19971021 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[19971021 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:06:25,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5701632. Throughput: 0: 285.7. Samples: 5714032. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 02:06:25,952][166323] Avg episode reward: [(0, '1134.202')]
[36m[2025-07-02 02:06:30,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5701632. Throughput: 0: 286.6. Samples: 5715808. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 02:06:30,948][166323] Avg episode reward: [(0, '1209.258')]
[36m[2025-07-02 02:06:35,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5701632. Throughput: 0: 284.4. Samples: 5716640. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 02:06:35,959][166323] Avg episode reward: [(0, '1187.661')]
[36m[2025-07-02 02:06:40,946][166323] Fps is (10 sec: 1638.7, 60 sec: 546.4, 300 sec: 333.3). Total num frames: 5718016. Throughput: 0: 281.7. Samples: 5718240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:06:40,946][166323] Avg episode reward: [(0, '1183.428')]
[36m[2025-07-02 02:06:45,985][166323] Fps is (10 sec: 1634.1, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5718016. Throughput: 0: 279.7. Samples: 5719920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:06:45,985][166323] Avg episode reward: [(0, '1226.770')]
[36m[2025-07-02 02:06:50,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5718016. Throughput: 0: 279.7. Samples: 5720736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:06:50,988][166323] Avg episode reward: [(0, '1227.703')]
[31m[19999612 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19999612 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[19999612 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:06:55,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5718016. Throughput: 0: 279.5. Samples: 5722352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:06:55,976][166323] Avg episode reward: [(0, '1189.930')]
[36m[2025-07-02 02:07:00,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5718016. Throughput: 0: 284.1. Samples: 5724080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:07:00,988][166323] Avg episode reward: [(0, '1197.952')]
[36m[2025-07-02 02:07:05,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5718016. Throughput: 0: 282.0. Samples: 5724880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:07:05,949][166323] Avg episode reward: [(0, '1163.670')]
[31m[20019116 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20019117 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[20019117 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:07:10,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5718016. Throughput: 0: 277.6. Samples: 5726528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:07:10,962][166323] Avg episode reward: [(0, '1209.945')]
[36m[2025-07-02 02:07:15,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5718016. Throughput: 0: 276.2. Samples: 5728240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:07:15,952][166323] Avg episode reward: [(0, '1177.051')]
[36m[2025-07-02 02:07:20,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5718016. Throughput: 0: 274.5. Samples: 5728992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:07:20,963][166323] Avg episode reward: [(0, '1151.097')]
[36m[2025-07-02 02:07:25,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5718016. Throughput: 0: 278.2. Samples: 5730768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:07:25,973][166323] Avg episode reward: [(0, '1139.467')]
[36m[2025-07-02 02:07:30,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5718016. Throughput: 0: 278.5. Samples: 5732448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:07:30,969][166323] Avg episode reward: [(0, '1154.266')]
[36m[2025-07-02 02:07:35,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5718016. Throughput: 0: 278.6. Samples: 5733264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:07:35,947][166323] Avg episode reward: [(0, '1181.966')]
[37m[1m[2025-07-02 02:07:35,999][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000011160_5718016.pth...
[36m[2025-07-02 02:07:36,003][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000011032_5652480.pth
[36m[2025-07-02 02:07:40,973][166323] Fps is (10 sec: 1637.8, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5734400. Throughput: 0: 279.8. Samples: 5734944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:07:40,973][166323] Avg episode reward: [(0, '1193.294')]
[36m[2025-07-02 02:07:45,988][166323] Fps is (10 sec: 1631.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5734400. Throughput: 0: 277.3. Samples: 5736560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:07:45,988][166323] Avg episode reward: [(0, '1182.665')]
[36m[2025-07-02 02:07:50,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5734400. Throughput: 0: 276.0. Samples: 5737312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:07:50,994][166323] Avg episode reward: [(0, '1168.825')]
[31m[20059903 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20059904 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[20059904 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:07:55,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5734400. Throughput: 0: 274.5. Samples: 5738880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:07:55,963][166323] Avg episode reward: [(0, '1183.105')]
[36m[2025-07-02 02:08:00,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5734400. Throughput: 0: 272.5. Samples: 5740512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:08:00,992][166323] Avg episode reward: [(0, '1169.506')]
[31m[20074384 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20074385 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[20074385 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:08:05,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5734400. Throughput: 0: 272.6. Samples: 5741264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:08:05,990][166323] Avg episode reward: [(0, '1176.310')]
[31m[20079099 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20079099 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[20079099 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:08:10,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5734400. Throughput: 0: 271.6. Samples: 5742992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:08:10,978][166323] Avg episode reward: [(0, '1081.218')]
[31m[20082655 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20082655 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[20082656 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:08:15,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5734400. Throughput: 0: 270.6. Samples: 5744624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:08:15,963][166323] Avg episode reward: [(0, '1097.388')]
[36m[2025-07-02 02:08:20,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5734400. Throughput: 0: 269.5. Samples: 5745392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:08:20,945][166323] Avg episode reward: [(0, '1136.979')]
[36m[2025-07-02 02:08:25,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5734400. Throughput: 0: 269.8. Samples: 5747088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:08:25,979][166323] Avg episode reward: [(0, '1128.220')]
[36m[2025-07-02 02:08:30,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5734400. Throughput: 0: 274.0. Samples: 5748880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:08:30,953][166323] Avg episode reward: [(0, '1188.659')]
[36m[2025-07-02 02:08:35,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5734400. Throughput: 0: 276.1. Samples: 5749728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:08:35,962][166323] Avg episode reward: [(0, '1151.873')]
[36m[2025-07-02 02:08:40,981][166323] Fps is (10 sec: 1633.7, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5750784. Throughput: 0: 277.9. Samples: 5751392. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 02:08:40,981][166323] Avg episode reward: [(0, '1190.144')]
[36m[2025-07-02 02:08:45,967][166323] Fps is (10 sec: 1637.6, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5750784. Throughput: 0: 279.6. Samples: 5753088. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 02:08:45,967][166323] Avg episode reward: [(0, '1171.524')]
[36m[2025-07-02 02:08:50,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5750784. Throughput: 0: 281.0. Samples: 5753904. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 02:08:50,964][166323] Avg episode reward: [(0, '1127.344')]
[36m[2025-07-02 02:08:55,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5750784. Throughput: 0: 278.6. Samples: 5755520. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 02:08:55,944][166323] Avg episode reward: [(0, '1162.852')]
[36m[2025-07-02 02:09:00,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5750784. Throughput: 0: 277.6. Samples: 5757120. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 02:09:00,984][166323] Avg episode reward: [(0, '1216.753')]
[36m[2025-07-02 02:09:05,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5750784. Throughput: 0: 281.9. Samples: 5758080. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 02:09:05,948][166323] Avg episode reward: [(0, '1250.674')]
[36m[2025-07-02 02:09:10,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5750784. Throughput: 0: 279.4. Samples: 5759664. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 02:09:10,993][166323] Avg episode reward: [(0, '1261.307')]
[36m[2025-07-02 02:09:15,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5750784. Throughput: 0: 275.2. Samples: 5761264. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 02:09:15,951][166323] Avg episode reward: [(0, '1276.690')]
[36m[2025-07-02 02:09:20,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5750784. Throughput: 0: 274.6. Samples: 5762080. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 02:09:20,945][166323] Avg episode reward: [(0, '1244.927')]
[31m[20153969 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20153969 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[20153969 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:09:25,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5750784. Throughput: 0: 275.2. Samples: 5763776. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 02:09:25,984][166323] Avg episode reward: [(0, '1250.672')]
[31m[20158790 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20158790 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[20158791 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:09:30,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5750784. Throughput: 0: 276.7. Samples: 5765536. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 02:09:30,955][166323] Avg episode reward: [(0, '1200.283')]
[36m[2025-07-02 02:09:35,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5750784. Throughput: 0: 277.6. Samples: 5766400. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 02:09:35,983][166323] Avg episode reward: [(0, '1174.993')]
[37m[1m[2025-07-02 02:09:36,033][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000011224_5750784.pth...
[36m[2025-07-02 02:09:36,037][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000011096_5685248.pth
[36m[2025-07-02 02:09:40,957][166323] Fps is (10 sec: 1638.1, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5767168. Throughput: 0: 279.4. Samples: 5768096. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:09:40,957][166323] Avg episode reward: [(0, '1162.325')]
[36m[2025-07-02 02:09:45,957][166323] Fps is (10 sec: 1642.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5767168. Throughput: 0: 279.3. Samples: 5769680. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:09:45,957][166323] Avg episode reward: [(0, '1163.268')]
[31m[20174725 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20174725 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[20174725 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:09:50,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5767168. Throughput: 0: 275.3. Samples: 5770480. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:09:50,994][166323] Avg episode reward: [(0, '1153.635')]
[36m[2025-07-02 02:09:55,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5767168. Throughput: 0: 276.9. Samples: 5772112. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:09:55,950][166323] Avg episode reward: [(0, '1208.315')]
[36m[2025-07-02 02:10:00,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5767168. Throughput: 0: 278.7. Samples: 5773808. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:10:00,954][166323] Avg episode reward: [(0, '1213.228')]
[36m[2025-07-02 02:10:05,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5767168. Throughput: 0: 278.5. Samples: 5774624. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:10:05,993][166323] Avg episode reward: [(0, '1126.636')]
[36m[2025-07-02 02:10:10,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5767168. Throughput: 0: 276.4. Samples: 5776208. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:10:10,956][166323] Avg episode reward: [(0, '1166.118')]
[36m[2025-07-02 02:10:16,004][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 5767168. Throughput: 0: 273.8. Samples: 5777872. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:10:16,004][166323] Avg episode reward: [(0, '1172.429')]
[36m[2025-07-02 02:10:20,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5767168. Throughput: 0: 272.2. Samples: 5778640. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:10:20,957][166323] Avg episode reward: [(0, '1183.974')]
[36m[2025-07-02 02:10:25,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5767168. Throughput: 0: 272.2. Samples: 5780352. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:10:25,977][166323] Avg episode reward: [(0, '1199.304')]
[36m[2025-07-02 02:10:30,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5767168. Throughput: 0: 272.7. Samples: 5781952. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:10:30,951][166323] Avg episode reward: [(0, '1143.854')]
[36m[2025-07-02 02:10:35,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5767168. Throughput: 0: 274.4. Samples: 5782816. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:10:35,954][166323] Avg episode reward: [(0, '1120.937')]
[36m[2025-07-02 02:10:40,977][166323] Fps is (10 sec: 1634.1, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5783552. Throughput: 0: 273.6. Samples: 5784432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:10:40,977][166323] Avg episode reward: [(0, '1169.038')]
[36m[2025-07-02 02:10:45,947][166323] Fps is (10 sec: 1639.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5783552. Throughput: 0: 272.4. Samples: 5786064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:10:45,947][166323] Avg episode reward: [(0, '1109.940')]
[36m[2025-07-02 02:10:50,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5783552. Throughput: 0: 273.8. Samples: 5786944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:10:50,984][166323] Avg episode reward: [(0, '1125.138')]
[36m[2025-07-02 02:10:55,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5783552. Throughput: 0: 275.6. Samples: 5788608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:10:55,949][166323] Avg episode reward: [(0, '1127.963')]
[36m[2025-07-02 02:11:00,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5783552. Throughput: 0: 275.4. Samples: 5790256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:11:00,966][166323] Avg episode reward: [(0, '1196.787')]
[36m[2025-07-02 02:11:05,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5783552. Throughput: 0: 275.0. Samples: 5791024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:11:05,991][166323] Avg episode reward: [(0, '1225.695')]
[36m[2025-07-02 02:11:10,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5783552. Throughput: 0: 275.4. Samples: 5792736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:11:10,948][166323] Avg episode reward: [(0, '1228.647')]
[36m[2025-07-02 02:11:15,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5783552. Throughput: 0: 277.2. Samples: 5794432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:11:15,974][166323] Avg episode reward: [(0, '1319.193')]
[37m[1m[2025-07-02 02:11:16,032][166323] Saving new best policy, reward=1319.193!
[31m[20265173 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20265173 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[20265173 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:11:20,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5783552. Throughput: 0: 275.8. Samples: 5795232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:11:20,972][166323] Avg episode reward: [(0, '1341.231')]
[37m[1m[2025-07-02 02:11:21,036][166323] Saving new best policy, reward=1341.231!
[36m[2025-07-02 02:11:25,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5783552. Throughput: 0: 279.9. Samples: 5797024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:11:25,969][166323] Avg episode reward: [(0, '1297.783')]
[36m[2025-07-02 02:11:30,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5783552. Throughput: 0: 282.4. Samples: 5798784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:11:30,991][166323] Avg episode reward: [(0, '1276.198')]
[36m[2025-07-02 02:11:35,960][166323] Fps is (10 sec: 1639.7, 60 sec: 546.1, 300 sec: 277.7). Total num frames: 5799936. Throughput: 0: 281.4. Samples: 5799600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:11:35,961][166323] Avg episode reward: [(0, '1261.238')]
[37m[1m[2025-07-02 02:11:36,022][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000011320_5799936.pth...
[36m[2025-07-02 02:11:36,029][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000011160_5718016.pth
[36m[2025-07-02 02:11:40,989][166323] Fps is (10 sec: 1638.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5799936. Throughput: 0: 278.5. Samples: 5801152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:11:40,989][166323] Avg episode reward: [(0, '1217.075')]
[36m[2025-07-02 02:11:45,977][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5799936. Throughput: 0: 280.5. Samples: 5802880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:11:45,978][166323] Avg episode reward: [(0, '1136.698')]
[36m[2025-07-02 02:11:50,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5799936. Throughput: 0: 282.5. Samples: 5803728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:11:50,956][166323] Avg episode reward: [(0, '1134.203')]
[36m[2025-07-02 02:11:55,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5799936. Throughput: 0: 280.4. Samples: 5805360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:11:55,962][166323] Avg episode reward: [(0, '1094.526')]
[36m[2025-07-02 02:12:00,997][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 5799936. Throughput: 0: 278.3. Samples: 5806960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:12:00,997][166323] Avg episode reward: [(0, '1165.993')]
[36m[2025-07-02 02:12:05,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5799936. Throughput: 0: 277.9. Samples: 5807744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:12:05,994][166323] Avg episode reward: [(0, '1184.444')]
[36m[2025-07-02 02:12:10,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5799936. Throughput: 0: 275.7. Samples: 5809424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:12:10,943][166323] Avg episode reward: [(0, '1148.091')]
[36m[2025-07-02 02:12:15,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5799936. Throughput: 0: 272.3. Samples: 5811024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:12:15,947][166323] Avg episode reward: [(0, '1190.788')]
[36m[2025-07-02 02:12:20,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5799936. Throughput: 0: 270.4. Samples: 5811776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:12:20,985][166323] Avg episode reward: [(0, '1281.139')]
[36m[2025-07-02 02:12:26,013][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5799936. Throughput: 0: 275.1. Samples: 5813536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:12:26,014][166323] Avg episode reward: [(0, '1269.760')]
[36m[2025-07-02 02:12:30,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5799936. Throughput: 0: 272.1. Samples: 5815120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:12:30,964][166323] Avg episode reward: [(0, '1299.615')]
[36m[2025-07-02 02:12:35,984][166323] Fps is (10 sec: 1643.2, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5816320. Throughput: 0: 269.3. Samples: 5815856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:12:35,984][166323] Avg episode reward: [(0, '1270.151')]
[36m[2025-07-02 02:12:40,957][166323] Fps is (10 sec: 1639.5, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5816320. Throughput: 0: 271.7. Samples: 5817584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:12:40,957][166323] Avg episode reward: [(0, '1229.901')]
[36m[2025-07-02 02:12:45,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5816320. Throughput: 0: 274.2. Samples: 5819296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:12:45,987][166323] Avg episode reward: [(0, '1250.950')]
[36m[2025-07-02 02:12:50,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5816320. Throughput: 0: 275.7. Samples: 5820144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:12:50,977][166323] Avg episode reward: [(0, '1251.423')]
[36m[2025-07-02 02:12:55,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5816320. Throughput: 0: 276.4. Samples: 5821872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:12:55,984][166323] Avg episode reward: [(0, '1213.420')]
[36m[2025-07-02 02:13:00,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5816320. Throughput: 0: 282.0. Samples: 5823712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:13:00,944][166323] Avg episode reward: [(0, '1164.438')]
[36m[2025-07-02 02:13:05,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5816320. Throughput: 0: 285.7. Samples: 5824624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:13:05,962][166323] Avg episode reward: [(0, '1177.016')]
[36m[2025-07-02 02:13:10,977][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5816320. Throughput: 0: 284.3. Samples: 5826320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:13:10,977][166323] Avg episode reward: [(0, '1213.719')]
[36m[2025-07-02 02:13:16,011][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 5816320. Throughput: 0: 284.9. Samples: 5827952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:13:16,011][166323] Avg episode reward: [(0, '1189.797')]
[31m[20387265 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20387266 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[20387266 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:13:20,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5816320. Throughput: 0: 284.9. Samples: 5828672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:13:20,962][166323] Avg episode reward: [(0, '1190.448')]
[36m[2025-07-02 02:13:25,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5816320. Throughput: 0: 280.0. Samples: 5830192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:13:25,979][166323] Avg episode reward: [(0, '1176.927')]
[31m[20396539 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20396539 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[20396539 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:13:30,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5816320. Throughput: 0: 281.5. Samples: 5831952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:13:30,948][166323] Avg episode reward: [(0, '1142.872')]
[36m[2025-07-02 02:13:35,989][166323] Fps is (10 sec: 1636.8, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5832704. Throughput: 0: 279.7. Samples: 5832736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:13:35,989][166323] Avg episode reward: [(0, '1199.473')]
[37m[1m[2025-07-02 02:13:36,071][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000011384_5832704.pth...
[36m[2025-07-02 02:13:36,075][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000011224_5750784.pth
[36m[2025-07-02 02:13:40,957][166323] Fps is (10 sec: 1636.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5832704. Throughput: 0: 276.1. Samples: 5834288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:13:40,957][166323] Avg episode reward: [(0, '1151.968')]
[36m[2025-07-02 02:13:45,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5832704. Throughput: 0: 273.8. Samples: 5836032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:13:45,946][166323] Avg episode reward: [(0, '1146.009')]
[36m[2025-07-02 02:13:50,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5832704. Throughput: 0: 274.1. Samples: 5836960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:13:50,969][166323] Avg episode reward: [(0, '1160.586')]
[36m[2025-07-02 02:13:55,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5832704. Throughput: 0: 276.5. Samples: 5838752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:13:55,946][166323] Avg episode reward: [(0, '1160.605')]
[36m[2025-07-02 02:14:00,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5832704. Throughput: 0: 280.0. Samples: 5840544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:14:00,984][166323] Avg episode reward: [(0, '1161.358')]
[36m[2025-07-02 02:14:06,002][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5832704. Throughput: 0: 283.5. Samples: 5841440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:14:06,003][166323] Avg episode reward: [(0, '1108.303')]
[36m[2025-07-02 02:14:10,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5832704. Throughput: 0: 289.8. Samples: 5843232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:14:10,969][166323] Avg episode reward: [(0, '1171.405')]
[36m[2025-07-02 02:14:15,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5832704. Throughput: 0: 288.1. Samples: 5844928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:14:15,986][166323] Avg episode reward: [(0, '1186.276')]
[36m[2025-07-02 02:14:20,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5832704. Throughput: 0: 288.6. Samples: 5845712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:14:20,950][166323] Avg episode reward: [(0, '1233.751')]
[36m[2025-07-02 02:14:25,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5832704. Throughput: 0: 289.0. Samples: 5847296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:14:25,970][166323] Avg episode reward: [(0, '1186.752')]
[31m[20454852 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20454853 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[20454853 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:14:30,976][166323] Fps is (10 sec: 1634.1, 60 sec: 545.9, 300 sec: 333.2). Total num frames: 5849088. Throughput: 0: 288.9. Samples: 5849040. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 02:14:30,977][166323] Avg episode reward: [(0, '1186.909')]
[36m[2025-07-02 02:14:35,958][166323] Fps is (10 sec: 1640.2, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5849088. Throughput: 0: 285.9. Samples: 5849824. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 02:14:35,958][166323] Avg episode reward: [(0, '1148.133')]
[36m[2025-07-02 02:14:40,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5849088. Throughput: 0: 286.0. Samples: 5851632. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 02:14:40,986][166323] Avg episode reward: [(0, '1176.505')]
[36m[2025-07-02 02:14:45,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5849088. Throughput: 0: 284.6. Samples: 5853344. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 02:14:45,952][166323] Avg episode reward: [(0, '1160.422')]
[36m[2025-07-02 02:14:50,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5849088. Throughput: 0: 284.4. Samples: 5854224. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 02:14:50,955][166323] Avg episode reward: [(0, '1137.942')]
[36m[2025-07-02 02:14:55,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5849088. Throughput: 0: 283.8. Samples: 5856000. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 02:14:55,966][166323] Avg episode reward: [(0, '1180.218')]
[36m[2025-07-02 02:15:00,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5849088. Throughput: 0: 285.2. Samples: 5857760. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 02:15:00,985][166323] Avg episode reward: [(0, '1194.359')]
[36m[2025-07-02 02:15:05,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5849088. Throughput: 0: 283.8. Samples: 5858496. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 02:15:05,989][166323] Avg episode reward: [(0, '1259.931')]
[36m[2025-07-02 02:15:10,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5849088. Throughput: 0: 285.5. Samples: 5860144. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 02:15:10,966][166323] Avg episode reward: [(0, '1243.517')]
[36m[2025-07-02 02:15:15,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5849088. Throughput: 0: 284.6. Samples: 5861840. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 02:15:15,947][166323] Avg episode reward: [(0, '1177.186')]
[36m[2025-07-02 02:15:20,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5849088. Throughput: 0: 284.4. Samples: 5862624. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 02:15:20,969][166323] Avg episode reward: [(0, '1182.338')]
[36m[2025-07-02 02:15:26,006][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 5849088. Throughput: 0: 281.8. Samples: 5864320. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 02:15:26,007][166323] Avg episode reward: [(0, '1258.359')]
[31m[20516251 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20516252 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[20516252 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:15:30,954][166323] Fps is (10 sec: 1640.8, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 5865472. Throughput: 0: 280.5. Samples: 5865968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:15:30,954][166323] Avg episode reward: [(0, '1222.666')]
[36m[2025-07-02 02:15:35,957][166323] Fps is (10 sec: 1646.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5865472. Throughput: 0: 282.3. Samples: 5866928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:15:35,957][166323] Avg episode reward: [(0, '1184.539')]
[37m[1m[2025-07-02 02:15:36,006][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000011448_5865472.pth...
[36m[2025-07-02 02:15:36,010][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000011320_5799936.pth
[36m[2025-07-02 02:15:40,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5865472. Throughput: 0: 281.5. Samples: 5868672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:15:40,987][166323] Avg episode reward: [(0, '1132.607')]
[36m[2025-07-02 02:15:45,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5865472. Throughput: 0: 279.4. Samples: 5870336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:15:45,988][166323] Avg episode reward: [(0, '1176.509')]
[36m[2025-07-02 02:15:50,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5865472. Throughput: 0: 281.7. Samples: 5871168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:15:50,968][166323] Avg episode reward: [(0, '1137.303')]
[36m[2025-07-02 02:15:55,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5865472. Throughput: 0: 280.2. Samples: 5872752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:15:55,962][166323] Avg episode reward: [(0, '1082.170')]
[36m[2025-07-02 02:16:00,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5865472. Throughput: 0: 278.9. Samples: 5874400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:16:00,988][166323] Avg episode reward: [(0, '1062.669')]
[36m[2025-07-02 02:16:05,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5865472. Throughput: 0: 279.1. Samples: 5875184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:16:05,964][166323] Avg episode reward: [(0, '1077.655')]
[36m[2025-07-02 02:16:10,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5865472. Throughput: 0: 278.1. Samples: 5876816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:16:10,947][166323] Avg episode reward: [(0, '1068.949')]
[36m[2025-07-02 02:16:16,013][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 5865472. Throughput: 0: 277.0. Samples: 5878448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:16:16,013][166323] Avg episode reward: [(0, '1073.398')]
[36m[2025-07-02 02:16:20,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5865472. Throughput: 0: 273.5. Samples: 5879232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:16:20,944][166323] Avg episode reward: [(0, '1095.008')]
[36m[2025-07-02 02:16:25,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5865472. Throughput: 0: 272.2. Samples: 5880912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:16:25,959][166323] Avg episode reward: [(0, '1145.279')]
[36m[2025-07-02 02:16:30,962][166323] Fps is (10 sec: 1635.4, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5881856. Throughput: 0: 270.7. Samples: 5882512. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 02:16:30,962][166323] Avg episode reward: [(0, '1173.815')]
[36m[2025-07-02 02:16:36,005][166323] Fps is (10 sec: 1630.8, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 5881856. Throughput: 0: 270.7. Samples: 5883360. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 02:16:36,006][166323] Avg episode reward: [(0, '1196.512')]
[36m[2025-07-02 02:16:40,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5881856. Throughput: 0: 274.2. Samples: 5885088. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 02:16:40,956][166323] Avg episode reward: [(0, '1156.008')]
[36m[2025-07-02 02:16:46,004][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 5881856. Throughput: 0: 275.8. Samples: 5886816. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 02:16:46,004][166323] Avg episode reward: [(0, '1201.477')]
[36m[2025-07-02 02:16:50,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5881856. Throughput: 0: 274.8. Samples: 5887552. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 02:16:50,966][166323] Avg episode reward: [(0, '1171.489')]
[36m[2025-07-02 02:16:55,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5881856. Throughput: 0: 273.7. Samples: 5889136. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 02:16:55,956][166323] Avg episode reward: [(0, '1199.401')]
[31m[20606941 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20606942 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[20606942 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:17:00,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5881856. Throughput: 0: 275.7. Samples: 5890848. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 02:17:00,993][166323] Avg episode reward: [(0, '1193.905')]
[36m[2025-07-02 02:17:05,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5881856. Throughput: 0: 276.7. Samples: 5891696. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 02:17:05,984][166323] Avg episode reward: [(0, '1210.120')]
[36m[2025-07-02 02:17:10,980][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5881856. Throughput: 0: 280.8. Samples: 5893552. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 02:17:10,980][166323] Avg episode reward: [(0, '1192.869')]
[36m[2025-07-02 02:17:15,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5881856. Throughput: 0: 282.8. Samples: 5895248. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 02:17:15,993][166323] Avg episode reward: [(0, '1261.439')]
[36m[2025-07-02 02:17:20,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5881856. Throughput: 0: 283.0. Samples: 5896080. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 02:17:20,960][166323] Avg episode reward: [(0, '1276.282')]
[36m[2025-07-02 02:17:26,003][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5881856. Throughput: 0: 281.3. Samples: 5897760. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 02:17:26,003][166323] Avg episode reward: [(0, '1296.367')]
[33m[20636290 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[20636291 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.85107421875
[33mCrash Rate: 0.13818359375
[33mTimeout Rate: 0.0107421875 (navigation_task.py:265)
[33m[20636291 ms][navigation_task] - WARNING : 
[33mSuccesses: 1743
[33mCrashes : 283
[33mTimeouts: 22 (navigation_task.py:268)
[36m[2025-07-02 02:17:30,960][166323] Fps is (10 sec: 1638.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5898240. Throughput: 0: 282.9. Samples: 5899536. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:17:30,960][166323] Avg episode reward: [(0, '1271.006')]
[36m[2025-07-02 02:17:36,006][166323] Fps is (10 sec: 1637.7, 60 sec: 273.1, 300 sec: 277.6). Total num frames: 5898240. Throughput: 0: 286.7. Samples: 5900464. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:17:36,007][166323] Avg episode reward: [(0, '1272.011')]
[37m[1m[2025-07-02 02:17:36,060][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000011512_5898240.pth...
[36m[2025-07-02 02:17:36,063][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000011384_5832704.pth
[36m[2025-07-02 02:17:40,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5898240. Throughput: 0: 290.5. Samples: 5902208. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:17:40,955][166323] Avg episode reward: [(0, '1283.068')]
[36m[2025-07-02 02:17:45,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5898240. Throughput: 0: 291.3. Samples: 5903952. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:17:45,978][166323] Avg episode reward: [(0, '1302.192')]
[36m[2025-07-02 02:17:50,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5898240. Throughput: 0: 290.9. Samples: 5904784. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:17:50,973][166323] Avg episode reward: [(0, '1269.531')]
[36m[2025-07-02 02:17:55,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5898240. Throughput: 0: 289.5. Samples: 5906576. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:17:55,965][166323] Avg episode reward: [(0, '1244.270')]
[36m[2025-07-02 02:18:00,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5898240. Throughput: 0: 286.4. Samples: 5908128. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:18:00,969][166323] Avg episode reward: [(0, '1224.884')]
[36m[2025-07-02 02:18:05,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5898240. Throughput: 0: 284.7. Samples: 5908896. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:18:05,975][166323] Avg episode reward: [(0, '1221.197')]
[36m[2025-07-02 02:18:10,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5898240. Throughput: 0: 284.0. Samples: 5910528. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:18:10,967][166323] Avg episode reward: [(0, '1229.779')]
[36m[2025-07-02 02:18:15,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5898240. Throughput: 0: 279.0. Samples: 5912096. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:18:15,973][166323] Avg episode reward: [(0, '1225.259')]
[36m[2025-07-02 02:18:20,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5898240. Throughput: 0: 279.0. Samples: 5913008. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:18:20,975][166323] Avg episode reward: [(0, '1161.815')]
[36m[2025-07-02 02:18:25,956][166323] Fps is (10 sec: 1641.1, 60 sec: 546.6, 300 sec: 333.2). Total num frames: 5914624. Throughput: 0: 276.6. Samples: 5914656. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:18:25,956][166323] Avg episode reward: [(0, '1208.107')]
[36m[2025-07-02 02:18:30,975][166323] Fps is (10 sec: 1638.3, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5914624. Throughput: 0: 274.1. Samples: 5916288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:18:30,976][166323] Avg episode reward: [(0, '1208.591')]
[31m[20701492 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20701492 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[20701492 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:18:35,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5914624. Throughput: 0: 274.2. Samples: 5917120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:18:35,966][166323] Avg episode reward: [(0, '1199.662')]
[36m[2025-07-02 02:18:40,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5914624. Throughput: 0: 273.9. Samples: 5918896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:18:40,948][166323] Avg episode reward: [(0, '1229.031')]
[31m[20714346 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20714347 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[20714347 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:18:45,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5914624. Throughput: 0: 276.0. Samples: 5920544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:18:45,947][166323] Avg episode reward: [(0, '1237.452')]
[36m[2025-07-02 02:18:50,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5914624. Throughput: 0: 277.4. Samples: 5921376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:18:50,969][166323] Avg episode reward: [(0, '1205.745')]
[36m[2025-07-02 02:18:55,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5914624. Throughput: 0: 277.2. Samples: 5923008. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:18:55,992][166323] Avg episode reward: [(0, '1255.844')]
[36m[2025-07-02 02:19:00,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5914624. Throughput: 0: 276.5. Samples: 5924544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:19:00,994][166323] Avg episode reward: [(0, '1181.054')]
[36m[2025-07-02 02:19:05,997][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5914624. Throughput: 0: 273.6. Samples: 5925328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:19:05,997][166323] Avg episode reward: [(0, '1208.881')]
[36m[2025-07-02 02:19:10,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5914624. Throughput: 0: 273.0. Samples: 5926944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:19:10,974][166323] Avg episode reward: [(0, '1235.329')]
[36m[2025-07-02 02:19:15,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5914624. Throughput: 0: 271.6. Samples: 5928512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:19:15,979][166323] Avg episode reward: [(0, '1194.567')]
[36m[2025-07-02 02:19:20,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5914624. Throughput: 0: 270.8. Samples: 5929312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:19:20,983][166323] Avg episode reward: [(0, '1196.704')]
[36m[2025-07-02 02:19:25,964][166323] Fps is (10 sec: 1640.8, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5931008. Throughput: 0: 266.6. Samples: 5930896. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 02:19:25,964][166323] Avg episode reward: [(0, '1276.901')]
[36m[2025-07-02 02:19:30,953][166323] Fps is (10 sec: 1643.3, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5931008. Throughput: 0: 264.9. Samples: 5932464. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 02:19:30,953][166323] Avg episode reward: [(0, '1235.480')]
[36m[2025-07-02 02:19:35,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5931008. Throughput: 0: 263.8. Samples: 5933248. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 02:19:35,977][166323] Avg episode reward: [(0, '1214.691')]
[37m[1m[2025-07-02 02:19:36,028][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000011576_5931008.pth...
[36m[2025-07-02 02:19:36,035][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000011448_5865472.pth
[31m[20769230 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20769230 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[20769231 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[20769305 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20769306 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[20769306 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:19:40,980][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5931008. Throughput: 0: 263.2. Samples: 5934848. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 02:19:40,980][166323] Avg episode reward: [(0, '1168.425')]
[36m[2025-07-02 02:19:45,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5931008. Throughput: 0: 265.3. Samples: 5936480. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 02:19:45,984][166323] Avg episode reward: [(0, '1186.346')]
[36m[2025-07-02 02:19:50,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5931008. Throughput: 0: 264.7. Samples: 5937232. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 02:19:50,961][166323] Avg episode reward: [(0, '1233.284')]
[36m[2025-07-02 02:19:55,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5931008. Throughput: 0: 265.9. Samples: 5938912. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 02:19:55,981][166323] Avg episode reward: [(0, '1185.443')]
[36m[2025-07-02 02:20:00,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5931008. Throughput: 0: 267.9. Samples: 5940560. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 02:20:00,957][166323] Avg episode reward: [(0, '1149.923')]
[36m[2025-07-02 02:20:05,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5931008. Throughput: 0: 268.5. Samples: 5941392. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 02:20:05,976][166323] Avg episode reward: [(0, '1222.128')]
[36m[2025-07-02 02:20:10,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5931008. Throughput: 0: 272.1. Samples: 5943136. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 02:20:10,950][166323] Avg episode reward: [(0, '1210.671')]
[36m[2025-07-02 02:20:15,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5931008. Throughput: 0: 274.4. Samples: 5944816. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 02:20:15,974][166323] Avg episode reward: [(0, '1304.715')]
[36m[2025-07-02 02:20:20,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5931008. Throughput: 0: 276.3. Samples: 5945680. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 02:20:20,970][166323] Avg episode reward: [(0, '1287.960')]
[36m[2025-07-02 02:20:25,950][166323] Fps is (10 sec: 1642.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5947392. Throughput: 0: 276.8. Samples: 5947296. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 02:20:25,950][166323] Avg episode reward: [(0, '1216.801')]
[36m[2025-07-02 02:20:30,972][166323] Fps is (10 sec: 1638.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5947392. Throughput: 0: 275.3. Samples: 5948864. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 02:20:30,972][166323] Avg episode reward: [(0, '1251.661')]
[36m[2025-07-02 02:20:36,004][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5947392. Throughput: 0: 277.1. Samples: 5949712. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 02:20:36,004][166323] Avg episode reward: [(0, '1220.042')]
[31m[20826438 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20826439 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[20826439 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:20:40,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5947392. Throughput: 0: 278.1. Samples: 5951424. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 02:20:40,967][166323] Avg episode reward: [(0, '1156.865')]
[36m[2025-07-02 02:20:45,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 5947392. Throughput: 0: 278.8. Samples: 5953104. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 02:20:45,944][166323] Avg episode reward: [(0, '1172.426')]
[36m[2025-07-02 02:20:50,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5947392. Throughput: 0: 280.1. Samples: 5954000. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 02:20:50,981][166323] Avg episode reward: [(0, '1203.816')]
[36m[2025-07-02 02:20:55,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5947392. Throughput: 0: 281.9. Samples: 5955824. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 02:20:55,954][166323] Avg episode reward: [(0, '1164.296')]
[31m[20846433 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20846433 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[20846433 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:21:00,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5947392. Throughput: 0: 284.8. Samples: 5957632. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 02:21:00,968][166323] Avg episode reward: [(0, '1152.799')]
[36m[2025-07-02 02:21:05,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5947392. Throughput: 0: 282.1. Samples: 5958368. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 02:21:05,951][166323] Avg episode reward: [(0, '1149.195')]
[36m[2025-07-02 02:21:10,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5947392. Throughput: 0: 284.2. Samples: 5960096. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 02:21:10,993][166323] Avg episode reward: [(0, '1133.327')]
[36m[2025-07-02 02:21:15,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5947392. Throughput: 0: 286.1. Samples: 5961744. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 02:21:15,986][166323] Avg episode reward: [(0, '1150.634')]
[36m[2025-07-02 02:21:21,011][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 5947392. Throughput: 0: 287.2. Samples: 5962640. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 02:21:21,011][166323] Avg episode reward: [(0, '1153.674')]
[36m[2025-07-02 02:21:25,985][166323] Fps is (10 sec: 1638.6, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5963776. Throughput: 0: 281.5. Samples: 5964096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:21:25,985][166323] Avg episode reward: [(0, '1207.667')]
[36m[2025-07-02 02:21:30,966][166323] Fps is (10 sec: 1645.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5963776. Throughput: 0: 283.6. Samples: 5965872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:21:30,966][166323] Avg episode reward: [(0, '1163.453')]
[36m[2025-07-02 02:21:35,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5963776. Throughput: 0: 281.6. Samples: 5966672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:21:35,986][166323] Avg episode reward: [(0, '1116.930')]
[37m[1m[2025-07-02 02:21:36,075][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000011640_5963776.pth...
[36m[2025-07-02 02:21:36,078][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000011512_5898240.pth
[36m[2025-07-02 02:21:40,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5963776. Throughput: 0: 276.1. Samples: 5968256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:21:40,980][166323] Avg episode reward: [(0, '1168.552')]
[36m[2025-07-02 02:21:45,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5963776. Throughput: 0: 271.6. Samples: 5969856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:21:45,967][166323] Avg episode reward: [(0, '1191.510')]
[36m[2025-07-02 02:21:50,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5963776. Throughput: 0: 276.2. Samples: 5970800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:21:50,970][166323] Avg episode reward: [(0, '1226.275')]
[36m[2025-07-02 02:21:55,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5963776. Throughput: 0: 276.0. Samples: 5972512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:21:55,970][166323] Avg episode reward: [(0, '1195.904')]
[36m[2025-07-02 02:22:00,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5963776. Throughput: 0: 277.8. Samples: 5974240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:22:00,973][166323] Avg episode reward: [(0, '1228.359')]
[36m[2025-07-02 02:22:05,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5963776. Throughput: 0: 276.4. Samples: 5975072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:22:05,983][166323] Avg episode reward: [(0, '1232.862')]
[36m[2025-07-02 02:22:11,008][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5963776. Throughput: 0: 278.6. Samples: 5976640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:22:11,009][166323] Avg episode reward: [(0, '1284.695')]
[36m[2025-07-02 02:22:15,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5963776. Throughput: 0: 276.2. Samples: 5978304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:22:15,972][166323] Avg episode reward: [(0, '1280.671')]
[36m[2025-07-02 02:22:20,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5963776. Throughput: 0: 276.4. Samples: 5979104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:22:20,971][166323] Avg episode reward: [(0, '1264.073')]
[36m[2025-07-02 02:22:25,995][166323] Fps is (10 sec: 1634.7, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5980160. Throughput: 0: 279.0. Samples: 5980816. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 02:22:25,995][166323] Avg episode reward: [(0, '1267.074')]
[36m[2025-07-02 02:22:30,991][166323] Fps is (10 sec: 1635.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5980160. Throughput: 0: 278.6. Samples: 5982400. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 02:22:30,992][166323] Avg episode reward: [(0, '1298.050')]
[36m[2025-07-02 02:22:35,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5980160. Throughput: 0: 274.5. Samples: 5983152. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 02:22:35,960][166323] Avg episode reward: [(0, '1287.994')]
[36m[2025-07-02 02:22:40,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5980160. Throughput: 0: 275.7. Samples: 5984912. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 02:22:40,946][166323] Avg episode reward: [(0, '1209.779')]
[36m[2025-07-02 02:22:45,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5980160. Throughput: 0: 274.0. Samples: 5986560. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 02:22:45,944][166323] Avg episode reward: [(0, '1234.390')]
[36m[2025-07-02 02:22:50,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5980160. Throughput: 0: 271.9. Samples: 5987312. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 02:22:50,994][166323] Avg episode reward: [(0, '1248.330')]
[36m[2025-07-02 02:22:55,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5980160. Throughput: 0: 273.0. Samples: 5988912. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 02:22:55,965][166323] Avg episode reward: [(0, '1191.629')]
[36m[2025-07-02 02:23:00,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5980160. Throughput: 0: 272.5. Samples: 5990560. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 02:23:00,952][166323] Avg episode reward: [(0, '1168.966')]
[36m[2025-07-02 02:23:05,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5980160. Throughput: 0: 273.1. Samples: 5991392. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 02:23:05,961][166323] Avg episode reward: [(0, '1144.699')]
[36m[2025-07-02 02:23:10,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5980160. Throughput: 0: 272.5. Samples: 5993072. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 02:23:10,977][166323] Avg episode reward: [(0, '1183.459')]
[36m[2025-07-02 02:23:16,008][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5980160. Throughput: 0: 274.0. Samples: 5994736. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 02:23:16,008][166323] Avg episode reward: [(0, '1171.657')]
[36m[2025-07-02 02:23:20,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 5980160. Throughput: 0: 275.8. Samples: 5995568. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 02:23:20,978][166323] Avg episode reward: [(0, '1174.927')]
[36m[2025-07-02 02:23:26,004][166323] Fps is (10 sec: 1639.1, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5996544. Throughput: 0: 269.2. Samples: 5997040. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 02:23:26,004][166323] Avg episode reward: [(0, '1126.878')]
[36m[2025-07-02 02:23:30,968][166323] Fps is (10 sec: 1640.1, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5996544. Throughput: 0: 271.9. Samples: 5998800. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 02:23:30,968][166323] Avg episode reward: [(0, '1217.559')]
[36m[2025-07-02 02:23:35,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5996544. Throughput: 0: 274.8. Samples: 5999680. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 02:23:35,996][166323] Avg episode reward: [(0, '1185.832')]
[37m[1m[2025-07-02 02:23:36,057][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000011704_5996544.pth...
[36m[2025-07-02 02:23:36,061][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000011576_5931008.pth
[36m[2025-07-02 02:23:40,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5996544. Throughput: 0: 276.3. Samples: 6001344. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 02:23:40,954][166323] Avg episode reward: [(0, '1202.422')]
[36m[2025-07-02 02:23:45,973][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 5996544. Throughput: 0: 276.1. Samples: 6002992. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 02:23:45,973][166323] Avg episode reward: [(0, '1213.164')]
[36m[2025-07-02 02:23:50,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5996544. Throughput: 0: 274.4. Samples: 6003744. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 02:23:50,978][166323] Avg episode reward: [(0, '1266.979')]
[36m[2025-07-02 02:23:55,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5996544. Throughput: 0: 275.4. Samples: 6005456. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 02:23:55,945][166323] Avg episode reward: [(0, '1256.112')]
[36m[2025-07-02 02:24:00,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5996544. Throughput: 0: 278.4. Samples: 6007248. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 02:24:00,956][166323] Avg episode reward: [(0, '1268.427')]
[36m[2025-07-02 02:24:05,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 5996544. Throughput: 0: 278.1. Samples: 6008080. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 02:24:05,962][166323] Avg episode reward: [(0, '1239.216')]
[36m[2025-07-02 02:24:10,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 5996544. Throughput: 0: 285.6. Samples: 6009888. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 02:24:10,993][166323] Avg episode reward: [(0, '1246.577')]
[36m[2025-07-02 02:24:15,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 5996544. Throughput: 0: 282.2. Samples: 6011504. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 02:24:15,987][166323] Avg episode reward: [(0, '1230.535')]
[36m[2025-07-02 02:24:21,008][166323] Fps is (10 sec: 1636.0, 60 sec: 545.9, 300 sec: 277.7). Total num frames: 6012928. Throughput: 0: 281.9. Samples: 6012368. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 02:24:21,008][166323] Avg episode reward: [(0, '1227.762')]
[31m[21053440 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21053440 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[21053440 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:24:25,989][166323] Fps is (10 sec: 1638.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6012928. Throughput: 0: 279.6. Samples: 6013936. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 02:24:25,989][166323] Avg episode reward: [(0, '1198.245')]
[36m[2025-07-02 02:24:30,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6012928. Throughput: 0: 282.5. Samples: 6015712. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 02:24:30,994][166323] Avg episode reward: [(0, '1203.391')]
[36m[2025-07-02 02:24:35,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6012928. Throughput: 0: 282.7. Samples: 6016464. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 02:24:35,972][166323] Avg episode reward: [(0, '1224.624')]
[36m[2025-07-02 02:24:40,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6012928. Throughput: 0: 279.9. Samples: 6018064. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 02:24:40,993][166323] Avg episode reward: [(0, '1217.056')]
[36m[2025-07-02 02:24:45,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6012928. Throughput: 0: 279.5. Samples: 6019824. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 02:24:45,953][166323] Avg episode reward: [(0, '1270.320')]
[36m[2025-07-02 02:24:50,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6012928. Throughput: 0: 279.1. Samples: 6020640. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 02:24:50,968][166323] Avg episode reward: [(0, '1289.476')]
[36m[2025-07-02 02:24:55,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6012928. Throughput: 0: 275.1. Samples: 6022256. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 02:24:55,957][166323] Avg episode reward: [(0, '1270.172')]
[36m[2025-07-02 02:25:01,014][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 6012928. Throughput: 0: 274.3. Samples: 6023856. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 02:25:01,014][166323] Avg episode reward: [(0, '1308.804')]
[36m[2025-07-02 02:25:05,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6012928. Throughput: 0: 275.4. Samples: 6024752. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 02:25:05,969][166323] Avg episode reward: [(0, '1284.147')]
[36m[2025-07-02 02:25:10,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6012928. Throughput: 0: 275.6. Samples: 6026336. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 02:25:10,988][166323] Avg episode reward: [(0, '1297.410')]
[36m[2025-07-02 02:25:15,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6012928. Throughput: 0: 273.4. Samples: 6028016. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 02:25:15,996][166323] Avg episode reward: [(0, '1282.885')]
[36m[2025-07-02 02:25:20,965][166323] Fps is (10 sec: 1642.1, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6029312. Throughput: 0: 277.0. Samples: 6028928. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 02:25:20,965][166323] Avg episode reward: [(0, '1271.908')]
[36m[2025-07-02 02:25:25,948][166323] Fps is (10 sec: 1646.3, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6029312. Throughput: 0: 278.3. Samples: 6030576. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 02:25:25,949][166323] Avg episode reward: [(0, '1281.341')]
[36m[2025-07-02 02:25:30,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6029312. Throughput: 0: 277.0. Samples: 6032288. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 02:25:30,948][166323] Avg episode reward: [(0, '1293.614')]
[36m[2025-07-02 02:25:35,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6029312. Throughput: 0: 277.6. Samples: 6033136. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 02:25:35,978][166323] Avg episode reward: [(0, '1282.327')]
[37m[1m[2025-07-02 02:25:36,033][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000011768_6029312.pth...
[36m[2025-07-02 02:25:36,039][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000011640_5963776.pth
[36m[2025-07-02 02:25:40,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6029312. Throughput: 0: 280.1. Samples: 6034864. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 02:25:40,973][166323] Avg episode reward: [(0, '1299.081')]
[31m[21133847 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21133847 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[21133847 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:25:45,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6029312. Throughput: 0: 281.3. Samples: 6036496. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 02:25:45,947][166323] Avg episode reward: [(0, '1205.439')]
[36m[2025-07-02 02:25:50,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6029312. Throughput: 0: 278.1. Samples: 6037264. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 02:25:50,963][166323] Avg episode reward: [(0, '1194.290')]
[36m[2025-07-02 02:25:55,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6029312. Throughput: 0: 278.9. Samples: 6038880. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 02:25:55,962][166323] Avg episode reward: [(0, '1121.156')]
[36m[2025-07-02 02:26:00,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 6029312. Throughput: 0: 277.3. Samples: 6040480. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 02:26:00,946][166323] Avg episode reward: [(0, '1058.484')]
[36m[2025-07-02 02:26:05,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6029312. Throughput: 0: 274.1. Samples: 6041264. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 02:26:05,978][166323] Avg episode reward: [(0, '1077.672')]
[36m[2025-07-02 02:26:10,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6029312. Throughput: 0: 274.2. Samples: 6042912. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 02:26:10,945][166323] Avg episode reward: [(0, '1049.905')]
[36m[2025-07-02 02:26:15,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6029312. Throughput: 0: 271.6. Samples: 6044512. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 02:26:15,960][166323] Avg episode reward: [(0, '1086.860')]
[36m[2025-07-02 02:26:20,956][166323] Fps is (10 sec: 1636.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6045696. Throughput: 0: 272.8. Samples: 6045408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:26:20,957][166323] Avg episode reward: [(0, '1160.512')]
[36m[2025-07-02 02:26:25,946][166323] Fps is (10 sec: 1640.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6045696. Throughput: 0: 266.5. Samples: 6046848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:26:25,946][166323] Avg episode reward: [(0, '1160.053')]
[31m[21178562 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21178563 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[21178563 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:26:30,974][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6045696. Throughput: 0: 267.9. Samples: 6048560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:26:30,974][166323] Avg episode reward: [(0, '1197.192')]
[36m[2025-07-02 02:26:35,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6045696. Throughput: 0: 270.6. Samples: 6049440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:26:35,953][166323] Avg episode reward: [(0, '1254.595')]
[36m[2025-07-02 02:26:40,998][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6045696. Throughput: 0: 271.4. Samples: 6051104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:26:40,999][166323] Avg episode reward: [(0, '1224.905')]
[36m[2025-07-02 02:26:45,975][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6045696. Throughput: 0: 274.0. Samples: 6052816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:26:45,975][166323] Avg episode reward: [(0, '1236.823')]
[31m[21198115 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21198116 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[21198116 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:26:50,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6045696. Throughput: 0: 274.2. Samples: 6053600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:26:50,974][166323] Avg episode reward: [(0, '1166.114')]
[36m[2025-07-02 02:26:55,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6045696. Throughput: 0: 272.8. Samples: 6055200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:26:55,992][166323] Avg episode reward: [(0, '1191.158')]
[36m[2025-07-02 02:27:00,973][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6045696. Throughput: 0: 273.7. Samples: 6056832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:27:00,973][166323] Avg episode reward: [(0, '1145.996')]
[36m[2025-07-02 02:27:05,997][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6045696. Throughput: 0: 272.5. Samples: 6057680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:27:05,997][166323] Avg episode reward: [(0, '1141.927')]
[36m[2025-07-02 02:27:11,003][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 6045696. Throughput: 0: 274.1. Samples: 6059200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:27:11,003][166323] Avg episode reward: [(0, '1038.347')]
[36m[2025-07-02 02:27:15,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6045696. Throughput: 0: 273.4. Samples: 6060864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:27:15,972][166323] Avg episode reward: [(0, '1109.648')]
[31m[21228236 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21228236 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[21228237 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:27:20,987][166323] Fps is (10 sec: 1641.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6062080. Throughput: 0: 271.8. Samples: 6061680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:27:20,987][166323] Avg episode reward: [(0, '1103.966')]
[31m[21233835 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21233835 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[21233835 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:27:25,949][166323] Fps is (10 sec: 1642.1, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6062080. Throughput: 0: 274.1. Samples: 6063424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:27:25,949][166323] Avg episode reward: [(0, '1194.578')]
[36m[2025-07-02 02:27:30,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6062080. Throughput: 0: 271.8. Samples: 6065040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:27:30,955][166323] Avg episode reward: [(0, '1181.418')]
[31m[21243068 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21243068 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[21243068 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:27:35,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6062080. Throughput: 0: 273.3. Samples: 6065904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:27:35,989][166323] Avg episode reward: [(0, '1131.938')]
[37m[1m[2025-07-02 02:27:36,044][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000011832_6062080.pth...
[36m[2025-07-02 02:27:36,049][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000011704_5996544.pth
[36m[2025-07-02 02:27:40,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6062080. Throughput: 0: 275.3. Samples: 6067584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:27:40,975][166323] Avg episode reward: [(0, '1169.446')]
[36m[2025-07-02 02:27:45,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6062080. Throughput: 0: 274.3. Samples: 6069184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:27:45,998][166323] Avg episode reward: [(0, '1206.600')]
[36m[2025-07-02 02:27:50,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6062080. Throughput: 0: 271.7. Samples: 6069904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:27:50,983][166323] Avg episode reward: [(0, '1129.778')]
[36m[2025-07-02 02:27:55,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6062080. Throughput: 0: 271.5. Samples: 6071408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:27:55,974][166323] Avg episode reward: [(0, '1175.014')]
[36m[2025-07-02 02:28:00,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6062080. Throughput: 0: 277.1. Samples: 6073328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:28:00,950][166323] Avg episode reward: [(0, '1193.691')]
[36m[2025-07-02 02:28:05,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6062080. Throughput: 0: 277.4. Samples: 6074160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:28:05,975][166323] Avg episode reward: [(0, '1216.434')]
[36m[2025-07-02 02:28:10,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6062080. Throughput: 0: 276.5. Samples: 6075872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:28:10,963][166323] Avg episode reward: [(0, '1216.747')]
[36m[2025-07-02 02:28:15,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6062080. Throughput: 0: 278.8. Samples: 6077584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:28:15,950][166323] Avg episode reward: [(0, '1274.499')]
[36m[2025-07-02 02:28:21,001][166323] Fps is (10 sec: 1632.2, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6078464. Throughput: 0: 276.5. Samples: 6078352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:28:21,001][166323] Avg episode reward: [(0, '1229.035')]
[36m[2025-07-02 02:28:25,978][166323] Fps is (10 sec: 1633.8, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6078464. Throughput: 0: 273.0. Samples: 6079872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:28:25,978][166323] Avg episode reward: [(0, '1247.523')]
[36m[2025-07-02 02:28:30,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6078464. Throughput: 0: 272.6. Samples: 6081440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:28:30,950][166323] Avg episode reward: [(0, '1218.334')]
[36m[2025-07-02 02:28:35,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6078464. Throughput: 0: 276.1. Samples: 6082320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:28:35,944][166323] Avg episode reward: [(0, '1209.983')]
[36m[2025-07-02 02:28:40,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6078464. Throughput: 0: 279.9. Samples: 6084000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:28:40,966][166323] Avg episode reward: [(0, '1234.822')]
[36m[2025-07-02 02:28:45,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6078464. Throughput: 0: 274.8. Samples: 6085696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:28:45,965][166323] Avg episode reward: [(0, '1258.311')]
[36m[2025-07-02 02:28:50,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6078464. Throughput: 0: 277.5. Samples: 6086640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:28:50,954][166323] Avg episode reward: [(0, '1239.419')]
[36m[2025-07-02 02:28:55,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6078464. Throughput: 0: 277.0. Samples: 6088336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:28:55,967][166323] Avg episode reward: [(0, '1239.160')]
[33m[21325068 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[21325068 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.86669921875
[33mCrash Rate: 0.1220703125
[33mTimeout Rate: 0.01123046875 (navigation_task.py:265)
[33m[21325068 ms][navigation_task] - WARNING : 
[33mSuccesses: 1775
[33mCrashes : 250
[33mTimeouts: 23 (navigation_task.py:268)
[36m[2025-07-02 02:29:00,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6078464. Throughput: 0: 274.1. Samples: 6089920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:29:00,947][166323] Avg episode reward: [(0, '1273.346')]
[36m[2025-07-02 02:29:05,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6078464. Throughput: 0: 274.7. Samples: 6090704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:29:05,967][166323] Avg episode reward: [(0, '1209.753')]
[36m[2025-07-02 02:29:10,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6078464. Throughput: 0: 274.3. Samples: 6092208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:29:10,954][166323] Avg episode reward: [(0, '1234.357')]
[36m[2025-07-02 02:29:15,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 6078464. Throughput: 0: 279.4. Samples: 6094016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:29:15,959][166323] Avg episode reward: [(0, '1265.057')]
[31m[21345559 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21345559 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[21345559 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:29:20,967][166323] Fps is (10 sec: 1636.3, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6094848. Throughput: 0: 279.0. Samples: 6094880. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 02:29:20,967][166323] Avg episode reward: [(0, '1148.413')]
[36m[2025-07-02 02:29:25,979][166323] Fps is (10 sec: 1635.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6094848. Throughput: 0: 276.9. Samples: 6096464. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 02:29:25,979][166323] Avg episode reward: [(0, '1129.531')]
[36m[2025-07-02 02:29:30,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6094848. Throughput: 0: 276.8. Samples: 6098160. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 02:29:30,989][166323] Avg episode reward: [(0, '1189.554')]
[36m[2025-07-02 02:29:35,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6094848. Throughput: 0: 275.2. Samples: 6099024. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 02:29:35,959][166323] Avg episode reward: [(0, '1246.890')]
[37m[1m[2025-07-02 02:29:36,028][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000011896_6094848.pth...
[36m[2025-07-02 02:29:36,033][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000011768_6029312.pth
[36m[2025-07-02 02:29:41,002][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 6094848. Throughput: 0: 271.4. Samples: 6100560. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 02:29:41,003][166323] Avg episode reward: [(0, '1254.353')]
[36m[2025-07-02 02:29:45,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6094848. Throughput: 0: 273.6. Samples: 6102240. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 02:29:45,983][166323] Avg episode reward: [(0, '1253.201')]
[36m[2025-07-02 02:29:50,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6094848. Throughput: 0: 275.3. Samples: 6103088. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 02:29:50,951][166323] Avg episode reward: [(0, '1226.007')]
[36m[2025-07-02 02:29:55,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 6094848. Throughput: 0: 277.3. Samples: 6104688. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 02:29:55,954][166323] Avg episode reward: [(0, '1294.990')]
[36m[2025-07-02 02:30:00,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6094848. Throughput: 0: 277.5. Samples: 6106512. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 02:30:00,988][166323] Avg episode reward: [(0, '1331.612')]
[36m[2025-07-02 02:30:05,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6094848. Throughput: 0: 278.5. Samples: 6107408. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 02:30:05,954][166323] Avg episode reward: [(0, '1343.619')]
[37m[1m[2025-07-02 02:30:06,008][166323] Saving new best policy, reward=1343.619!
[36m[2025-07-02 02:30:10,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6094848. Throughput: 0: 278.1. Samples: 6108976. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 02:30:10,963][166323] Avg episode reward: [(0, '1323.331')]
[36m[2025-07-02 02:30:15,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 6094848. Throughput: 0: 278.2. Samples: 6110672. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 02:30:15,956][166323] Avg episode reward: [(0, '1318.057')]
[36m[2025-07-02 02:30:20,950][166323] Fps is (10 sec: 1640.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6111232. Throughput: 0: 274.9. Samples: 6111392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:30:20,950][166323] Avg episode reward: [(0, '1317.354')]
[36m[2025-07-02 02:30:25,979][166323] Fps is (10 sec: 1634.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6111232. Throughput: 0: 280.7. Samples: 6113184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:30:25,979][166323] Avg episode reward: [(0, '1328.355')]
[36m[2025-07-02 02:30:30,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6111232. Throughput: 0: 280.0. Samples: 6114832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:30:30,953][166323] Avg episode reward: [(0, '1330.990')]
[36m[2025-07-02 02:30:35,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6111232. Throughput: 0: 279.7. Samples: 6115680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:30:35,963][166323] Avg episode reward: [(0, '1318.710')]
[36m[2025-07-02 02:30:40,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6111232. Throughput: 0: 280.6. Samples: 6117328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:30:40,995][166323] Avg episode reward: [(0, '1311.107')]
[31m[21433576 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21433577 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[21433577 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:30:45,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6111232. Throughput: 0: 276.4. Samples: 6118944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:30:45,974][166323] Avg episode reward: [(0, '1233.929')]
[31m[21436432 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21436432 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[21436432 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:30:50,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6111232. Throughput: 0: 277.6. Samples: 6119904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:30:50,971][166323] Avg episode reward: [(0, '1252.377')]
[36m[2025-07-02 02:30:55,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6111232. Throughput: 0: 282.3. Samples: 6121680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:30:55,961][166323] Avg episode reward: [(0, '1251.089')]
[36m[2025-07-02 02:31:01,009][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6111232. Throughput: 0: 282.0. Samples: 6123376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:31:01,010][166323] Avg episode reward: [(0, '1167.193')]
[36m[2025-07-02 02:31:05,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6111232. Throughput: 0: 284.4. Samples: 6124192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:31:05,960][166323] Avg episode reward: [(0, '1123.913')]
[36m[2025-07-02 02:31:10,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6111232. Throughput: 0: 284.0. Samples: 6125952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:31:10,944][166323] Avg episode reward: [(0, '1099.904')]
[36m[2025-07-02 02:31:15,971][166323] Fps is (10 sec: 1636.6, 60 sec: 546.0, 300 sec: 277.7). Total num frames: 6127616. Throughput: 0: 285.4. Samples: 6127680. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 02:31:15,971][166323] Avg episode reward: [(0, '1088.510')]
[36m[2025-07-02 02:31:20,953][166323] Fps is (10 sec: 1636.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6127616. Throughput: 0: 286.3. Samples: 6128560. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 02:31:20,953][166323] Avg episode reward: [(0, '1172.116')]
[36m[2025-07-02 02:31:25,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6127616. Throughput: 0: 287.9. Samples: 6130272. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 02:31:25,959][166323] Avg episode reward: [(0, '1120.457')]
[36m[2025-07-02 02:31:30,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6127616. Throughput: 0: 289.3. Samples: 6131968. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 02:31:30,986][166323] Avg episode reward: [(0, '1127.890')]
[36m[2025-07-02 02:31:35,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6127616. Throughput: 0: 285.7. Samples: 6132752. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 02:31:35,946][166323] Avg episode reward: [(0, '1133.265')]
[37m[1m[2025-07-02 02:31:35,997][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000011960_6127616.pth...
[36m[2025-07-02 02:31:36,001][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000011832_6062080.pth
[36m[2025-07-02 02:31:40,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6127616. Throughput: 0: 283.2. Samples: 6134432. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 02:31:40,988][166323] Avg episode reward: [(0, '1140.767')]
[36m[2025-07-02 02:31:45,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6127616. Throughput: 0: 285.9. Samples: 6136224. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 02:31:45,947][166323] Avg episode reward: [(0, '1149.574')]
[36m[2025-07-02 02:31:50,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6127616. Throughput: 0: 284.6. Samples: 6137008. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 02:31:50,991][166323] Avg episode reward: [(0, '1133.437')]
[36m[2025-07-02 02:31:55,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6127616. Throughput: 0: 281.0. Samples: 6138608. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 02:31:55,979][166323] Avg episode reward: [(0, '1113.772')]
[36m[2025-07-02 02:32:00,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6127616. Throughput: 0: 277.4. Samples: 6140160. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 02:32:00,954][166323] Avg episode reward: [(0, '1159.682')]
[36m[2025-07-02 02:32:05,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6127616. Throughput: 0: 276.2. Samples: 6140992. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 02:32:05,967][166323] Avg episode reward: [(0, '1157.468')]
[36m[2025-07-02 02:32:11,002][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 6127616. Throughput: 0: 274.6. Samples: 6142640. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 02:32:11,003][166323] Avg episode reward: [(0, '1157.049')]
[31m[21522582 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21522582 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[21522583 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:32:15,968][166323] Fps is (10 sec: 1638.3, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6144000. Throughput: 0: 271.8. Samples: 6144192. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:32:15,968][166323] Avg episode reward: [(0, '1168.419')]
[36m[2025-07-02 02:32:20,965][166323] Fps is (10 sec: 1644.5, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6144000. Throughput: 0: 273.3. Samples: 6145056. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:32:20,966][166323] Avg episode reward: [(0, '1148.854')]
[36m[2025-07-02 02:32:25,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6144000. Throughput: 0: 273.9. Samples: 6146752. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:32:25,973][166323] Avg episode reward: [(0, '1194.983')]
[36m[2025-07-02 02:32:30,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6144000. Throughput: 0: 274.3. Samples: 6148576. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:32:30,974][166323] Avg episode reward: [(0, '1206.149')]
[36m[2025-07-02 02:32:36,021][166323] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 277.7). Total num frames: 6144000. Throughput: 0: 275.4. Samples: 6149408. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:32:36,022][166323] Avg episode reward: [(0, '1150.119')]
[36m[2025-07-02 02:32:40,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6144000. Throughput: 0: 277.3. Samples: 6151088. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:32:40,978][166323] Avg episode reward: [(0, '1159.880')]
[36m[2025-07-02 02:32:45,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6144000. Throughput: 0: 281.4. Samples: 6152832. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:32:45,985][166323] Avg episode reward: [(0, '1185.474')]
[36m[2025-07-02 02:32:50,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6144000. Throughput: 0: 280.8. Samples: 6153632. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:32:50,986][166323] Avg episode reward: [(0, '1195.979')]
[36m[2025-07-02 02:32:55,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6144000. Throughput: 0: 282.4. Samples: 6155344. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:32:55,993][166323] Avg episode reward: [(0, '1084.113')]
[36m[2025-07-02 02:33:00,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6144000. Throughput: 0: 285.7. Samples: 6157056. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:33:00,990][166323] Avg episode reward: [(0, '1098.966')]
[36m[2025-07-02 02:33:05,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6144000. Throughput: 0: 283.3. Samples: 6157808. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:33:05,977][166323] Avg episode reward: [(0, '1114.403')]
[36m[2025-07-02 02:33:10,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6144000. Throughput: 0: 285.3. Samples: 6159584. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:33:10,946][166323] Avg episode reward: [(0, '1146.186')]
[36m[2025-07-02 02:33:15,991][166323] Fps is (10 sec: 1635.9, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6160384. Throughput: 0: 281.1. Samples: 6161232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:33:15,992][166323] Avg episode reward: [(0, '1119.596')]
[36m[2025-07-02 02:33:20,957][166323] Fps is (10 sec: 1636.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6160384. Throughput: 0: 282.0. Samples: 6162080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:33:20,957][166323] Avg episode reward: [(0, '1080.904')]
[36m[2025-07-02 02:33:25,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6160384. Throughput: 0: 280.2. Samples: 6163696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:33:25,974][166323] Avg episode reward: [(0, '1149.730')]
[36m[2025-07-02 02:33:30,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6160384. Throughput: 0: 279.6. Samples: 6165408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:33:30,970][166323] Avg episode reward: [(0, '1209.587')]
[36m[2025-07-02 02:33:35,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6160384. Throughput: 0: 278.1. Samples: 6166144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:33:35,985][166323] Avg episode reward: [(0, '1220.903')]
[37m[1m[2025-07-02 02:33:36,044][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000012024_6160384.pth...
[36m[2025-07-02 02:33:36,048][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000011896_6094848.pth
[31m[21609274 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21609274 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[21609274 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:33:40,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6160384. Throughput: 0: 275.4. Samples: 6167728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:33:40,961][166323] Avg episode reward: [(0, '1214.619')]
[36m[2025-07-02 02:33:45,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6160384. Throughput: 0: 273.6. Samples: 6169360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:33:45,974][166323] Avg episode reward: [(0, '1218.531')]
[36m[2025-07-02 02:33:50,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6160384. Throughput: 0: 275.9. Samples: 6170224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:33:50,985][166323] Avg episode reward: [(0, '1225.997')]
[36m[2025-07-02 02:33:55,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6160384. Throughput: 0: 271.6. Samples: 6171808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:33:55,950][166323] Avg episode reward: [(0, '1261.921')]
[36m[2025-07-02 02:34:00,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6160384. Throughput: 0: 274.8. Samples: 6173584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:34:00,944][166323] Avg episode reward: [(0, '1262.837')]
[36m[2025-07-02 02:34:05,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6160384. Throughput: 0: 274.1. Samples: 6174416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:34:05,968][166323] Avg episode reward: [(0, '1247.136')]
[31m[21634762 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21634762 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[21634762 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:34:10,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6160384. Throughput: 0: 275.6. Samples: 6176096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:34:10,971][166323] Avg episode reward: [(0, '1184.111')]
[36m[2025-07-02 02:34:15,945][166323] Fps is (10 sec: 1642.1, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6176768. Throughput: 0: 276.1. Samples: 6177824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:34:15,945][166323] Avg episode reward: [(0, '1216.603')]
[36m[2025-07-02 02:34:20,989][166323] Fps is (10 sec: 1635.4, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6176768. Throughput: 0: 278.4. Samples: 6178672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:34:20,989][166323] Avg episode reward: [(0, '1194.566')]
[36m[2025-07-02 02:34:25,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6176768. Throughput: 0: 278.3. Samples: 6180256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:34:25,982][166323] Avg episode reward: [(0, '1152.751')]
[36m[2025-07-02 02:34:30,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6176768. Throughput: 0: 277.3. Samples: 6181840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:34:30,968][166323] Avg episode reward: [(0, '1158.750')]
[36m[2025-07-02 02:34:35,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6176768. Throughput: 0: 279.3. Samples: 6182784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:34:35,955][166323] Avg episode reward: [(0, '1131.108')]
[36m[2025-07-02 02:34:40,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6176768. Throughput: 0: 278.7. Samples: 6184352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:34:40,962][166323] Avg episode reward: [(0, '1204.756')]
[36m[2025-07-02 02:34:46,035][166323] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 277.6). Total num frames: 6176768. Throughput: 0: 280.0. Samples: 6186208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:34:46,035][166323] Avg episode reward: [(0, '1234.448')]
[36m[2025-07-02 02:34:50,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6176768. Throughput: 0: 281.6. Samples: 6187088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:34:50,969][166323] Avg episode reward: [(0, '1221.446')]
[36m[2025-07-02 02:34:55,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6176768. Throughput: 0: 282.0. Samples: 6188784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:34:55,970][166323] Avg episode reward: [(0, '1257.283')]
[36m[2025-07-02 02:35:00,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6176768. Throughput: 0: 282.4. Samples: 6190544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:35:00,988][166323] Avg episode reward: [(0, '1301.327')]
[36m[2025-07-02 02:35:05,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6176768. Throughput: 0: 282.5. Samples: 6191376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:35:05,952][166323] Avg episode reward: [(0, '1316.821')]
[36m[2025-07-02 02:35:11,023][166323] Fps is (10 sec: 1632.6, 60 sec: 545.7, 300 sec: 333.2). Total num frames: 6193152. Throughput: 0: 282.1. Samples: 6192960. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 02:35:11,023][166323] Avg episode reward: [(0, '1316.134')]
[36m[2025-07-02 02:35:15,945][166323] Fps is (10 sec: 1639.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6193152. Throughput: 0: 281.4. Samples: 6194496. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 02:35:15,946][166323] Avg episode reward: [(0, '1288.601')]
[36m[2025-07-02 02:35:20,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6193152. Throughput: 0: 279.5. Samples: 6195360. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 02:35:20,943][166323] Avg episode reward: [(0, '1265.277')]
[36m[2025-07-02 02:35:25,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6193152. Throughput: 0: 283.4. Samples: 6197104. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 02:35:25,955][166323] Avg episode reward: [(0, '1253.685')]
[31m[21718623 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21718623 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[21718623 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:35:30,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6193152. Throughput: 0: 281.7. Samples: 6198864. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 02:35:30,966][166323] Avg episode reward: [(0, '1206.348')]
[36m[2025-07-02 02:35:35,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6193152. Throughput: 0: 279.8. Samples: 6199680. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 02:35:35,979][166323] Avg episode reward: [(0, '1175.467')]
[37m[1m[2025-07-02 02:35:36,029][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000012088_6193152.pth...
[36m[2025-07-02 02:35:36,033][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000011960_6127616.pth
[36m[2025-07-02 02:35:40,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6193152. Throughput: 0: 279.2. Samples: 6201344. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 02:35:40,960][166323] Avg episode reward: [(0, '1162.824')]
[36m[2025-07-02 02:35:45,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 6193152. Throughput: 0: 280.7. Samples: 6203168. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 02:35:45,968][166323] Avg episode reward: [(0, '1193.898')]
[36m[2025-07-02 02:35:50,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6193152. Throughput: 0: 282.7. Samples: 6204096. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 02:35:50,953][166323] Avg episode reward: [(0, '1201.528')]
[31m[21743780 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21743780 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[21743780 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[21744129 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21744129 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[21744130 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:35:55,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6193152. Throughput: 0: 285.1. Samples: 6205776. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 02:35:55,974][166323] Avg episode reward: [(0, '1190.761')]
[36m[2025-07-02 02:36:00,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6193152. Throughput: 0: 285.7. Samples: 6207360. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 02:36:00,970][166323] Avg episode reward: [(0, '1157.742')]
[36m[2025-07-02 02:36:05,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6193152. Throughput: 0: 283.9. Samples: 6208144. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 02:36:05,972][166323] Avg episode reward: [(0, '1183.648')]
[36m[2025-07-02 02:36:10,949][166323] Fps is (10 sec: 1641.9, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 6209536. Throughput: 0: 282.7. Samples: 6209824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:36:10,949][166323] Avg episode reward: [(0, '1203.464')]
[36m[2025-07-02 02:36:15,978][166323] Fps is (10 sec: 1637.3, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6209536. Throughput: 0: 282.2. Samples: 6211568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:36:15,979][166323] Avg episode reward: [(0, '1221.340')]
[36m[2025-07-02 02:36:20,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6209536. Throughput: 0: 282.6. Samples: 6212400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:36:20,990][166323] Avg episode reward: [(0, '1184.635')]
[36m[2025-07-02 02:36:25,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6209536. Throughput: 0: 285.4. Samples: 6214192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:36:25,975][166323] Avg episode reward: [(0, '1188.896')]
[36m[2025-07-02 02:36:30,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6209536. Throughput: 0: 282.0. Samples: 6215856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:36:30,961][166323] Avg episode reward: [(0, '1219.783')]
[36m[2025-07-02 02:36:35,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6209536. Throughput: 0: 281.4. Samples: 6216768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:36:35,983][166323] Avg episode reward: [(0, '1227.364')]
[31m[21788765 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21788765 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[21788765 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:36:40,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6209536. Throughput: 0: 280.1. Samples: 6218384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:36:40,979][166323] Avg episode reward: [(0, '1209.163')]
[36m[2025-07-02 02:36:45,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6209536. Throughput: 0: 283.0. Samples: 6220096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:36:45,971][166323] Avg episode reward: [(0, '1216.996')]
[36m[2025-07-02 02:36:50,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6209536. Throughput: 0: 282.2. Samples: 6220848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:36:50,993][166323] Avg episode reward: [(0, '1182.799')]
[36m[2025-07-02 02:36:55,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6209536. Throughput: 0: 282.3. Samples: 6222528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:36:55,951][166323] Avg episode reward: [(0, '1218.329')]
[31m[21805704 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21805705 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[21805705 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:37:00,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6209536. Throughput: 0: 281.9. Samples: 6224256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:37:00,990][166323] Avg episode reward: [(0, '1239.920')]
[36m[2025-07-02 02:37:05,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6209536. Throughput: 0: 281.6. Samples: 6225072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:37:05,989][166323] Avg episode reward: [(0, '1181.787')]
[36m[2025-07-02 02:37:10,948][166323] Fps is (10 sec: 1645.3, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6225920. Throughput: 0: 275.0. Samples: 6226560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:37:10,948][166323] Avg episode reward: [(0, '1216.925')]
[36m[2025-07-02 02:37:15,980][166323] Fps is (10 sec: 1639.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6225920. Throughput: 0: 276.1. Samples: 6228288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:37:15,981][166323] Avg episode reward: [(0, '1245.326')]
[36m[2025-07-02 02:37:20,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6225920. Throughput: 0: 276.3. Samples: 6229200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:37:20,985][166323] Avg episode reward: [(0, '1175.115')]
[36m[2025-07-02 02:37:25,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6225920. Throughput: 0: 280.3. Samples: 6230992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:37:25,954][166323] Avg episode reward: [(0, '1191.278')]
[36m[2025-07-02 02:37:30,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 6225920. Throughput: 0: 280.7. Samples: 6232720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:37:30,950][166323] Avg episode reward: [(0, '1133.605')]
[36m[2025-07-02 02:37:35,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6225920. Throughput: 0: 283.7. Samples: 6233600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:37:35,944][166323] Avg episode reward: [(0, '1086.385')]
[37m[1m[2025-07-02 02:37:35,995][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000012152_6225920.pth...
[36m[2025-07-02 02:37:36,000][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000012024_6160384.pth
[36m[2025-07-02 02:37:40,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6225920. Throughput: 0: 284.4. Samples: 6235328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:37:40,965][166323] Avg episode reward: [(0, '1160.349')]
[36m[2025-07-02 02:37:45,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6225920. Throughput: 0: 281.2. Samples: 6236896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:37:45,944][166323] Avg episode reward: [(0, '1139.842')]
[36m[2025-07-02 02:37:50,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6225920. Throughput: 0: 281.0. Samples: 6237712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:37:50,968][166323] Avg episode reward: [(0, '1150.923')]
[36m[2025-07-02 02:37:55,978][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6225920. Throughput: 0: 284.3. Samples: 6239360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:37:55,978][166323] Avg episode reward: [(0, '1224.995')]
[36m[2025-07-02 02:38:01,023][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6225920. Throughput: 0: 284.5. Samples: 6241104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:38:01,024][166323] Avg episode reward: [(0, '1247.402')]
[36m[2025-07-02 02:38:05,967][166323] Fps is (10 sec: 1640.2, 60 sec: 546.3, 300 sec: 333.2). Total num frames: 6242304. Throughput: 0: 282.1. Samples: 6241888. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:38:05,967][166323] Avg episode reward: [(0, '1283.907')]
[36m[2025-07-02 02:38:10,952][166323] Fps is (10 sec: 1650.3, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6242304. Throughput: 0: 280.9. Samples: 6243632. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:38:10,952][166323] Avg episode reward: [(0, '1281.826')]
[31m[21883399 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21883399 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[21883400 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:38:15,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6242304. Throughput: 0: 278.7. Samples: 6245264. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:38:15,953][166323] Avg episode reward: [(0, '1213.111')]
[36m[2025-07-02 02:38:20,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6242304. Throughput: 0: 276.0. Samples: 6246032. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:38:20,980][166323] Avg episode reward: [(0, '1250.404')]
[36m[2025-07-02 02:38:25,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6242304. Throughput: 0: 277.3. Samples: 6247808. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:38:25,968][166323] Avg episode reward: [(0, '1196.412')]
[36m[2025-07-02 02:38:30,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6242304. Throughput: 0: 280.7. Samples: 6249536. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:38:30,969][166323] Avg episode reward: [(0, '1163.501')]
[36m[2025-07-02 02:38:35,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6242304. Throughput: 0: 280.2. Samples: 6250320. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:38:35,966][166323] Avg episode reward: [(0, '1194.024')]
[31m[21908739 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21908740 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[21908740 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:38:40,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6242304. Throughput: 0: 280.5. Samples: 6251984. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:38:40,987][166323] Avg episode reward: [(0, '1144.379')]
[36m[2025-07-02 02:38:45,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6242304. Throughput: 0: 280.6. Samples: 6253712. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:38:45,955][166323] Avg episode reward: [(0, '1199.711')]
[36m[2025-07-02 02:38:50,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6242304. Throughput: 0: 282.2. Samples: 6254592. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:38:50,982][166323] Avg episode reward: [(0, '1162.370')]
[36m[2025-07-02 02:38:55,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6242304. Throughput: 0: 279.4. Samples: 6256208. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:38:55,966][166323] Avg episode reward: [(0, '1203.719')]
[36m[2025-07-02 02:39:00,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6242304. Throughput: 0: 279.2. Samples: 6257840. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 02:39:00,994][166323] Avg episode reward: [(0, '1192.905')]
[36m[2025-07-02 02:39:05,971][166323] Fps is (10 sec: 1637.6, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 6258688. Throughput: 0: 279.9. Samples: 6258624. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 02:39:05,971][166323] Avg episode reward: [(0, '1210.374')]
[36m[2025-07-02 02:39:11,016][166323] Fps is (10 sec: 1634.8, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 6258688. Throughput: 0: 278.8. Samples: 6260368. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 02:39:11,016][166323] Avg episode reward: [(0, '1199.185')]
[36m[2025-07-02 02:39:15,979][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6258688. Throughput: 0: 275.5. Samples: 6261936. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 02:39:15,979][166323] Avg episode reward: [(0, '1188.885')]
[36m[2025-07-02 02:39:20,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6258688. Throughput: 0: 276.7. Samples: 6262768. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 02:39:20,961][166323] Avg episode reward: [(0, '1182.760')]
[36m[2025-07-02 02:39:25,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6258688. Throughput: 0: 274.3. Samples: 6264320. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 02:39:25,965][166323] Avg episode reward: [(0, '1222.769')]
[36m[2025-07-02 02:39:30,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6258688. Throughput: 0: 272.1. Samples: 6265968. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 02:39:31,000][166323] Avg episode reward: [(0, '1195.407')]
[36m[2025-07-02 02:39:35,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6258688. Throughput: 0: 272.9. Samples: 6266864. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 02:39:35,943][166323] Avg episode reward: [(0, '1196.829')]
[37m[1m[2025-07-02 02:39:36,001][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000012216_6258688.pth...
[36m[2025-07-02 02:39:36,005][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000012088_6193152.pth
[36m[2025-07-02 02:39:40,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 6258688. Throughput: 0: 272.7. Samples: 6268480. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 02:39:40,966][166323] Avg episode reward: [(0, '1154.716')]
[36m[2025-07-02 02:39:45,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6258688. Throughput: 0: 273.1. Samples: 6270128. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 02:39:45,982][166323] Avg episode reward: [(0, '1181.755')]
[36m[2025-07-02 02:39:50,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6258688. Throughput: 0: 275.3. Samples: 6271008. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 02:39:50,947][166323] Avg episode reward: [(0, '1182.028')]
[36m[2025-07-02 02:39:55,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6258688. Throughput: 0: 272.2. Samples: 6272608. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 02:39:55,989][166323] Avg episode reward: [(0, '1157.982')]
[36m[2025-07-02 02:40:00,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6258688. Throughput: 0: 275.0. Samples: 6274304. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 02:40:00,946][166323] Avg episode reward: [(0, '1111.921')]
[36m[2025-07-02 02:40:06,027][166323] Fps is (10 sec: 1632.1, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 6275072. Throughput: 0: 274.4. Samples: 6275136. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 02:40:06,027][166323] Avg episode reward: [(0, '1104.089')]
[31m[21998869 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21998870 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[21998870 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:40:10,984][166323] Fps is (10 sec: 1632.2, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6275072. Throughput: 0: 282.2. Samples: 6277024. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 02:40:10,984][166323] Avg episode reward: [(0, '1168.647')]
[36m[2025-07-02 02:40:15,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6275072. Throughput: 0: 286.7. Samples: 6278864. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 02:40:15,978][166323] Avg episode reward: [(0, '1123.722')]
[33m[22004882 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[22004882 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.87109375
[33mCrash Rate: 0.12158203125
[33mTimeout Rate: 0.00732421875 (navigation_task.py:265)
[33m[22004883 ms][navigation_task] - WARNING : 
[33mSuccesses: 1784
[33mCrashes : 249
[33mTimeouts: 15 (navigation_task.py:268)
[36m[2025-07-02 02:40:20,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6275072. Throughput: 0: 285.6. Samples: 6279728. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 02:40:20,980][166323] Avg episode reward: [(0, '1151.685')]
[31m[22014499 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[22014499 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[22014500 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:40:25,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6275072. Throughput: 0: 285.7. Samples: 6281344. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 02:40:25,992][166323] Avg episode reward: [(0, '1116.382')]
[36m[2025-07-02 02:40:31,005][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6275072. Throughput: 0: 285.7. Samples: 6282992. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 02:40:31,006][166323] Avg episode reward: [(0, '1203.255')]
[36m[2025-07-02 02:40:35,970][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6275072. Throughput: 0: 285.4. Samples: 6283856. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 02:40:35,970][166323] Avg episode reward: [(0, '1194.083')]
[36m[2025-07-02 02:40:40,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6275072. Throughput: 0: 286.6. Samples: 6285504. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 02:40:40,990][166323] Avg episode reward: [(0, '1171.591')]
[31m[22033410 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[22033410 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[22033410 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:40:45,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6275072. Throughput: 0: 289.1. Samples: 6287312. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 02:40:45,947][166323] Avg episode reward: [(0, '1203.161')]
[36m[2025-07-02 02:40:50,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6275072. Throughput: 0: 291.0. Samples: 6288208. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 02:40:50,951][166323] Avg episode reward: [(0, '1237.824')]
[36m[2025-07-02 02:40:55,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6275072. Throughput: 0: 286.6. Samples: 6289920. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 02:40:55,975][166323] Avg episode reward: [(0, '1248.907')]
[31m[22046313 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[22046314 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[22046315 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:41:00,948][166323] Fps is (10 sec: 1639.0, 60 sec: 546.1, 300 sec: 333.3). Total num frames: 6291456. Throughput: 0: 280.7. Samples: 6291488. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 02:41:00,948][166323] Avg episode reward: [(0, '1294.998')]
[36m[2025-07-02 02:41:05,950][166323] Fps is (10 sec: 1642.4, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 6291456. Throughput: 0: 279.3. Samples: 6292288. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 02:41:05,950][166323] Avg episode reward: [(0, '1279.207')]
[36m[2025-07-02 02:41:10,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6291456. Throughput: 0: 277.2. Samples: 6293808. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 02:41:10,955][166323] Avg episode reward: [(0, '1274.745')]
[36m[2025-07-02 02:41:15,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6291456. Throughput: 0: 276.8. Samples: 6295440. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 02:41:15,978][166323] Avg episode reward: [(0, '1198.814')]
[31m[22067289 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[22067289 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[22067289 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:41:20,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6291456. Throughput: 0: 275.8. Samples: 6296272. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 02:41:20,982][166323] Avg episode reward: [(0, '1254.719')]
[36m[2025-07-02 02:41:25,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6291456. Throughput: 0: 276.1. Samples: 6297920. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 02:41:25,955][166323] Avg episode reward: [(0, '1222.886')]
[36m[2025-07-02 02:41:30,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6291456. Throughput: 0: 274.8. Samples: 6299680. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 02:41:30,953][166323] Avg episode reward: [(0, '1248.154')]
[36m[2025-07-02 02:41:35,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6291456. Throughput: 0: 272.9. Samples: 6300496. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 02:41:35,984][166323] Avg episode reward: [(0, '1289.885')]
[37m[1m[2025-07-02 02:41:36,063][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000012280_6291456.pth...
[36m[2025-07-02 02:41:36,068][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000012152_6225920.pth
[31m[22086880 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[22086880 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[22086881 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:41:41,010][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6291456. Throughput: 0: 271.8. Samples: 6302160. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 02:41:41,011][166323] Avg episode reward: [(0, '1260.962')]
[36m[2025-07-02 02:41:45,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6291456. Throughput: 0: 274.8. Samples: 6303856. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 02:41:45,948][166323] Avg episode reward: [(0, '1239.541')]
[36m[2025-07-02 02:41:50,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6291456. Throughput: 0: 274.3. Samples: 6304640. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 02:41:50,984][166323] Avg episode reward: [(0, '1269.780')]
[36m[2025-07-02 02:41:55,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6291456. Throughput: 0: 276.3. Samples: 6306240. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 02:41:55,953][166323] Avg episode reward: [(0, '1227.673')]
[36m[2025-07-02 02:42:01,031][166323] Fps is (10 sec: 1630.7, 60 sec: 272.7, 300 sec: 333.2). Total num frames: 6307840. Throughput: 0: 274.5. Samples: 6307808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:42:01,032][166323] Avg episode reward: [(0, '1203.527')]
[36m[2025-07-02 02:42:05,982][166323] Fps is (10 sec: 1633.6, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6307840. Throughput: 0: 273.8. Samples: 6308592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:42:05,983][166323] Avg episode reward: [(0, '1153.134')]
[36m[2025-07-02 02:42:10,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6307840. Throughput: 0: 271.6. Samples: 6310144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:42:10,960][166323] Avg episode reward: [(0, '1144.472')]
[36m[2025-07-02 02:42:15,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6307840. Throughput: 0: 269.4. Samples: 6311808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:42:15,968][166323] Avg episode reward: [(0, '1181.762')]
[31m[22127289 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[22127289 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[22127289 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:42:20,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6307840. Throughput: 0: 273.3. Samples: 6312800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:42:20,999][166323] Avg episode reward: [(0, '1193.320')]
[36m[2025-07-02 02:42:25,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6307840. Throughput: 0: 270.5. Samples: 6314320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:42:25,963][166323] Avg episode reward: [(0, '1196.203')]
[36m[2025-07-02 02:42:30,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6307840. Throughput: 0: 268.9. Samples: 6315968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:42:30,987][166323] Avg episode reward: [(0, '1187.195')]
[36m[2025-07-02 02:42:35,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6307840. Throughput: 0: 270.5. Samples: 6316816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:42:35,997][166323] Avg episode reward: [(0, '1200.785')]
[36m[2025-07-02 02:42:40,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6307840. Throughput: 0: 270.4. Samples: 6318416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:42:40,975][166323] Avg episode reward: [(0, '1231.069')]
[36m[2025-07-02 02:42:45,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6307840. Throughput: 0: 273.0. Samples: 6320080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:42:45,984][166323] Avg episode reward: [(0, '1212.665')]
[36m[2025-07-02 02:42:50,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6307840. Throughput: 0: 274.5. Samples: 6320944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:42:50,984][166323] Avg episode reward: [(0, '1240.931')]
[36m[2025-07-02 02:42:55,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.8). Total num frames: 6307840. Throughput: 0: 276.6. Samples: 6322592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:42:55,964][166323] Avg episode reward: [(0, '1232.745')]
[36m[2025-07-02 02:43:00,977][166323] Fps is (10 sec: 1639.7, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6324224. Throughput: 0: 274.8. Samples: 6324176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:43:00,977][166323] Avg episode reward: [(0, '1212.957')]
[36m[2025-07-02 02:43:05,955][166323] Fps is (10 sec: 1639.8, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6324224. Throughput: 0: 270.8. Samples: 6324976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:43:05,955][166323] Avg episode reward: [(0, '1252.315')]
[36m[2025-07-02 02:43:10,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6324224. Throughput: 0: 276.2. Samples: 6326752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:43:10,969][166323] Avg episode reward: [(0, '1218.988')]
[36m[2025-07-02 02:43:15,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6324224. Throughput: 0: 277.1. Samples: 6328432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:43:15,973][166323] Avg episode reward: [(0, '1233.465')]
[31m[22187414 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[22187415 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[22187416 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[22187753 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[22187753 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[22187754 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:43:20,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6324224. Throughput: 0: 278.4. Samples: 6329328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:43:20,944][166323] Avg episode reward: [(0, '1205.895')]
[36m[2025-07-02 02:43:26,008][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6324224. Throughput: 0: 278.9. Samples: 6330976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:43:26,008][166323] Avg episode reward: [(0, '1175.900')]
[36m[2025-07-02 02:43:30,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6324224. Throughput: 0: 277.9. Samples: 6332576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:43:30,948][166323] Avg episode reward: [(0, '1172.236')]
[36m[2025-07-02 02:43:35,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6324224. Throughput: 0: 276.8. Samples: 6333392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:43:35,956][166323] Avg episode reward: [(0, '1159.153')]
[37m[1m[2025-07-02 02:43:36,014][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000012344_6324224.pth...
[36m[2025-07-02 02:43:36,019][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000012216_6258688.pth
[36m[2025-07-02 02:43:40,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6324224. Throughput: 0: 276.3. Samples: 6335024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:43:40,964][166323] Avg episode reward: [(0, '1154.303')]
[36m[2025-07-02 02:43:45,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6324224. Throughput: 0: 278.1. Samples: 6336688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:43:45,963][166323] Avg episode reward: [(0, '1176.319')]
[36m[2025-07-02 02:43:50,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6324224. Throughput: 0: 282.0. Samples: 6337664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:43:50,944][166323] Avg episode reward: [(0, '1135.147')]
[36m[2025-07-02 02:43:55,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6324224. Throughput: 0: 280.7. Samples: 6339392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:43:55,997][166323] Avg episode reward: [(0, '1119.005')]
[36m[2025-07-02 02:44:00,990][166323] Fps is (10 sec: 1630.9, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6340608. Throughput: 0: 279.0. Samples: 6340992. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 02:44:00,990][166323] Avg episode reward: [(0, '1156.088')]
[36m[2025-07-02 02:44:05,976][166323] Fps is (10 sec: 1641.8, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6340608. Throughput: 0: 276.4. Samples: 6341776. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 02:44:05,976][166323] Avg episode reward: [(0, '1173.515')]
[36m[2025-07-02 02:44:10,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6340608. Throughput: 0: 276.2. Samples: 6343392. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 02:44:10,960][166323] Avg episode reward: [(0, '1155.796')]
[36m[2025-07-02 02:44:15,995][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6340608. Throughput: 0: 277.0. Samples: 6345056. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 02:44:15,995][166323] Avg episode reward: [(0, '1136.672')]
[36m[2025-07-02 02:44:20,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6340608. Throughput: 0: 278.1. Samples: 6345904. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 02:44:20,944][166323] Avg episode reward: [(0, '1191.296')]
[36m[2025-07-02 02:44:25,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6340608. Throughput: 0: 279.8. Samples: 6347616. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 02:44:25,968][166323] Avg episode reward: [(0, '1191.849')]
[36m[2025-07-02 02:44:30,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6340608. Throughput: 0: 277.7. Samples: 6349184. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 02:44:30,962][166323] Avg episode reward: [(0, '1216.711')]
[36m[2025-07-02 02:44:35,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6340608. Throughput: 0: 274.5. Samples: 6350016. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 02:44:35,947][166323] Avg episode reward: [(0, '1217.438')]
[36m[2025-07-02 02:44:40,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6340608. Throughput: 0: 273.7. Samples: 6351696. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 02:44:40,945][166323] Avg episode reward: [(0, '1202.943')]
[36m[2025-07-02 02:44:45,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6340608. Throughput: 0: 278.6. Samples: 6353520. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 02:44:45,956][166323] Avg episode reward: [(0, '1217.303')]
[36m[2025-07-02 02:44:50,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 6340608. Throughput: 0: 281.5. Samples: 6354448. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 02:44:50,999][166323] Avg episode reward: [(0, '1218.317')]
[36m[2025-07-02 02:44:55,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6340608. Throughput: 0: 283.7. Samples: 6356160. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 02:44:55,960][166323] Avg episode reward: [(0, '1226.248')]
[36m[2025-07-02 02:45:00,948][166323] Fps is (10 sec: 1646.8, 60 sec: 273.3, 300 sec: 277.8). Total num frames: 6356992. Throughput: 0: 285.1. Samples: 6357872. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 02:45:00,948][166323] Avg episode reward: [(0, '1248.120')]
[36m[2025-07-02 02:45:05,989][166323] Fps is (10 sec: 1633.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6356992. Throughput: 0: 285.2. Samples: 6358752. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 02:45:05,989][166323] Avg episode reward: [(0, '1244.112')]
[36m[2025-07-02 02:45:10,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6356992. Throughput: 0: 284.5. Samples: 6360416. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 02:45:10,954][166323] Avg episode reward: [(0, '1253.958')]
[36m[2025-07-02 02:45:15,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6356992. Throughput: 0: 285.8. Samples: 6362048. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 02:45:15,975][166323] Avg episode reward: [(0, '1299.743')]
[36m[2025-07-02 02:45:20,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6356992. Throughput: 0: 287.0. Samples: 6362928. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 02:45:20,943][166323] Avg episode reward: [(0, '1267.671')]
[31m[22311853 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[22311853 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[22311853 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:45:25,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6356992. Throughput: 0: 284.5. Samples: 6364512. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 02:45:25,989][166323] Avg episode reward: [(0, '1259.126')]
[36m[2025-07-02 02:45:30,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6356992. Throughput: 0: 284.6. Samples: 6366336. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 02:45:30,990][166323] Avg episode reward: [(0, '1289.631')]
[36m[2025-07-02 02:45:35,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6356992. Throughput: 0: 282.0. Samples: 6367136. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 02:45:35,989][166323] Avg episode reward: [(0, '1298.180')]
[37m[1m[2025-07-02 02:45:36,040][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000012408_6356992.pth...
[36m[2025-07-02 02:45:36,044][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000012280_6291456.pth
[36m[2025-07-02 02:45:40,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6356992. Throughput: 0: 283.9. Samples: 6368944. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 02:45:40,992][166323] Avg episode reward: [(0, '1293.715')]
[36m[2025-07-02 02:45:45,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6356992. Throughput: 0: 282.9. Samples: 6370608. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 02:45:45,968][166323] Avg episode reward: [(0, '1198.255')]
[36m[2025-07-02 02:45:51,000][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6356992. Throughput: 0: 284.4. Samples: 6371552. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 02:45:51,000][166323] Avg episode reward: [(0, '1175.730')]
[36m[2025-07-02 02:45:55,986][166323] Fps is (10 sec: 1635.5, 60 sec: 545.9, 300 sec: 277.7). Total num frames: 6373376. Throughput: 0: 284.6. Samples: 6373232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:45:55,986][166323] Avg episode reward: [(0, '1226.806')]
[31m[22344859 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[22344860 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[22344861 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:46:00,974][166323] Fps is (10 sec: 1642.7, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6373376. Throughput: 0: 284.1. Samples: 6374832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:46:00,974][166323] Avg episode reward: [(0, '1210.162')]
[36m[2025-07-02 02:46:05,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6373376. Throughput: 0: 283.6. Samples: 6375696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:46:05,967][166323] Avg episode reward: [(0, '1217.430')]
[36m[2025-07-02 02:46:10,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6373376. Throughput: 0: 284.1. Samples: 6377296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:46:10,995][166323] Avg episode reward: [(0, '1185.853')]
[36m[2025-07-02 02:46:15,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6373376. Throughput: 0: 277.0. Samples: 6378800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:46:15,988][166323] Avg episode reward: [(0, '1251.964')]
[36m[2025-07-02 02:46:20,976][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6373376. Throughput: 0: 277.8. Samples: 6379632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:46:20,977][166323] Avg episode reward: [(0, '1290.438')]
[36m[2025-07-02 02:46:25,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6373376. Throughput: 0: 275.2. Samples: 6381328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:46:25,988][166323] Avg episode reward: [(0, '1308.903')]
[36m[2025-07-02 02:46:30,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6373376. Throughput: 0: 281.3. Samples: 6383264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:46:30,958][166323] Avg episode reward: [(0, '1298.646')]
[36m[2025-07-02 02:46:35,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6373376. Throughput: 0: 279.7. Samples: 6384128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:46:35,964][166323] Avg episode reward: [(0, '1267.248')]
[31m[22386330 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[22386330 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[22386331 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:46:40,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6373376. Throughput: 0: 277.0. Samples: 6385696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:46:40,989][166323] Avg episode reward: [(0, '1232.003')]
[36m[2025-07-02 02:46:45,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6373376. Throughput: 0: 279.9. Samples: 6387424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:46:45,956][166323] Avg episode reward: [(0, '1253.792')]
[36m[2025-07-02 02:46:50,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6373376. Throughput: 0: 278.3. Samples: 6388224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:46:50,979][166323] Avg episode reward: [(0, '1242.639')]
[36m[2025-07-02 02:46:55,982][166323] Fps is (10 sec: 1634.1, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6389760. Throughput: 0: 281.0. Samples: 6389936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:46:55,983][166323] Avg episode reward: [(0, '1237.564')]
[36m[2025-07-02 02:47:00,959][166323] Fps is (10 sec: 1641.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6389760. Throughput: 0: 285.0. Samples: 6391616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:47:00,959][166323] Avg episode reward: [(0, '1261.957')]
[36m[2025-07-02 02:47:05,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6389760. Throughput: 0: 285.8. Samples: 6392496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:47:05,988][166323] Avg episode reward: [(0, '1272.003')]
[36m[2025-07-02 02:47:10,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6389760. Throughput: 0: 285.0. Samples: 6394144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:47:10,961][166323] Avg episode reward: [(0, '1213.551')]
[36m[2025-07-02 02:47:15,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6389760. Throughput: 0: 281.2. Samples: 6395920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:47:15,970][166323] Avg episode reward: [(0, '1230.298')]
[36m[2025-07-02 02:47:20,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6389760. Throughput: 0: 281.7. Samples: 6396800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:47:20,949][166323] Avg episode reward: [(0, '1251.709')]
[36m[2025-07-02 02:47:25,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6389760. Throughput: 0: 282.6. Samples: 6398400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:47:25,947][166323] Avg episode reward: [(0, '1207.406')]
[36m[2025-07-02 02:47:30,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6389760. Throughput: 0: 284.0. Samples: 6400208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:47:30,976][166323] Avg episode reward: [(0, '1245.858')]
[36m[2025-07-02 02:47:35,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6389760. Throughput: 0: 285.3. Samples: 6401056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:47:35,952][166323] Avg episode reward: [(0, '1231.258')]
[37m[1m[2025-07-02 02:47:36,038][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000012472_6389760.pth...
[36m[2025-07-02 02:47:36,045][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000012344_6324224.pth
[36m[2025-07-02 02:47:40,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6389760. Throughput: 0: 280.8. Samples: 6402576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:47:40,995][166323] Avg episode reward: [(0, '1198.734')]
[36m[2025-07-02 02:47:46,006][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 6389760. Throughput: 0: 277.4. Samples: 6404112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:47:46,006][166323] Avg episode reward: [(0, '1267.288')]
[36m[2025-07-02 02:47:50,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6389760. Throughput: 0: 276.0. Samples: 6404912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:47:50,969][166323] Avg episode reward: [(0, '1275.262')]
[36m[2025-07-02 02:47:55,975][166323] Fps is (10 sec: 1643.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6406144. Throughput: 0: 274.8. Samples: 6406512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:47:55,975][166323] Avg episode reward: [(0, '1268.044')]
[36m[2025-07-02 02:48:00,950][166323] Fps is (10 sec: 1641.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6406144. Throughput: 0: 267.1. Samples: 6407936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:48:00,950][166323] Avg episode reward: [(0, '1319.954')]
[36m[2025-07-02 02:48:05,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6406144. Throughput: 0: 267.0. Samples: 6408816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:48:05,950][166323] Avg episode reward: [(0, '1317.882')]
[36m[2025-07-02 02:48:11,003][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6406144. Throughput: 0: 268.1. Samples: 6410480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:48:11,004][166323] Avg episode reward: [(0, '1274.093')]
[36m[2025-07-02 02:48:15,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6406144. Throughput: 0: 265.3. Samples: 6412144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:48:15,962][166323] Avg episode reward: [(0, '1300.647')]
[36m[2025-07-02 02:48:20,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6406144. Throughput: 0: 264.3. Samples: 6412960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:48:20,990][166323] Avg episode reward: [(0, '1236.996')]
[36m[2025-07-02 02:48:25,975][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6406144. Throughput: 0: 268.9. Samples: 6414672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:48:25,975][166323] Avg episode reward: [(0, '1159.291')]
[36m[2025-07-02 02:48:30,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6406144. Throughput: 0: 270.2. Samples: 6416256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:48:30,950][166323] Avg episode reward: [(0, '1163.367')]
[36m[2025-07-02 02:48:35,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6406144. Throughput: 0: 271.1. Samples: 6417104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:48:35,948][166323] Avg episode reward: [(0, '1148.829')]
[36m[2025-07-02 02:48:40,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6406144. Throughput: 0: 279.4. Samples: 6419088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:48:40,983][166323] Avg episode reward: [(0, '1173.360')]
[36m[2025-07-02 02:48:45,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6406144. Throughput: 0: 285.3. Samples: 6420784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:48:45,984][166323] Avg episode reward: [(0, '1164.556')]
[36m[2025-07-02 02:48:50,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6406144. Throughput: 0: 283.1. Samples: 6421568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:48:50,987][166323] Avg episode reward: [(0, '1182.756')]
[36m[2025-07-02 02:48:55,953][166323] Fps is (10 sec: 1643.4, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6422528. Throughput: 0: 281.2. Samples: 6423120. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:48:55,953][166323] Avg episode reward: [(0, '1258.668')]
[31m[22524518 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[22524518 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[22524518 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:49:00,982][166323] Fps is (10 sec: 1639.1, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6422528. Throughput: 0: 281.5. Samples: 6424816. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:49:00,982][166323] Avg episode reward: [(0, '1178.419')]
[36m[2025-07-02 02:49:05,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6422528. Throughput: 0: 282.2. Samples: 6425648. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:49:05,945][166323] Avg episode reward: [(0, '1198.005')]
[36m[2025-07-02 02:49:10,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6422528. Throughput: 0: 279.4. Samples: 6427248. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:49:10,992][166323] Avg episode reward: [(0, '1228.719')]
[31m[22542622 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[22542622 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[22542622 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:49:15,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6422528. Throughput: 0: 281.2. Samples: 6428912. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:49:15,951][166323] Avg episode reward: [(0, '1236.077')]
[31m[22549191 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[22549191 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[22549191 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:49:20,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6422528. Throughput: 0: 280.7. Samples: 6429744. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:49:20,983][166323] Avg episode reward: [(0, '1206.602')]
[36m[2025-07-02 02:49:25,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6422528. Throughput: 0: 274.8. Samples: 6431456. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:49:25,993][166323] Avg episode reward: [(0, '1152.682')]
[36m[2025-07-02 02:49:30,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6422528. Throughput: 0: 276.0. Samples: 6433200. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:49:30,975][166323] Avg episode reward: [(0, '1172.841')]
[36m[2025-07-02 02:49:35,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6422528. Throughput: 0: 277.1. Samples: 6434032. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:49:35,971][166323] Avg episode reward: [(0, '1193.260')]
[37m[1m[2025-07-02 02:49:36,021][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000012536_6422528.pth...
[36m[2025-07-02 02:49:36,025][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000012408_6356992.pth
[31m[22564636 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[22564637 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[22564637 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:49:40,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6422528. Throughput: 0: 280.7. Samples: 6435760. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:49:40,978][166323] Avg episode reward: [(0, '1175.940')]
[36m[2025-07-02 02:49:45,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6422528. Throughput: 0: 280.9. Samples: 6437456. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 02:49:45,978][166323] Avg episode reward: [(0, '1181.404')]
[36m[2025-07-02 02:49:50,975][166323] Fps is (10 sec: 1638.9, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 6438912. Throughput: 0: 282.1. Samples: 6438352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:49:50,976][166323] Avg episode reward: [(0, '1214.167')]
[36m[2025-07-02 02:49:55,989][166323] Fps is (10 sec: 1636.6, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6438912. Throughput: 0: 286.2. Samples: 6440128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:49:55,989][166323] Avg episode reward: [(0, '1202.804')]
[36m[2025-07-02 02:50:00,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6438912. Throughput: 0: 285.7. Samples: 6441776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:50:00,980][166323] Avg episode reward: [(0, '1209.745')]
[36m[2025-07-02 02:50:06,013][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 6438912. Throughput: 0: 286.4. Samples: 6442640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:50:06,014][166323] Avg episode reward: [(0, '1186.024')]
[36m[2025-07-02 02:50:10,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6438912. Throughput: 0: 283.2. Samples: 6444192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:50:10,970][166323] Avg episode reward: [(0, '1264.854')]
[36m[2025-07-02 02:50:15,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6438912. Throughput: 0: 285.7. Samples: 6446048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:50:15,946][166323] Avg episode reward: [(0, '1251.453')]
[36m[2025-07-02 02:50:20,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6438912. Throughput: 0: 285.4. Samples: 6446880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:50:20,985][166323] Avg episode reward: [(0, '1214.781')]
[36m[2025-07-02 02:50:25,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6438912. Throughput: 0: 284.9. Samples: 6448576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:50:25,967][166323] Avg episode reward: [(0, '1243.524')]
[36m[2025-07-02 02:50:30,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6438912. Throughput: 0: 282.0. Samples: 6450144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:50:30,977][166323] Avg episode reward: [(0, '1276.509')]
[36m[2025-07-02 02:50:35,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6438912. Throughput: 0: 281.0. Samples: 6450992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:50:35,951][166323] Avg episode reward: [(0, '1262.427')]
[36m[2025-07-02 02:50:40,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6438912. Throughput: 0: 279.1. Samples: 6452688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:50:40,994][166323] Avg episode reward: [(0, '1311.378')]
[36m[2025-07-02 02:50:45,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6438912. Throughput: 0: 281.0. Samples: 6454416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:50:45,964][166323] Avg episode reward: [(0, '1296.741')]
[36m[2025-07-02 02:50:50,996][166323] Fps is (10 sec: 1638.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6455296. Throughput: 0: 282.8. Samples: 6455360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:50:50,996][166323] Avg episode reward: [(0, '1286.578')]
[31m[22642276 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[22642276 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[22642277 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:50:55,991][166323] Fps is (10 sec: 1633.9, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6455296. Throughput: 0: 287.2. Samples: 6457120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:50:55,992][166323] Avg episode reward: [(0, '1240.871')]
[36m[2025-07-02 02:51:00,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6455296. Throughput: 0: 285.9. Samples: 6458928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:51:00,999][166323] Avg episode reward: [(0, '1218.893')]
[36m[2025-07-02 02:51:05,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6455296. Throughput: 0: 287.8. Samples: 6459824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:51:05,953][166323] Avg episode reward: [(0, '1171.913')]
[36m[2025-07-02 02:51:10,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6455296. Throughput: 0: 287.8. Samples: 6461520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:51:10,945][166323] Avg episode reward: [(0, '1135.599')]
[36m[2025-07-02 02:51:15,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 6455296. Throughput: 0: 291.4. Samples: 6463264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:51:16,000][166323] Avg episode reward: [(0, '1128.679')]
[36m[2025-07-02 02:51:20,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6455296. Throughput: 0: 292.2. Samples: 6464144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:51:20,963][166323] Avg episode reward: [(0, '1106.162')]
[36m[2025-07-02 02:51:25,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6455296. Throughput: 0: 288.3. Samples: 6465648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:51:25,952][166323] Avg episode reward: [(0, '1177.439')]
[31m[22678597 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[22678597 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[22678597 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:51:30,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6455296. Throughput: 0: 289.1. Samples: 6467424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:51:30,953][166323] Avg episode reward: [(0, '1152.060')]
[36m[2025-07-02 02:51:35,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6455296. Throughput: 0: 288.9. Samples: 6468352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:51:35,964][166323] Avg episode reward: [(0, '1210.330')]
[37m[1m[2025-07-02 02:51:36,031][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000012600_6455296.pth...
[36m[2025-07-02 02:51:36,036][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000012472_6389760.pth
[33m[22685978 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[22685979 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.8701171875
[33mCrash Rate: 0.123046875
[33mTimeout Rate: 0.0068359375 (navigation_task.py:265)
[33m[22685979 ms][navigation_task] - WARNING : 
[33mSuccesses: 1782
[33mCrashes : 252
[33mTimeouts: 14 (navigation_task.py:268)
[36m[2025-07-02 02:51:40,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6455296. Throughput: 0: 286.7. Samples: 6470016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:51:40,973][166323] Avg episode reward: [(0, '1214.964')]
[36m[2025-07-02 02:51:45,972][166323] Fps is (10 sec: 1637.1, 60 sec: 546.1, 300 sec: 333.2). Total num frames: 6471680. Throughput: 0: 283.9. Samples: 6471696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:51:45,972][166323] Avg episode reward: [(0, '1230.449')]
[36m[2025-07-02 02:51:50,990][166323] Fps is (10 sec: 1635.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6471680. Throughput: 0: 278.9. Samples: 6472384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:51:50,990][166323] Avg episode reward: [(0, '1283.715')]
[36m[2025-07-02 02:51:55,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6471680. Throughput: 0: 281.9. Samples: 6474208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:51:55,954][166323] Avg episode reward: [(0, '1269.771')]
[36m[2025-07-02 02:52:00,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6471680. Throughput: 0: 279.3. Samples: 6475824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:52:00,970][166323] Avg episode reward: [(0, '1188.071')]
[36m[2025-07-02 02:52:05,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6471680. Throughput: 0: 278.7. Samples: 6476688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:52:05,968][166323] Avg episode reward: [(0, '1190.530')]
[36m[2025-07-02 02:52:10,971][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6471680. Throughput: 0: 280.4. Samples: 6478272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:52:10,971][166323] Avg episode reward: [(0, '1168.826')]
[36m[2025-07-02 02:52:15,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6471680. Throughput: 0: 279.3. Samples: 6480000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:52:15,988][166323] Avg episode reward: [(0, '1178.695')]
[36m[2025-07-02 02:52:20,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6471680. Throughput: 0: 279.5. Samples: 6480928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:52:20,962][166323] Avg episode reward: [(0, '1189.105')]
[36m[2025-07-02 02:52:25,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6471680. Throughput: 0: 277.0. Samples: 6482480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:52:25,967][166323] Avg episode reward: [(0, '1146.185')]
[36m[2025-07-02 02:52:30,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6471680. Throughput: 0: 276.6. Samples: 6484144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:52:30,968][166323] Avg episode reward: [(0, '1183.557')]
[36m[2025-07-02 02:52:35,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6471680. Throughput: 0: 278.5. Samples: 6484912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:52:35,971][166323] Avg episode reward: [(0, '1280.344')]
[36m[2025-07-02 02:52:40,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6471680. Throughput: 0: 274.0. Samples: 6486544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:52:40,974][166323] Avg episode reward: [(0, '1250.220')]
[36m[2025-07-02 02:52:45,968][166323] Fps is (10 sec: 1638.8, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 6488064. Throughput: 0: 277.0. Samples: 6488288. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 02:52:45,968][166323] Avg episode reward: [(0, '1273.620')]
[36m[2025-07-02 02:52:50,964][166323] Fps is (10 sec: 1640.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6488064. Throughput: 0: 277.4. Samples: 6489168. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 02:52:50,964][166323] Avg episode reward: [(0, '1286.826')]
[36m[2025-07-02 02:52:55,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6488064. Throughput: 0: 280.2. Samples: 6490880. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 02:52:55,965][166323] Avg episode reward: [(0, '1233.626')]
[36m[2025-07-02 02:53:00,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6488064. Throughput: 0: 279.3. Samples: 6492560. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 02:53:00,963][166323] Avg episode reward: [(0, '1236.171')]
[31m[22773315 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[22773315 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[22773316 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:53:05,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 6488064. Throughput: 0: 277.4. Samples: 6493408. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 02:53:05,944][166323] Avg episode reward: [(0, '1220.795')]
[36m[2025-07-02 02:53:10,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6488064. Throughput: 0: 282.1. Samples: 6495184. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 02:53:10,996][166323] Avg episode reward: [(0, '1211.705')]
[36m[2025-07-02 02:53:15,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6488064. Throughput: 0: 285.2. Samples: 6496976. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 02:53:15,961][166323] Avg episode reward: [(0, '1210.902')]
[36m[2025-07-02 02:53:20,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6488064. Throughput: 0: 284.3. Samples: 6497712. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 02:53:20,994][166323] Avg episode reward: [(0, '1200.271')]
[36m[2025-07-02 02:53:25,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6488064. Throughput: 0: 286.2. Samples: 6499424. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 02:53:25,981][166323] Avg episode reward: [(0, '1146.870')]
[36m[2025-07-02 02:53:30,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6488064. Throughput: 0: 284.4. Samples: 6501088. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 02:53:30,982][166323] Avg episode reward: [(0, '1124.130')]
[36m[2025-07-02 02:53:35,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6488064. Throughput: 0: 283.0. Samples: 6501904. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 02:53:35,967][166323] Avg episode reward: [(0, '1106.802')]
[37m[1m[2025-07-02 02:53:36,017][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000012664_6488064.pth...
[36m[2025-07-02 02:53:36,021][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000012536_6422528.pth
[31m[22808912 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[22808913 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[22808913 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:53:40,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6488064. Throughput: 0: 285.5. Samples: 6503728. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 02:53:40,960][166323] Avg episode reward: [(0, '1009.078')]
[31m[22814280 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[22814280 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[22814280 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:53:45,965][166323] Fps is (10 sec: 1638.6, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 6504448. Throughput: 0: 287.6. Samples: 6505504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:53:45,965][166323] Avg episode reward: [(0, '1058.152')]
[36m[2025-07-02 02:53:51,006][166323] Fps is (10 sec: 1630.9, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 6504448. Throughput: 0: 289.0. Samples: 6506432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:53:51,006][166323] Avg episode reward: [(0, '1073.956')]
[36m[2025-07-02 02:53:55,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6504448. Throughput: 0: 284.7. Samples: 6507984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:53:55,954][166323] Avg episode reward: [(0, '1076.495')]
[36m[2025-07-02 02:54:00,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6504448. Throughput: 0: 283.9. Samples: 6509760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:54:00,985][166323] Avg episode reward: [(0, '1109.663')]
[36m[2025-07-02 02:54:05,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6504448. Throughput: 0: 285.2. Samples: 6510544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:54:05,989][166323] Avg episode reward: [(0, '1150.630')]
[36m[2025-07-02 02:54:10,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6504448. Throughput: 0: 281.4. Samples: 6512080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:54:10,951][166323] Avg episode reward: [(0, '1153.610')]
[36m[2025-07-02 02:54:15,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6504448. Throughput: 0: 282.0. Samples: 6513776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:54:15,969][166323] Avg episode reward: [(0, '1161.500')]
[36m[2025-07-02 02:54:20,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6504448. Throughput: 0: 283.2. Samples: 6514640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:54:20,944][166323] Avg episode reward: [(0, '1176.830')]
[36m[2025-07-02 02:54:25,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6504448. Throughput: 0: 280.6. Samples: 6516352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:54:25,951][166323] Avg episode reward: [(0, '1150.451')]
[31m[22856547 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[22856547 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[22856548 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:54:30,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6504448. Throughput: 0: 278.3. Samples: 6518032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:54:30,980][166323] Avg episode reward: [(0, '1166.143')]
[36m[2025-07-02 02:54:35,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6504448. Throughput: 0: 277.6. Samples: 6518912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:54:35,964][166323] Avg episode reward: [(0, '1142.625')]
[36m[2025-07-02 02:54:40,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6504448. Throughput: 0: 277.4. Samples: 6520464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:54:40,946][166323] Avg episode reward: [(0, '1104.427')]
[36m[2025-07-02 02:54:45,970][166323] Fps is (10 sec: 1637.4, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6520832. Throughput: 0: 273.9. Samples: 6522080. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 02:54:45,970][166323] Avg episode reward: [(0, '1119.703')]
[36m[2025-07-02 02:54:50,976][166323] Fps is (10 sec: 1633.6, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6520832. Throughput: 0: 276.3. Samples: 6522976. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 02:54:50,976][166323] Avg episode reward: [(0, '1124.728')]
[36m[2025-07-02 02:54:55,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6520832. Throughput: 0: 282.7. Samples: 6524800. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 02:54:55,950][166323] Avg episode reward: [(0, '1152.393')]
[36m[2025-07-02 02:55:00,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6520832. Throughput: 0: 282.9. Samples: 6526512. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 02:55:00,981][166323] Avg episode reward: [(0, '1174.751')]
[36m[2025-07-02 02:55:05,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6520832. Throughput: 0: 283.9. Samples: 6527424. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 02:55:05,979][166323] Avg episode reward: [(0, '1174.224')]
[36m[2025-07-02 02:55:10,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6520832. Throughput: 0: 285.3. Samples: 6529200. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 02:55:10,984][166323] Avg episode reward: [(0, '1211.887')]
[36m[2025-07-02 02:55:15,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6520832. Throughput: 0: 289.4. Samples: 6531056. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 02:55:15,977][166323] Avg episode reward: [(0, '1194.306')]
[36m[2025-07-02 02:55:20,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6520832. Throughput: 0: 290.6. Samples: 6531984. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 02:55:20,950][166323] Avg episode reward: [(0, '1233.728')]
[36m[2025-07-02 02:55:25,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6520832. Throughput: 0: 294.3. Samples: 6533712. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 02:55:25,962][166323] Avg episode reward: [(0, '1227.507')]
[36m[2025-07-02 02:55:30,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6520832. Throughput: 0: 296.1. Samples: 6535408. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 02:55:30,983][166323] Avg episode reward: [(0, '1213.891')]
[36m[2025-07-02 02:55:35,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6520832. Throughput: 0: 295.0. Samples: 6536256. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 02:55:35,994][166323] Avg episode reward: [(0, '1268.795')]
[37m[1m[2025-07-02 02:55:36,065][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000012728_6520832.pth...
[36m[2025-07-02 02:55:36,069][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000012600_6455296.pth
[36m[2025-07-02 02:55:40,946][166323] Fps is (10 sec: 1644.4, 60 sec: 546.1, 300 sec: 333.3). Total num frames: 6537216. Throughput: 0: 288.4. Samples: 6537776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:55:40,947][166323] Avg episode reward: [(0, '1284.229')]
[36m[2025-07-02 02:55:45,999][166323] Fps is (10 sec: 1637.5, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6537216. Throughput: 0: 286.5. Samples: 6539408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:55:46,000][166323] Avg episode reward: [(0, '1201.294')]
[36m[2025-07-02 02:55:50,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6537216. Throughput: 0: 283.4. Samples: 6540176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:55:50,980][166323] Avg episode reward: [(0, '1241.575')]
[36m[2025-07-02 02:55:55,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6537216. Throughput: 0: 279.6. Samples: 6541776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:55:55,963][166323] Avg episode reward: [(0, '1214.997')]
[36m[2025-07-02 02:56:00,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6537216. Throughput: 0: 277.0. Samples: 6543520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:56:00,967][166323] Avg episode reward: [(0, '1194.110')]
[36m[2025-07-02 02:56:05,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6537216. Throughput: 0: 275.0. Samples: 6544368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:56:05,984][166323] Avg episode reward: [(0, '1182.326')]
[36m[2025-07-02 02:56:10,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6537216. Throughput: 0: 271.6. Samples: 6545936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:56:10,977][166323] Avg episode reward: [(0, '1131.292')]
[36m[2025-07-02 02:56:15,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6537216. Throughput: 0: 274.4. Samples: 6547744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:56:15,944][166323] Avg episode reward: [(0, '1120.714')]
[36m[2025-07-02 02:56:20,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6537216. Throughput: 0: 272.0. Samples: 6548496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:56:20,986][166323] Avg episode reward: [(0, '1192.174')]
[36m[2025-07-02 02:56:25,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6537216. Throughput: 0: 274.3. Samples: 6550128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:56:25,971][166323] Avg episode reward: [(0, '1166.501')]
[36m[2025-07-02 02:56:30,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6537216. Throughput: 0: 275.1. Samples: 6551776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:56:30,956][166323] Avg episode reward: [(0, '1176.047')]
[36m[2025-07-02 02:56:35,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6537216. Throughput: 0: 276.4. Samples: 6552608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:56:35,953][166323] Avg episode reward: [(0, '1182.598')]
[36m[2025-07-02 02:56:40,944][166323] Fps is (10 sec: 1640.3, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6553600. Throughput: 0: 275.7. Samples: 6554176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:56:40,944][166323] Avg episode reward: [(0, '1202.185')]
[36m[2025-07-02 02:56:45,971][166323] Fps is (10 sec: 1635.5, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6553600. Throughput: 0: 272.0. Samples: 6555760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:56:45,971][166323] Avg episode reward: [(0, '1192.046')]
[36m[2025-07-02 02:56:50,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6553600. Throughput: 0: 271.8. Samples: 6556592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:56:50,956][166323] Avg episode reward: [(0, '1184.242')]
[36m[2025-07-02 02:56:55,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6553600. Throughput: 0: 273.4. Samples: 6558240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:56:55,973][166323] Avg episode reward: [(0, '1186.562')]
[36m[2025-07-02 02:57:00,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6553600. Throughput: 0: 268.2. Samples: 6559824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:57:00,986][166323] Avg episode reward: [(0, '1201.373')]
[36m[2025-07-02 02:57:05,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6553600. Throughput: 0: 269.2. Samples: 6560608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:57:05,977][166323] Avg episode reward: [(0, '1144.367')]
[36m[2025-07-02 02:57:10,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6553600. Throughput: 0: 271.7. Samples: 6562352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:57:10,969][166323] Avg episode reward: [(0, '1111.975')]
[36m[2025-07-02 02:57:15,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6553600. Throughput: 0: 272.1. Samples: 6564016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:57:15,946][166323] Avg episode reward: [(0, '1110.513')]
[36m[2025-07-02 02:57:20,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6553600. Throughput: 0: 271.3. Samples: 6564816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:57:20,943][166323] Avg episode reward: [(0, '1155.258')]
[36m[2025-07-02 02:57:25,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6553600. Throughput: 0: 271.6. Samples: 6566400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:57:25,946][166323] Avg episode reward: [(0, '1157.789')]
[36m[2025-07-02 02:57:31,007][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 6553600. Throughput: 0: 273.2. Samples: 6568064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:57:31,008][166323] Avg episode reward: [(0, '1117.782')]
[36m[2025-07-02 02:57:35,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6553600. Throughput: 0: 273.7. Samples: 6568912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 02:57:35,970][166323] Avg episode reward: [(0, '1151.612')]
[37m[1m[2025-07-02 02:57:36,029][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000012792_6553600.pth...
[36m[2025-07-02 02:57:36,033][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000012664_6488064.pth
[36m[2025-07-02 02:57:40,967][166323] Fps is (10 sec: 1645.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6569984. Throughput: 0: 273.1. Samples: 6570528. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 02:57:40,967][166323] Avg episode reward: [(0, '1206.055')]
[31m[23050836 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[23050836 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[23050836 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:57:46,001][166323] Fps is (10 sec: 1633.2, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6569984. Throughput: 0: 275.8. Samples: 6572240. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 02:57:46,001][166323] Avg episode reward: [(0, '1274.632')]
[36m[2025-07-02 02:57:50,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6569984. Throughput: 0: 277.4. Samples: 6573088. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 02:57:50,967][166323] Avg episode reward: [(0, '1290.295')]
[36m[2025-07-02 02:57:55,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6569984. Throughput: 0: 275.8. Samples: 6574768. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 02:57:55,989][166323] Avg episode reward: [(0, '1284.919')]
[36m[2025-07-02 02:58:00,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6569984. Throughput: 0: 276.0. Samples: 6576448. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 02:58:00,985][166323] Avg episode reward: [(0, '1285.404')]
[36m[2025-07-02 02:58:05,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6569984. Throughput: 0: 279.3. Samples: 6577392. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 02:58:05,975][166323] Avg episode reward: [(0, '1346.082')]
[37m[1m[2025-07-02 02:58:06,026][166323] Saving new best policy, reward=1346.082!
[36m[2025-07-02 02:58:11,022][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 6569984. Throughput: 0: 279.3. Samples: 6578992. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 02:58:11,022][166323] Avg episode reward: [(0, '1336.536')]
[36m[2025-07-02 02:58:15,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6569984. Throughput: 0: 283.0. Samples: 6580784. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 02:58:15,950][166323] Avg episode reward: [(0, '1266.199')]
[36m[2025-07-02 02:58:20,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6569984. Throughput: 0: 281.4. Samples: 6581568. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 02:58:20,951][166323] Avg episode reward: [(0, '1249.487')]
[36m[2025-07-02 02:58:25,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6569984. Throughput: 0: 282.8. Samples: 6583248. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 02:58:25,944][166323] Avg episode reward: [(0, '1225.134')]
[36m[2025-07-02 02:58:30,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6569984. Throughput: 0: 280.8. Samples: 6584864. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 02:58:30,961][166323] Avg episode reward: [(0, '1175.849')]
[36m[2025-07-02 02:58:35,999][166323] Fps is (10 sec: 1629.4, 60 sec: 545.9, 300 sec: 333.2). Total num frames: 6586368. Throughput: 0: 279.6. Samples: 6585680. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 02:58:35,999][166323] Avg episode reward: [(0, '1168.530')]
[36m[2025-07-02 02:58:40,997][166323] Fps is (10 sec: 1632.4, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6586368. Throughput: 0: 278.7. Samples: 6587312. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 02:58:40,997][166323] Avg episode reward: [(0, '1158.988')]
[36m[2025-07-02 02:58:45,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6586368. Throughput: 0: 277.1. Samples: 6588912. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 02:58:45,972][166323] Avg episode reward: [(0, '1231.613')]
[36m[2025-07-02 02:58:50,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6586368. Throughput: 0: 273.8. Samples: 6589712. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 02:58:50,973][166323] Avg episode reward: [(0, '1267.760')]
[36m[2025-07-02 02:58:55,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6586368. Throughput: 0: 277.4. Samples: 6591456. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 02:58:55,949][166323] Avg episode reward: [(0, '1242.881')]
[31m[23126728 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[23126728 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[23126728 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 02:59:00,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6586368. Throughput: 0: 272.3. Samples: 6593040. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 02:59:00,963][166323] Avg episode reward: [(0, '1241.998')]
[36m[2025-07-02 02:59:05,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6586368. Throughput: 0: 275.9. Samples: 6593984. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 02:59:05,957][166323] Avg episode reward: [(0, '1309.219')]
[36m[2025-07-02 02:59:10,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6586368. Throughput: 0: 274.0. Samples: 6595584. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 02:59:10,969][166323] Avg episode reward: [(0, '1312.461')]
[36m[2025-07-02 02:59:15,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6586368. Throughput: 0: 278.2. Samples: 6597392. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 02:59:15,992][166323] Avg episode reward: [(0, '1322.606')]
[36m[2025-07-02 02:59:21,015][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 6586368. Throughput: 0: 277.2. Samples: 6598160. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 02:59:21,015][166323] Avg episode reward: [(0, '1303.839')]
[36m[2025-07-02 02:59:25,976][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6586368. Throughput: 0: 279.6. Samples: 6599888. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 02:59:25,976][166323] Avg episode reward: [(0, '1320.156')]
[36m[2025-07-02 02:59:30,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6586368. Throughput: 0: 283.7. Samples: 6601680. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 02:59:30,984][166323] Avg episode reward: [(0, '1291.502')]
[36m[2025-07-02 02:59:35,989][166323] Fps is (10 sec: 1636.1, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 6602752. Throughput: 0: 285.0. Samples: 6602544. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 02:59:35,990][166323] Avg episode reward: [(0, '1265.554')]
[37m[1m[2025-07-02 02:59:36,060][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000012888_6602752.pth...
[36m[2025-07-02 02:59:36,065][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000012728_6520832.pth
[36m[2025-07-02 02:59:40,948][166323] Fps is (10 sec: 1644.3, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6602752. Throughput: 0: 281.6. Samples: 6604128. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 02:59:40,949][166323] Avg episode reward: [(0, '1218.902')]
[36m[2025-07-02 02:59:45,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6602752. Throughput: 0: 286.5. Samples: 6605936. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 02:59:45,981][166323] Avg episode reward: [(0, '1212.891')]
[36m[2025-07-02 02:59:50,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6602752. Throughput: 0: 284.3. Samples: 6606784. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 02:59:50,976][166323] Avg episode reward: [(0, '1162.247')]
[36m[2025-07-02 02:59:55,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6602752. Throughput: 0: 287.7. Samples: 6608528. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 02:59:55,960][166323] Avg episode reward: [(0, '1076.339')]
[36m[2025-07-02 03:00:00,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6602752. Throughput: 0: 287.7. Samples: 6610336. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 03:00:00,985][166323] Avg episode reward: [(0, '1108.092')]
[36m[2025-07-02 03:00:05,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6602752. Throughput: 0: 292.6. Samples: 6611312. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 03:00:05,966][166323] Avg episode reward: [(0, '1111.976')]
[36m[2025-07-02 03:00:10,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6602752. Throughput: 0: 291.0. Samples: 6612992. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 03:00:11,000][166323] Avg episode reward: [(0, '1095.214')]
[36m[2025-07-02 03:00:15,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6602752. Throughput: 0: 290.1. Samples: 6614736. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 03:00:15,985][166323] Avg episode reward: [(0, '1076.229')]
[31m[23204767 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[23204767 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[23204767 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[23204819 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[23204819 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[23204820 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:00:20,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6602752. Throughput: 0: 290.9. Samples: 6615632. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 03:00:20,981][166323] Avg episode reward: [(0, '1042.870')]
[36m[2025-07-02 03:00:25,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6602752. Throughput: 0: 291.7. Samples: 6617264. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 03:00:25,978][166323] Avg episode reward: [(0, '1133.350')]
[36m[2025-07-02 03:00:30,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6602752. Throughput: 0: 285.9. Samples: 6618800. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 03:00:30,982][166323] Avg episode reward: [(0, '1068.974')]
[36m[2025-07-02 03:00:36,000][166323] Fps is (10 sec: 1634.8, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 6619136. Throughput: 0: 283.9. Samples: 6619568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:00:36,000][166323] Avg episode reward: [(0, '1073.281')]
[36m[2025-07-02 03:00:40,998][166323] Fps is (10 sec: 1635.7, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 6619136. Throughput: 0: 282.4. Samples: 6621248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:00:40,999][166323] Avg episode reward: [(0, '1114.910')]
[36m[2025-07-02 03:00:45,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6619136. Throughput: 0: 280.9. Samples: 6622976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:00:45,989][166323] Avg episode reward: [(0, '1100.216')]
[36m[2025-07-02 03:00:50,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6619136. Throughput: 0: 279.2. Samples: 6623872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:00:50,955][166323] Avg episode reward: [(0, '1207.492')]
[36m[2025-07-02 03:00:55,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6619136. Throughput: 0: 278.8. Samples: 6625536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:00:55,994][166323] Avg episode reward: [(0, '1186.518')]
[36m[2025-07-02 03:01:00,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6619136. Throughput: 0: 276.4. Samples: 6627168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:01:00,971][166323] Avg episode reward: [(0, '1161.525')]
[36m[2025-07-02 03:01:05,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6619136. Throughput: 0: 276.2. Samples: 6628064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:01:05,994][166323] Avg episode reward: [(0, '1177.077')]
[36m[2025-07-02 03:01:10,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6619136. Throughput: 0: 278.7. Samples: 6629808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:01:10,983][166323] Avg episode reward: [(0, '1261.417')]
[36m[2025-07-02 03:01:15,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6619136. Throughput: 0: 282.4. Samples: 6631504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:01:15,975][166323] Avg episode reward: [(0, '1256.241')]
[36m[2025-07-02 03:01:20,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6619136. Throughput: 0: 286.6. Samples: 6632448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:01:20,946][166323] Avg episode reward: [(0, '1227.756')]
[36m[2025-07-02 03:01:25,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6619136. Throughput: 0: 284.0. Samples: 6634016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:01:25,949][166323] Avg episode reward: [(0, '1215.763')]
[36m[2025-07-02 03:01:30,952][166323] Fps is (10 sec: 1637.2, 60 sec: 546.4, 300 sec: 333.2). Total num frames: 6635520. Throughput: 0: 282.2. Samples: 6635664. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 03:01:30,953][166323] Avg episode reward: [(0, '1191.405')]
[36m[2025-07-02 03:01:35,987][166323] Fps is (10 sec: 1632.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6635520. Throughput: 0: 279.3. Samples: 6636448. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 03:01:35,987][166323] Avg episode reward: [(0, '1192.995')]
[37m[1m[2025-07-02 03:01:36,044][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000012952_6635520.pth...
[36m[2025-07-02 03:01:36,052][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000012792_6553600.pth
[36m[2025-07-02 03:01:41,000][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6635520. Throughput: 0: 279.1. Samples: 6638096. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 03:01:41,001][166323] Avg episode reward: [(0, '1246.560')]
[36m[2025-07-02 03:01:45,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6635520. Throughput: 0: 279.1. Samples: 6639728. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 03:01:45,978][166323] Avg episode reward: [(0, '1214.634')]
[36m[2025-07-02 03:01:50,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6635520. Throughput: 0: 281.2. Samples: 6640704. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 03:01:50,951][166323] Avg episode reward: [(0, '1244.471')]
[36m[2025-07-02 03:01:55,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6635520. Throughput: 0: 280.9. Samples: 6642448. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 03:01:55,976][166323] Avg episode reward: [(0, '1232.876')]
[36m[2025-07-02 03:02:00,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6635520. Throughput: 0: 281.8. Samples: 6644192. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 03:02:00,992][166323] Avg episode reward: [(0, '1279.616')]
[36m[2025-07-02 03:02:05,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6635520. Throughput: 0: 277.0. Samples: 6644912. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 03:02:05,946][166323] Avg episode reward: [(0, '1248.204')]
[36m[2025-07-02 03:02:10,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 6635520. Throughput: 0: 278.1. Samples: 6646544. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 03:02:10,999][166323] Avg episode reward: [(0, '1223.880')]
[36m[2025-07-02 03:02:15,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6635520. Throughput: 0: 279.8. Samples: 6648256. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 03:02:15,953][166323] Avg episode reward: [(0, '1210.319')]
[36m[2025-07-02 03:02:20,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6635520. Throughput: 0: 281.5. Samples: 6649104. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 03:02:20,950][166323] Avg episode reward: [(0, '1215.730')]
[36m[2025-07-02 03:02:25,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 6635520. Throughput: 0: 282.7. Samples: 6650800. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 03:02:25,945][166323] Avg episode reward: [(0, '1198.636')]
[36m[2025-07-02 03:02:30,957][166323] Fps is (10 sec: 1637.2, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 6651904. Throughput: 0: 280.7. Samples: 6652352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:02:30,957][166323] Avg episode reward: [(0, '1187.370')]
[36m[2025-07-02 03:02:35,987][166323] Fps is (10 sec: 1631.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6651904. Throughput: 0: 276.8. Samples: 6653168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:02:35,987][166323] Avg episode reward: [(0, '1150.221')]
[36m[2025-07-02 03:02:41,022][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6651904. Throughput: 0: 272.1. Samples: 6654704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:02:41,022][166323] Avg episode reward: [(0, '1193.281')]
[31m[23353420 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[23353421 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[23353421 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:02:45,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6651904. Throughput: 0: 266.3. Samples: 6656176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:02:45,998][166323] Avg episode reward: [(0, '1174.987')]
[33m[23359497 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[23359498 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.8515625
[33mCrash Rate: 0.13623046875
[33mTimeout Rate: 0.01220703125 (navigation_task.py:265)
[33m[23359498 ms][navigation_task] - WARNING : 
[33mSuccesses: 1744
[33mCrashes : 279
[33mTimeouts: 25 (navigation_task.py:268)
[36m[2025-07-02 03:02:50,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6651904. Throughput: 0: 267.1. Samples: 6656944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:02:50,987][166323] Avg episode reward: [(0, '1136.100')]
[36m[2025-07-02 03:02:55,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6651904. Throughput: 0: 270.0. Samples: 6658688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:02:55,985][166323] Avg episode reward: [(0, '1144.770')]
[36m[2025-07-02 03:03:00,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6651904. Throughput: 0: 268.0. Samples: 6660320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:03:00,975][166323] Avg episode reward: [(0, '1152.116')]
[36m[2025-07-02 03:03:05,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 6651904. Throughput: 0: 269.2. Samples: 6661216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:03:05,946][166323] Avg episode reward: [(0, '1202.062')]
[36m[2025-07-02 03:03:10,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6651904. Throughput: 0: 269.6. Samples: 6662944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:03:10,988][166323] Avg episode reward: [(0, '1176.329')]
[36m[2025-07-02 03:03:15,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6651904. Throughput: 0: 271.9. Samples: 6664592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:03:15,975][166323] Avg episode reward: [(0, '1193.730')]
[36m[2025-07-02 03:03:20,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6651904. Throughput: 0: 274.3. Samples: 6665504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:03:20,953][166323] Avg episode reward: [(0, '1158.011')]
[36m[2025-07-02 03:03:25,980][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6651904. Throughput: 0: 278.3. Samples: 6667216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:03:25,980][166323] Avg episode reward: [(0, '1190.353')]
[36m[2025-07-02 03:03:30,943][166323] Fps is (10 sec: 1640.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6668288. Throughput: 0: 283.4. Samples: 6668912. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:03:30,943][166323] Avg episode reward: [(0, '1197.646')]
[36m[2025-07-02 03:03:35,950][166323] Fps is (10 sec: 1643.3, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6668288. Throughput: 0: 284.7. Samples: 6669744. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:03:35,950][166323] Avg episode reward: [(0, '1255.951')]
[37m[1m[2025-07-02 03:03:36,008][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013016_6668288.pth...
[36m[2025-07-02 03:03:36,012][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000012888_6602752.pth
[36m[2025-07-02 03:03:40,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 6668288. Throughput: 0: 283.9. Samples: 6671456. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:03:40,959][166323] Avg episode reward: [(0, '1247.782')]
[36m[2025-07-02 03:03:45,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6668288. Throughput: 0: 285.9. Samples: 6673184. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:03:45,977][166323] Avg episode reward: [(0, '1274.513')]
[36m[2025-07-02 03:03:50,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 6668288. Throughput: 0: 283.8. Samples: 6674000. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:03:50,999][166323] Avg episode reward: [(0, '1296.738')]
[36m[2025-07-02 03:03:55,995][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6668288. Throughput: 0: 283.3. Samples: 6675696. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:03:55,996][166323] Avg episode reward: [(0, '1310.243')]
[36m[2025-07-02 03:04:00,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6668288. Throughput: 0: 283.6. Samples: 6677360. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:04:00,995][166323] Avg episode reward: [(0, '1255.911')]
[36m[2025-07-02 03:04:05,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6668288. Throughput: 0: 279.9. Samples: 6678096. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:04:05,947][166323] Avg episode reward: [(0, '1211.616')]
[36m[2025-07-02 03:04:10,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6668288. Throughput: 0: 278.6. Samples: 6679744. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:04:10,954][166323] Avg episode reward: [(0, '1182.493')]
[36m[2025-07-02 03:04:16,004][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6668288. Throughput: 0: 274.1. Samples: 6681264. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:04:16,004][166323] Avg episode reward: [(0, '1208.261')]
[36m[2025-07-02 03:04:20,998][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6668288. Throughput: 0: 272.1. Samples: 6682000. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:04:20,998][166323] Avg episode reward: [(0, '1230.439')]
[36m[2025-07-02 03:04:25,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6668288. Throughput: 0: 268.1. Samples: 6683520. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:04:25,964][166323] Avg episode reward: [(0, '1156.496')]
[36m[2025-07-02 03:04:30,984][166323] Fps is (10 sec: 1640.6, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6684672. Throughput: 0: 267.0. Samples: 6685200. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 03:04:30,985][166323] Avg episode reward: [(0, '1193.129')]
[36m[2025-07-02 03:04:35,950][166323] Fps is (10 sec: 1640.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6684672. Throughput: 0: 266.2. Samples: 6685968. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 03:04:35,950][166323] Avg episode reward: [(0, '1224.062')]
[36m[2025-07-02 03:04:40,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6684672. Throughput: 0: 265.2. Samples: 6687616. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 03:04:40,943][166323] Avg episode reward: [(0, '1254.866')]
[36m[2025-07-02 03:04:45,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6684672. Throughput: 0: 266.0. Samples: 6689328. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 03:04:45,983][166323] Avg episode reward: [(0, '1274.045')]
[36m[2025-07-02 03:04:50,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6684672. Throughput: 0: 267.6. Samples: 6690144. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 03:04:50,975][166323] Avg episode reward: [(0, '1269.646')]
[36m[2025-07-02 03:04:55,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6684672. Throughput: 0: 269.9. Samples: 6691888. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 03:04:55,947][166323] Avg episode reward: [(0, '1237.827')]
[36m[2025-07-02 03:05:00,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6684672. Throughput: 0: 274.6. Samples: 6693616. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 03:05:00,980][166323] Avg episode reward: [(0, '1322.310')]
[36m[2025-07-02 03:05:05,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6684672. Throughput: 0: 279.1. Samples: 6694544. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 03:05:05,947][166323] Avg episode reward: [(0, '1331.608')]
[36m[2025-07-02 03:05:10,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6684672. Throughput: 0: 286.2. Samples: 6696400. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 03:05:10,960][166323] Avg episode reward: [(0, '1328.287')]
[36m[2025-07-02 03:05:15,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6684672. Throughput: 0: 287.0. Samples: 6698112. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 03:05:15,975][166323] Avg episode reward: [(0, '1278.981')]
[36m[2025-07-02 03:05:20,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6684672. Throughput: 0: 289.1. Samples: 6698976. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 03:05:20,947][166323] Avg episode reward: [(0, '1245.025')]
[36m[2025-07-02 03:05:26,106][166323] Fps is (10 sec: 1617.3, 60 sec: 544.8, 300 sec: 333.1). Total num frames: 6701056. Throughput: 0: 290.2. Samples: 6700720. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 03:05:26,106][166323] Avg episode reward: [(0, '1285.035')]
[36m[2025-07-02 03:05:30,981][166323] Fps is (10 sec: 1632.9, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6701056. Throughput: 0: 290.5. Samples: 6702400. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 03:05:30,981][166323] Avg episode reward: [(0, '1277.526')]
[36m[2025-07-02 03:05:35,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6701056. Throughput: 0: 290.6. Samples: 6703216. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 03:05:35,952][166323] Avg episode reward: [(0, '1274.593')]
[37m[1m[2025-07-02 03:05:36,018][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013080_6701056.pth...
[36m[2025-07-02 03:05:36,025][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000012952_6635520.pth
[36m[2025-07-02 03:05:40,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 6701056. Throughput: 0: 287.7. Samples: 6704848. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 03:05:40,992][166323] Avg episode reward: [(0, '1263.571')]
[36m[2025-07-02 03:05:45,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6701056. Throughput: 0: 288.4. Samples: 6706592. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 03:05:45,967][166323] Avg episode reward: [(0, '1276.406')]
[36m[2025-07-02 03:05:50,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6701056. Throughput: 0: 283.1. Samples: 6707296. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 03:05:50,990][166323] Avg episode reward: [(0, '1297.354')]
[36m[2025-07-02 03:05:55,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6701056. Throughput: 0: 281.4. Samples: 6709072. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 03:05:55,985][166323] Avg episode reward: [(0, '1318.203')]
[36m[2025-07-02 03:06:00,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6701056. Throughput: 0: 277.8. Samples: 6710608. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 03:06:00,963][166323] Avg episode reward: [(0, '1334.448')]
[36m[2025-07-02 03:06:05,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6701056. Throughput: 0: 279.2. Samples: 6711552. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 03:06:05,994][166323] Avg episode reward: [(0, '1276.695')]
[36m[2025-07-02 03:06:10,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6701056. Throughput: 0: 280.7. Samples: 6713312. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 03:06:10,970][166323] Avg episode reward: [(0, '1228.522')]
[36m[2025-07-02 03:06:15,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6701056. Throughput: 0: 279.2. Samples: 6714960. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 03:06:15,969][166323] Avg episode reward: [(0, '1206.942')]
[36m[2025-07-02 03:06:20,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6701056. Throughput: 0: 283.0. Samples: 6715952. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 03:06:20,960][166323] Avg episode reward: [(0, '1186.531')]
[36m[2025-07-02 03:06:25,953][166323] Fps is (10 sec: 1641.0, 60 sec: 273.8, 300 sec: 277.7). Total num frames: 6717440. Throughput: 0: 284.7. Samples: 6717648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:06:25,953][166323] Avg episode reward: [(0, '1134.064')]
[36m[2025-07-02 03:06:30,989][166323] Fps is (10 sec: 1633.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6717440. Throughput: 0: 286.1. Samples: 6719472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:06:30,989][166323] Avg episode reward: [(0, '1125.483')]
[36m[2025-07-02 03:06:35,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6717440. Throughput: 0: 289.3. Samples: 6720304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:06:35,958][166323] Avg episode reward: [(0, '1189.305')]
[31m[23589417 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[23589417 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[23589417 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:06:40,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6717440. Throughput: 0: 287.8. Samples: 6722016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:06:40,957][166323] Avg episode reward: [(0, '1171.026')]
[36m[2025-07-02 03:06:45,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6717440. Throughput: 0: 290.9. Samples: 6723696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:06:45,955][166323] Avg episode reward: [(0, '1175.619')]
[36m[2025-07-02 03:06:50,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6717440. Throughput: 0: 287.5. Samples: 6724480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:06:50,957][166323] Avg episode reward: [(0, '1259.106')]
[36m[2025-07-02 03:06:55,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6717440. Throughput: 0: 286.2. Samples: 6726192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:06:55,974][166323] Avg episode reward: [(0, '1238.290')]
[36m[2025-07-02 03:07:01,004][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 6717440. Throughput: 0: 286.0. Samples: 6727840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:07:01,004][166323] Avg episode reward: [(0, '1215.850')]
[36m[2025-07-02 03:07:06,023][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6717440. Throughput: 0: 280.8. Samples: 6728608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:07:06,024][166323] Avg episode reward: [(0, '1210.267')]
[36m[2025-07-02 03:07:10,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6717440. Throughput: 0: 280.7. Samples: 6730288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:07:10,979][166323] Avg episode reward: [(0, '1164.210')]
[36m[2025-07-02 03:07:15,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6717440. Throughput: 0: 274.3. Samples: 6731808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:07:15,956][166323] Avg episode reward: [(0, '1185.796')]
[36m[2025-07-02 03:07:20,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6717440. Throughput: 0: 275.1. Samples: 6732688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:07:20,977][166323] Avg episode reward: [(0, '1188.265')]
[36m[2025-07-02 03:07:25,997][166323] Fps is (10 sec: 1631.7, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6733824. Throughput: 0: 272.8. Samples: 6734304. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 03:07:25,998][166323] Avg episode reward: [(0, '1143.956')]
[31m[23638836 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[23638837 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[23638837 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:07:31,007][166323] Fps is (10 sec: 1633.4, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6733824. Throughput: 0: 267.8. Samples: 6735760. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 03:07:31,008][166323] Avg episode reward: [(0, '1216.299')]
[36m[2025-07-02 03:07:35,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6733824. Throughput: 0: 268.2. Samples: 6736560. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 03:07:35,995][166323] Avg episode reward: [(0, '1116.935')]
[37m[1m[2025-07-02 03:07:36,082][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013144_6733824.pth...
[36m[2025-07-02 03:07:36,087][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013016_6668288.pth
[36m[2025-07-02 03:07:40,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6733824. Throughput: 0: 266.6. Samples: 6738192. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 03:07:40,989][166323] Avg episode reward: [(0, '1137.730')]
[31m[23652317 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[23652317 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[23652318 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:07:45,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6733824. Throughput: 0: 268.3. Samples: 6739904. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 03:07:45,965][166323] Avg episode reward: [(0, '1204.873')]
[36m[2025-07-02 03:07:50,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6733824. Throughput: 0: 268.7. Samples: 6740688. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 03:07:50,981][166323] Avg episode reward: [(0, '1121.015')]
[31m[23662055 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[23662056 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[23662056 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:07:55,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6733824. Throughput: 0: 267.5. Samples: 6742320. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 03:07:55,953][166323] Avg episode reward: [(0, '1099.367')]
[36m[2025-07-02 03:08:00,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6733824. Throughput: 0: 273.5. Samples: 6744112. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 03:08:00,951][166323] Avg episode reward: [(0, '1120.175')]
[36m[2025-07-02 03:08:05,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6733824. Throughput: 0: 273.4. Samples: 6744992. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 03:08:05,977][166323] Avg episode reward: [(0, '1160.956')]
[36m[2025-07-02 03:08:10,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6733824. Throughput: 0: 274.2. Samples: 6746640. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 03:08:10,990][166323] Avg episode reward: [(0, '1219.200')]
[36m[2025-07-02 03:08:15,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6733824. Throughput: 0: 277.3. Samples: 6748224. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 03:08:15,959][166323] Avg episode reward: [(0, '1181.682')]
[36m[2025-07-02 03:08:20,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6733824. Throughput: 0: 277.6. Samples: 6749040. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 03:08:20,947][166323] Avg episode reward: [(0, '1241.808')]
[36m[2025-07-02 03:08:25,996][166323] Fps is (10 sec: 1632.2, 60 sec: 273.1, 300 sec: 277.6). Total num frames: 6750208. Throughput: 0: 276.6. Samples: 6750640. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 03:08:25,997][166323] Avg episode reward: [(0, '1322.989')]
[36m[2025-07-02 03:08:30,966][166323] Fps is (10 sec: 1635.2, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6750208. Throughput: 0: 272.3. Samples: 6752160. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 03:08:30,966][166323] Avg episode reward: [(0, '1318.331')]
[36m[2025-07-02 03:08:35,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6750208. Throughput: 0: 274.6. Samples: 6753040. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 03:08:35,966][166323] Avg episode reward: [(0, '1307.300')]
[36m[2025-07-02 03:08:41,019][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6750208. Throughput: 0: 273.7. Samples: 6754656. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 03:08:41,019][166323] Avg episode reward: [(0, '1336.038')]
[36m[2025-07-02 03:08:45,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6750208. Throughput: 0: 268.9. Samples: 6756224. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 03:08:45,993][166323] Avg episode reward: [(0, '1352.932')]
[31m[23714562 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[23714563 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[23714563 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[37m[1m[2025-07-02 03:08:46,055][166323] Saving new best policy, reward=1352.932!
[36m[2025-07-02 03:08:50,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6750208. Throughput: 0: 267.2. Samples: 6757008. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 03:08:50,950][166323] Avg episode reward: [(0, '1306.372')]
[36m[2025-07-02 03:08:55,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6750208. Throughput: 0: 268.7. Samples: 6758720. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 03:08:55,945][166323] Avg episode reward: [(0, '1295.251')]
[36m[2025-07-02 03:09:00,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6750208. Throughput: 0: 274.0. Samples: 6760560. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 03:09:00,987][166323] Avg episode reward: [(0, '1302.432')]
[36m[2025-07-02 03:09:05,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6750208. Throughput: 0: 275.5. Samples: 6761440. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 03:09:05,961][166323] Avg episode reward: [(0, '1344.426')]
[36m[2025-07-02 03:09:10,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6750208. Throughput: 0: 276.6. Samples: 6763088. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 03:09:10,993][166323] Avg episode reward: [(0, '1325.390')]
[36m[2025-07-02 03:09:15,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6750208. Throughput: 0: 279.0. Samples: 6764720. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 03:09:15,979][166323] Avg episode reward: [(0, '1320.025')]
[36m[2025-07-02 03:09:20,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6750208. Throughput: 0: 278.8. Samples: 6765584. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 03:09:20,955][166323] Avg episode reward: [(0, '1359.961')]
[37m[1m[2025-07-02 03:09:21,003][166323] Saving new best policy, reward=1359.961!
[36m[2025-07-02 03:09:25,985][166323] Fps is (10 sec: 1637.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6766592. Throughput: 0: 282.2. Samples: 6767344. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:09:25,986][166323] Avg episode reward: [(0, '1330.598')]
[36m[2025-07-02 03:09:30,974][166323] Fps is (10 sec: 1635.3, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6766592. Throughput: 0: 286.7. Samples: 6769120. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:09:30,974][166323] Avg episode reward: [(0, '1256.641')]
[36m[2025-07-02 03:09:36,009][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 6766592. Throughput: 0: 288.7. Samples: 6770016. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:09:36,010][166323] Avg episode reward: [(0, '1220.422')]
[37m[1m[2025-07-02 03:09:36,090][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013208_6766592.pth...
[36m[2025-07-02 03:09:36,094][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013080_6701056.pth
[36m[2025-07-02 03:09:40,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6766592. Throughput: 0: 287.9. Samples: 6771680. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:09:40,959][166323] Avg episode reward: [(0, '1194.139')]
[36m[2025-07-02 03:09:45,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6766592. Throughput: 0: 283.0. Samples: 6773296. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:09:45,999][166323] Avg episode reward: [(0, '1151.452')]
[36m[2025-07-02 03:09:50,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6766592. Throughput: 0: 281.2. Samples: 6774096. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:09:50,970][166323] Avg episode reward: [(0, '1146.241')]
[31m[23783981 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[23783981 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[23783981 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:09:55,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6766592. Throughput: 0: 278.6. Samples: 6775616. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:09:55,967][166323] Avg episode reward: [(0, '1123.704')]
[36m[2025-07-02 03:10:00,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6766592. Throughput: 0: 278.7. Samples: 6777264. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:10:00,983][166323] Avg episode reward: [(0, '1126.863')]
[36m[2025-07-02 03:10:06,003][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6766592. Throughput: 0: 274.9. Samples: 6777968. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:10:06,003][166323] Avg episode reward: [(0, '1129.199')]
[36m[2025-07-02 03:10:10,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6766592. Throughput: 0: 275.0. Samples: 6779712. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:10:10,959][166323] Avg episode reward: [(0, '1176.720')]
[36m[2025-07-02 03:10:15,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6766592. Throughput: 0: 270.0. Samples: 6781264. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:10:15,949][166323] Avg episode reward: [(0, '1122.519')]
[31m[23805673 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[23805673 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[23805673 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:10:20,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.3). Total num frames: 6766592. Throughput: 0: 268.4. Samples: 6782080. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:10:20,952][166323] Avg episode reward: [(0, '1054.734')]
[31m[23813311 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[23813311 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[23813312 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:10:25,991][166323] Fps is (10 sec: 1631.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6782976. Throughput: 0: 268.6. Samples: 6783776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:10:25,991][166323] Avg episode reward: [(0, '1046.495')]
[36m[2025-07-02 03:10:30,955][166323] Fps is (10 sec: 1637.9, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6782976. Throughput: 0: 266.2. Samples: 6785264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:10:30,955][166323] Avg episode reward: [(0, '1048.115')]
[31m[23819839 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[23819840 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[23819840 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[23822208 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[23822209 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[23822209 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:10:35,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6782976. Throughput: 0: 268.1. Samples: 6786160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:10:35,964][166323] Avg episode reward: [(0, '1075.344')]
[36m[2025-07-02 03:10:40,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6782976. Throughput: 0: 271.4. Samples: 6787824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:10:40,946][166323] Avg episode reward: [(0, '994.708')]
[36m[2025-07-02 03:10:45,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6782976. Throughput: 0: 269.5. Samples: 6789392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:10:45,995][166323] Avg episode reward: [(0, '1059.398')]
[36m[2025-07-02 03:10:50,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6782976. Throughput: 0: 274.1. Samples: 6790288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:10:50,959][166323] Avg episode reward: [(0, '1153.693')]
[36m[2025-07-02 03:10:55,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6782976. Throughput: 0: 271.9. Samples: 6791952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:10:55,976][166323] Avg episode reward: [(0, '1188.524')]
[36m[2025-07-02 03:11:00,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6782976. Throughput: 0: 275.6. Samples: 6793664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:11:00,949][166323] Avg episode reward: [(0, '1184.997')]
[36m[2025-07-02 03:11:05,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6782976. Throughput: 0: 274.9. Samples: 6794464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:11:05,998][166323] Avg episode reward: [(0, '1134.785')]
[36m[2025-07-02 03:11:10,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6782976. Throughput: 0: 275.1. Samples: 6796144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:11:10,945][166323] Avg episode reward: [(0, '1176.881')]
[36m[2025-07-02 03:11:15,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6782976. Throughput: 0: 278.6. Samples: 6797808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:11:15,972][166323] Avg episode reward: [(0, '1214.633')]
[36m[2025-07-02 03:11:20,963][166323] Fps is (10 sec: 1635.4, 60 sec: 546.0, 300 sec: 277.7). Total num frames: 6799360. Throughput: 0: 279.5. Samples: 6798736. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 03:11:20,963][166323] Avg episode reward: [(0, '1224.154')]
[36m[2025-07-02 03:11:25,992][166323] Fps is (10 sec: 1635.1, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6799360. Throughput: 0: 279.9. Samples: 6800432. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 03:11:25,992][166323] Avg episode reward: [(0, '1236.728')]
[36m[2025-07-02 03:11:30,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6799360. Throughput: 0: 281.5. Samples: 6802064. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 03:11:30,999][166323] Avg episode reward: [(0, '1231.451')]
[36m[2025-07-02 03:11:35,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6799360. Throughput: 0: 280.9. Samples: 6802928. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 03:11:35,963][166323] Avg episode reward: [(0, '1251.363')]
[37m[1m[2025-07-02 03:11:36,052][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013272_6799360.pth...
[36m[2025-07-02 03:11:36,059][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013144_6733824.pth
[31m[23886395 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[23886395 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[23886395 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:11:40,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6799360. Throughput: 0: 279.8. Samples: 6804544. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 03:11:40,991][166323] Avg episode reward: [(0, '1333.047')]
[36m[2025-07-02 03:11:45,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6799360. Throughput: 0: 280.4. Samples: 6806288. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 03:11:45,967][166323] Avg episode reward: [(0, '1344.253')]
[36m[2025-07-02 03:11:50,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6799360. Throughput: 0: 280.7. Samples: 6807088. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 03:11:50,967][166323] Avg episode reward: [(0, '1291.861')]
[31m[23903473 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[23903473 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[23903474 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:11:55,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6799360. Throughput: 0: 278.5. Samples: 6808688. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 03:11:55,986][166323] Avg episode reward: [(0, '1230.940')]
[36m[2025-07-02 03:12:00,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6799360. Throughput: 0: 274.0. Samples: 6810144. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 03:12:00,988][166323] Avg episode reward: [(0, '1257.802')]
[36m[2025-07-02 03:12:06,028][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 6799360. Throughput: 0: 271.6. Samples: 6810976. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 03:12:06,029][166323] Avg episode reward: [(0, '1222.326')]
[36m[2025-07-02 03:12:10,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6799360. Throughput: 0: 267.0. Samples: 6812448. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 03:12:10,990][166323] Avg episode reward: [(0, '1256.297')]
[36m[2025-07-02 03:12:15,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6799360. Throughput: 0: 271.1. Samples: 6814256. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 03:12:15,970][166323] Avg episode reward: [(0, '1256.952')]
[36m[2025-07-02 03:12:20,959][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 222.2). Total num frames: 6799360. Throughput: 0: 269.2. Samples: 6815040. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 03:12:20,959][166323] Avg episode reward: [(0, '1269.218')]
[36m[2025-07-02 03:12:25,986][166323] Fps is (10 sec: 1635.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6815744. Throughput: 0: 270.2. Samples: 6816704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:12:25,986][166323] Avg episode reward: [(0, '1281.682')]
[31m[23937250 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[23937250 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[23937250 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:12:30,967][166323] Fps is (10 sec: 1637.2, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6815744. Throughput: 0: 266.7. Samples: 6818288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:12:30,967][166323] Avg episode reward: [(0, '1272.785')]
[36m[2025-07-02 03:12:35,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6815744. Throughput: 0: 267.0. Samples: 6819104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:12:35,977][166323] Avg episode reward: [(0, '1254.147')]
[36m[2025-07-02 03:12:40,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6815744. Throughput: 0: 267.3. Samples: 6820704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:12:40,945][166323] Avg episode reward: [(0, '1250.036')]
[36m[2025-07-02 03:12:45,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6815744. Throughput: 0: 268.4. Samples: 6822224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:12:46,000][166323] Avg episode reward: [(0, '1254.357')]
[31m[23956717 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[23956718 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[23956718 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:12:50,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6815744. Throughput: 0: 269.5. Samples: 6823088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:12:50,969][166323] Avg episode reward: [(0, '1152.293')]
[36m[2025-07-02 03:12:55,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6815744. Throughput: 0: 272.2. Samples: 6824688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:12:55,961][166323] Avg episode reward: [(0, '1134.327')]
[36m[2025-07-02 03:13:00,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6815744. Throughput: 0: 269.2. Samples: 6826368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:13:00,971][166323] Avg episode reward: [(0, '1130.237')]
[36m[2025-07-02 03:13:06,022][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6815744. Throughput: 0: 269.5. Samples: 6827184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:13:06,023][166323] Avg episode reward: [(0, '1156.481')]
[36m[2025-07-02 03:13:10,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6815744. Throughput: 0: 266.9. Samples: 6828704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:13:10,954][166323] Avg episode reward: [(0, '1109.655')]
[36m[2025-07-02 03:13:15,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6815744. Throughput: 0: 271.9. Samples: 6830528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:13:15,990][166323] Avg episode reward: [(0, '1130.547')]
[36m[2025-07-02 03:13:21,083][166323] Fps is (10 sec: 1617.5, 60 sec: 545.0, 300 sec: 277.6). Total num frames: 6832128. Throughput: 0: 272.8. Samples: 6831408. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 03:13:21,084][166323] Avg episode reward: [(0, '1155.375')]
[36m[2025-07-02 03:13:25,970][166323] Fps is (10 sec: 1641.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6832128. Throughput: 0: 275.0. Samples: 6833088. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 03:13:25,970][166323] Avg episode reward: [(0, '1148.235')]
[31m[23998186 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[23998186 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[23998187 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[23998240 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[23998240 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[23998241 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:13:30,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6832128. Throughput: 0: 283.9. Samples: 6834992. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 03:13:30,969][166323] Avg episode reward: [(0, '1140.232')]
[36m[2025-07-02 03:13:35,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 6832128. Throughput: 0: 283.9. Samples: 6835856. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 03:13:35,948][166323] Avg episode reward: [(0, '1151.513')]
[37m[1m[2025-07-02 03:13:35,999][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013336_6832128.pth...
[36m[2025-07-02 03:13:36,004][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013208_6766592.pth
[36m[2025-07-02 03:13:40,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6832128. Throughput: 0: 287.2. Samples: 6837616. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 03:13:40,968][166323] Avg episode reward: [(0, '1150.725')]
[36m[2025-07-02 03:13:45,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6832128. Throughput: 0: 286.2. Samples: 6839248. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 03:13:45,977][166323] Avg episode reward: [(0, '1105.970')]
[36m[2025-07-02 03:13:50,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6832128. Throughput: 0: 285.5. Samples: 6840016. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 03:13:50,968][166323] Avg episode reward: [(0, '1096.271')]
[36m[2025-07-02 03:13:55,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6832128. Throughput: 0: 290.4. Samples: 6841776. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 03:13:55,965][166323] Avg episode reward: [(0, '1079.306')]
[36m[2025-07-02 03:14:01,024][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 6832128. Throughput: 0: 284.9. Samples: 6843360. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 03:14:01,024][166323] Avg episode reward: [(0, '1108.299')]
[36m[2025-07-02 03:14:05,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6832128. Throughput: 0: 284.1. Samples: 6844160. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 03:14:05,972][166323] Avg episode reward: [(0, '1166.080')]
[36m[2025-07-02 03:14:10,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6832128. Throughput: 0: 281.2. Samples: 6845744. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 03:14:10,969][166323] Avg episode reward: [(0, '1182.115')]
[33m[24039580 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[24039580 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.85791015625
[33mCrash Rate: 0.13623046875
[33mTimeout Rate: 0.005859375 (navigation_task.py:265)
[33m[24039580 ms][navigation_task] - WARNING : 
[33mSuccesses: 1757
[33mCrashes : 279
[33mTimeouts: 12 (navigation_task.py:268)
[36m[2025-07-02 03:14:15,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6832128. Throughput: 0: 274.1. Samples: 6847328. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 03:14:15,972][166323] Avg episode reward: [(0, '1190.687')]
[36m[2025-07-02 03:14:20,982][166323] Fps is (10 sec: 1636.4, 60 sec: 273.5, 300 sec: 277.7). Total num frames: 6848512. Throughput: 0: 273.9. Samples: 6848192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:14:20,982][166323] Avg episode reward: [(0, '1238.852')]
[36m[2025-07-02 03:14:25,975][166323] Fps is (10 sec: 1637.8, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6848512. Throughput: 0: 269.5. Samples: 6849744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:14:25,976][166323] Avg episode reward: [(0, '1265.662')]
[36m[2025-07-02 03:14:30,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6848512. Throughput: 0: 270.6. Samples: 6851424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:14:30,970][166323] Avg episode reward: [(0, '1282.938')]
[31m[24059532 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[24059533 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[24059533 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:14:35,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6848512. Throughput: 0: 270.3. Samples: 6852176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:14:35,948][166323] Avg episode reward: [(0, '1210.624')]
[36m[2025-07-02 03:14:40,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6848512. Throughput: 0: 266.9. Samples: 6853792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:14:40,987][166323] Avg episode reward: [(0, '1203.620')]
[31m[24071911 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[24071912 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[24071912 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:14:45,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6848512. Throughput: 0: 268.9. Samples: 6855440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:14:45,948][166323] Avg episode reward: [(0, '1227.542')]
[36m[2025-07-02 03:14:50,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6848512. Throughput: 0: 267.1. Samples: 6856176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:14:50,965][166323] Avg episode reward: [(0, '1204.025')]
[36m[2025-07-02 03:14:56,001][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6848512. Throughput: 0: 267.2. Samples: 6857776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:14:56,001][166323] Avg episode reward: [(0, '1219.355')]
[36m[2025-07-02 03:15:00,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 6848512. Throughput: 0: 269.3. Samples: 6859440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:15:00,947][166323] Avg episode reward: [(0, '1208.884')]
[31m[24094014 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[24094014 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[24094014 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:15:05,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6848512. Throughput: 0: 268.4. Samples: 6860272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:15:05,981][166323] Avg episode reward: [(0, '1204.540')]
[36m[2025-07-02 03:15:11,024][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 6848512. Throughput: 0: 268.9. Samples: 6861856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:15:11,024][166323] Avg episode reward: [(0, '1166.559')]
[36m[2025-07-02 03:15:16,005][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 6848512. Throughput: 0: 267.9. Samples: 6863488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:15:16,006][166323] Avg episode reward: [(0, '1158.979')]
[36m[2025-07-02 03:15:20,987][166323] Fps is (10 sec: 1644.5, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6864896. Throughput: 0: 268.6. Samples: 6864272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:15:20,987][166323] Avg episode reward: [(0, '1156.139')]
[36m[2025-07-02 03:15:26,008][166323] Fps is (10 sec: 1637.9, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 6864896. Throughput: 0: 271.5. Samples: 6866016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:15:26,009][166323] Avg episode reward: [(0, '1176.978')]
[36m[2025-07-02 03:15:30,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6864896. Throughput: 0: 270.2. Samples: 6867600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:15:30,957][166323] Avg episode reward: [(0, '1167.114')]
[36m[2025-07-02 03:15:35,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 6864896. Throughput: 0: 271.1. Samples: 6868384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:15:35,994][166323] Avg episode reward: [(0, '1181.278')]
[37m[1m[2025-07-02 03:15:36,064][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013400_6864896.pth...
[36m[2025-07-02 03:15:36,068][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013272_6799360.pth
[36m[2025-07-02 03:15:40,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6864896. Throughput: 0: 273.9. Samples: 6870096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:15:40,977][166323] Avg episode reward: [(0, '1153.599')]
[36m[2025-07-02 03:15:45,978][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6864896. Throughput: 0: 273.9. Samples: 6871776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:15:45,978][166323] Avg episode reward: [(0, '1198.333')]
[36m[2025-07-02 03:15:50,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6864896. Throughput: 0: 274.7. Samples: 6872624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:15:50,954][166323] Avg episode reward: [(0, '1185.067')]
[36m[2025-07-02 03:15:55,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6864896. Throughput: 0: 278.9. Samples: 6874384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:15:55,943][166323] Avg episode reward: [(0, '1164.734')]
[36m[2025-07-02 03:16:00,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6864896. Throughput: 0: 283.4. Samples: 6876224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:16:00,951][166323] Avg episode reward: [(0, '1181.649')]
[36m[2025-07-02 03:16:05,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6864896. Throughput: 0: 284.8. Samples: 6877088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:16:05,989][166323] Avg episode reward: [(0, '1133.331')]
[36m[2025-07-02 03:16:10,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6864896. Throughput: 0: 282.1. Samples: 6878704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:16:10,984][166323] Avg episode reward: [(0, '1106.046')]
[36m[2025-07-02 03:16:16,007][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.1). Total num frames: 6864896. Throughput: 0: 284.5. Samples: 6880416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:16:16,007][166323] Avg episode reward: [(0, '1063.163')]
[36m[2025-07-02 03:16:20,965][166323] Fps is (10 sec: 1641.5, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6881280. Throughput: 0: 284.6. Samples: 6881184. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 03:16:20,965][166323] Avg episode reward: [(0, '1115.914')]
[36m[2025-07-02 03:16:25,944][166323] Fps is (10 sec: 1648.7, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 6881280. Throughput: 0: 283.6. Samples: 6882848. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 03:16:25,945][166323] Avg episode reward: [(0, '1181.471')]
[31m[24175397 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[24175398 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[24175398 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:16:30,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6881280. Throughput: 0: 283.0. Samples: 6884512. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 03:16:30,984][166323] Avg episode reward: [(0, '1052.153')]
[36m[2025-07-02 03:16:35,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6881280. Throughput: 0: 283.9. Samples: 6885408. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 03:16:35,978][166323] Avg episode reward: [(0, '1082.443')]
[36m[2025-07-02 03:16:40,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6881280. Throughput: 0: 283.6. Samples: 6887152. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 03:16:40,969][166323] Avg episode reward: [(0, '1129.495')]
[36m[2025-07-02 03:16:45,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6881280. Throughput: 0: 281.5. Samples: 6888896. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 03:16:45,961][166323] Avg episode reward: [(0, '1151.953')]
[36m[2025-07-02 03:16:50,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6881280. Throughput: 0: 281.0. Samples: 6889728. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 03:16:50,969][166323] Avg episode reward: [(0, '1091.359')]
[36m[2025-07-02 03:16:55,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6881280. Throughput: 0: 281.2. Samples: 6891360. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 03:16:55,988][166323] Avg episode reward: [(0, '1060.199')]
[36m[2025-07-02 03:17:00,977][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6881280. Throughput: 0: 282.9. Samples: 6893136. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 03:17:00,977][166323] Avg episode reward: [(0, '1141.581')]
[31m[24211165 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[24211166 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[24211166 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:17:05,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6881280. Throughput: 0: 282.9. Samples: 6893920. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 03:17:05,987][166323] Avg episode reward: [(0, '1114.430')]
[36m[2025-07-02 03:17:10,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6881280. Throughput: 0: 281.9. Samples: 6895536. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 03:17:10,949][166323] Avg episode reward: [(0, '1141.547')]
[36m[2025-07-02 03:17:15,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6881280. Throughput: 0: 279.1. Samples: 6897072. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 03:17:15,985][166323] Avg episode reward: [(0, '1147.392')]
[36m[2025-07-02 03:17:20,960][166323] Fps is (10 sec: 1636.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6897664. Throughput: 0: 276.4. Samples: 6897840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:17:20,960][166323] Avg episode reward: [(0, '1195.881')]
[36m[2025-07-02 03:17:25,965][166323] Fps is (10 sec: 1641.7, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6897664. Throughput: 0: 273.4. Samples: 6899456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:17:25,965][166323] Avg episode reward: [(0, '1235.733')]
[36m[2025-07-02 03:17:30,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6897664. Throughput: 0: 270.6. Samples: 6901072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:17:30,951][166323] Avg episode reward: [(0, '1285.356')]
[36m[2025-07-02 03:17:36,003][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 6897664. Throughput: 0: 271.1. Samples: 6901936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:17:36,004][166323] Avg episode reward: [(0, '1283.789')]
[37m[1m[2025-07-02 03:17:36,081][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013464_6897664.pth...
[36m[2025-07-02 03:17:36,088][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013336_6832128.pth
[36m[2025-07-02 03:17:40,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6897664. Throughput: 0: 271.9. Samples: 6903584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:17:40,947][166323] Avg episode reward: [(0, '1279.031')]
[36m[2025-07-02 03:17:45,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6897664. Throughput: 0: 270.4. Samples: 6905296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:17:45,949][166323] Avg episode reward: [(0, '1257.562')]
[36m[2025-07-02 03:17:50,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6897664. Throughput: 0: 271.8. Samples: 6906144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:17:50,966][166323] Avg episode reward: [(0, '1213.255')]
[36m[2025-07-02 03:17:55,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6897664. Throughput: 0: 272.8. Samples: 6907824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:17:55,994][166323] Avg episode reward: [(0, '1188.447')]
[36m[2025-07-02 03:18:00,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6897664. Throughput: 0: 275.6. Samples: 6909472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:18:00,980][166323] Avg episode reward: [(0, '1149.813')]
[36m[2025-07-02 03:18:05,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6897664. Throughput: 0: 277.5. Samples: 6910336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:18:05,989][166323] Avg episode reward: [(0, '1108.874')]
[36m[2025-07-02 03:18:10,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6897664. Throughput: 0: 277.6. Samples: 6911952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:18:10,973][166323] Avg episode reward: [(0, '1153.125')]
[36m[2025-07-02 03:18:15,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.3). Total num frames: 6897664. Throughput: 0: 280.1. Samples: 6913680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:18:15,958][166323] Avg episode reward: [(0, '1206.237')]
[36m[2025-07-02 03:18:20,995][166323] Fps is (10 sec: 1634.7, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6914048. Throughput: 0: 276.3. Samples: 6914368. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:18:20,996][166323] Avg episode reward: [(0, '1201.396')]
[36m[2025-07-02 03:18:25,959][166323] Fps is (10 sec: 1638.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6914048. Throughput: 0: 274.4. Samples: 6915936. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:18:25,960][166323] Avg episode reward: [(0, '1225.779')]
[36m[2025-07-02 03:18:30,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6914048. Throughput: 0: 274.4. Samples: 6917648. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:18:30,968][166323] Avg episode reward: [(0, '1223.940')]
[36m[2025-07-02 03:18:36,001][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6914048. Throughput: 0: 273.9. Samples: 6918480. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:18:36,002][166323] Avg episode reward: [(0, '1240.781')]
[36m[2025-07-02 03:18:40,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6914048. Throughput: 0: 272.4. Samples: 6920080. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:18:40,991][166323] Avg episode reward: [(0, '1244.833')]
[36m[2025-07-02 03:18:45,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6914048. Throughput: 0: 272.1. Samples: 6921712. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:18:45,970][166323] Avg episode reward: [(0, '1212.537')]
[36m[2025-07-02 03:18:50,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6914048. Throughput: 0: 269.5. Samples: 6922464. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:18:50,988][166323] Avg episode reward: [(0, '1156.470')]
[31m[24323961 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[24323961 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[24323962 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:18:55,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.8). Total num frames: 6914048. Throughput: 0: 271.4. Samples: 6924160. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:18:55,952][166323] Avg episode reward: [(0, '1130.770')]
[36m[2025-07-02 03:19:00,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6914048. Throughput: 0: 269.2. Samples: 6925792. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:19:00,946][166323] Avg episode reward: [(0, '1148.455')]
[36m[2025-07-02 03:19:05,997][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6914048. Throughput: 0: 273.4. Samples: 6926672. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:19:05,998][166323] Avg episode reward: [(0, '1117.322')]
[36m[2025-07-02 03:19:10,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6914048. Throughput: 0: 275.1. Samples: 6928320. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:19:10,969][166323] Avg episode reward: [(0, '1104.707')]
[36m[2025-07-02 03:19:15,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 6914048. Throughput: 0: 278.1. Samples: 6930160. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:19:15,957][166323] Avg episode reward: [(0, '1188.306')]
[36m[2025-07-02 03:19:20,985][166323] Fps is (10 sec: 1635.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6930432. Throughput: 0: 278.9. Samples: 6931024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:19:20,986][166323] Avg episode reward: [(0, '1223.628')]
[36m[2025-07-02 03:19:25,945][166323] Fps is (10 sec: 1640.3, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6930432. Throughput: 0: 282.6. Samples: 6932784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:19:25,945][166323] Avg episode reward: [(0, '1189.772')]
[31m[24358797 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[24358797 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[24358797 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:19:30,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6930432. Throughput: 0: 286.2. Samples: 6934592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:19:30,971][166323] Avg episode reward: [(0, '1175.404')]
[36m[2025-07-02 03:19:35,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6930432. Throughput: 0: 286.9. Samples: 6935360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:19:35,945][166323] Avg episode reward: [(0, '1194.331')]
[37m[1m[2025-07-02 03:19:36,005][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013528_6930432.pth...
[36m[2025-07-02 03:19:36,011][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013400_6864896.pth
[36m[2025-07-02 03:19:40,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6930432. Throughput: 0: 286.2. Samples: 6937040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:19:40,962][166323] Avg episode reward: [(0, '1191.291')]
[36m[2025-07-02 03:19:45,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6930432. Throughput: 0: 287.9. Samples: 6938752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:19:45,966][166323] Avg episode reward: [(0, '1192.188')]
[36m[2025-07-02 03:19:50,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6930432. Throughput: 0: 287.0. Samples: 6939584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:19:50,991][166323] Avg episode reward: [(0, '1165.519')]
[36m[2025-07-02 03:19:55,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6930432. Throughput: 0: 288.1. Samples: 6941280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:19:55,948][166323] Avg episode reward: [(0, '1167.309')]
[36m[2025-07-02 03:20:01,010][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 6930432. Throughput: 0: 283.4. Samples: 6942928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:20:01,010][166323] Avg episode reward: [(0, '1198.410')]
[36m[2025-07-02 03:20:06,032][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6930432. Throughput: 0: 281.3. Samples: 6943696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:20:06,033][166323] Avg episode reward: [(0, '1222.687')]
[36m[2025-07-02 03:20:10,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6930432. Throughput: 0: 280.0. Samples: 6945392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:20:10,968][166323] Avg episode reward: [(0, '1253.821')]
[36m[2025-07-02 03:20:16,000][166323] Fps is (10 sec: 1643.7, 60 sec: 545.7, 300 sec: 277.7). Total num frames: 6946816. Throughput: 0: 277.9. Samples: 6947104. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:20:16,000][166323] Avg episode reward: [(0, '1253.807')]
[36m[2025-07-02 03:20:20,965][166323] Fps is (10 sec: 1638.9, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6946816. Throughput: 0: 281.1. Samples: 6948016. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:20:20,965][166323] Avg episode reward: [(0, '1282.476')]
[36m[2025-07-02 03:20:25,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6946816. Throughput: 0: 280.7. Samples: 6949680. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:20:25,984][166323] Avg episode reward: [(0, '1287.319')]
[36m[2025-07-02 03:20:30,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6946816. Throughput: 0: 282.9. Samples: 6951488. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:20:30,980][166323] Avg episode reward: [(0, '1336.427')]
[36m[2025-07-02 03:20:35,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6946816. Throughput: 0: 281.8. Samples: 6952256. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:20:35,959][166323] Avg episode reward: [(0, '1342.307')]
[36m[2025-07-02 03:20:40,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6946816. Throughput: 0: 281.7. Samples: 6953968. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:20:40,991][166323] Avg episode reward: [(0, '1343.349')]
[36m[2025-07-02 03:20:45,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6946816. Throughput: 0: 281.1. Samples: 6955568. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:20:45,983][166323] Avg episode reward: [(0, '1283.108')]
[36m[2025-07-02 03:20:50,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6946816. Throughput: 0: 282.5. Samples: 6956384. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:20:50,948][166323] Avg episode reward: [(0, '1283.644')]
[36m[2025-07-02 03:20:55,974][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6946816. Throughput: 0: 284.0. Samples: 6958176. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:20:55,975][166323] Avg episode reward: [(0, '1227.350')]
[36m[2025-07-02 03:21:00,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6946816. Throughput: 0: 282.9. Samples: 6959824. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:21:00,969][166323] Avg episode reward: [(0, '1193.719')]
[36m[2025-07-02 03:21:05,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.5, 300 sec: 277.7). Total num frames: 6946816. Throughput: 0: 279.6. Samples: 6960592. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:21:05,947][166323] Avg episode reward: [(0, '1158.126')]
[36m[2025-07-02 03:21:10,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6946816. Throughput: 0: 281.1. Samples: 6962320. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:21:10,951][166323] Avg episode reward: [(0, '1140.256')]
[36m[2025-07-02 03:21:15,982][166323] Fps is (10 sec: 1632.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6963200. Throughput: 0: 278.0. Samples: 6964000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:21:15,982][166323] Avg episode reward: [(0, '1226.746')]
[36m[2025-07-02 03:21:20,975][166323] Fps is (10 sec: 1634.4, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6963200. Throughput: 0: 280.8. Samples: 6964896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:21:20,975][166323] Avg episode reward: [(0, '1257.893')]
[36m[2025-07-02 03:21:25,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6963200. Throughput: 0: 281.4. Samples: 6966624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:21:25,973][166323] Avg episode reward: [(0, '1196.225')]
[36m[2025-07-02 03:21:31,000][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6963200. Throughput: 0: 283.6. Samples: 6968336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:21:31,001][166323] Avg episode reward: [(0, '1231.950')]
[36m[2025-07-02 03:21:35,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6963200. Throughput: 0: 284.4. Samples: 6969184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:21:35,963][166323] Avg episode reward: [(0, '1257.231')]
[37m[1m[2025-07-02 03:21:36,016][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013592_6963200.pth...
[36m[2025-07-02 03:21:36,020][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013464_6897664.pth
[36m[2025-07-02 03:21:40,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6963200. Throughput: 0: 281.8. Samples: 6970848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:21:40,948][166323] Avg episode reward: [(0, '1273.472')]
[36m[2025-07-02 03:21:45,997][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6963200. Throughput: 0: 281.8. Samples: 6972512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:21:45,998][166323] Avg episode reward: [(0, '1220.297')]
[36m[2025-07-02 03:21:50,979][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6963200. Throughput: 0: 282.5. Samples: 6973312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:21:50,980][166323] Avg episode reward: [(0, '1145.493')]
[36m[2025-07-02 03:21:55,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6963200. Throughput: 0: 281.6. Samples: 6974992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:21:55,944][166323] Avg episode reward: [(0, '1105.515')]
[36m[2025-07-02 03:22:00,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6963200. Throughput: 0: 282.9. Samples: 6976720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:22:00,946][166323] Avg episode reward: [(0, '1148.847')]
[36m[2025-07-02 03:22:05,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6963200. Throughput: 0: 283.8. Samples: 6977664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:22:05,964][166323] Avg episode reward: [(0, '1126.735')]
[36m[2025-07-02 03:22:10,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6963200. Throughput: 0: 280.0. Samples: 6979216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:22:10,947][166323] Avg episode reward: [(0, '1146.472')]
[36m[2025-07-02 03:22:15,958][166323] Fps is (10 sec: 1639.4, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6979584. Throughput: 0: 276.2. Samples: 6980752. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 03:22:15,958][166323] Avg episode reward: [(0, '1119.154')]
[36m[2025-07-02 03:22:20,993][166323] Fps is (10 sec: 1630.9, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6979584. Throughput: 0: 274.3. Samples: 6981536. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 03:22:20,993][166323] Avg episode reward: [(0, '1186.013')]
[36m[2025-07-02 03:22:25,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6979584. Throughput: 0: 273.3. Samples: 6983152. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 03:22:25,967][166323] Avg episode reward: [(0, '1186.445')]
[36m[2025-07-02 03:22:30,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6979584. Throughput: 0: 272.3. Samples: 6984752. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 03:22:30,954][166323] Avg episode reward: [(0, '1238.466')]
[36m[2025-07-02 03:22:35,998][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 6979584. Throughput: 0: 274.7. Samples: 6985680. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 03:22:35,998][166323] Avg episode reward: [(0, '1212.530')]
[31m[24544662 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[24544663 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[24544663 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:22:40,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6979584. Throughput: 0: 271.5. Samples: 6987216. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 03:22:40,971][166323] Avg episode reward: [(0, '1189.598')]
[36m[2025-07-02 03:22:45,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6979584. Throughput: 0: 264.9. Samples: 6988640. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 03:22:45,947][166323] Avg episode reward: [(0, '1220.081')]
[36m[2025-07-02 03:22:50,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6979584. Throughput: 0: 264.4. Samples: 6989568. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 03:22:50,984][166323] Avg episode reward: [(0, '1241.352')]
[36m[2025-07-02 03:22:55,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6979584. Throughput: 0: 268.0. Samples: 6991280. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 03:22:55,962][166323] Avg episode reward: [(0, '1143.040')]
[36m[2025-07-02 03:23:00,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6979584. Throughput: 0: 271.8. Samples: 6992992. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 03:23:00,991][166323] Avg episode reward: [(0, '1185.266')]
[36m[2025-07-02 03:23:06,004][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6979584. Throughput: 0: 274.1. Samples: 6993872. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 03:23:06,004][166323] Avg episode reward: [(0, '1200.370')]
[36m[2025-07-02 03:23:11,006][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 6979584. Throughput: 0: 271.8. Samples: 6995392. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 03:23:11,007][166323] Avg episode reward: [(0, '1232.331')]
[31m[24580997 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[24580997 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[24580997 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:23:15,990][166323] Fps is (10 sec: 1640.7, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 6995968. Throughput: 0: 271.8. Samples: 6996992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:23:15,990][166323] Avg episode reward: [(0, '1208.458')]
[36m[2025-07-02 03:23:20,989][166323] Fps is (10 sec: 1641.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6995968. Throughput: 0: 271.0. Samples: 6997872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:23:20,989][166323] Avg episode reward: [(0, '1204.046')]
[36m[2025-07-02 03:23:25,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6995968. Throughput: 0: 270.2. Samples: 6999376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:23:25,973][166323] Avg episode reward: [(0, '1195.024')]
[36m[2025-07-02 03:23:30,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6995968. Throughput: 0: 278.3. Samples: 7001168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:23:30,960][166323] Avg episode reward: [(0, '1219.804')]
[36m[2025-07-02 03:23:35,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6995968. Throughput: 0: 275.6. Samples: 7001968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:23:35,979][166323] Avg episode reward: [(0, '1252.440')]
[37m[1m[2025-07-02 03:23:36,072][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013656_6995968.pth...
[36m[2025-07-02 03:23:36,076][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013528_6930432.pth
[31m[24608217 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[24608218 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[24608218 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:23:40,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6995968. Throughput: 0: 273.1. Samples: 7003568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:23:40,956][166323] Avg episode reward: [(0, '1189.936')]
[36m[2025-07-02 03:23:45,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6995968. Throughput: 0: 273.2. Samples: 7005280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:23:45,969][166323] Avg episode reward: [(0, '1195.860')]
[36m[2025-07-02 03:23:50,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 6995968. Throughput: 0: 272.5. Samples: 7006128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:23:50,983][166323] Avg episode reward: [(0, '1216.161')]
[36m[2025-07-02 03:23:55,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 6995968. Throughput: 0: 277.9. Samples: 7007888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:23:55,975][166323] Avg episode reward: [(0, '1230.827')]
[36m[2025-07-02 03:24:00,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6995968. Throughput: 0: 280.0. Samples: 7009584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:24:00,967][166323] Avg episode reward: [(0, '1226.047')]
[36m[2025-07-02 03:24:05,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 6995968. Throughput: 0: 278.1. Samples: 7010384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:24:05,984][166323] Avg episode reward: [(0, '1234.659')]
[36m[2025-07-02 03:24:10,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 6995968. Throughput: 0: 280.3. Samples: 7011984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:24:10,954][166323] Avg episode reward: [(0, '1264.032')]
[36m[2025-07-02 03:24:15,964][166323] Fps is (10 sec: 1641.6, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7012352. Throughput: 0: 276.9. Samples: 7013632. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 03:24:15,964][166323] Avg episode reward: [(0, '1291.535')]
[36m[2025-07-02 03:24:21,002][166323] Fps is (10 sec: 1630.6, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 7012352. Throughput: 0: 279.0. Samples: 7014528. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 03:24:21,003][166323] Avg episode reward: [(0, '1320.404')]
[36m[2025-07-02 03:24:25,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7012352. Throughput: 0: 278.3. Samples: 7016096. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 03:24:25,970][166323] Avg episode reward: [(0, '1253.536')]
[36m[2025-07-02 03:24:30,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7012352. Throughput: 0: 277.0. Samples: 7017744. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 03:24:30,963][166323] Avg episode reward: [(0, '1234.345')]
[36m[2025-07-02 03:24:35,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7012352. Throughput: 0: 277.4. Samples: 7018608. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 03:24:35,967][166323] Avg episode reward: [(0, '1215.212')]
[36m[2025-07-02 03:24:40,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7012352. Throughput: 0: 276.1. Samples: 7020304. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 03:24:40,945][166323] Avg episode reward: [(0, '1153.669')]
[36m[2025-07-02 03:24:45,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7012352. Throughput: 0: 275.9. Samples: 7022000. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 03:24:45,962][166323] Avg episode reward: [(0, '1160.578')]
[36m[2025-07-02 03:24:50,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7012352. Throughput: 0: 276.6. Samples: 7022832. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 03:24:50,994][166323] Avg episode reward: [(0, '1155.840')]
[36m[2025-07-02 03:24:55,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 7012352. Throughput: 0: 278.4. Samples: 7024512. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 03:24:55,947][166323] Avg episode reward: [(0, '1116.997')]
[36m[2025-07-02 03:25:00,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7012352. Throughput: 0: 275.0. Samples: 7026016. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 03:25:00,993][166323] Avg episode reward: [(0, '1157.025')]
[36m[2025-07-02 03:25:05,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7012352. Throughput: 0: 275.7. Samples: 7026928. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 03:25:05,986][166323] Avg episode reward: [(0, '1205.922')]
[36m[2025-07-02 03:25:11,003][166323] Fps is (10 sec: 1636.7, 60 sec: 545.7, 300 sec: 277.7). Total num frames: 7028736. Throughput: 0: 276.8. Samples: 7028560. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:25:11,003][166323] Avg episode reward: [(0, '1156.997')]
[36m[2025-07-02 03:25:15,991][166323] Fps is (10 sec: 1637.6, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7028736. Throughput: 0: 274.0. Samples: 7030080. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:25:15,991][166323] Avg episode reward: [(0, '1159.758')]
[36m[2025-07-02 03:25:20,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7028736. Throughput: 0: 271.9. Samples: 7030848. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:25:20,987][166323] Avg episode reward: [(0, '1126.827')]
[33m[24711341 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[24711341 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.83642578125
[33mCrash Rate: 0.15478515625
[33mTimeout Rate: 0.0087890625 (navigation_task.py:265)
[33m[24711341 ms][navigation_task] - WARNING : 
[33mSuccesses: 1713
[33mCrashes : 317
[33mTimeouts: 18 (navigation_task.py:268)
[31m[24711489 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[24711489 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[24711489 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:25:25,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7028736. Throughput: 0: 271.8. Samples: 7032544. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:25:25,975][166323] Avg episode reward: [(0, '1091.700')]
[36m[2025-07-02 03:25:30,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7028736. Throughput: 0: 274.2. Samples: 7034336. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:25:30,957][166323] Avg episode reward: [(0, '1162.287')]
[36m[2025-07-02 03:25:35,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7028736. Throughput: 0: 272.5. Samples: 7035088. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:25:35,974][166323] Avg episode reward: [(0, '1119.269')]
[37m[1m[2025-07-02 03:25:36,032][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013720_7028736.pth...
[36m[2025-07-02 03:25:36,036][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013592_6963200.pth
[36m[2025-07-02 03:25:40,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7028736. Throughput: 0: 272.1. Samples: 7036768. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:25:40,991][166323] Avg episode reward: [(0, '1175.851')]
[31m[24732414 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[24732414 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[24732415 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:25:45,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7028736. Throughput: 0: 273.3. Samples: 7038304. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:25:45,950][166323] Avg episode reward: [(0, '1245.143')]
[36m[2025-07-02 03:25:50,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7028736. Throughput: 0: 272.0. Samples: 7039168. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:25:50,989][166323] Avg episode reward: [(0, '1267.516')]
[36m[2025-07-02 03:25:56,008][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 7028736. Throughput: 0: 269.8. Samples: 7040704. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:25:56,008][166323] Avg episode reward: [(0, '1269.906')]
[36m[2025-07-02 03:26:00,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7028736. Throughput: 0: 274.4. Samples: 7042416. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:26:00,955][166323] Avg episode reward: [(0, '1316.772')]
[36m[2025-07-02 03:26:05,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7028736. Throughput: 0: 277.9. Samples: 7043344. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:26:05,956][166323] Avg episode reward: [(0, '1313.326')]
[36m[2025-07-02 03:26:10,960][166323] Fps is (10 sec: 1637.4, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7045120. Throughput: 0: 279.2. Samples: 7045104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:26:10,961][166323] Avg episode reward: [(0, '1372.848')]
[37m[1m[2025-07-02 03:26:11,033][166323] Saving new best policy, reward=1372.848!
[36m[2025-07-02 03:26:16,001][166323] Fps is (10 sec: 1630.9, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7045120. Throughput: 0: 273.1. Samples: 7046640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:26:16,002][166323] Avg episode reward: [(0, '1389.296')]
[37m[1m[2025-07-02 03:26:16,008][166323] Saving new best policy, reward=1389.296!
[36m[2025-07-02 03:26:20,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7045120. Throughput: 0: 276.0. Samples: 7047504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:26:20,964][166323] Avg episode reward: [(0, '1394.811')]
[37m[1m[2025-07-02 03:26:21,012][166323] Saving new best policy, reward=1394.811!
[36m[2025-07-02 03:26:25,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7045120. Throughput: 0: 278.0. Samples: 7049280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:26:25,991][166323] Avg episode reward: [(0, '1391.565')]
[36m[2025-07-02 03:26:30,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7045120. Throughput: 0: 279.8. Samples: 7050896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:26:30,957][166323] Avg episode reward: [(0, '1400.562')]
[37m[1m[2025-07-02 03:26:31,005][166323] Saving new best policy, reward=1400.562!
[36m[2025-07-02 03:26:35,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7045120. Throughput: 0: 278.4. Samples: 7051696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:26:35,982][166323] Avg episode reward: [(0, '1431.793')]
[37m[1m[2025-07-02 03:26:36,040][166323] Saving new best policy, reward=1431.793!
[36m[2025-07-02 03:26:40,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7045120. Throughput: 0: 281.1. Samples: 7053344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:26:40,978][166323] Avg episode reward: [(0, '1431.981')]
[37m[1m[2025-07-02 03:26:41,026][166323] Saving new best policy, reward=1431.981!
[31m[24791438 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[24791439 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[24791439 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:26:46,033][166323] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 277.6). Total num frames: 7045120. Throughput: 0: 282.2. Samples: 7055136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:26:46,033][166323] Avg episode reward: [(0, '1362.477')]
[36m[2025-07-02 03:26:50,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 7045120. Throughput: 0: 278.5. Samples: 7055888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:26:50,995][166323] Avg episode reward: [(0, '1310.670')]
[36m[2025-07-02 03:26:55,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7045120. Throughput: 0: 276.5. Samples: 7057552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:26:55,983][166323] Avg episode reward: [(0, '1309.517')]
[36m[2025-07-02 03:27:00,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7045120. Throughput: 0: 276.7. Samples: 7059088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:27:00,985][166323] Avg episode reward: [(0, '1280.497')]
[36m[2025-07-02 03:27:05,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7045120. Throughput: 0: 275.9. Samples: 7059920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:27:05,971][166323] Avg episode reward: [(0, '1258.520')]
[31m[24816733 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[24816733 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[24816733 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:27:10,954][166323] Fps is (10 sec: 1643.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7061504. Throughput: 0: 275.1. Samples: 7061648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:27:10,954][166323] Avg episode reward: [(0, '1192.418')]
[36m[2025-07-02 03:27:15,995][166323] Fps is (10 sec: 1634.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7061504. Throughput: 0: 275.3. Samples: 7063296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:27:15,995][166323] Avg episode reward: [(0, '1187.406')]
[36m[2025-07-02 03:27:20,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7061504. Throughput: 0: 276.2. Samples: 7064128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:27:20,992][166323] Avg episode reward: [(0, '1178.590')]
[36m[2025-07-02 03:27:25,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7061504. Throughput: 0: 274.8. Samples: 7065712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:27:25,986][166323] Avg episode reward: [(0, '1134.441')]
[31m[24835591 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[24835591 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[24835591 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:27:30,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7061504. Throughput: 0: 274.8. Samples: 7067488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:27:30,989][166323] Avg episode reward: [(0, '1118.503')]
[36m[2025-07-02 03:27:35,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7061504. Throughput: 0: 276.1. Samples: 7068304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:27:35,971][166323] Avg episode reward: [(0, '1127.574')]
[37m[1m[2025-07-02 03:27:36,021][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013784_7061504.pth...
[36m[2025-07-02 03:27:36,025][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013656_6995968.pth
[31m[24845806 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[24845807 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[24845808 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:27:40,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7061504. Throughput: 0: 277.1. Samples: 7070016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:27:40,962][166323] Avg episode reward: [(0, '1112.306')]
[36m[2025-07-02 03:27:45,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 7061504. Throughput: 0: 280.3. Samples: 7071696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:27:45,965][166323] Avg episode reward: [(0, '1107.756')]
[36m[2025-07-02 03:27:50,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7061504. Throughput: 0: 278.9. Samples: 7072464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:27:50,945][166323] Avg episode reward: [(0, '1055.928')]
[36m[2025-07-02 03:27:55,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7061504. Throughput: 0: 280.9. Samples: 7074288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:27:55,957][166323] Avg episode reward: [(0, '1142.234')]
[36m[2025-07-02 03:28:00,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7061504. Throughput: 0: 282.1. Samples: 7075984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:28:00,968][166323] Avg episode reward: [(0, '1166.729')]
[36m[2025-07-02 03:28:05,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7061504. Throughput: 0: 283.9. Samples: 7076896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:28:05,961][166323] Avg episode reward: [(0, '1160.102')]
[36m[2025-07-02 03:28:10,966][166323] Fps is (10 sec: 1638.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7077888. Throughput: 0: 287.1. Samples: 7078624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:28:10,966][166323] Avg episode reward: [(0, '1163.978')]
[31m[24880405 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[24880405 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[24880405 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:28:15,990][166323] Fps is (10 sec: 1633.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7077888. Throughput: 0: 282.7. Samples: 7080208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:28:15,990][166323] Avg episode reward: [(0, '1209.538')]
[31m[24886425 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[24886425 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[24886425 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:28:20,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7077888. Throughput: 0: 283.5. Samples: 7081056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:28:20,959][166323] Avg episode reward: [(0, '1212.243')]
[36m[2025-07-02 03:28:25,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7077888. Throughput: 0: 280.2. Samples: 7082624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:28:25,951][166323] Avg episode reward: [(0, '1241.856')]
[36m[2025-07-02 03:28:30,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7077888. Throughput: 0: 283.1. Samples: 7084432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:28:30,958][166323] Avg episode reward: [(0, '1246.821')]
[36m[2025-07-02 03:28:35,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7077888. Throughput: 0: 286.1. Samples: 7085344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:28:35,971][166323] Avg episode reward: [(0, '1237.950')]
[36m[2025-07-02 03:28:41,022][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 7077888. Throughput: 0: 281.6. Samples: 7086976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:28:41,022][166323] Avg episode reward: [(0, '1242.794')]
[36m[2025-07-02 03:28:45,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7077888. Throughput: 0: 282.9. Samples: 7088720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:28:45,988][166323] Avg episode reward: [(0, '1248.984')]
[36m[2025-07-02 03:28:50,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7077888. Throughput: 0: 280.7. Samples: 7089536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:28:50,987][166323] Avg episode reward: [(0, '1270.211')]
[31m[24920613 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[24920613 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[24920613 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:28:55,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7077888. Throughput: 0: 282.0. Samples: 7091312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:28:55,965][166323] Avg episode reward: [(0, '1201.942')]
[36m[2025-07-02 03:29:00,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7077888. Throughput: 0: 282.1. Samples: 7092896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:29:00,968][166323] Avg episode reward: [(0, '1248.834')]
[36m[2025-07-02 03:29:05,975][166323] Fps is (10 sec: 1636.8, 60 sec: 546.0, 300 sec: 333.2). Total num frames: 7094272. Throughput: 0: 282.2. Samples: 7093760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:29:05,975][166323] Avg episode reward: [(0, '1249.887')]
[36m[2025-07-02 03:29:11,004][166323] Fps is (10 sec: 1632.5, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7094272. Throughput: 0: 284.1. Samples: 7095424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:29:11,005][166323] Avg episode reward: [(0, '1282.772')]
[36m[2025-07-02 03:29:15,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7094272. Throughput: 0: 282.7. Samples: 7097152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:29:15,946][166323] Avg episode reward: [(0, '1172.296')]
[36m[2025-07-02 03:29:20,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7094272. Throughput: 0: 279.1. Samples: 7097904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:29:20,967][166323] Avg episode reward: [(0, '1216.045')]
[36m[2025-07-02 03:29:25,983][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7094272. Throughput: 0: 277.9. Samples: 7099472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:29:25,984][166323] Avg episode reward: [(0, '1256.958')]
[36m[2025-07-02 03:29:30,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7094272. Throughput: 0: 278.6. Samples: 7101248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:29:30,949][166323] Avg episode reward: [(0, '1200.175')]
[36m[2025-07-02 03:29:35,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7094272. Throughput: 0: 278.8. Samples: 7102080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:29:35,972][166323] Avg episode reward: [(0, '1200.165')]
[37m[1m[2025-07-02 03:29:36,041][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013848_7094272.pth...
[36m[2025-07-02 03:29:36,045][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013720_7028736.pth
[31m[24966203 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[24966204 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[24966204 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:29:40,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7094272. Throughput: 0: 276.5. Samples: 7103760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:29:40,987][166323] Avg episode reward: [(0, '1145.843')]
[36m[2025-07-02 03:29:46,010][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7094272. Throughput: 0: 278.9. Samples: 7105456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:29:46,010][166323] Avg episode reward: [(0, '1232.604')]
[36m[2025-07-02 03:29:50,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7094272. Throughput: 0: 278.7. Samples: 7106304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:29:50,977][166323] Avg episode reward: [(0, '1307.358')]
[36m[2025-07-02 03:29:55,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7094272. Throughput: 0: 279.8. Samples: 7108000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:29:55,951][166323] Avg episode reward: [(0, '1256.185')]
[36m[2025-07-02 03:30:00,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7094272. Throughput: 0: 278.1. Samples: 7109680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:30:00,988][166323] Avg episode reward: [(0, '1236.493')]
[36m[2025-07-02 03:30:05,995][166323] Fps is (10 sec: 1631.3, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7110656. Throughput: 0: 281.1. Samples: 7110560. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 03:30:05,995][166323] Avg episode reward: [(0, '1246.937')]
[31m[24998937 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[24998937 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[24998937 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:30:10,992][166323] Fps is (10 sec: 1637.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7110656. Throughput: 0: 285.5. Samples: 7112320. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 03:30:10,992][166323] Avg episode reward: [(0, '1187.989')]
[36m[2025-07-02 03:30:15,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7110656. Throughput: 0: 283.5. Samples: 7114016. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 03:30:15,987][166323] Avg episode reward: [(0, '1232.183')]
[36m[2025-07-02 03:30:20,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7110656. Throughput: 0: 286.9. Samples: 7114992. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 03:30:20,982][166323] Avg episode reward: [(0, '1211.504')]
[36m[2025-07-02 03:30:25,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7110656. Throughput: 0: 287.9. Samples: 7116704. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 03:30:25,945][166323] Avg episode reward: [(0, '1204.173')]
[36m[2025-07-02 03:30:30,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7110656. Throughput: 0: 288.3. Samples: 7118416. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 03:30:30,966][166323] Avg episode reward: [(0, '1214.460')]
[36m[2025-07-02 03:30:35,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7110656. Throughput: 0: 289.4. Samples: 7119328. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 03:30:35,982][166323] Avg episode reward: [(0, '1228.605')]
[36m[2025-07-02 03:30:40,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7110656. Throughput: 0: 288.5. Samples: 7120992. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 03:30:40,985][166323] Avg episode reward: [(0, '1265.696')]
[36m[2025-07-02 03:30:45,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7110656. Throughput: 0: 286.1. Samples: 7122544. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 03:30:45,956][166323] Avg episode reward: [(0, '1256.573')]
[36m[2025-07-02 03:30:50,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7110656. Throughput: 0: 287.0. Samples: 7123472. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 03:30:50,983][166323] Avg episode reward: [(0, '1276.315')]
[36m[2025-07-02 03:30:55,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7110656. Throughput: 0: 285.8. Samples: 7125168. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 03:30:55,953][166323] Avg episode reward: [(0, '1228.347')]
[36m[2025-07-02 03:31:01,121][166323] Fps is (10 sec: 1616.0, 60 sec: 544.9, 300 sec: 333.0). Total num frames: 7127040. Throughput: 0: 286.1. Samples: 7126928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:31:01,121][166323] Avg episode reward: [(0, '1251.664')]
[36m[2025-07-02 03:31:05,980][166323] Fps is (10 sec: 1634.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7127040. Throughput: 0: 283.7. Samples: 7127760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:31:05,980][166323] Avg episode reward: [(0, '1234.415')]
[36m[2025-07-02 03:31:10,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7127040. Throughput: 0: 281.5. Samples: 7129376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:31:10,957][166323] Avg episode reward: [(0, '1228.870')]
[36m[2025-07-02 03:31:15,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7127040. Throughput: 0: 277.9. Samples: 7130928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:31:15,984][166323] Avg episode reward: [(0, '1207.200')]
[36m[2025-07-02 03:31:20,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7127040. Throughput: 0: 279.2. Samples: 7131888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:31:20,973][166323] Avg episode reward: [(0, '1241.521')]
[36m[2025-07-02 03:31:25,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7127040. Throughput: 0: 281.5. Samples: 7133648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:31:25,948][166323] Avg episode reward: [(0, '1224.541')]
[36m[2025-07-02 03:31:30,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7127040. Throughput: 0: 283.0. Samples: 7135280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:31:30,956][166323] Avg episode reward: [(0, '1228.521')]
[36m[2025-07-02 03:31:36,018][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7127040. Throughput: 0: 281.7. Samples: 7136160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:31:36,018][166323] Avg episode reward: [(0, '1233.158')]
[37m[1m[2025-07-02 03:31:36,026][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013912_7127040.pth...
[36m[2025-07-02 03:31:36,031][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013784_7061504.pth
[36m[2025-07-02 03:31:40,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 7127040. Throughput: 0: 284.3. Samples: 7137968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:31:40,973][166323] Avg episode reward: [(0, '1276.512')]
[36m[2025-07-02 03:31:45,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7127040. Throughput: 0: 281.6. Samples: 7139552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:31:45,944][166323] Avg episode reward: [(0, '1276.848')]
[36m[2025-07-02 03:31:50,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7127040. Throughput: 0: 282.0. Samples: 7140448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:31:50,973][166323] Avg episode reward: [(0, '1272.970')]
[36m[2025-07-02 03:31:55,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7127040. Throughput: 0: 282.0. Samples: 7142064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:31:55,946][166323] Avg episode reward: [(0, '1264.093')]
[36m[2025-07-02 03:32:00,989][166323] Fps is (10 sec: 1635.8, 60 sec: 273.7, 300 sec: 333.2). Total num frames: 7143424. Throughput: 0: 281.9. Samples: 7143616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:32:00,989][166323] Avg episode reward: [(0, '1227.895')]
[36m[2025-07-02 03:32:05,954][166323] Fps is (10 sec: 1637.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7143424. Throughput: 0: 278.9. Samples: 7144432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:32:05,955][166323] Avg episode reward: [(0, '1260.176')]
[36m[2025-07-02 03:32:10,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7143424. Throughput: 0: 274.9. Samples: 7146032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:32:10,997][166323] Avg episode reward: [(0, '1269.651')]
[36m[2025-07-02 03:32:15,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7143424. Throughput: 0: 273.0. Samples: 7147568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:32:15,968][166323] Avg episode reward: [(0, '1234.204')]
[36m[2025-07-02 03:32:20,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7143424. Throughput: 0: 272.7. Samples: 7148416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:32:20,956][166323] Avg episode reward: [(0, '1240.469')]
[36m[2025-07-02 03:32:25,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7143424. Throughput: 0: 268.5. Samples: 7150048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:32:25,960][166323] Avg episode reward: [(0, '1205.004')]
[36m[2025-07-02 03:32:30,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7143424. Throughput: 0: 269.5. Samples: 7151680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:32:30,951][166323] Avg episode reward: [(0, '1219.551')]
[31m[25141588 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[25141588 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[25141588 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:32:35,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7143424. Throughput: 0: 267.4. Samples: 7152480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:32:35,974][166323] Avg episode reward: [(0, '1221.103')]
[36m[2025-07-02 03:32:41,024][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 7143424. Throughput: 0: 270.5. Samples: 7154256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:32:41,025][166323] Avg episode reward: [(0, '1266.627')]
[31m[25151170 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[25151170 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[25151170 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:32:45,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7143424. Throughput: 0: 269.5. Samples: 7155744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:32:45,985][166323] Avg episode reward: [(0, '1205.575')]
[36m[2025-07-02 03:32:50,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7143424. Throughput: 0: 266.3. Samples: 7156416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:32:50,962][166323] Avg episode reward: [(0, '1235.712')]
[36m[2025-07-02 03:32:55,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7143424. Throughput: 0: 268.0. Samples: 7158080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:32:55,956][166323] Avg episode reward: [(0, '1252.501')]
[31m[25165746 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[25165746 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[25165746 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:33:01,001][166323] Fps is (10 sec: 1632.1, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 7159808. Throughput: 0: 270.4. Samples: 7159744. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 03:33:01,001][166323] Avg episode reward: [(0, '1236.969')]
[36m[2025-07-02 03:33:05,955][166323] Fps is (10 sec: 1638.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7159808. Throughput: 0: 269.9. Samples: 7160560. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 03:33:05,956][166323] Avg episode reward: [(0, '1242.045')]
[36m[2025-07-02 03:33:10,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7159808. Throughput: 0: 271.5. Samples: 7162272. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 03:33:10,976][166323] Avg episode reward: [(0, '1301.624')]
[36m[2025-07-02 03:33:15,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7159808. Throughput: 0: 275.9. Samples: 7164096. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 03:33:15,947][166323] Avg episode reward: [(0, '1238.828')]
[36m[2025-07-02 03:33:20,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7159808. Throughput: 0: 277.5. Samples: 7164960. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 03:33:20,952][166323] Avg episode reward: [(0, '1308.416')]
[36m[2025-07-02 03:33:25,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7159808. Throughput: 0: 275.8. Samples: 7166656. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 03:33:25,985][166323] Avg episode reward: [(0, '1264.962')]
[36m[2025-07-02 03:33:30,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7159808. Throughput: 0: 280.1. Samples: 7168352. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 03:33:30,995][166323] Avg episode reward: [(0, '1256.866')]
[36m[2025-07-02 03:33:35,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7159808. Throughput: 0: 281.6. Samples: 7169088. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 03:33:35,967][166323] Avg episode reward: [(0, '1256.115')]
[37m[1m[2025-07-02 03:33:36,024][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013976_7159808.pth...
[36m[2025-07-02 03:33:36,027][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013848_7094272.pth
[36m[2025-07-02 03:33:40,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 7159808. Throughput: 0: 284.1. Samples: 7170864. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 03:33:40,950][166323] Avg episode reward: [(0, '1189.381')]
[31m[25213757 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[25213757 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[25213758 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:33:45,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7159808. Throughput: 0: 281.8. Samples: 7172416. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 03:33:45,971][166323] Avg episode reward: [(0, '1157.859')]
[36m[2025-07-02 03:33:50,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7159808. Throughput: 0: 283.3. Samples: 7173312. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 03:33:50,962][166323] Avg episode reward: [(0, '1180.397')]
[31m[25224446 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[25224446 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[25224447 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:33:56,021][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 7159808. Throughput: 0: 284.2. Samples: 7175072. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 03:33:56,021][166323] Avg episode reward: [(0, '1126.366')]
[36m[2025-07-02 03:34:00,953][166323] Fps is (10 sec: 1639.9, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7176192. Throughput: 0: 278.7. Samples: 7176640. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 03:34:00,953][166323] Avg episode reward: [(0, '1141.031')]
[36m[2025-07-02 03:34:05,982][166323] Fps is (10 sec: 1644.9, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7176192. Throughput: 0: 276.4. Samples: 7177408. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 03:34:05,982][166323] Avg episode reward: [(0, '1163.375')]
[36m[2025-07-02 03:34:11,007][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 7176192. Throughput: 0: 275.4. Samples: 7179056. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 03:34:11,007][166323] Avg episode reward: [(0, '1200.647')]
[36m[2025-07-02 03:34:15,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7176192. Throughput: 0: 273.3. Samples: 7180640. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 03:34:15,964][166323] Avg episode reward: [(0, '1236.418')]
[36m[2025-07-02 03:34:20,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7176192. Throughput: 0: 276.2. Samples: 7181520. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 03:34:20,970][166323] Avg episode reward: [(0, '1277.918')]
[31m[25253917 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[25253917 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[25253918 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:34:26,013][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 7176192. Throughput: 0: 274.8. Samples: 7183248. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 03:34:26,014][166323] Avg episode reward: [(0, '1192.096')]
[36m[2025-07-02 03:34:30,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7176192. Throughput: 0: 276.6. Samples: 7184864. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 03:34:30,976][166323] Avg episode reward: [(0, '1185.774')]
[31m[25259879 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[25259880 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[25259880 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:34:35,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7176192. Throughput: 0: 277.1. Samples: 7185776. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 03:34:35,944][166323] Avg episode reward: [(0, '1174.564')]
[36m[2025-07-02 03:34:40,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7176192. Throughput: 0: 274.1. Samples: 7187392. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 03:34:40,972][166323] Avg episode reward: [(0, '1219.158')]
[36m[2025-07-02 03:34:45,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7176192. Throughput: 0: 276.7. Samples: 7189088. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 03:34:45,945][166323] Avg episode reward: [(0, '1173.673')]
[36m[2025-07-02 03:34:50,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7176192. Throughput: 0: 275.9. Samples: 7189824. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 03:34:50,989][166323] Avg episode reward: [(0, '1143.650')]
[36m[2025-07-02 03:34:55,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 7176192. Throughput: 0: 275.5. Samples: 7191440. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 03:34:55,957][166323] Avg episode reward: [(0, '1172.092')]
[36m[2025-07-02 03:35:00,990][166323] Fps is (10 sec: 1638.1, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7192576. Throughput: 0: 278.2. Samples: 7193168. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 03:35:00,991][166323] Avg episode reward: [(0, '1221.632')]
[36m[2025-07-02 03:35:05,984][166323] Fps is (10 sec: 1633.9, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7192576. Throughput: 0: 276.5. Samples: 7193968. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 03:35:05,984][166323] Avg episode reward: [(0, '1277.248')]
[36m[2025-07-02 03:35:10,995][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7192576. Throughput: 0: 278.2. Samples: 7195760. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 03:35:10,996][166323] Avg episode reward: [(0, '1244.877')]
[36m[2025-07-02 03:35:15,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7192576. Throughput: 0: 279.7. Samples: 7197456. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 03:35:15,997][166323] Avg episode reward: [(0, '1250.547')]
[36m[2025-07-02 03:35:20,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7192576. Throughput: 0: 276.8. Samples: 7198240. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 03:35:20,964][166323] Avg episode reward: [(0, '1322.907')]
[36m[2025-07-02 03:35:25,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7192576. Throughput: 0: 274.8. Samples: 7199760. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 03:35:25,974][166323] Avg episode reward: [(0, '1313.522')]
[36m[2025-07-02 03:35:30,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7192576. Throughput: 0: 273.6. Samples: 7201408. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 03:35:30,979][166323] Avg episode reward: [(0, '1357.578')]
[36m[2025-07-02 03:35:35,979][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7192576. Throughput: 0: 275.3. Samples: 7202208. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 03:35:35,979][166323] Avg episode reward: [(0, '1337.796')]
[37m[1m[2025-07-02 03:35:36,056][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000014040_7192576.pth...
[36m[2025-07-02 03:35:36,060][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013912_7127040.pth
[36m[2025-07-02 03:35:40,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7192576. Throughput: 0: 276.1. Samples: 7203872. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 03:35:40,983][166323] Avg episode reward: [(0, '1342.699')]
[36m[2025-07-02 03:35:45,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7192576. Throughput: 0: 274.7. Samples: 7205520. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 03:35:45,959][166323] Avg episode reward: [(0, '1312.317')]
[36m[2025-07-02 03:35:50,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7192576. Throughput: 0: 277.2. Samples: 7206432. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 03:35:50,951][166323] Avg episode reward: [(0, '1313.376')]
[36m[2025-07-02 03:35:55,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.3). Total num frames: 7192576. Throughput: 0: 276.9. Samples: 7208208. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 03:35:55,951][166323] Avg episode reward: [(0, '1278.524')]
[36m[2025-07-02 03:36:00,960][166323] Fps is (10 sec: 1637.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7208960. Throughput: 0: 276.1. Samples: 7209872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:36:00,960][166323] Avg episode reward: [(0, '1299.922')]
[36m[2025-07-02 03:36:05,980][166323] Fps is (10 sec: 1633.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7208960. Throughput: 0: 280.4. Samples: 7210864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:36:05,981][166323] Avg episode reward: [(0, '1309.239')]
[36m[2025-07-02 03:36:10,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7208960. Throughput: 0: 281.9. Samples: 7212448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:36:10,986][166323] Avg episode reward: [(0, '1303.472')]
[36m[2025-07-02 03:36:15,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7208960. Throughput: 0: 280.9. Samples: 7214048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:36:15,976][166323] Avg episode reward: [(0, '1281.827')]
[31m[25368168 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[25368168 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[25368169 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:36:20,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7208960. Throughput: 0: 281.8. Samples: 7214880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:36:20,947][166323] Avg episode reward: [(0, '1293.387')]
[36m[2025-07-02 03:36:25,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7208960. Throughput: 0: 280.6. Samples: 7216496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:36:25,970][166323] Avg episode reward: [(0, '1287.314')]
[36m[2025-07-02 03:36:30,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7208960. Throughput: 0: 281.1. Samples: 7218176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:36:30,989][166323] Avg episode reward: [(0, '1229.841')]
[36m[2025-07-02 03:36:35,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7208960. Throughput: 0: 282.8. Samples: 7219168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:36:35,983][166323] Avg episode reward: [(0, '1256.266')]
[36m[2025-07-02 03:36:40,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7208960. Throughput: 0: 281.9. Samples: 7220896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:36:40,961][166323] Avg episode reward: [(0, '1259.760')]
[33m[25389675 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[25389676 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.8642578125
[33mCrash Rate: 0.12744140625
[33mTimeout Rate: 0.00830078125 (navigation_task.py:265)
[33m[25389676 ms][navigation_task] - WARNING : 
[33mSuccesses: 1770
[33mCrashes : 261
[33mTimeouts: 17 (navigation_task.py:268)
[36m[2025-07-02 03:36:45,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7208960. Throughput: 0: 284.3. Samples: 7222672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:36:45,980][166323] Avg episode reward: [(0, '1237.092')]
[36m[2025-07-02 03:36:50,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7208960. Throughput: 0: 280.0. Samples: 7223456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:36:50,947][166323] Avg episode reward: [(0, '1204.272')]
[36m[2025-07-02 03:36:55,945][166323] Fps is (10 sec: 1644.0, 60 sec: 546.2, 300 sec: 277.7). Total num frames: 7225344. Throughput: 0: 285.4. Samples: 7225280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:36:55,946][166323] Avg episode reward: [(0, '1210.017')]
[36m[2025-07-02 03:37:00,960][166323] Fps is (10 sec: 1636.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7225344. Throughput: 0: 286.7. Samples: 7226944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:37:00,960][166323] Avg episode reward: [(0, '1271.246')]
[36m[2025-07-02 03:37:06,005][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7225344. Throughput: 0: 286.6. Samples: 7227792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:37:06,005][166323] Avg episode reward: [(0, '1225.348')]
[36m[2025-07-02 03:37:10,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7225344. Throughput: 0: 286.6. Samples: 7229392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:37:10,970][166323] Avg episode reward: [(0, '1184.297')]
[36m[2025-07-02 03:37:15,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7225344. Throughput: 0: 285.4. Samples: 7231008. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:37:15,946][166323] Avg episode reward: [(0, '1173.422')]
[36m[2025-07-02 03:37:20,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7225344. Throughput: 0: 281.0. Samples: 7231808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:37:20,959][166323] Avg episode reward: [(0, '1190.788')]
[36m[2025-07-02 03:37:25,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7225344. Throughput: 0: 282.0. Samples: 7233584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:37:25,960][166323] Avg episode reward: [(0, '1199.463')]
[36m[2025-07-02 03:37:30,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7225344. Throughput: 0: 282.8. Samples: 7235392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:37:30,959][166323] Avg episode reward: [(0, '1152.893')]
[36m[2025-07-02 03:37:35,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7225344. Throughput: 0: 285.0. Samples: 7236288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:37:35,976][166323] Avg episode reward: [(0, '1152.904')]
[37m[1m[2025-07-02 03:37:36,063][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000014104_7225344.pth...
[36m[2025-07-02 03:37:36,067][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000013976_7159808.pth
[36m[2025-07-02 03:37:40,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7225344. Throughput: 0: 278.7. Samples: 7237824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:37:40,949][166323] Avg episode reward: [(0, '1222.339')]
[36m[2025-07-02 03:37:45,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7225344. Throughput: 0: 280.5. Samples: 7239568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:37:45,959][166323] Avg episode reward: [(0, '1165.118')]
[36m[2025-07-02 03:37:50,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7225344. Throughput: 0: 280.8. Samples: 7240416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:37:50,970][166323] Avg episode reward: [(0, '1142.828')]
[36m[2025-07-02 03:37:56,031][166323] Fps is (10 sec: 1626.6, 60 sec: 272.7, 300 sec: 277.7). Total num frames: 7241728. Throughput: 0: 281.9. Samples: 7242096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:37:56,031][166323] Avg episode reward: [(0, '1140.344')]
[36m[2025-07-02 03:38:00,957][166323] Fps is (10 sec: 1640.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7241728. Throughput: 0: 284.4. Samples: 7243808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:38:00,957][166323] Avg episode reward: [(0, '1118.662')]
[36m[2025-07-02 03:38:05,995][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7241728. Throughput: 0: 287.4. Samples: 7244752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:38:05,995][166323] Avg episode reward: [(0, '1080.484')]
[36m[2025-07-02 03:38:10,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7241728. Throughput: 0: 288.1. Samples: 7246544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:38:10,949][166323] Avg episode reward: [(0, '1083.007')]
[36m[2025-07-02 03:38:15,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7241728. Throughput: 0: 285.1. Samples: 7248224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:38:15,968][166323] Avg episode reward: [(0, '1130.640')]
[36m[2025-07-02 03:38:20,998][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7241728. Throughput: 0: 286.1. Samples: 7249168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:38:20,999][166323] Avg episode reward: [(0, '1059.984')]
[36m[2025-07-02 03:38:25,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7241728. Throughput: 0: 289.4. Samples: 7250848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:38:25,956][166323] Avg episode reward: [(0, '1165.387')]
[36m[2025-07-02 03:38:30,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7241728. Throughput: 0: 289.4. Samples: 7252592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:38:30,968][166323] Avg episode reward: [(0, '1186.880')]
[36m[2025-07-02 03:38:35,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7241728. Throughput: 0: 289.4. Samples: 7253440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:38:35,979][166323] Avg episode reward: [(0, '1222.886')]
[36m[2025-07-02 03:38:40,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7241728. Throughput: 0: 291.7. Samples: 7255200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:38:40,947][166323] Avg episode reward: [(0, '1214.443')]
[36m[2025-07-02 03:38:45,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7241728. Throughput: 0: 289.3. Samples: 7256832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:38:45,984][166323] Avg episode reward: [(0, '1241.482')]
[36m[2025-07-02 03:38:50,990][166323] Fps is (10 sec: 1631.4, 60 sec: 545.9, 300 sec: 333.3). Total num frames: 7258112. Throughput: 0: 287.3. Samples: 7257680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:38:50,990][166323] Avg episode reward: [(0, '1261.514')]
[36m[2025-07-02 03:38:55,982][166323] Fps is (10 sec: 1638.6, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7258112. Throughput: 0: 284.6. Samples: 7259360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:38:55,983][166323] Avg episode reward: [(0, '1287.649')]
[36m[2025-07-02 03:39:01,007][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 7258112. Throughput: 0: 283.8. Samples: 7261008. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:39:01,007][166323] Avg episode reward: [(0, '1308.196')]
[36m[2025-07-02 03:39:05,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.8). Total num frames: 7258112. Throughput: 0: 281.2. Samples: 7261808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:39:05,948][166323] Avg episode reward: [(0, '1259.715')]
[36m[2025-07-02 03:39:10,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7258112. Throughput: 0: 279.9. Samples: 7263440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:39:10,947][166323] Avg episode reward: [(0, '1255.949')]
[36m[2025-07-02 03:39:15,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7258112. Throughput: 0: 281.1. Samples: 7265248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:39:15,986][166323] Avg episode reward: [(0, '1247.761')]
[36m[2025-07-02 03:39:20,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7258112. Throughput: 0: 281.0. Samples: 7266080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:39:20,966][166323] Avg episode reward: [(0, '1161.957')]
[36m[2025-07-02 03:39:25,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7258112. Throughput: 0: 277.6. Samples: 7267696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:39:25,968][166323] Avg episode reward: [(0, '1156.604')]
[36m[2025-07-02 03:39:30,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7258112. Throughput: 0: 277.0. Samples: 7269296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:39:30,978][166323] Avg episode reward: [(0, '1109.825')]
[36m[2025-07-02 03:39:35,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7258112. Throughput: 0: 276.3. Samples: 7270112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:39:35,982][166323] Avg episode reward: [(0, '1135.329')]
[37m[1m[2025-07-02 03:39:36,032][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000014168_7258112.pth...
[36m[2025-07-02 03:39:36,036][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000014040_7192576.pth
[36m[2025-07-02 03:39:40,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7258112. Throughput: 0: 276.7. Samples: 7271808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:39:40,964][166323] Avg episode reward: [(0, '1170.288')]
[36m[2025-07-02 03:39:45,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7258112. Throughput: 0: 277.2. Samples: 7273472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:39:45,971][166323] Avg episode reward: [(0, '1181.998')]
[36m[2025-07-02 03:39:50,945][166323] Fps is (10 sec: 1641.5, 60 sec: 273.3, 300 sec: 333.2). Total num frames: 7274496. Throughput: 0: 275.9. Samples: 7274224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:39:50,945][166323] Avg episode reward: [(0, '1184.795')]
[31m[25581690 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[25581690 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[25581690 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:39:55,964][166323] Fps is (10 sec: 1639.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7274496. Throughput: 0: 276.2. Samples: 7275872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:39:55,964][166323] Avg episode reward: [(0, '1225.281')]
[36m[2025-07-02 03:40:00,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7274496. Throughput: 0: 272.6. Samples: 7277504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:40:00,949][166323] Avg episode reward: [(0, '1255.895')]
[36m[2025-07-02 03:40:06,027][166323] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 277.7). Total num frames: 7274496. Throughput: 0: 269.5. Samples: 7278224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:40:06,028][166323] Avg episode reward: [(0, '1284.491')]
[36m[2025-07-02 03:40:10,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7274496. Throughput: 0: 270.3. Samples: 7279856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:40:10,962][166323] Avg episode reward: [(0, '1219.941')]
[36m[2025-07-02 03:40:15,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7274496. Throughput: 0: 273.9. Samples: 7281616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:40:15,951][166323] Avg episode reward: [(0, '1196.099')]
[36m[2025-07-02 03:40:20,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7274496. Throughput: 0: 274.6. Samples: 7282464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:40:20,969][166323] Avg episode reward: [(0, '1181.608')]
[31m[25611276 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[25611277 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[25611277 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:40:25,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7274496. Throughput: 0: 276.2. Samples: 7284240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:40:25,969][166323] Avg episode reward: [(0, '1198.432')]
[36m[2025-07-02 03:40:30,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7274496. Throughput: 0: 277.4. Samples: 7285952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:40:30,968][166323] Avg episode reward: [(0, '1234.656')]
[36m[2025-07-02 03:40:35,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7274496. Throughput: 0: 280.4. Samples: 7286848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:40:35,969][166323] Avg episode reward: [(0, '1215.936')]
[36m[2025-07-02 03:40:40,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7274496. Throughput: 0: 281.9. Samples: 7288560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:40:40,979][166323] Avg episode reward: [(0, '1292.368')]
[36m[2025-07-02 03:40:45,998][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7274496. Throughput: 0: 279.5. Samples: 7290096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:40:45,998][166323] Avg episode reward: [(0, '1343.538')]
[36m[2025-07-02 03:40:50,988][166323] Fps is (10 sec: 1636.8, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 7290880. Throughput: 0: 280.1. Samples: 7290816. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:40:50,988][166323] Avg episode reward: [(0, '1290.830')]
[36m[2025-07-02 03:40:55,947][166323] Fps is (10 sec: 1646.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7290880. Throughput: 0: 279.9. Samples: 7292448. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:40:55,947][166323] Avg episode reward: [(0, '1291.648')]
[36m[2025-07-02 03:41:01,002][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 7290880. Throughput: 0: 279.5. Samples: 7294208. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:41:01,002][166323] Avg episode reward: [(0, '1256.648')]
[36m[2025-07-02 03:41:05,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7290880. Throughput: 0: 279.4. Samples: 7295040. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:41:05,986][166323] Avg episode reward: [(0, '1217.133')]
[36m[2025-07-02 03:41:11,007][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7290880. Throughput: 0: 277.4. Samples: 7296736. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:41:11,008][166323] Avg episode reward: [(0, '1246.973')]
[36m[2025-07-02 03:41:15,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7290880. Throughput: 0: 273.3. Samples: 7298256. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:41:15,986][166323] Avg episode reward: [(0, '1219.820')]
[36m[2025-07-02 03:41:20,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7290880. Throughput: 0: 272.9. Samples: 7299136. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:41:20,994][166323] Avg episode reward: [(0, '1230.630')]
[36m[2025-07-02 03:41:25,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7290880. Throughput: 0: 269.4. Samples: 7300688. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:41:25,991][166323] Avg episode reward: [(0, '1277.922')]
[36m[2025-07-02 03:41:30,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7290880. Throughput: 0: 270.6. Samples: 7302272. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:41:30,989][166323] Avg episode reward: [(0, '1230.132')]
[36m[2025-07-02 03:41:35,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7290880. Throughput: 0: 273.9. Samples: 7303136. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:41:35,975][166323] Avg episode reward: [(0, '1303.423')]
[37m[1m[2025-07-02 03:41:36,028][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000014232_7290880.pth...
[36m[2025-07-02 03:41:36,032][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000014104_7225344.pth
[36m[2025-07-02 03:41:40,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7290880. Throughput: 0: 273.2. Samples: 7304752. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:41:40,988][166323] Avg episode reward: [(0, '1310.241')]
[36m[2025-07-02 03:41:45,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7290880. Throughput: 0: 274.8. Samples: 7306560. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 03:41:45,948][166323] Avg episode reward: [(0, '1313.827')]
[36m[2025-07-02 03:41:50,945][166323] Fps is (10 sec: 1645.5, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7307264. Throughput: 0: 274.4. Samples: 7307376. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 03:41:50,945][166323] Avg episode reward: [(0, '1347.158')]
[36m[2025-07-02 03:41:55,977][166323] Fps is (10 sec: 1633.7, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7307264. Throughput: 0: 274.7. Samples: 7309088. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 03:41:55,977][166323] Avg episode reward: [(0, '1339.380')]
[36m[2025-07-02 03:42:00,997][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7307264. Throughput: 0: 277.3. Samples: 7310736. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 03:42:00,998][166323] Avg episode reward: [(0, '1317.612')]
[36m[2025-07-02 03:42:06,016][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7307264. Throughput: 0: 273.3. Samples: 7311440. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 03:42:06,016][166323] Avg episode reward: [(0, '1284.114')]
[36m[2025-07-02 03:42:10,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 7307264. Throughput: 0: 276.6. Samples: 7313120. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 03:42:10,943][166323] Avg episode reward: [(0, '1253.247')]
[36m[2025-07-02 03:42:15,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7307264. Throughput: 0: 277.2. Samples: 7314736. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 03:42:15,947][166323] Avg episode reward: [(0, '1211.292')]
[36m[2025-07-02 03:42:20,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7307264. Throughput: 0: 278.4. Samples: 7315664. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 03:42:20,975][166323] Avg episode reward: [(0, '1193.035')]
[36m[2025-07-02 03:42:25,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7307264. Throughput: 0: 280.1. Samples: 7317344. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 03:42:25,945][166323] Avg episode reward: [(0, '1181.451')]
[36m[2025-07-02 03:42:30,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7307264. Throughput: 0: 276.3. Samples: 7318992. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 03:42:30,950][166323] Avg episode reward: [(0, '1160.225')]
[36m[2025-07-02 03:42:35,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7307264. Throughput: 0: 276.4. Samples: 7319824. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 03:42:35,977][166323] Avg episode reward: [(0, '1196.448')]
[36m[2025-07-02 03:42:40,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7307264. Throughput: 0: 272.4. Samples: 7321344. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 03:42:40,966][166323] Avg episode reward: [(0, '1208.942')]
[31m[25754338 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[25754339 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[25754339 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:42:45,974][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7307264. Throughput: 0: 273.6. Samples: 7323040. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 03:42:45,974][166323] Avg episode reward: [(0, '1148.578')]
[36m[2025-07-02 03:42:50,964][166323] Fps is (10 sec: 1638.7, 60 sec: 273.0, 300 sec: 277.8). Total num frames: 7323648. Throughput: 0: 276.2. Samples: 7323856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:42:50,964][166323] Avg episode reward: [(0, '1172.957')]
[36m[2025-07-02 03:42:55,957][166323] Fps is (10 sec: 1641.1, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7323648. Throughput: 0: 276.9. Samples: 7325584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:42:55,958][166323] Avg episode reward: [(0, '1107.044')]
[36m[2025-07-02 03:43:00,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7323648. Throughput: 0: 280.0. Samples: 7327344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:43:00,974][166323] Avg episode reward: [(0, '1119.814')]
[36m[2025-07-02 03:43:05,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7323648. Throughput: 0: 279.1. Samples: 7328224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:43:05,973][166323] Avg episode reward: [(0, '1187.488')]
[36m[2025-07-02 03:43:10,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7323648. Throughput: 0: 277.3. Samples: 7329824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:43:10,956][166323] Avg episode reward: [(0, '1175.981')]
[31m[25781063 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[25781064 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[25781064 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:43:15,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7323648. Throughput: 0: 278.1. Samples: 7331520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:43:15,992][166323] Avg episode reward: [(0, '1172.806')]
[36m[2025-07-02 03:43:20,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7323648. Throughput: 0: 278.0. Samples: 7332336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:43:20,983][166323] Avg episode reward: [(0, '1225.009')]
[36m[2025-07-02 03:43:25,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7323648. Throughput: 0: 280.7. Samples: 7333968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:43:25,945][166323] Avg episode reward: [(0, '1214.670')]
[36m[2025-07-02 03:43:30,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7323648. Throughput: 0: 281.8. Samples: 7335712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:43:30,949][166323] Avg episode reward: [(0, '1271.779')]
[36m[2025-07-02 03:43:35,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7323648. Throughput: 0: 281.1. Samples: 7336512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:43:35,981][166323] Avg episode reward: [(0, '1287.080')]
[37m[1m[2025-07-02 03:43:36,043][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000014296_7323648.pth...
[36m[2025-07-02 03:43:36,047][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000014168_7258112.pth
[36m[2025-07-02 03:43:40,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7323648. Throughput: 0: 278.8. Samples: 7338128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:43:40,958][166323] Avg episode reward: [(0, '1262.445')]
[36m[2025-07-02 03:43:46,014][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 7323648. Throughput: 0: 278.1. Samples: 7339872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:43:46,014][166323] Avg episode reward: [(0, '1236.519')]
[36m[2025-07-02 03:43:50,969][166323] Fps is (10 sec: 1636.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7340032. Throughput: 0: 275.2. Samples: 7340608. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:43:50,969][166323] Avg episode reward: [(0, '1211.786')]
[36m[2025-07-02 03:43:55,962][166323] Fps is (10 sec: 1647.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7340032. Throughput: 0: 274.8. Samples: 7342192. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:43:55,962][166323] Avg episode reward: [(0, '1193.556')]
[36m[2025-07-02 03:44:00,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7340032. Throughput: 0: 274.1. Samples: 7343856. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:44:00,991][166323] Avg episode reward: [(0, '1151.960')]
[36m[2025-07-02 03:44:05,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7340032. Throughput: 0: 275.6. Samples: 7344736. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:44:05,970][166323] Avg episode reward: [(0, '1165.055')]
[36m[2025-07-02 03:44:10,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7340032. Throughput: 0: 273.7. Samples: 7346288. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:44:10,950][166323] Avg episode reward: [(0, '1148.125')]
[36m[2025-07-02 03:44:15,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7340032. Throughput: 0: 271.9. Samples: 7347952. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:44:15,958][166323] Avg episode reward: [(0, '1121.768')]
[36m[2025-07-02 03:44:20,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7340032. Throughput: 0: 272.2. Samples: 7348752. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:44:20,953][166323] Avg episode reward: [(0, '1127.789')]
[36m[2025-07-02 03:44:25,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7340032. Throughput: 0: 272.0. Samples: 7350368. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:44:25,953][166323] Avg episode reward: [(0, '1180.864')]
[36m[2025-07-02 03:44:30,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7340032. Throughput: 0: 271.2. Samples: 7352064. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:44:30,967][166323] Avg episode reward: [(0, '1176.988')]
[36m[2025-07-02 03:44:35,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7340032. Throughput: 0: 270.8. Samples: 7352800. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:44:35,987][166323] Avg episode reward: [(0, '1123.248')]
[36m[2025-07-02 03:44:40,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7340032. Throughput: 0: 272.5. Samples: 7354464. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:44:40,993][166323] Avg episode reward: [(0, '1151.158')]
[36m[2025-07-02 03:44:45,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 222.1). Total num frames: 7340032. Throughput: 0: 272.5. Samples: 7356112. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:44:45,965][166323] Avg episode reward: [(0, '1134.624')]
[36m[2025-07-02 03:44:50,960][166323] Fps is (10 sec: 1643.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7356416. Throughput: 0: 269.6. Samples: 7356864. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 03:44:50,960][166323] Avg episode reward: [(0, '1222.281')]
[36m[2025-07-02 03:44:56,003][166323] Fps is (10 sec: 1632.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 7356416. Throughput: 0: 273.5. Samples: 7358608. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 03:44:56,004][166323] Avg episode reward: [(0, '1249.798')]
[36m[2025-07-02 03:45:00,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 7356416. Throughput: 0: 274.4. Samples: 7360304. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 03:45:00,967][166323] Avg episode reward: [(0, '1235.063')]
[36m[2025-07-02 03:45:05,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7356416. Throughput: 0: 275.6. Samples: 7361152. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 03:45:05,954][166323] Avg episode reward: [(0, '1263.214')]
[36m[2025-07-02 03:45:10,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7356416. Throughput: 0: 279.7. Samples: 7362960. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 03:45:10,976][166323] Avg episode reward: [(0, '1254.059')]
[36m[2025-07-02 03:45:15,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7356416. Throughput: 0: 279.1. Samples: 7364624. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 03:45:15,967][166323] Avg episode reward: [(0, '1252.768')]
[36m[2025-07-02 03:45:20,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7356416. Throughput: 0: 282.6. Samples: 7365504. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 03:45:20,946][166323] Avg episode reward: [(0, '1247.201')]
[36m[2025-07-02 03:45:25,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7356416. Throughput: 0: 282.1. Samples: 7367152. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 03:45:25,976][166323] Avg episode reward: [(0, '1222.508')]
[36m[2025-07-02 03:45:30,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7356416. Throughput: 0: 281.7. Samples: 7368784. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 03:45:30,949][166323] Avg episode reward: [(0, '1205.401')]
[36m[2025-07-02 03:45:35,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7356416. Throughput: 0: 282.2. Samples: 7369568. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 03:45:35,983][166323] Avg episode reward: [(0, '1248.966')]
[37m[1m[2025-07-02 03:45:36,079][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000014360_7356416.pth...
[36m[2025-07-02 03:45:36,088][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000014232_7290880.pth
[36m[2025-07-02 03:45:40,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7356416. Throughput: 0: 280.8. Samples: 7371232. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 03:45:40,965][166323] Avg episode reward: [(0, '1236.350')]
[31m[25929843 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[25929844 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[25929844 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:45:45,949][166323] Fps is (10 sec: 1643.9, 60 sec: 546.3, 300 sec: 277.7). Total num frames: 7372800. Throughput: 0: 280.3. Samples: 7372912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:45:45,949][166323] Avg episode reward: [(0, '1300.468')]
[36m[2025-07-02 03:45:50,978][166323] Fps is (10 sec: 1636.3, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7372800. Throughput: 0: 281.4. Samples: 7373824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:45:50,978][166323] Avg episode reward: [(0, '1309.925')]
[36m[2025-07-02 03:45:56,001][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7372800. Throughput: 0: 275.0. Samples: 7375344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:45:56,002][166323] Avg episode reward: [(0, '1299.522')]
[36m[2025-07-02 03:46:00,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7372800. Throughput: 0: 275.8. Samples: 7377040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:46:00,981][166323] Avg episode reward: [(0, '1284.323')]
[36m[2025-07-02 03:46:06,023][166323] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 277.7). Total num frames: 7372800. Throughput: 0: 275.8. Samples: 7377936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:46:06,023][166323] Avg episode reward: [(0, '1338.819')]
[36m[2025-07-02 03:46:10,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7372800. Throughput: 0: 276.6. Samples: 7379600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:46:10,980][166323] Avg episode reward: [(0, '1228.371')]
[36m[2025-07-02 03:46:15,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7372800. Throughput: 0: 276.9. Samples: 7381248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:46:15,956][166323] Avg episode reward: [(0, '1236.852')]
[36m[2025-07-02 03:46:20,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7372800. Throughput: 0: 276.0. Samples: 7381984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:46:20,963][166323] Avg episode reward: [(0, '1201.983')]
[36m[2025-07-02 03:46:25,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7372800. Throughput: 0: 276.3. Samples: 7383664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:46:25,963][166323] Avg episode reward: [(0, '1197.243')]
[36m[2025-07-02 03:46:30,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7372800. Throughput: 0: 279.1. Samples: 7385472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:46:30,957][166323] Avg episode reward: [(0, '1205.568')]
[36m[2025-07-02 03:46:35,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7372800. Throughput: 0: 278.0. Samples: 7386336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:46:35,978][166323] Avg episode reward: [(0, '1163.586')]
[36m[2025-07-02 03:46:40,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7372800. Throughput: 0: 281.9. Samples: 7388016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:46:40,960][166323] Avg episode reward: [(0, '1182.123')]
[36m[2025-07-02 03:46:45,960][166323] Fps is (10 sec: 1641.3, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7389184. Throughput: 0: 280.3. Samples: 7389648. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 03:46:45,961][166323] Avg episode reward: [(0, '1245.875')]
[36m[2025-07-02 03:46:50,944][166323] Fps is (10 sec: 1641.1, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7389184. Throughput: 0: 280.7. Samples: 7390544. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 03:46:50,944][166323] Avg episode reward: [(0, '1246.407')]
[36m[2025-07-02 03:46:55,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7389184. Throughput: 0: 279.5. Samples: 7392176. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 03:46:55,974][166323] Avg episode reward: [(0, '1260.242')]
[36m[2025-07-02 03:47:00,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 7389184. Throughput: 0: 279.9. Samples: 7393840. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 03:47:00,945][166323] Avg episode reward: [(0, '1184.597')]
[36m[2025-07-02 03:47:05,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7389184. Throughput: 0: 280.1. Samples: 7394592. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 03:47:05,970][166323] Avg episode reward: [(0, '1220.103')]
[36m[2025-07-02 03:47:10,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7389184. Throughput: 0: 278.9. Samples: 7396208. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 03:47:10,943][166323] Avg episode reward: [(0, '1252.910')]
[36m[2025-07-02 03:47:15,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7389184. Throughput: 0: 274.5. Samples: 7397824. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 03:47:15,959][166323] Avg episode reward: [(0, '1228.578')]
[36m[2025-07-02 03:47:20,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7389184. Throughput: 0: 273.1. Samples: 7398624. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 03:47:20,978][166323] Avg episode reward: [(0, '1202.320')]
[36m[2025-07-02 03:47:25,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7389184. Throughput: 0: 270.8. Samples: 7400208. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 03:47:25,981][166323] Avg episode reward: [(0, '1169.543')]
[36m[2025-07-02 03:47:30,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7389184. Throughput: 0: 272.9. Samples: 7401936. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 03:47:30,991][166323] Avg episode reward: [(0, '1205.454')]
[31m[26041357 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[26041357 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[26041357 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:47:35,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7389184. Throughput: 0: 272.4. Samples: 7402816. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 03:47:35,990][166323] Avg episode reward: [(0, '1192.413')]
[37m[1m[2025-07-02 03:47:36,040][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000014424_7389184.pth...
[36m[2025-07-02 03:47:36,044][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000014296_7323648.pth
[36m[2025-07-02 03:47:40,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7389184. Throughput: 0: 272.6. Samples: 7404448. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 03:47:40,985][166323] Avg episode reward: [(0, '1172.703')]
[36m[2025-07-02 03:47:45,950][166323] Fps is (10 sec: 1644.9, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7405568. Throughput: 0: 272.7. Samples: 7406112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:47:45,950][166323] Avg episode reward: [(0, '1176.825')]
[36m[2025-07-02 03:47:50,985][166323] Fps is (10 sec: 1638.3, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7405568. Throughput: 0: 274.7. Samples: 7406960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:47:50,985][166323] Avg episode reward: [(0, '1161.958')]
[36m[2025-07-02 03:47:55,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7405568. Throughput: 0: 274.0. Samples: 7408544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:47:55,958][166323] Avg episode reward: [(0, '1207.546')]
[33m[26066313 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[26066313 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.8544921875
[33mCrash Rate: 0.1376953125
[33mTimeout Rate: 0.0078125 (navigation_task.py:265)
[33m[26066314 ms][navigation_task] - WARNING : 
[33mSuccesses: 1750
[33mCrashes : 282
[33mTimeouts: 16 (navigation_task.py:268)
[36m[2025-07-02 03:48:00,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7405568. Throughput: 0: 275.9. Samples: 7410240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:48:00,953][166323] Avg episode reward: [(0, '1245.717')]
[36m[2025-07-02 03:48:05,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7405568. Throughput: 0: 278.3. Samples: 7411152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:48:05,987][166323] Avg episode reward: [(0, '1182.095')]
[31m[26078469 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[26078469 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[26078470 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:48:10,971][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7405568. Throughput: 0: 283.4. Samples: 7412960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:48:10,971][166323] Avg episode reward: [(0, '1174.708')]
[31m[26082312 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[26082312 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[26082313 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:48:15,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7405568. Throughput: 0: 282.5. Samples: 7414640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:48:15,957][166323] Avg episode reward: [(0, '1246.268')]
[31m[26088421 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[26088421 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[26088421 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:48:20,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7405568. Throughput: 0: 283.6. Samples: 7415568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:48:20,950][166323] Avg episode reward: [(0, '1271.560')]
[36m[2025-07-02 03:48:25,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7405568. Throughput: 0: 286.9. Samples: 7417360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:48:25,983][166323] Avg episode reward: [(0, '1260.288')]
[36m[2025-07-02 03:48:30,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7405568. Throughput: 0: 285.7. Samples: 7418976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:48:30,979][166323] Avg episode reward: [(0, '1242.227')]
[36m[2025-07-02 03:48:35,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7405568. Throughput: 0: 287.8. Samples: 7419904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:48:35,964][166323] Avg episode reward: [(0, '1306.130')]
[36m[2025-07-02 03:48:40,980][166323] Fps is (10 sec: 1638.3, 60 sec: 546.2, 300 sec: 333.3). Total num frames: 7421952. Throughput: 0: 291.8. Samples: 7421680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:48:40,980][166323] Avg episode reward: [(0, '1295.500')]
[36m[2025-07-02 03:48:45,977][166323] Fps is (10 sec: 1636.3, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7421952. Throughput: 0: 290.0. Samples: 7423296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:48:45,977][166323] Avg episode reward: [(0, '1334.404')]
[36m[2025-07-02 03:48:50,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7421952. Throughput: 0: 288.2. Samples: 7424112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:48:50,962][166323] Avg episode reward: [(0, '1337.231')]
[36m[2025-07-02 03:48:55,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7421952. Throughput: 0: 283.8. Samples: 7425728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:48:55,956][166323] Avg episode reward: [(0, '1312.025')]
[36m[2025-07-02 03:49:00,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7421952. Throughput: 0: 283.1. Samples: 7427376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:49:00,944][166323] Avg episode reward: [(0, '1269.428')]
[36m[2025-07-02 03:49:05,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7421952. Throughput: 0: 281.0. Samples: 7428224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:49:05,985][166323] Avg episode reward: [(0, '1271.920')]
[36m[2025-07-02 03:49:10,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7421952. Throughput: 0: 283.1. Samples: 7430096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:49:10,969][166323] Avg episode reward: [(0, '1253.586')]
[36m[2025-07-02 03:49:15,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7421952. Throughput: 0: 282.7. Samples: 7431696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:49:15,977][166323] Avg episode reward: [(0, '1270.218')]
[36m[2025-07-02 03:49:20,977][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7421952. Throughput: 0: 279.4. Samples: 7432480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:49:20,977][166323] Avg episode reward: [(0, '1210.212')]
[36m[2025-07-02 03:49:25,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7421952. Throughput: 0: 274.3. Samples: 7434016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:49:25,945][166323] Avg episode reward: [(0, '1200.601')]
[36m[2025-07-02 03:49:30,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7421952. Throughput: 0: 278.2. Samples: 7435808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:49:30,945][166323] Avg episode reward: [(0, '1247.421')]
[36m[2025-07-02 03:49:35,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7421952. Throughput: 0: 278.2. Samples: 7436640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:49:35,994][166323] Avg episode reward: [(0, '1245.995')]
[37m[1m[2025-07-02 03:49:36,052][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000014488_7421952.pth...
[36m[2025-07-02 03:49:36,056][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000014360_7356416.pth
[36m[2025-07-02 03:49:40,965][166323] Fps is (10 sec: 1635.0, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 7438336. Throughput: 0: 280.8. Samples: 7438368. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 03:49:40,966][166323] Avg episode reward: [(0, '1254.781')]
[36m[2025-07-02 03:49:46,018][166323] Fps is (10 sec: 1634.4, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 7438336. Throughput: 0: 279.7. Samples: 7439984. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 03:49:46,018][166323] Avg episode reward: [(0, '1242.927')]
[31m[26177829 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[26177829 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[26177830 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:49:50,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7438336. Throughput: 0: 280.3. Samples: 7440832. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 03:49:50,966][166323] Avg episode reward: [(0, '1202.175')]
[36m[2025-07-02 03:49:55,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7438336. Throughput: 0: 273.5. Samples: 7442400. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 03:49:55,961][166323] Avg episode reward: [(0, '1266.955')]
[36m[2025-07-02 03:50:00,980][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7438336. Throughput: 0: 273.8. Samples: 7444016. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 03:50:00,980][166323] Avg episode reward: [(0, '1208.687')]
[36m[2025-07-02 03:50:05,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7438336. Throughput: 0: 276.9. Samples: 7444944. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 03:50:05,997][166323] Avg episode reward: [(0, '1192.253')]
[36m[2025-07-02 03:50:10,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7438336. Throughput: 0: 281.1. Samples: 7446672. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 03:50:10,971][166323] Avg episode reward: [(0, '1226.999')]
[36m[2025-07-02 03:50:15,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7438336. Throughput: 0: 277.1. Samples: 7448288. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 03:50:15,990][166323] Avg episode reward: [(0, '1196.677')]
[31m[26206128 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[26206129 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[26206129 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:50:21,000][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7438336. Throughput: 0: 278.7. Samples: 7449184. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 03:50:21,000][166323] Avg episode reward: [(0, '1177.635')]
[36m[2025-07-02 03:50:25,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7438336. Throughput: 0: 275.3. Samples: 7450752. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 03:50:25,950][166323] Avg episode reward: [(0, '1165.293')]
[36m[2025-07-02 03:50:30,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7438336. Throughput: 0: 279.1. Samples: 7452528. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 03:50:30,957][166323] Avg episode reward: [(0, '1141.615')]
[36m[2025-07-02 03:50:35,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7438336. Throughput: 0: 277.8. Samples: 7453328. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 03:50:35,957][166323] Avg episode reward: [(0, '1156.076')]
[36m[2025-07-02 03:50:40,953][166323] Fps is (10 sec: 1639.1, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7454720. Throughput: 0: 279.9. Samples: 7454992. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:50:40,953][166323] Avg episode reward: [(0, '1193.920')]
[36m[2025-07-02 03:50:45,950][166323] Fps is (10 sec: 1639.4, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 7454720. Throughput: 0: 277.9. Samples: 7456512. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:50:45,950][166323] Avg episode reward: [(0, '1219.565')]
[36m[2025-07-02 03:50:50,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7454720. Throughput: 0: 277.6. Samples: 7457424. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:50:50,959][166323] Avg episode reward: [(0, '1215.332')]
[36m[2025-07-02 03:50:55,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7454720. Throughput: 0: 274.5. Samples: 7459024. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:50:55,967][166323] Avg episode reward: [(0, '1229.809')]
[36m[2025-07-02 03:51:00,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7454720. Throughput: 0: 276.4. Samples: 7460720. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:51:00,973][166323] Avg episode reward: [(0, '1264.629')]
[36m[2025-07-02 03:51:05,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7454720. Throughput: 0: 273.8. Samples: 7461504. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:51:05,993][166323] Avg episode reward: [(0, '1325.044')]
[36m[2025-07-02 03:51:10,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7454720. Throughput: 0: 275.0. Samples: 7463136. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:51:10,991][166323] Avg episode reward: [(0, '1334.633')]
[36m[2025-07-02 03:51:15,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7454720. Throughput: 0: 272.9. Samples: 7464816. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:51:15,988][166323] Avg episode reward: [(0, '1327.461')]
[36m[2025-07-02 03:51:20,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7454720. Throughput: 0: 272.6. Samples: 7465600. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:51:20,969][166323] Avg episode reward: [(0, '1298.963')]
[31m[26274367 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[26274368 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[26274368 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:51:26,015][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 7454720. Throughput: 0: 272.7. Samples: 7467280. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:51:26,016][166323] Avg episode reward: [(0, '1312.063')]
[31m[26278361 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[26278361 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[26278361 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:51:30,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7454720. Throughput: 0: 275.9. Samples: 7468928. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:51:30,944][166323] Avg episode reward: [(0, '1298.832')]
[36m[2025-07-02 03:51:35,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7454720. Throughput: 0: 274.5. Samples: 7469776. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 03:51:35,956][166323] Avg episode reward: [(0, '1256.382')]
[37m[1m[2025-07-02 03:51:36,007][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000014552_7454720.pth...
[36m[2025-07-02 03:51:36,011][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000014424_7389184.pth
[36m[2025-07-02 03:51:40,960][166323] Fps is (10 sec: 1635.8, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7471104. Throughput: 0: 278.8. Samples: 7471568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:51:40,960][166323] Avg episode reward: [(0, '1242.767')]
[36m[2025-07-02 03:51:45,968][166323] Fps is (10 sec: 1636.5, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7471104. Throughput: 0: 278.4. Samples: 7473248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:51:45,968][166323] Avg episode reward: [(0, '1183.234')]
[36m[2025-07-02 03:51:50,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7471104. Throughput: 0: 281.8. Samples: 7474176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:51:50,969][166323] Avg episode reward: [(0, '1118.930')]
[36m[2025-07-02 03:51:55,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7471104. Throughput: 0: 283.8. Samples: 7475904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:51:55,982][166323] Avg episode reward: [(0, '1149.732')]
[31m[26308366 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[26308366 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[26308366 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:52:00,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7471104. Throughput: 0: 283.4. Samples: 7477568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:52:00,976][166323] Avg episode reward: [(0, '1193.343')]
[31m[26309684 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[26309684 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[26309684 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:52:05,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7471104. Throughput: 0: 285.2. Samples: 7478432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:52:05,967][166323] Avg episode reward: [(0, '1164.682')]
[36m[2025-07-02 03:52:10,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7471104. Throughput: 0: 286.3. Samples: 7480144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:52:10,953][166323] Avg episode reward: [(0, '1159.135')]
[36m[2025-07-02 03:52:15,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7471104. Throughput: 0: 286.2. Samples: 7481808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:52:15,950][166323] Avg episode reward: [(0, '1126.709')]
[31m[26325838 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[26325839 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[26325839 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:52:20,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7471104. Throughput: 0: 285.7. Samples: 7482640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:52:20,979][166323] Avg episode reward: [(0, '1134.444')]
[36m[2025-07-02 03:52:25,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7471104. Throughput: 0: 283.0. Samples: 7484304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:52:25,962][166323] Avg episode reward: [(0, '1104.775')]
[36m[2025-07-02 03:52:30,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7471104. Throughput: 0: 284.9. Samples: 7486064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:52:30,956][166323] Avg episode reward: [(0, '1066.976')]
[36m[2025-07-02 03:52:36,005][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 7471104. Throughput: 0: 280.3. Samples: 7486800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:52:36,006][166323] Avg episode reward: [(0, '1114.889')]
[36m[2025-07-02 03:52:41,005][166323] Fps is (10 sec: 1630.5, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 7487488. Throughput: 0: 276.5. Samples: 7488352. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 03:52:41,005][166323] Avg episode reward: [(0, '1184.187')]
[31m[26351933 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[26351933 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[26351933 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:52:45,973][166323] Fps is (10 sec: 1643.8, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7487488. Throughput: 0: 274.9. Samples: 7489936. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 03:52:45,973][166323] Avg episode reward: [(0, '1173.796')]
[36m[2025-07-02 03:52:50,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7487488. Throughput: 0: 274.2. Samples: 7490768. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 03:52:50,964][166323] Avg episode reward: [(0, '1220.039')]
[36m[2025-07-02 03:52:55,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7487488. Throughput: 0: 272.8. Samples: 7492432. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 03:52:55,990][166323] Avg episode reward: [(0, '1274.856')]
[36m[2025-07-02 03:53:00,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7487488. Throughput: 0: 274.3. Samples: 7494160. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 03:53:00,982][166323] Avg episode reward: [(0, '1303.723')]
[31m[26370660 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[26370660 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[26370660 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:53:05,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7487488. Throughput: 0: 275.3. Samples: 7495024. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 03:53:05,967][166323] Avg episode reward: [(0, '1307.650')]
[36m[2025-07-02 03:53:10,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7487488. Throughput: 0: 278.4. Samples: 7496832. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 03:53:10,961][166323] Avg episode reward: [(0, '1327.067')]
[36m[2025-07-02 03:53:15,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7487488. Throughput: 0: 275.1. Samples: 7498448. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 03:53:15,969][166323] Avg episode reward: [(0, '1325.494')]
[36m[2025-07-02 03:53:21,013][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7487488. Throughput: 0: 276.9. Samples: 7499264. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 03:53:21,013][166323] Avg episode reward: [(0, '1345.486')]
[36m[2025-07-02 03:53:25,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7487488. Throughput: 0: 281.0. Samples: 7500992. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 03:53:25,989][166323] Avg episode reward: [(0, '1387.573')]
[36m[2025-07-02 03:53:30,998][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7487488. Throughput: 0: 281.4. Samples: 7502608. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 03:53:30,998][166323] Avg episode reward: [(0, '1339.798')]
[36m[2025-07-02 03:53:35,987][166323] Fps is (10 sec: 1638.6, 60 sec: 546.3, 300 sec: 277.7). Total num frames: 7503872. Throughput: 0: 281.5. Samples: 7503440. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:53:35,988][166323] Avg episode reward: [(0, '1330.240')]
[37m[1m[2025-07-02 03:53:36,051][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000014648_7503872.pth...
[36m[2025-07-02 03:53:36,055][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000014488_7421952.pth
[36m[2025-07-02 03:53:40,981][166323] Fps is (10 sec: 1641.2, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7503872. Throughput: 0: 282.4. Samples: 7505136. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:53:40,981][166323] Avg episode reward: [(0, '1318.258')]
[36m[2025-07-02 03:53:45,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7503872. Throughput: 0: 276.2. Samples: 7506592. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:53:45,990][166323] Avg episode reward: [(0, '1312.808')]
[36m[2025-07-02 03:53:50,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7503872. Throughput: 0: 274.0. Samples: 7507360. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:53:50,981][166323] Avg episode reward: [(0, '1300.706')]
[36m[2025-07-02 03:53:55,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7503872. Throughput: 0: 269.9. Samples: 7508976. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:53:55,957][166323] Avg episode reward: [(0, '1333.567')]
[36m[2025-07-02 03:54:00,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7503872. Throughput: 0: 268.6. Samples: 7510528. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:54:00,951][166323] Avg episode reward: [(0, '1289.838')]
[36m[2025-07-02 03:54:05,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7503872. Throughput: 0: 268.4. Samples: 7511328. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:54:05,964][166323] Avg episode reward: [(0, '1348.444')]
[36m[2025-07-02 03:54:10,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7503872. Throughput: 0: 267.8. Samples: 7513040. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:54:10,986][166323] Avg episode reward: [(0, '1359.517')]
[36m[2025-07-02 03:54:15,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7503872. Throughput: 0: 269.2. Samples: 7514720. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:54:15,985][166323] Avg episode reward: [(0, '1308.518')]
[36m[2025-07-02 03:54:20,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7503872. Throughput: 0: 268.5. Samples: 7515520. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:54:20,975][166323] Avg episode reward: [(0, '1237.814')]
[36m[2025-07-02 03:54:25,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7503872. Throughput: 0: 265.8. Samples: 7517088. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:54:25,945][166323] Avg episode reward: [(0, '1192.931')]
[36m[2025-07-02 03:54:30,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7503872. Throughput: 0: 272.4. Samples: 7518848. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:54:30,984][166323] Avg episode reward: [(0, '1178.466')]
[36m[2025-07-02 03:54:35,963][166323] Fps is (10 sec: 1635.4, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7520256. Throughput: 0: 275.0. Samples: 7519728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:54:35,964][166323] Avg episode reward: [(0, '1162.318')]
[36m[2025-07-02 03:54:40,979][166323] Fps is (10 sec: 1639.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7520256. Throughput: 0: 274.0. Samples: 7521312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:54:40,980][166323] Avg episode reward: [(0, '1157.290')]
[36m[2025-07-02 03:54:45,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7520256. Throughput: 0: 280.0. Samples: 7523136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:54:45,976][166323] Avg episode reward: [(0, '1188.168')]
[36m[2025-07-02 03:54:50,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7520256. Throughput: 0: 280.8. Samples: 7523968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:54:50,972][166323] Avg episode reward: [(0, '1221.543')]
[36m[2025-07-02 03:54:55,998][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7520256. Throughput: 0: 279.4. Samples: 7525616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:54:55,999][166323] Avg episode reward: [(0, '1211.711')]
[36m[2025-07-02 03:55:01,007][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 7520256. Throughput: 0: 280.0. Samples: 7527328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:55:01,007][166323] Avg episode reward: [(0, '1210.701')]
[36m[2025-07-02 03:55:06,019][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 7520256. Throughput: 0: 279.5. Samples: 7528112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:55:06,019][166323] Avg episode reward: [(0, '1193.581')]
[36m[2025-07-02 03:55:10,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7520256. Throughput: 0: 280.7. Samples: 7529728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:55:10,982][166323] Avg episode reward: [(0, '1210.386')]
[36m[2025-07-02 03:55:15,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7520256. Throughput: 0: 279.1. Samples: 7531408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:55:15,994][166323] Avg episode reward: [(0, '1219.402')]
[36m[2025-07-02 03:55:20,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7520256. Throughput: 0: 279.2. Samples: 7532288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:55:20,950][166323] Avg episode reward: [(0, '1188.058')]
[36m[2025-07-02 03:55:26,002][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 7520256. Throughput: 0: 276.5. Samples: 7533760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:55:26,003][166323] Avg episode reward: [(0, '1220.092')]
[31m[26516352 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[26516352 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[26516353 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:55:30,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7520256. Throughput: 0: 273.6. Samples: 7535440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:55:30,949][166323] Avg episode reward: [(0, '1293.705')]
[36m[2025-07-02 03:55:35,980][166323] Fps is (10 sec: 1642.1, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7536640. Throughput: 0: 273.0. Samples: 7536256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:55:35,980][166323] Avg episode reward: [(0, '1303.423')]
[37m[1m[2025-07-02 03:55:36,030][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000014712_7536640.pth...
[36m[2025-07-02 03:55:36,034][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000014552_7454720.pth
[36m[2025-07-02 03:55:40,951][166323] Fps is (10 sec: 1638.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7536640. Throughput: 0: 273.7. Samples: 7537920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:55:40,951][166323] Avg episode reward: [(0, '1322.088')]
[36m[2025-07-02 03:55:45,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7536640. Throughput: 0: 274.9. Samples: 7539696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:55:45,993][166323] Avg episode reward: [(0, '1332.666')]
[36m[2025-07-02 03:55:50,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7536640. Throughput: 0: 274.5. Samples: 7540448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:55:50,953][166323] Avg episode reward: [(0, '1307.510')]
[36m[2025-07-02 03:55:55,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7536640. Throughput: 0: 272.1. Samples: 7541968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:55:55,973][166323] Avg episode reward: [(0, '1316.553')]
[36m[2025-07-02 03:56:00,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7536640. Throughput: 0: 274.3. Samples: 7543744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:56:00,966][166323] Avg episode reward: [(0, '1273.303')]
[36m[2025-07-02 03:56:05,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7536640. Throughput: 0: 272.3. Samples: 7544544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:56:05,964][166323] Avg episode reward: [(0, '1291.589')]
[36m[2025-07-02 03:56:10,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7536640. Throughput: 0: 274.8. Samples: 7546112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:56:10,946][166323] Avg episode reward: [(0, '1265.323')]
[36m[2025-07-02 03:56:16,011][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7536640. Throughput: 0: 276.9. Samples: 7547920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:56:16,011][166323] Avg episode reward: [(0, '1245.311')]
[36m[2025-07-02 03:56:20,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.8). Total num frames: 7536640. Throughput: 0: 275.3. Samples: 7548640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:56:20,957][166323] Avg episode reward: [(0, '1252.686')]
[36m[2025-07-02 03:56:25,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7536640. Throughput: 0: 271.9. Samples: 7550160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:56:25,975][166323] Avg episode reward: [(0, '1207.510')]
[36m[2025-07-02 03:56:30,978][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7536640. Throughput: 0: 271.4. Samples: 7551904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:56:30,978][166323] Avg episode reward: [(0, '1221.718')]
[36m[2025-07-02 03:56:35,948][166323] Fps is (10 sec: 1642.9, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7553024. Throughput: 0: 272.0. Samples: 7552688. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:56:35,948][166323] Avg episode reward: [(0, '1225.218')]
[36m[2025-07-02 03:56:40,956][166323] Fps is (10 sec: 1641.9, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7553024. Throughput: 0: 275.3. Samples: 7554352. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:56:40,957][166323] Avg episode reward: [(0, '1200.836')]
[36m[2025-07-02 03:56:45,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7553024. Throughput: 0: 275.1. Samples: 7556128. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:56:45,990][166323] Avg episode reward: [(0, '1237.547')]
[36m[2025-07-02 03:56:50,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7553024. Throughput: 0: 275.2. Samples: 7556928. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:56:50,963][166323] Avg episode reward: [(0, '1252.636')]
[36m[2025-07-02 03:56:56,025][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 7553024. Throughput: 0: 277.9. Samples: 7558640. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:56:56,026][166323] Avg episode reward: [(0, '1263.807')]
[31m[26604611 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[26604611 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[26604612 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:57:01,008][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7553024. Throughput: 0: 272.0. Samples: 7560160. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:57:01,008][166323] Avg episode reward: [(0, '1309.774')]
[36m[2025-07-02 03:57:05,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7553024. Throughput: 0: 274.8. Samples: 7561008. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:57:05,972][166323] Avg episode reward: [(0, '1278.522')]
[36m[2025-07-02 03:57:11,010][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 7553024. Throughput: 0: 278.5. Samples: 7562704. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:57:11,011][166323] Avg episode reward: [(0, '1302.219')]
[36m[2025-07-02 03:57:15,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7553024. Throughput: 0: 275.7. Samples: 7564304. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:57:15,954][166323] Avg episode reward: [(0, '1300.759')]
[36m[2025-07-02 03:57:21,002][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7553024. Throughput: 0: 276.6. Samples: 7565152. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:57:21,002][166323] Avg episode reward: [(0, '1298.549')]
[36m[2025-07-02 03:57:25,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7553024. Throughput: 0: 277.5. Samples: 7566848. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:57:25,982][166323] Avg episode reward: [(0, '1297.914')]
[36m[2025-07-02 03:57:30,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7553024. Throughput: 0: 276.3. Samples: 7568560. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 03:57:30,980][166323] Avg episode reward: [(0, '1243.827')]
[36m[2025-07-02 03:57:35,947][166323] Fps is (10 sec: 1644.1, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7569408. Throughput: 0: 276.0. Samples: 7569344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:57:35,948][166323] Avg episode reward: [(0, '1202.762')]
[37m[1m[2025-07-02 03:57:36,001][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000014776_7569408.pth...
[36m[2025-07-02 03:57:36,005][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000014648_7503872.pth
[36m[2025-07-02 03:57:40,964][166323] Fps is (10 sec: 1641.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7569408. Throughput: 0: 274.5. Samples: 7570976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:57:40,964][166323] Avg episode reward: [(0, '1221.002')]
[36m[2025-07-02 03:57:45,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7569408. Throughput: 0: 278.2. Samples: 7572672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:57:45,976][166323] Avg episode reward: [(0, '1246.547')]
[36m[2025-07-02 03:57:50,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7569408. Throughput: 0: 277.6. Samples: 7573504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:57:50,981][166323] Avg episode reward: [(0, '1244.077')]
[36m[2025-07-02 03:57:55,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7569408. Throughput: 0: 276.5. Samples: 7575136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:57:55,979][166323] Avg episode reward: [(0, '1178.318')]
[31m[26666573 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[26666573 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[26666574 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:58:00,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7569408. Throughput: 0: 279.0. Samples: 7576864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:58:00,972][166323] Avg episode reward: [(0, '1214.045')]
[36m[2025-07-02 03:58:06,008][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7569408. Throughput: 0: 278.0. Samples: 7577664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:58:06,008][166323] Avg episode reward: [(0, '1278.433')]
[36m[2025-07-02 03:58:10,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7569408. Throughput: 0: 276.6. Samples: 7579296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:58:10,992][166323] Avg episode reward: [(0, '1288.795')]
[36m[2025-07-02 03:58:15,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 7569408. Throughput: 0: 276.8. Samples: 7581008. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:58:15,950][166323] Avg episode reward: [(0, '1321.700')]
[36m[2025-07-02 03:58:20,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7569408. Throughput: 0: 275.4. Samples: 7581744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:58:20,973][166323] Avg episode reward: [(0, '1335.566')]
[36m[2025-07-02 03:58:25,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7569408. Throughput: 0: 279.6. Samples: 7583552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:58:25,946][166323] Avg episode reward: [(0, '1345.004')]
[36m[2025-07-02 03:58:30,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 7569408. Throughput: 0: 276.2. Samples: 7585104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:58:30,989][166323] Avg episode reward: [(0, '1310.458')]
[36m[2025-07-02 03:58:36,017][166323] Fps is (10 sec: 1626.8, 60 sec: 272.7, 300 sec: 277.7). Total num frames: 7585792. Throughput: 0: 273.9. Samples: 7585840. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 03:58:36,018][166323] Avg episode reward: [(0, '1313.122')]
[31m[26706325 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[26706326 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[26706326 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:58:40,979][166323] Fps is (10 sec: 1640.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7585792. Throughput: 0: 270.9. Samples: 7587328. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 03:58:40,979][166323] Avg episode reward: [(0, '1273.101')]
[36m[2025-07-02 03:58:45,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7585792. Throughput: 0: 269.2. Samples: 7588976. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 03:58:45,966][166323] Avg episode reward: [(0, '1270.392')]
[36m[2025-07-02 03:58:50,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7585792. Throughput: 0: 268.4. Samples: 7589728. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 03:58:50,953][166323] Avg episode reward: [(0, '1248.383')]
[36m[2025-07-02 03:58:55,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7585792. Throughput: 0: 271.1. Samples: 7591488. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 03:58:55,970][166323] Avg episode reward: [(0, '1229.943')]
[36m[2025-07-02 03:59:00,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7585792. Throughput: 0: 270.7. Samples: 7593200. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 03:59:00,991][166323] Avg episode reward: [(0, '1226.607')]
[31m[26733991 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[26733992 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[26733992 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 03:59:05,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7585792. Throughput: 0: 273.1. Samples: 7594032. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 03:59:05,973][166323] Avg episode reward: [(0, '1297.740')]
[36m[2025-07-02 03:59:10,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7585792. Throughput: 0: 271.4. Samples: 7595776. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 03:59:10,981][166323] Avg episode reward: [(0, '1270.124')]
[36m[2025-07-02 03:59:15,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7585792. Throughput: 0: 276.2. Samples: 7597520. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 03:59:15,944][166323] Avg episode reward: [(0, '1237.910')]
[36m[2025-07-02 03:59:20,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 7585792. Throughput: 0: 278.9. Samples: 7598384. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 03:59:20,999][166323] Avg episode reward: [(0, '1195.297')]
[33m[26751115 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[26751115 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.869140625
[33mCrash Rate: 0.11962890625
[33mTimeout Rate: 0.01123046875 (navigation_task.py:265)
[33m[26751115 ms][navigation_task] - WARNING : 
[33mSuccesses: 1780
[33mCrashes : 245
[33mTimeouts: 23 (navigation_task.py:268)
[36m[2025-07-02 03:59:25,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7585792. Throughput: 0: 280.0. Samples: 7599920. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 03:59:25,952][166323] Avg episode reward: [(0, '1194.219')]
[36m[2025-07-02 03:59:30,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.1). Total num frames: 7585792. Throughput: 0: 281.5. Samples: 7601648. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 03:59:30,988][166323] Avg episode reward: [(0, '1171.709')]
[36m[2025-07-02 03:59:35,992][166323] Fps is (10 sec: 1631.7, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7602176. Throughput: 0: 282.8. Samples: 7602464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:59:35,992][166323] Avg episode reward: [(0, '1180.583')]
[37m[1m[2025-07-02 03:59:36,049][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000014840_7602176.pth...
[36m[2025-07-02 03:59:36,053][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000014712_7536640.pth
[36m[2025-07-02 03:59:40,992][166323] Fps is (10 sec: 1637.8, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7602176. Throughput: 0: 279.7. Samples: 7604080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:59:40,992][166323] Avg episode reward: [(0, '1139.830')]
[36m[2025-07-02 03:59:45,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7602176. Throughput: 0: 278.3. Samples: 7605712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:59:45,951][166323] Avg episode reward: [(0, '1120.092')]
[36m[2025-07-02 03:59:50,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7602176. Throughput: 0: 277.8. Samples: 7606528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:59:50,950][166323] Avg episode reward: [(0, '1123.006')]
[36m[2025-07-02 03:59:55,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7602176. Throughput: 0: 278.5. Samples: 7608304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 03:59:55,961][166323] Avg episode reward: [(0, '1151.505')]
[36m[2025-07-02 04:00:00,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7602176. Throughput: 0: 278.2. Samples: 7610048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:00:00,980][166323] Avg episode reward: [(0, '1148.827')]
[36m[2025-07-02 04:00:05,995][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7602176. Throughput: 0: 277.7. Samples: 7610880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:00:05,995][166323] Avg episode reward: [(0, '1151.953')]
[36m[2025-07-02 04:00:10,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7602176. Throughput: 0: 279.6. Samples: 7612512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:00:10,991][166323] Avg episode reward: [(0, '1159.008')]
[36m[2025-07-02 04:00:16,018][166323] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 277.6). Total num frames: 7602176. Throughput: 0: 279.6. Samples: 7614240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:00:16,018][166323] Avg episode reward: [(0, '1202.473')]
[36m[2025-07-02 04:00:20,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7602176. Throughput: 0: 280.8. Samples: 7615088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:00:20,952][166323] Avg episode reward: [(0, '1233.280')]
[36m[2025-07-02 04:00:25,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7602176. Throughput: 0: 281.9. Samples: 7616752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:00:25,950][166323] Avg episode reward: [(0, '1197.806')]
[36m[2025-07-02 04:00:30,944][166323] Fps is (10 sec: 1639.6, 60 sec: 546.5, 300 sec: 277.7). Total num frames: 7618560. Throughput: 0: 286.3. Samples: 7618592. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 04:00:30,944][166323] Avg episode reward: [(0, '1240.428')]
[36m[2025-07-02 04:00:35,980][166323] Fps is (10 sec: 1633.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7618560. Throughput: 0: 284.3. Samples: 7619328. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 04:00:35,980][166323] Avg episode reward: [(0, '1262.403')]
[36m[2025-07-02 04:00:40,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7618560. Throughput: 0: 278.3. Samples: 7620832. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 04:00:40,971][166323] Avg episode reward: [(0, '1265.437')]
[31m[26830666 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[26830666 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[26830667 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:00:45,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7618560. Throughput: 0: 275.4. Samples: 7622432. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 04:00:45,946][166323] Avg episode reward: [(0, '1274.512')]
[36m[2025-07-02 04:00:50,978][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7618560. Throughput: 0: 277.4. Samples: 7623360. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 04:00:50,978][166323] Avg episode reward: [(0, '1301.500')]
[36m[2025-07-02 04:00:55,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7618560. Throughput: 0: 278.1. Samples: 7625024. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 04:00:55,975][166323] Avg episode reward: [(0, '1286.713')]
[36m[2025-07-02 04:01:00,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7618560. Throughput: 0: 281.7. Samples: 7626896. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 04:01:00,948][166323] Avg episode reward: [(0, '1249.311')]
[36m[2025-07-02 04:01:05,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7618560. Throughput: 0: 281.9. Samples: 7627776. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 04:01:05,965][166323] Avg episode reward: [(0, '1229.445')]
[36m[2025-07-02 04:01:10,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7618560. Throughput: 0: 283.2. Samples: 7629504. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 04:01:10,980][166323] Avg episode reward: [(0, '1200.310')]
[36m[2025-07-02 04:01:15,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 7618560. Throughput: 0: 281.5. Samples: 7631264. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 04:01:15,954][166323] Avg episode reward: [(0, '1221.745')]
[36m[2025-07-02 04:01:20,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7618560. Throughput: 0: 282.8. Samples: 7632048. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 04:01:20,961][166323] Avg episode reward: [(0, '1198.776')]
[36m[2025-07-02 04:01:25,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7618560. Throughput: 0: 285.7. Samples: 7633680. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 04:01:25,945][166323] Avg episode reward: [(0, '1179.870')]
[36m[2025-07-02 04:01:30,988][166323] Fps is (10 sec: 1634.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7634944. Throughput: 0: 284.9. Samples: 7635264. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:01:30,988][166323] Avg episode reward: [(0, '1240.665')]
[36m[2025-07-02 04:01:35,984][166323] Fps is (10 sec: 1631.9, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7634944. Throughput: 0: 283.7. Samples: 7636128. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:01:35,984][166323] Avg episode reward: [(0, '1267.319')]
[37m[1m[2025-07-02 04:01:36,040][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000014904_7634944.pth...
[36m[2025-07-02 04:01:36,044][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000014776_7569408.pth
[36m[2025-07-02 04:01:40,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7634944. Throughput: 0: 282.4. Samples: 7637728. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:01:40,962][166323] Avg episode reward: [(0, '1194.623')]
[36m[2025-07-02 04:01:45,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7634944. Throughput: 0: 280.1. Samples: 7639504. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:01:45,962][166323] Avg episode reward: [(0, '1249.212')]
[36m[2025-07-02 04:01:50,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 7634944. Throughput: 0: 281.7. Samples: 7640448. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:01:50,951][166323] Avg episode reward: [(0, '1172.062')]
[31m[26902049 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[26902050 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[26902050 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:01:55,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7634944. Throughput: 0: 277.9. Samples: 7642000. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:01:55,952][166323] Avg episode reward: [(0, '1234.669')]
[36m[2025-07-02 04:02:00,977][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7634944. Throughput: 0: 278.3. Samples: 7643792. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:02:00,977][166323] Avg episode reward: [(0, '1240.587')]
[36m[2025-07-02 04:02:06,008][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7634944. Throughput: 0: 278.1. Samples: 7644576. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:02:06,009][166323] Avg episode reward: [(0, '1220.749')]
[36m[2025-07-02 04:02:10,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7634944. Throughput: 0: 280.3. Samples: 7646304. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:02:10,984][166323] Avg episode reward: [(0, '1300.628')]
[36m[2025-07-02 04:02:15,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7634944. Throughput: 0: 282.3. Samples: 7647968. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:02:15,991][166323] Avg episode reward: [(0, '1307.200')]
[36m[2025-07-02 04:02:20,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7634944. Throughput: 0: 279.7. Samples: 7648704. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:02:20,950][166323] Avg episode reward: [(0, '1292.560')]
[36m[2025-07-02 04:02:25,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7634944. Throughput: 0: 280.4. Samples: 7650352. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:02:25,985][166323] Avg episode reward: [(0, '1320.205')]
[36m[2025-07-02 04:02:30,956][166323] Fps is (10 sec: 1637.3, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7651328. Throughput: 0: 275.6. Samples: 7651904. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 04:02:30,956][166323] Avg episode reward: [(0, '1304.156')]
[36m[2025-07-02 04:02:36,003][166323] Fps is (10 sec: 1635.4, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7651328. Throughput: 0: 273.8. Samples: 7652784. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 04:02:36,003][166323] Avg episode reward: [(0, '1239.569')]
[36m[2025-07-02 04:02:40,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7651328. Throughput: 0: 275.1. Samples: 7654384. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 04:02:40,966][166323] Avg episode reward: [(0, '1227.814')]
[36m[2025-07-02 04:02:45,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7651328. Throughput: 0: 274.4. Samples: 7656144. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 04:02:45,996][166323] Avg episode reward: [(0, '1174.127')]
[36m[2025-07-02 04:02:50,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7651328. Throughput: 0: 274.6. Samples: 7656928. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 04:02:50,988][166323] Avg episode reward: [(0, '1173.284')]
[36m[2025-07-02 04:02:55,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7651328. Throughput: 0: 270.5. Samples: 7658480. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 04:02:55,994][166323] Avg episode reward: [(0, '1212.987')]
[36m[2025-07-02 04:03:00,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7651328. Throughput: 0: 270.3. Samples: 7660128. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 04:03:00,978][166323] Avg episode reward: [(0, '1204.261')]
[36m[2025-07-02 04:03:05,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7651328. Throughput: 0: 271.2. Samples: 7660912. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 04:03:05,964][166323] Avg episode reward: [(0, '1176.926')]
[36m[2025-07-02 04:03:10,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7651328. Throughput: 0: 270.9. Samples: 7662544. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 04:03:10,992][166323] Avg episode reward: [(0, '1221.181')]
[36m[2025-07-02 04:03:15,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7651328. Throughput: 0: 275.2. Samples: 7664288. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 04:03:15,954][166323] Avg episode reward: [(0, '1213.159')]
[36m[2025-07-02 04:03:20,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7651328. Throughput: 0: 274.0. Samples: 7665104. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 04:03:20,974][166323] Avg episode reward: [(0, '1309.561')]
[36m[2025-07-02 04:03:25,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7651328. Throughput: 0: 274.2. Samples: 7666720. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 04:03:25,959][166323] Avg episode reward: [(0, '1303.415')]
[36m[2025-07-02 04:03:30,989][166323] Fps is (10 sec: 1635.9, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7667712. Throughput: 0: 268.1. Samples: 7668208. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 04:03:30,989][166323] Avg episode reward: [(0, '1271.656')]
[36m[2025-07-02 04:03:36,011][166323] Fps is (10 sec: 1629.9, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7667712. Throughput: 0: 268.3. Samples: 7669008. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 04:03:36,011][166323] Avg episode reward: [(0, '1271.474')]
[37m[1m[2025-07-02 04:03:36,017][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000014968_7667712.pth...
[36m[2025-07-02 04:03:36,024][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000014840_7602176.pth
[31m[27008183 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[27008183 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[27008183 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[27008865 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[27008865 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[27008866 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:03:40,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7667712. Throughput: 0: 271.0. Samples: 7670672. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 04:03:40,983][166323] Avg episode reward: [(0, '1260.079')]
[36m[2025-07-02 04:03:45,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7667712. Throughput: 0: 274.2. Samples: 7672464. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 04:03:45,970][166323] Avg episode reward: [(0, '1262.823')]
[36m[2025-07-02 04:03:50,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7667712. Throughput: 0: 274.1. Samples: 7673248. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 04:03:50,966][166323] Avg episode reward: [(0, '1239.595')]
[36m[2025-07-02 04:03:55,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7667712. Throughput: 0: 275.3. Samples: 7674928. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 04:03:55,984][166323] Avg episode reward: [(0, '1174.856')]
[36m[2025-07-02 04:04:00,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7667712. Throughput: 0: 271.5. Samples: 7676512. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 04:04:00,973][166323] Avg episode reward: [(0, '1153.615')]
[31m[27032718 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[27032719 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[27032719 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:04:05,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7667712. Throughput: 0: 272.3. Samples: 7677360. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 04:04:05,983][166323] Avg episode reward: [(0, '1162.468')]
[36m[2025-07-02 04:04:10,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7667712. Throughput: 0: 271.6. Samples: 7678944. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 04:04:10,964][166323] Avg episode reward: [(0, '1179.592')]
[36m[2025-07-02 04:04:16,006][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 7667712. Throughput: 0: 276.5. Samples: 7680656. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 04:04:16,006][166323] Avg episode reward: [(0, '1285.050')]
[36m[2025-07-02 04:04:20,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7667712. Throughput: 0: 275.7. Samples: 7681408. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 04:04:20,981][166323] Avg episode reward: [(0, '1288.289')]
[36m[2025-07-02 04:04:25,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7667712. Throughput: 0: 273.9. Samples: 7682992. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 04:04:25,960][166323] Avg episode reward: [(0, '1309.342')]
[31m[27058890 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[27058890 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[27058890 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:04:30,996][166323] Fps is (10 sec: 1635.9, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7684096. Throughput: 0: 265.4. Samples: 7684416. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 04:04:30,996][166323] Avg episode reward: [(0, '1281.552')]
[36m[2025-07-02 04:04:35,990][166323] Fps is (10 sec: 1633.4, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7684096. Throughput: 0: 267.2. Samples: 7685280. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 04:04:35,990][166323] Avg episode reward: [(0, '1305.876')]
[36m[2025-07-02 04:04:40,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7684096. Throughput: 0: 270.3. Samples: 7687088. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 04:04:40,968][166323] Avg episode reward: [(0, '1285.978')]
[36m[2025-07-02 04:04:45,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7684096. Throughput: 0: 272.3. Samples: 7688768. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 04:04:45,989][166323] Avg episode reward: [(0, '1255.902')]
[36m[2025-07-02 04:04:50,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7684096. Throughput: 0: 270.6. Samples: 7689536. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 04:04:50,974][166323] Avg episode reward: [(0, '1239.366')]
[36m[2025-07-02 04:04:55,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7684096. Throughput: 0: 271.7. Samples: 7691168. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 04:04:55,956][166323] Avg episode reward: [(0, '1207.590')]
[36m[2025-07-02 04:05:00,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7684096. Throughput: 0: 272.7. Samples: 7692912. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 04:05:00,946][166323] Avg episode reward: [(0, '1203.122')]
[36m[2025-07-02 04:05:05,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7684096. Throughput: 0: 274.3. Samples: 7693744. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 04:05:05,950][166323] Avg episode reward: [(0, '1257.927')]
[36m[2025-07-02 04:05:10,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7684096. Throughput: 0: 275.9. Samples: 7695408. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 04:05:10,969][166323] Avg episode reward: [(0, '1291.305')]
[36m[2025-07-02 04:05:15,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7684096. Throughput: 0: 282.2. Samples: 7697104. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 04:05:15,960][166323] Avg episode reward: [(0, '1284.328')]
[36m[2025-07-02 04:05:20,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7684096. Throughput: 0: 282.7. Samples: 7698000. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 04:05:20,981][166323] Avg episode reward: [(0, '1288.226')]
[31m[27112821 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[27112822 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[27112823 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:05:25,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 7684096. Throughput: 0: 279.4. Samples: 7699664. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 04:05:25,972][166323] Avg episode reward: [(0, '1293.408')]
[36m[2025-07-02 04:05:31,005][166323] Fps is (10 sec: 1634.5, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7700480. Throughput: 0: 278.3. Samples: 7701296. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 04:05:31,006][166323] Avg episode reward: [(0, '1238.748')]
[36m[2025-07-02 04:05:35,950][166323] Fps is (10 sec: 1641.9, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7700480. Throughput: 0: 279.3. Samples: 7702096. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 04:05:35,951][166323] Avg episode reward: [(0, '1273.823')]
[37m[1m[2025-07-02 04:05:36,002][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015032_7700480.pth...
[36m[2025-07-02 04:05:36,006][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000014904_7634944.pth
[36m[2025-07-02 04:05:40,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7700480. Throughput: 0: 284.0. Samples: 7703952. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 04:05:40,977][166323] Avg episode reward: [(0, '1256.562')]
[36m[2025-07-02 04:05:45,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7700480. Throughput: 0: 286.2. Samples: 7705792. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 04:05:45,956][166323] Avg episode reward: [(0, '1245.412')]
[36m[2025-07-02 04:05:50,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7700480. Throughput: 0: 283.9. Samples: 7706528. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 04:05:50,974][166323] Avg episode reward: [(0, '1267.119')]
[36m[2025-07-02 04:05:55,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7700480. Throughput: 0: 287.1. Samples: 7708320. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 04:05:55,947][166323] Avg episode reward: [(0, '1262.635')]
[36m[2025-07-02 04:06:00,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7700480. Throughput: 0: 288.6. Samples: 7710096. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 04:06:00,983][166323] Avg episode reward: [(0, '1272.363')]
[36m[2025-07-02 04:06:05,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7700480. Throughput: 0: 288.1. Samples: 7710960. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 04:06:05,963][166323] Avg episode reward: [(0, '1263.515')]
[36m[2025-07-02 04:06:10,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7700480. Throughput: 0: 286.9. Samples: 7712576. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 04:06:10,978][166323] Avg episode reward: [(0, '1294.792')]
[36m[2025-07-02 04:06:15,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7700480. Throughput: 0: 285.2. Samples: 7714128. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 04:06:15,996][166323] Avg episode reward: [(0, '1218.384')]
[36m[2025-07-02 04:06:20,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7700480. Throughput: 0: 286.5. Samples: 7714992. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 04:06:20,962][166323] Avg episode reward: [(0, '1179.189')]
[36m[2025-07-02 04:06:25,953][166323] Fps is (10 sec: 1645.4, 60 sec: 546.3, 300 sec: 277.7). Total num frames: 7716864. Throughput: 0: 283.5. Samples: 7716704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:06:25,954][166323] Avg episode reward: [(0, '1230.820')]
[36m[2025-07-02 04:06:30,959][166323] Fps is (10 sec: 1638.8, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7716864. Throughput: 0: 274.5. Samples: 7718144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:06:30,960][166323] Avg episode reward: [(0, '1229.156')]
[36m[2025-07-02 04:06:35,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7716864. Throughput: 0: 276.9. Samples: 7718992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:06:35,984][166323] Avg episode reward: [(0, '1219.194')]
[31m[27185579 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[27185579 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[27185579 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:06:40,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7716864. Throughput: 0: 272.3. Samples: 7720576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:06:40,952][166323] Avg episode reward: [(0, '1190.477')]
[36m[2025-07-02 04:06:45,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7716864. Throughput: 0: 267.6. Samples: 7722128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:06:45,948][166323] Avg episode reward: [(0, '1196.930')]
[36m[2025-07-02 04:06:50,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7716864. Throughput: 0: 268.0. Samples: 7723024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:06:50,981][166323] Avg episode reward: [(0, '1213.289')]
[36m[2025-07-02 04:06:55,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7716864. Throughput: 0: 270.6. Samples: 7724752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:06:55,967][166323] Avg episode reward: [(0, '1193.512')]
[36m[2025-07-02 04:07:00,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 7716864. Throughput: 0: 274.4. Samples: 7726464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:07:00,945][166323] Avg episode reward: [(0, '1184.301')]
[36m[2025-07-02 04:07:05,997][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7716864. Throughput: 0: 273.9. Samples: 7727328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:07:05,997][166323] Avg episode reward: [(0, '1207.443')]
[36m[2025-07-02 04:07:10,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7716864. Throughput: 0: 274.1. Samples: 7729040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:07:10,957][166323] Avg episode reward: [(0, '1212.282')]
[36m[2025-07-02 04:07:15,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7716864. Throughput: 0: 282.2. Samples: 7730848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:07:15,972][166323] Avg episode reward: [(0, '1232.121')]
[36m[2025-07-02 04:07:20,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7716864. Throughput: 0: 281.5. Samples: 7731664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:07:20,994][166323] Avg episode reward: [(0, '1270.865')]
[36m[2025-07-02 04:07:26,009][166323] Fps is (10 sec: 1632.3, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 7733248. Throughput: 0: 280.9. Samples: 7733232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:07:26,009][166323] Avg episode reward: [(0, '1295.629')]
[36m[2025-07-02 04:07:30,954][166323] Fps is (10 sec: 1644.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7733248. Throughput: 0: 283.3. Samples: 7734880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:07:30,955][166323] Avg episode reward: [(0, '1281.247')]
[36m[2025-07-02 04:07:35,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7733248. Throughput: 0: 282.2. Samples: 7735728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:07:36,000][166323] Avg episode reward: [(0, '1260.159')]
[37m[1m[2025-07-02 04:07:36,059][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015096_7733248.pth...
[36m[2025-07-02 04:07:36,066][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000014968_7667712.pth
[36m[2025-07-02 04:07:40,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7733248. Throughput: 0: 279.6. Samples: 7737328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:07:40,949][166323] Avg episode reward: [(0, '1289.193')]
[36m[2025-07-02 04:07:45,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7733248. Throughput: 0: 282.5. Samples: 7739184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:07:45,965][166323] Avg episode reward: [(0, '1321.001')]
[36m[2025-07-02 04:07:50,995][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7733248. Throughput: 0: 283.4. Samples: 7740080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:07:50,995][166323] Avg episode reward: [(0, '1312.816')]
[36m[2025-07-02 04:07:55,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7733248. Throughput: 0: 281.5. Samples: 7741712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:07:55,968][166323] Avg episode reward: [(0, '1306.192')]
[36m[2025-07-02 04:08:00,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7733248. Throughput: 0: 277.8. Samples: 7743344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:08:00,951][166323] Avg episode reward: [(0, '1325.149')]
[36m[2025-07-02 04:08:05,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7733248. Throughput: 0: 277.1. Samples: 7744128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:08:05,966][166323] Avg episode reward: [(0, '1329.066')]
[36m[2025-07-02 04:08:10,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7733248. Throughput: 0: 277.3. Samples: 7745696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:08:10,957][166323] Avg episode reward: [(0, '1353.130')]
[36m[2025-07-02 04:08:15,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7733248. Throughput: 0: 274.4. Samples: 7747232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:08:15,975][166323] Avg episode reward: [(0, '1336.447')]
[36m[2025-07-02 04:08:20,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7733248. Throughput: 0: 274.1. Samples: 7748048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:08:20,948][166323] Avg episode reward: [(0, '1358.187')]
[36m[2025-07-02 04:08:25,989][166323] Fps is (10 sec: 1636.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7749632. Throughput: 0: 275.7. Samples: 7749744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:08:25,989][166323] Avg episode reward: [(0, '1346.856')]
[36m[2025-07-02 04:08:30,956][166323] Fps is (10 sec: 1637.1, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7749632. Throughput: 0: 274.2. Samples: 7751520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:08:30,956][166323] Avg episode reward: [(0, '1335.324')]
[36m[2025-07-02 04:08:35,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7749632. Throughput: 0: 272.4. Samples: 7752336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:08:35,987][166323] Avg episode reward: [(0, '1327.128')]
[36m[2025-07-02 04:08:40,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7749632. Throughput: 0: 269.8. Samples: 7753856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:08:40,982][166323] Avg episode reward: [(0, '1279.900')]
[36m[2025-07-02 04:08:45,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7749632. Throughput: 0: 269.7. Samples: 7755488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:08:45,977][166323] Avg episode reward: [(0, '1316.958')]
[36m[2025-07-02 04:08:50,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7749632. Throughput: 0: 272.7. Samples: 7756400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:08:50,961][166323] Avg episode reward: [(0, '1282.561')]
[36m[2025-07-02 04:08:55,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7749632. Throughput: 0: 274.3. Samples: 7758048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:08:55,987][166323] Avg episode reward: [(0, '1274.332')]
[36m[2025-07-02 04:09:00,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7749632. Throughput: 0: 276.9. Samples: 7759696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:09:00,989][166323] Avg episode reward: [(0, '1221.014')]
[36m[2025-07-02 04:09:05,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7749632. Throughput: 0: 276.7. Samples: 7760512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:09:05,994][166323] Avg episode reward: [(0, '1245.524')]
[36m[2025-07-02 04:09:10,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7749632. Throughput: 0: 276.0. Samples: 7762160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:09:10,982][166323] Avg episode reward: [(0, '1261.927')]
[36m[2025-07-02 04:09:15,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7749632. Throughput: 0: 273.7. Samples: 7763840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:09:15,970][166323] Avg episode reward: [(0, '1297.219')]
[36m[2025-07-02 04:09:20,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7749632. Throughput: 0: 274.6. Samples: 7764688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:09:20,966][166323] Avg episode reward: [(0, '1292.461')]
[36m[2025-07-02 04:09:25,971][166323] Fps is (10 sec: 1638.3, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7766016. Throughput: 0: 276.0. Samples: 7766272. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 04:09:25,971][166323] Avg episode reward: [(0, '1274.220')]
[36m[2025-07-02 04:09:30,976][166323] Fps is (10 sec: 1636.7, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7766016. Throughput: 0: 280.5. Samples: 7768112. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 04:09:30,976][166323] Avg episode reward: [(0, '1298.163')]
[36m[2025-07-02 04:09:35,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7766016. Throughput: 0: 279.0. Samples: 7768960. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 04:09:35,977][166323] Avg episode reward: [(0, '1289.022')]
[37m[1m[2025-07-02 04:09:36,026][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015160_7766016.pth...
[36m[2025-07-02 04:09:36,030][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015032_7700480.pth
[36m[2025-07-02 04:09:40,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7766016. Throughput: 0: 277.3. Samples: 7770528. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 04:09:40,985][166323] Avg episode reward: [(0, '1290.036')]
[36m[2025-07-02 04:09:45,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7766016. Throughput: 0: 279.1. Samples: 7772256. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 04:09:45,993][166323] Avg episode reward: [(0, '1301.893')]
[36m[2025-07-02 04:09:50,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7766016. Throughput: 0: 281.3. Samples: 7773168. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 04:09:50,978][166323] Avg episode reward: [(0, '1273.578')]
[36m[2025-07-02 04:09:55,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7766016. Throughput: 0: 284.9. Samples: 7774976. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 04:09:55,966][166323] Avg episode reward: [(0, '1249.438')]
[36m[2025-07-02 04:10:00,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7766016. Throughput: 0: 287.2. Samples: 7776768. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 04:10:00,983][166323] Avg episode reward: [(0, '1299.388')]
[36m[2025-07-02 04:10:05,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7766016. Throughput: 0: 286.2. Samples: 7777568. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 04:10:05,975][166323] Avg episode reward: [(0, '1237.454')]
[31m[27396815 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[27396815 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[27396816 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:10:10,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7766016. Throughput: 0: 289.5. Samples: 7779296. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 04:10:10,956][166323] Avg episode reward: [(0, '1236.166')]
[36m[2025-07-02 04:10:15,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7766016. Throughput: 0: 283.9. Samples: 7780880. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 04:10:15,950][166323] Avg episode reward: [(0, '1237.317')]
[36m[2025-07-02 04:10:20,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7766016. Throughput: 0: 282.1. Samples: 7781648. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 04:10:20,953][166323] Avg episode reward: [(0, '1200.176')]
[36m[2025-07-02 04:10:25,945][166323] Fps is (10 sec: 1639.2, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 7782400. Throughput: 0: 285.1. Samples: 7783344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:10:25,945][166323] Avg episode reward: [(0, '1154.050')]
[36m[2025-07-02 04:10:30,976][166323] Fps is (10 sec: 1634.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7782400. Throughput: 0: 282.1. Samples: 7784944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:10:30,976][166323] Avg episode reward: [(0, '1183.268')]
[36m[2025-07-02 04:10:35,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7782400. Throughput: 0: 279.4. Samples: 7785744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:10:35,987][166323] Avg episode reward: [(0, '1202.474')]
[36m[2025-07-02 04:10:40,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7782400. Throughput: 0: 275.7. Samples: 7787376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:10:40,943][166323] Avg episode reward: [(0, '1244.620')]
[36m[2025-07-02 04:10:45,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7782400. Throughput: 0: 275.1. Samples: 7789136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:10:45,948][166323] Avg episode reward: [(0, '1253.579')]
[36m[2025-07-02 04:10:50,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7782400. Throughput: 0: 276.1. Samples: 7789984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:10:50,951][166323] Avg episode reward: [(0, '1217.222')]
[33m[27439726 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[27439726 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.87109375
[33mCrash Rate: 0.1142578125
[33mTimeout Rate: 0.0146484375 (navigation_task.py:265)
[33m[27439726 ms][navigation_task] - WARNING : 
[33mSuccesses: 1784
[33mCrashes : 234
[33mTimeouts: 30 (navigation_task.py:268)
[36m[2025-07-02 04:10:55,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7782400. Throughput: 0: 276.0. Samples: 7791728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:10:55,999][166323] Avg episode reward: [(0, '1256.616')]
[36m[2025-07-02 04:11:00,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7782400. Throughput: 0: 277.4. Samples: 7793376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:11:00,998][166323] Avg episode reward: [(0, '1271.500')]
[36m[2025-07-02 04:11:05,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7782400. Throughput: 0: 278.3. Samples: 7794176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:11:05,971][166323] Avg episode reward: [(0, '1291.702')]
[36m[2025-07-02 04:11:10,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7782400. Throughput: 0: 276.2. Samples: 7795776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:11:10,950][166323] Avg episode reward: [(0, '1289.137')]
[36m[2025-07-02 04:11:15,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7782400. Throughput: 0: 278.8. Samples: 7797488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:11:15,971][166323] Avg episode reward: [(0, '1307.836')]
[36m[2025-07-02 04:11:20,962][166323] Fps is (10 sec: 1636.5, 60 sec: 546.0, 300 sec: 277.7). Total num frames: 7798784. Throughput: 0: 279.6. Samples: 7798320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:11:20,962][166323] Avg episode reward: [(0, '1271.784')]
[36m[2025-07-02 04:11:25,962][166323] Fps is (10 sec: 1639.8, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7798784. Throughput: 0: 277.9. Samples: 7799888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:11:25,963][166323] Avg episode reward: [(0, '1267.895')]
[36m[2025-07-02 04:11:30,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7798784. Throughput: 0: 280.7. Samples: 7801776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:11:30,972][166323] Avg episode reward: [(0, '1240.653')]
[36m[2025-07-02 04:11:35,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7798784. Throughput: 0: 279.7. Samples: 7802576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:11:35,966][166323] Avg episode reward: [(0, '1201.070')]
[37m[1m[2025-07-02 04:11:36,031][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015224_7798784.pth...
[36m[2025-07-02 04:11:36,035][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015096_7733248.pth
[36m[2025-07-02 04:11:40,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7798784. Throughput: 0: 279.3. Samples: 7804288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:11:40,965][166323] Avg episode reward: [(0, '1181.085')]
[36m[2025-07-02 04:11:45,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7798784. Throughput: 0: 285.3. Samples: 7806208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:11:45,971][166323] Avg episode reward: [(0, '1182.241')]
[36m[2025-07-02 04:11:50,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7798784. Throughput: 0: 285.6. Samples: 7807024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:11:50,961][166323] Avg episode reward: [(0, '1160.022')]
[36m[2025-07-02 04:11:55,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7798784. Throughput: 0: 289.4. Samples: 7808800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:11:55,947][166323] Avg episode reward: [(0, '1143.003')]
[36m[2025-07-02 04:12:00,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7798784. Throughput: 0: 288.8. Samples: 7810480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:12:00,955][166323] Avg episode reward: [(0, '1118.197')]
[36m[2025-07-02 04:12:06,007][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 7798784. Throughput: 0: 290.9. Samples: 7811424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:12:06,007][166323] Avg episode reward: [(0, '1176.723')]
[36m[2025-07-02 04:12:10,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7798784. Throughput: 0: 292.0. Samples: 7813024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:12:10,947][166323] Avg episode reward: [(0, '1187.844')]
[36m[2025-07-02 04:12:15,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7798784. Throughput: 0: 287.0. Samples: 7814688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:12:15,957][166323] Avg episode reward: [(0, '1162.888')]
[31m[27529395 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[27529395 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[27529395 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:12:20,999][166323] Fps is (10 sec: 1630.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7815168. Throughput: 0: 283.2. Samples: 7815328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:12:20,999][166323] Avg episode reward: [(0, '1123.749')]
[36m[2025-07-02 04:12:25,951][166323] Fps is (10 sec: 1639.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7815168. Throughput: 0: 281.0. Samples: 7816928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:12:25,951][166323] Avg episode reward: [(0, '1141.549')]
[36m[2025-07-02 04:12:30,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7815168. Throughput: 0: 275.6. Samples: 7818608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:12:30,960][166323] Avg episode reward: [(0, '1225.777')]
[31m[27541582 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[27541582 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[27541582 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:12:35,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7815168. Throughput: 0: 276.1. Samples: 7819456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:12:35,988][166323] Avg episode reward: [(0, '1215.670')]
[36m[2025-07-02 04:12:40,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7815168. Throughput: 0: 274.1. Samples: 7821136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:12:40,946][166323] Avg episode reward: [(0, '1179.276')]
[36m[2025-07-02 04:12:45,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7815168. Throughput: 0: 272.9. Samples: 7822768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:12:45,989][166323] Avg episode reward: [(0, '1138.316')]
[36m[2025-07-02 04:12:50,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7815168. Throughput: 0: 269.9. Samples: 7823552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:12:50,950][166323] Avg episode reward: [(0, '1189.762')]
[36m[2025-07-02 04:12:55,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7815168. Throughput: 0: 271.5. Samples: 7825248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:12:55,963][166323] Avg episode reward: [(0, '1252.489')]
[31m[27568247 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[27568248 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[27568248 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:13:01,002][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7815168. Throughput: 0: 272.4. Samples: 7826960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:13:01,002][166323] Avg episode reward: [(0, '1245.177')]
[31m[27572421 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[27572421 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[27572421 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:13:05,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7815168. Throughput: 0: 278.6. Samples: 7827856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:13:05,964][166323] Avg episode reward: [(0, '1165.197')]
[36m[2025-07-02 04:13:10,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7815168. Throughput: 0: 281.6. Samples: 7829600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:13:10,954][166323] Avg episode reward: [(0, '1170.021')]
[36m[2025-07-02 04:13:16,000][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 7815168. Throughput: 0: 279.9. Samples: 7831216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:13:16,000][166323] Avg episode reward: [(0, '1118.464')]
[36m[2025-07-02 04:13:20,950][166323] Fps is (10 sec: 1639.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7831552. Throughput: 0: 277.9. Samples: 7831952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:13:20,950][166323] Avg episode reward: [(0, '1133.256')]
[36m[2025-07-02 04:13:25,994][166323] Fps is (10 sec: 1639.3, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7831552. Throughput: 0: 274.2. Samples: 7833488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:13:25,995][166323] Avg episode reward: [(0, '1147.850')]
[36m[2025-07-02 04:13:30,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7831552. Throughput: 0: 274.3. Samples: 7835104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:13:30,961][166323] Avg episode reward: [(0, '1185.008')]
[36m[2025-07-02 04:13:35,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7831552. Throughput: 0: 273.3. Samples: 7835856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:13:35,971][166323] Avg episode reward: [(0, '1190.738')]
[37m[1m[2025-07-02 04:13:36,031][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015288_7831552.pth...
[36m[2025-07-02 04:13:36,035][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015160_7766016.pth
[36m[2025-07-02 04:13:40,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7831552. Throughput: 0: 273.1. Samples: 7837536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:13:40,957][166323] Avg episode reward: [(0, '1226.493')]
[36m[2025-07-02 04:13:45,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7831552. Throughput: 0: 269.9. Samples: 7839104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:13:45,996][166323] Avg episode reward: [(0, '1276.784')]
[36m[2025-07-02 04:13:50,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7831552. Throughput: 0: 269.0. Samples: 7839968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:13:50,993][166323] Avg episode reward: [(0, '1272.367')]
[36m[2025-07-02 04:13:55,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7831552. Throughput: 0: 267.9. Samples: 7841664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:13:55,991][166323] Avg episode reward: [(0, '1301.084')]
[36m[2025-07-02 04:14:01,011][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7831552. Throughput: 0: 267.7. Samples: 7843264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:14:01,012][166323] Avg episode reward: [(0, '1272.896')]
[36m[2025-07-02 04:14:05,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7831552. Throughput: 0: 269.0. Samples: 7844064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:14:05,981][166323] Avg episode reward: [(0, '1270.019')]
[36m[2025-07-02 04:14:10,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7831552. Throughput: 0: 273.0. Samples: 7845760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:14:10,953][166323] Avg episode reward: [(0, '1278.349')]
[36m[2025-07-02 04:14:15,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7831552. Throughput: 0: 275.8. Samples: 7847520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:14:15,977][166323] Avg episode reward: [(0, '1281.169')]
[36m[2025-07-02 04:14:20,976][166323] Fps is (10 sec: 1634.5, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7847936. Throughput: 0: 279.4. Samples: 7848432. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 04:14:20,977][166323] Avg episode reward: [(0, '1233.073')]
[36m[2025-07-02 04:14:25,965][166323] Fps is (10 sec: 1640.3, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7847936. Throughput: 0: 278.0. Samples: 7850048. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 04:14:25,966][166323] Avg episode reward: [(0, '1256.307')]
[36m[2025-07-02 04:14:30,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7847936. Throughput: 0: 281.2. Samples: 7851744. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 04:14:30,952][166323] Avg episode reward: [(0, '1266.520')]
[31m[27660872 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[27660873 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[27660873 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:14:35,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7847936. Throughput: 0: 279.2. Samples: 7852528. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 04:14:35,983][166323] Avg episode reward: [(0, '1248.710')]
[36m[2025-07-02 04:14:40,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7847936. Throughput: 0: 275.7. Samples: 7854064. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 04:14:40,975][166323] Avg episode reward: [(0, '1235.056')]
[36m[2025-07-02 04:14:45,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7847936. Throughput: 0: 274.4. Samples: 7855600. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 04:14:45,961][166323] Avg episode reward: [(0, '1247.600')]
[36m[2025-07-02 04:14:50,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7847936. Throughput: 0: 277.6. Samples: 7856544. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 04:14:50,943][166323] Avg episode reward: [(0, '1231.050')]
[36m[2025-07-02 04:14:55,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7847936. Throughput: 0: 275.8. Samples: 7858176. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 04:14:55,976][166323] Avg episode reward: [(0, '1287.281')]
[36m[2025-07-02 04:15:00,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7847936. Throughput: 0: 274.2. Samples: 7859856. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 04:15:00,964][166323] Avg episode reward: [(0, '1293.897')]
[36m[2025-07-02 04:15:05,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7847936. Throughput: 0: 273.5. Samples: 7860736. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 04:15:05,958][166323] Avg episode reward: [(0, '1215.105')]
[36m[2025-07-02 04:15:10,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7847936. Throughput: 0: 276.0. Samples: 7862464. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 04:15:10,949][166323] Avg episode reward: [(0, '1192.493')]
[36m[2025-07-02 04:15:16,003][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 7847936. Throughput: 0: 272.0. Samples: 7864000. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 04:15:16,003][166323] Avg episode reward: [(0, '1202.782')]
[36m[2025-07-02 04:15:20,955][166323] Fps is (10 sec: 1637.5, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7864320. Throughput: 0: 272.2. Samples: 7864768. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 04:15:20,955][166323] Avg episode reward: [(0, '1125.478')]
[36m[2025-07-02 04:15:25,974][166323] Fps is (10 sec: 1643.1, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7864320. Throughput: 0: 273.8. Samples: 7866384. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 04:15:25,975][166323] Avg episode reward: [(0, '1145.542')]
[36m[2025-07-02 04:15:30,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7864320. Throughput: 0: 277.9. Samples: 7868112. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 04:15:30,984][166323] Avg episode reward: [(0, '1177.051')]
[36m[2025-07-02 04:15:35,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7864320. Throughput: 0: 276.1. Samples: 7868976. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 04:15:35,964][166323] Avg episode reward: [(0, '1224.389')]
[37m[1m[2025-07-02 04:15:36,024][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015352_7864320.pth...
[36m[2025-07-02 04:15:36,028][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015224_7798784.pth
[36m[2025-07-02 04:15:40,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7864320. Throughput: 0: 277.6. Samples: 7870672. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 04:15:40,988][166323] Avg episode reward: [(0, '1253.247')]
[36m[2025-07-02 04:15:45,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7864320. Throughput: 0: 278.1. Samples: 7872368. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 04:15:45,948][166323] Avg episode reward: [(0, '1245.623')]
[36m[2025-07-02 04:15:50,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7864320. Throughput: 0: 277.3. Samples: 7873216. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 04:15:50,961][166323] Avg episode reward: [(0, '1273.683')]
[36m[2025-07-02 04:15:55,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7864320. Throughput: 0: 274.0. Samples: 7874800. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 04:15:55,974][166323] Avg episode reward: [(0, '1239.679')]
[36m[2025-07-02 04:16:00,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7864320. Throughput: 0: 274.9. Samples: 7876368. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 04:16:00,992][166323] Avg episode reward: [(0, '1271.256')]
[31m[27753601 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[27753602 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[27753602 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:16:05,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7864320. Throughput: 0: 276.0. Samples: 7877184. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 04:16:05,949][166323] Avg episode reward: [(0, '1225.035')]
[36m[2025-07-02 04:16:10,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7864320. Throughput: 0: 277.6. Samples: 7878880. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 04:16:10,985][166323] Avg episode reward: [(0, '1206.169')]
[36m[2025-07-02 04:16:15,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.1). Total num frames: 7864320. Throughput: 0: 273.5. Samples: 7880416. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 04:16:15,977][166323] Avg episode reward: [(0, '1195.263')]
[31m[27768768 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[27768769 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[27768769 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:16:20,958][166323] Fps is (10 sec: 1642.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7880704. Throughput: 0: 270.3. Samples: 7881136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:16:20,959][166323] Avg episode reward: [(0, '1181.177')]
[36m[2025-07-02 04:16:25,972][166323] Fps is (10 sec: 1639.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7880704. Throughput: 0: 268.9. Samples: 7882768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:16:25,973][166323] Avg episode reward: [(0, '1228.979')]
[36m[2025-07-02 04:16:30,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7880704. Throughput: 0: 269.1. Samples: 7884480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:16:30,957][166323] Avg episode reward: [(0, '1212.017')]
[36m[2025-07-02 04:16:35,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7880704. Throughput: 0: 270.2. Samples: 7885376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:16:35,973][166323] Avg episode reward: [(0, '1218.477')]
[36m[2025-07-02 04:16:40,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7880704. Throughput: 0: 271.7. Samples: 7887024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:16:40,959][166323] Avg episode reward: [(0, '1258.855')]
[36m[2025-07-02 04:16:45,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7880704. Throughput: 0: 274.5. Samples: 7888720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:16:45,990][166323] Avg episode reward: [(0, '1187.466')]
[36m[2025-07-02 04:16:50,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7880704. Throughput: 0: 274.7. Samples: 7889552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:16:50,980][166323] Avg episode reward: [(0, '1195.792')]
[36m[2025-07-02 04:16:55,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7880704. Throughput: 0: 272.1. Samples: 7891120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:16:55,963][166323] Avg episode reward: [(0, '1200.436')]
[36m[2025-07-02 04:17:00,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7880704. Throughput: 0: 278.8. Samples: 7892960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:17:00,965][166323] Avg episode reward: [(0, '1226.096')]
[36m[2025-07-02 04:17:05,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7880704. Throughput: 0: 279.9. Samples: 7893728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:17:05,952][166323] Avg episode reward: [(0, '1278.405')]
[36m[2025-07-02 04:17:10,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7880704. Throughput: 0: 282.4. Samples: 7895472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:17:10,964][166323] Avg episode reward: [(0, '1245.801')]
[31m[27821599 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[27821599 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[27821600 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:17:15,949][166323] Fps is (10 sec: 1638.9, 60 sec: 546.4, 300 sec: 277.7). Total num frames: 7897088. Throughput: 0: 282.4. Samples: 7897184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:17:15,949][166323] Avg episode reward: [(0, '1266.120')]
[36m[2025-07-02 04:17:20,990][166323] Fps is (10 sec: 1634.1, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7897088. Throughput: 0: 282.9. Samples: 7898112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:17:20,990][166323] Avg episode reward: [(0, '1369.881')]
[36m[2025-07-02 04:17:25,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7897088. Throughput: 0: 281.7. Samples: 7899712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:17:25,994][166323] Avg episode reward: [(0, '1362.007')]
[36m[2025-07-02 04:17:30,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7897088. Throughput: 0: 283.1. Samples: 7901456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:17:30,984][166323] Avg episode reward: [(0, '1273.532')]
[36m[2025-07-02 04:17:35,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7897088. Throughput: 0: 284.1. Samples: 7902336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:17:35,985][166323] Avg episode reward: [(0, '1268.205')]
[37m[1m[2025-07-02 04:17:36,043][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015416_7897088.pth...
[36m[2025-07-02 04:17:36,048][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015288_7831552.pth
[31m[27848811 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[27848811 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[27848811 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:17:40,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7897088. Throughput: 0: 287.0. Samples: 7904032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:17:40,959][166323] Avg episode reward: [(0, '1210.965')]
[36m[2025-07-02 04:17:45,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7897088. Throughput: 0: 283.1. Samples: 7905696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:17:45,953][166323] Avg episode reward: [(0, '1257.519')]
[36m[2025-07-02 04:17:50,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7897088. Throughput: 0: 282.6. Samples: 7906448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:17:50,962][166323] Avg episode reward: [(0, '1232.415')]
[36m[2025-07-02 04:17:55,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7897088. Throughput: 0: 279.5. Samples: 7908048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:17:55,958][166323] Avg episode reward: [(0, '1202.091')]
[36m[2025-07-02 04:18:01,008][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7897088. Throughput: 0: 279.8. Samples: 7909792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:18:01,008][166323] Avg episode reward: [(0, '1244.731')]
[36m[2025-07-02 04:18:05,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7897088. Throughput: 0: 279.5. Samples: 7910688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:18:05,988][166323] Avg episode reward: [(0, '1255.674')]
[36m[2025-07-02 04:18:10,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7897088. Throughput: 0: 281.4. Samples: 7912368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:18:10,962][166323] Avg episode reward: [(0, '1243.125')]
[36m[2025-07-02 04:18:15,967][166323] Fps is (10 sec: 1641.9, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7913472. Throughput: 0: 282.8. Samples: 7914176. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:18:15,967][166323] Avg episode reward: [(0, '1303.151')]
[36m[2025-07-02 04:18:20,944][166323] Fps is (10 sec: 1641.2, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7913472. Throughput: 0: 281.5. Samples: 7914992. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:18:20,945][166323] Avg episode reward: [(0, '1267.284')]
[36m[2025-07-02 04:18:25,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7913472. Throughput: 0: 278.8. Samples: 7916576. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:18:25,957][166323] Avg episode reward: [(0, '1274.236')]
[36m[2025-07-02 04:18:30,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7913472. Throughput: 0: 280.2. Samples: 7918304. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:18:30,947][166323] Avg episode reward: [(0, '1254.641')]
[31m[27901826 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[27901826 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[27901826 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:18:36,000][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7913472. Throughput: 0: 282.1. Samples: 7919152. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:18:36,001][166323] Avg episode reward: [(0, '1204.979')]
[36m[2025-07-02 04:18:40,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7913472. Throughput: 0: 283.9. Samples: 7920832. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:18:40,993][166323] Avg episode reward: [(0, '1231.047')]
[36m[2025-07-02 04:18:45,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7913472. Throughput: 0: 282.0. Samples: 7922464. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:18:45,949][166323] Avg episode reward: [(0, '1244.922')]
[36m[2025-07-02 04:18:50,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7913472. Throughput: 0: 281.2. Samples: 7923344. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:18:50,992][166323] Avg episode reward: [(0, '1198.267')]
[36m[2025-07-02 04:18:55,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7913472. Throughput: 0: 280.2. Samples: 7924976. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:18:55,955][166323] Avg episode reward: [(0, '1189.244')]
[36m[2025-07-02 04:19:00,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7913472. Throughput: 0: 276.7. Samples: 7926624. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:19:00,952][166323] Avg episode reward: [(0, '1197.113')]
[36m[2025-07-02 04:19:05,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7913472. Throughput: 0: 277.5. Samples: 7927488. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:19:05,980][166323] Avg episode reward: [(0, '1205.779')]
[31m[27938867 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[27938867 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[27938867 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:19:10,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7913472. Throughput: 0: 280.0. Samples: 7929184. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:19:10,981][166323] Avg episode reward: [(0, '1205.555')]
[36m[2025-07-02 04:19:15,980][166323] Fps is (10 sec: 1638.3, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7929856. Throughput: 0: 277.5. Samples: 7930800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:19:15,980][166323] Avg episode reward: [(0, '1202.448')]
[36m[2025-07-02 04:19:20,947][166323] Fps is (10 sec: 1643.9, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7929856. Throughput: 0: 276.2. Samples: 7931568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:19:20,947][166323] Avg episode reward: [(0, '1114.789')]
[36m[2025-07-02 04:19:25,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7929856. Throughput: 0: 276.4. Samples: 7933264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:19:25,979][166323] Avg episode reward: [(0, '1130.194')]
[36m[2025-07-02 04:19:30,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7929856. Throughput: 0: 277.3. Samples: 7934944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:19:30,948][166323] Avg episode reward: [(0, '1148.860')]
[31m[27960324 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[27960324 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[27960325 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:19:35,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7929856. Throughput: 0: 276.8. Samples: 7935792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:19:35,956][166323] Avg episode reward: [(0, '1147.808')]
[37m[1m[2025-07-02 04:19:36,026][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015480_7929856.pth...
[36m[2025-07-02 04:19:36,030][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015352_7864320.pth
[36m[2025-07-02 04:19:40,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7929856. Throughput: 0: 277.3. Samples: 7937456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:19:40,962][166323] Avg episode reward: [(0, '1141.596')]
[31m[27971699 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[27971699 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[27971699 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:19:45,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7929856. Throughput: 0: 275.3. Samples: 7939024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:19:45,987][166323] Avg episode reward: [(0, '1184.432')]
[36m[2025-07-02 04:19:50,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7929856. Throughput: 0: 275.4. Samples: 7939872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:19:50,948][166323] Avg episode reward: [(0, '1199.617')]
[36m[2025-07-02 04:19:55,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7929856. Throughput: 0: 272.4. Samples: 7941440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:19:55,965][166323] Avg episode reward: [(0, '1228.442')]
[36m[2025-07-02 04:20:00,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7929856. Throughput: 0: 275.1. Samples: 7943168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:20:00,943][166323] Avg episode reward: [(0, '1237.077')]
[36m[2025-07-02 04:20:05,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7929856. Throughput: 0: 277.1. Samples: 7944048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:20:05,990][166323] Avg episode reward: [(0, '1274.243')]
[36m[2025-07-02 04:20:10,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7929856. Throughput: 0: 278.6. Samples: 7945792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:20:10,948][166323] Avg episode reward: [(0, '1279.041')]
[36m[2025-07-02 04:20:15,957][166323] Fps is (10 sec: 1643.7, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7946240. Throughput: 0: 278.7. Samples: 7947488. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:20:15,958][166323] Avg episode reward: [(0, '1300.280')]
[31m[28007058 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[28007058 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[28007058 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:20:20,991][166323] Fps is (10 sec: 1631.5, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7946240. Throughput: 0: 276.8. Samples: 7948256. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:20:20,991][166323] Avg episode reward: [(0, '1256.218')]
[36m[2025-07-02 04:20:25,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7946240. Throughput: 0: 280.2. Samples: 7950064. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:20:25,956][166323] Avg episode reward: [(0, '1215.739')]
[36m[2025-07-02 04:20:30,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7946240. Throughput: 0: 283.2. Samples: 7951760. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:20:30,962][166323] Avg episode reward: [(0, '1266.405')]
[36m[2025-07-02 04:20:36,003][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7946240. Throughput: 0: 282.3. Samples: 7952592. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:20:36,004][166323] Avg episode reward: [(0, '1279.867')]
[36m[2025-07-02 04:20:40,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7946240. Throughput: 0: 283.3. Samples: 7954192. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:20:40,982][166323] Avg episode reward: [(0, '1238.339')]
[36m[2025-07-02 04:20:45,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7946240. Throughput: 0: 280.9. Samples: 7955808. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:20:45,949][166323] Avg episode reward: [(0, '1238.998')]
[36m[2025-07-02 04:20:50,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7946240. Throughput: 0: 282.1. Samples: 7956736. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:20:50,962][166323] Avg episode reward: [(0, '1222.171')]
[36m[2025-07-02 04:20:55,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7946240. Throughput: 0: 279.7. Samples: 7958384. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:20:55,968][166323] Avg episode reward: [(0, '1319.919')]
[36m[2025-07-02 04:21:00,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7946240. Throughput: 0: 278.1. Samples: 7960000. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:21:00,944][166323] Avg episode reward: [(0, '1374.642')]
[36m[2025-07-02 04:21:05,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7946240. Throughput: 0: 280.2. Samples: 7960864. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:21:05,980][166323] Avg episode reward: [(0, '1303.626')]
[36m[2025-07-02 04:21:10,988][166323] Fps is (10 sec: 1631.2, 60 sec: 545.8, 300 sec: 333.2). Total num frames: 7962624. Throughput: 0: 276.8. Samples: 7962528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:21:10,989][166323] Avg episode reward: [(0, '1294.506')]
[36m[2025-07-02 04:21:15,990][166323] Fps is (10 sec: 1636.8, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7962624. Throughput: 0: 274.0. Samples: 7964096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:21:15,990][166323] Avg episode reward: [(0, '1283.320')]
[36m[2025-07-02 04:21:20,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7962624. Throughput: 0: 272.7. Samples: 7964848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:21:20,946][166323] Avg episode reward: [(0, '1268.196')]
[36m[2025-07-02 04:21:25,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7962624. Throughput: 0: 272.4. Samples: 7966448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:21:25,977][166323] Avg episode reward: [(0, '1264.558')]
[36m[2025-07-02 04:21:30,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7962624. Throughput: 0: 275.0. Samples: 7968192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:21:30,988][166323] Avg episode reward: [(0, '1241.618')]
[36m[2025-07-02 04:21:35,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7962624. Throughput: 0: 272.9. Samples: 7969024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:21:35,983][166323] Avg episode reward: [(0, '1219.341')]
[37m[1m[2025-07-02 04:21:36,067][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015544_7962624.pth...
[36m[2025-07-02 04:21:36,071][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015416_7897088.pth
[36m[2025-07-02 04:21:40,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7962624. Throughput: 0: 270.0. Samples: 7970528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:21:40,947][166323] Avg episode reward: [(0, '1269.096')]
[31m[28093215 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[28093215 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[28093216 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:21:45,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7962624. Throughput: 0: 270.8. Samples: 7972192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:21:45,962][166323] Avg episode reward: [(0, '1186.162')]
[36m[2025-07-02 04:21:50,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7962624. Throughput: 0: 269.7. Samples: 7972992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:21:50,945][166323] Avg episode reward: [(0, '1223.176')]
[36m[2025-07-02 04:21:55,997][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7962624. Throughput: 0: 272.0. Samples: 7974768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:21:55,997][166323] Avg episode reward: [(0, '1271.052')]
[36m[2025-07-02 04:22:00,976][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7962624. Throughput: 0: 272.4. Samples: 7976352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:22:00,977][166323] Avg episode reward: [(0, '1248.406')]
[36m[2025-07-02 04:22:05,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7962624. Throughput: 0: 272.6. Samples: 7977120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:22:05,957][166323] Avg episode reward: [(0, '1220.328')]
[36m[2025-07-02 04:22:10,947][166323] Fps is (10 sec: 1643.2, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 7979008. Throughput: 0: 277.2. Samples: 7978912. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 04:22:10,947][166323] Avg episode reward: [(0, '1208.839')]
[33m[28122852 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[28122852 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.859375
[33mCrash Rate: 0.12890625
[33mTimeout Rate: 0.01171875 (navigation_task.py:265)
[33m[28122852 ms][navigation_task] - WARNING : 
[33mSuccesses: 1760
[33mCrashes : 264
[33mTimeouts: 24 (navigation_task.py:268)
[36m[2025-07-02 04:22:15,982][166323] Fps is (10 sec: 1634.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7979008. Throughput: 0: 275.2. Samples: 7980576. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 04:22:15,983][166323] Avg episode reward: [(0, '1156.436')]
[36m[2025-07-02 04:22:20,977][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7979008. Throughput: 0: 274.2. Samples: 7981360. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 04:22:20,977][166323] Avg episode reward: [(0, '1193.111')]
[36m[2025-07-02 04:22:25,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7979008. Throughput: 0: 278.4. Samples: 7983056. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 04:22:25,951][166323] Avg episode reward: [(0, '1156.375')]
[36m[2025-07-02 04:22:30,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7979008. Throughput: 0: 281.5. Samples: 7984864. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 04:22:30,975][166323] Avg episode reward: [(0, '1129.566')]
[36m[2025-07-02 04:22:35,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7979008. Throughput: 0: 284.3. Samples: 7985792. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 04:22:35,974][166323] Avg episode reward: [(0, '1183.672')]
[36m[2025-07-02 04:22:40,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7979008. Throughput: 0: 281.4. Samples: 7987424. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 04:22:40,967][166323] Avg episode reward: [(0, '1186.846')]
[36m[2025-07-02 04:22:45,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 7979008. Throughput: 0: 279.7. Samples: 7988944. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 04:22:45,989][166323] Avg episode reward: [(0, '1171.234')]
[31m[28158125 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[28158125 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[28158125 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:22:50,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7979008. Throughput: 0: 278.7. Samples: 7989664. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 04:22:50,961][166323] Avg episode reward: [(0, '1252.126')]
[36m[2025-07-02 04:22:55,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7979008. Throughput: 0: 279.3. Samples: 7991488. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 04:22:55,967][166323] Avg episode reward: [(0, '1239.883')]
[36m[2025-07-02 04:23:00,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7979008. Throughput: 0: 284.1. Samples: 7993360. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 04:23:00,978][166323] Avg episode reward: [(0, '1272.389')]
[36m[2025-07-02 04:23:05,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7979008. Throughput: 0: 285.7. Samples: 7994208. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 04:23:05,947][166323] Avg episode reward: [(0, '1240.635')]
[36m[2025-07-02 04:23:10,967][166323] Fps is (10 sec: 1640.3, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7995392. Throughput: 0: 285.4. Samples: 7995904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:23:10,967][166323] Avg episode reward: [(0, '1257.382')]
[36m[2025-07-02 04:23:15,955][166323] Fps is (10 sec: 1637.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7995392. Throughput: 0: 283.1. Samples: 7997600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:23:15,955][166323] Avg episode reward: [(0, '1260.987')]
[36m[2025-07-02 04:23:20,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7995392. Throughput: 0: 283.3. Samples: 7998544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:23:20,983][166323] Avg episode reward: [(0, '1270.505')]
[31m[28190902 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[28190902 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[28190902 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:23:25,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7995392. Throughput: 0: 287.9. Samples: 8000384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:23:25,976][166323] Avg episode reward: [(0, '1233.110')]
[36m[2025-07-02 04:23:30,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7995392. Throughput: 0: 293.2. Samples: 8002128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:23:30,949][166323] Avg episode reward: [(0, '1227.514')]
[36m[2025-07-02 04:23:35,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7995392. Throughput: 0: 297.3. Samples: 8003040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:23:35,958][166323] Avg episode reward: [(0, '1236.458')]
[37m[1m[2025-07-02 04:23:36,019][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015608_7995392.pth...
[36m[2025-07-02 04:23:36,024][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015480_7929856.pth
[31m[28207715 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[28207716 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[28207716 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:23:40,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7995392. Throughput: 0: 294.2. Samples: 8004720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:23:40,943][166323] Avg episode reward: [(0, '1229.608')]
[36m[2025-07-02 04:23:45,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7995392. Throughput: 0: 290.1. Samples: 8006416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:23:45,988][166323] Avg episode reward: [(0, '1215.379')]
[36m[2025-07-02 04:23:50,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 7995392. Throughput: 0: 289.5. Samples: 8007248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:23:50,984][166323] Avg episode reward: [(0, '1191.038')]
[36m[2025-07-02 04:23:55,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 7995392. Throughput: 0: 290.2. Samples: 8008960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:23:55,949][166323] Avg episode reward: [(0, '1186.820')]
[36m[2025-07-02 04:24:00,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 7995392. Throughput: 0: 289.8. Samples: 8010640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:24:00,958][166323] Avg episode reward: [(0, '1199.229')]
[36m[2025-07-02 04:24:05,994][166323] Fps is (10 sec: 1631.1, 60 sec: 545.7, 300 sec: 333.2). Total num frames: 8011776. Throughput: 0: 290.1. Samples: 8011600. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 04:24:05,994][166323] Avg episode reward: [(0, '1238.547')]
[36m[2025-07-02 04:24:10,975][166323] Fps is (10 sec: 1635.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8011776. Throughput: 0: 286.9. Samples: 8013296. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 04:24:10,975][166323] Avg episode reward: [(0, '1234.488')]
[36m[2025-07-02 04:24:15,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8011776. Throughput: 0: 285.2. Samples: 8014960. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 04:24:15,946][166323] Avg episode reward: [(0, '1246.965')]
[31m[28248181 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[28248182 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[28248182 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:24:20,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8011776. Throughput: 0: 285.8. Samples: 8015904. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 04:24:20,971][166323] Avg episode reward: [(0, '1231.875')]
[36m[2025-07-02 04:24:25,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8011776. Throughput: 0: 283.6. Samples: 8017488. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 04:24:25,966][166323] Avg episode reward: [(0, '1236.076')]
[36m[2025-07-02 04:24:30,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8011776. Throughput: 0: 286.3. Samples: 8019296. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 04:24:30,982][166323] Avg episode reward: [(0, '1211.821')]
[36m[2025-07-02 04:24:35,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8011776. Throughput: 0: 288.0. Samples: 8020208. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 04:24:35,990][166323] Avg episode reward: [(0, '1168.629')]
[36m[2025-07-02 04:24:40,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8011776. Throughput: 0: 290.9. Samples: 8022048. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 04:24:40,945][166323] Avg episode reward: [(0, '1196.685')]
[36m[2025-07-02 04:24:45,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8011776. Throughput: 0: 295.3. Samples: 8023936. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 04:24:45,989][166323] Avg episode reward: [(0, '1226.964')]
[36m[2025-07-02 04:24:50,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8011776. Throughput: 0: 291.5. Samples: 8024704. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 04:24:50,946][166323] Avg episode reward: [(0, '1155.474')]
[31m[28283600 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[28283600 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[28283600 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:24:55,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8011776. Throughput: 0: 293.7. Samples: 8026512. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 04:24:55,968][166323] Avg episode reward: [(0, '1182.026')]
[36m[2025-07-02 04:25:00,967][166323] Fps is (10 sec: 1635.0, 60 sec: 546.0, 300 sec: 333.3). Total num frames: 8028160. Throughput: 0: 294.3. Samples: 8028208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:25:00,967][166323] Avg episode reward: [(0, '1179.055')]
[36m[2025-07-02 04:25:05,989][166323] Fps is (10 sec: 1634.8, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 8028160. Throughput: 0: 292.5. Samples: 8029072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:25:05,990][166323] Avg episode reward: [(0, '1168.507')]
[36m[2025-07-02 04:25:10,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8028160. Throughput: 0: 295.8. Samples: 8030800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:25:10,977][166323] Avg episode reward: [(0, '1153.455')]
[31m[28302488 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[28302488 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[28302489 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:25:15,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8028160. Throughput: 0: 293.0. Samples: 8032480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:25:15,985][166323] Avg episode reward: [(0, '1056.338')]
[36m[2025-07-02 04:25:20,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8028160. Throughput: 0: 291.4. Samples: 8033312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:25:20,955][166323] Avg episode reward: [(0, '1138.239')]
[31m[28314272 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[28314272 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[28314273 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:25:25,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8028160. Throughput: 0: 286.7. Samples: 8034960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:25:25,985][166323] Avg episode reward: [(0, '1210.805')]
[36m[2025-07-02 04:25:30,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8028160. Throughput: 0: 280.7. Samples: 8036560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:25:30,959][166323] Avg episode reward: [(0, '1234.667')]
[36m[2025-07-02 04:25:35,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8028160. Throughput: 0: 281.9. Samples: 8037392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:25:35,960][166323] Avg episode reward: [(0, '1227.770')]
[37m[1m[2025-07-02 04:25:36,022][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015672_8028160.pth...
[36m[2025-07-02 04:25:36,026][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015544_7962624.pth
[36m[2025-07-02 04:25:40,997][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 8028160. Throughput: 0: 277.9. Samples: 8039024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:25:40,997][166323] Avg episode reward: [(0, '1184.243')]
[36m[2025-07-02 04:25:45,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8028160. Throughput: 0: 277.5. Samples: 8040704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:25:45,992][166323] Avg episode reward: [(0, '1261.171')]
[36m[2025-07-02 04:25:50,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8028160. Throughput: 0: 275.7. Samples: 8041472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:25:50,968][166323] Avg episode reward: [(0, '1311.315')]
[36m[2025-07-02 04:25:55,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8028160. Throughput: 0: 275.6. Samples: 8043200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:25:55,963][166323] Avg episode reward: [(0, '1267.756')]
[36m[2025-07-02 04:26:00,967][166323] Fps is (10 sec: 1638.5, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 8044544. Throughput: 0: 273.9. Samples: 8044800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:26:00,967][166323] Avg episode reward: [(0, '1291.527')]
[36m[2025-07-02 04:26:05,967][166323] Fps is (10 sec: 1637.7, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8044544. Throughput: 0: 273.7. Samples: 8045632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:26:05,967][166323] Avg episode reward: [(0, '1270.367')]
[36m[2025-07-02 04:26:10,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8044544. Throughput: 0: 274.3. Samples: 8047296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:26:10,961][166323] Avg episode reward: [(0, '1321.200')]
[36m[2025-07-02 04:26:15,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8044544. Throughput: 0: 277.5. Samples: 8049056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:26:15,986][166323] Avg episode reward: [(0, '1318.862')]
[36m[2025-07-02 04:26:20,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8044544. Throughput: 0: 278.8. Samples: 8049936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:26:20,948][166323] Avg episode reward: [(0, '1303.097')]
[36m[2025-07-02 04:26:25,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8044544. Throughput: 0: 279.5. Samples: 8051600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:26:25,986][166323] Avg episode reward: [(0, '1291.068')]
[36m[2025-07-02 04:26:30,998][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8044544. Throughput: 0: 277.6. Samples: 8053200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:26:30,999][166323] Avg episode reward: [(0, '1245.096')]
[36m[2025-07-02 04:26:35,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 8044544. Throughput: 0: 280.0. Samples: 8054080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:26:35,995][166323] Avg episode reward: [(0, '1272.493')]
[36m[2025-07-02 04:26:41,008][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8044544. Throughput: 0: 278.8. Samples: 8055760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:26:41,009][166323] Avg episode reward: [(0, '1250.050')]
[31m[28391644 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[28391645 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[28391645 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:26:45,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8044544. Throughput: 0: 280.6. Samples: 8057424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:26:45,957][166323] Avg episode reward: [(0, '1187.924')]
[36m[2025-07-02 04:26:50,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8044544. Throughput: 0: 280.8. Samples: 8058272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:26:50,983][166323] Avg episode reward: [(0, '1226.739')]
[36m[2025-07-02 04:26:55,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8044544. Throughput: 0: 281.3. Samples: 8059952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:26:55,954][166323] Avg episode reward: [(0, '1193.822')]
[31m[28406293 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[28406293 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[28406294 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:27:00,959][166323] Fps is (10 sec: 1642.2, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 8060928. Throughput: 0: 278.6. Samples: 8061584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:27:00,959][166323] Avg episode reward: [(0, '1270.542')]
[36m[2025-07-02 04:27:05,959][166323] Fps is (10 sec: 1637.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8060928. Throughput: 0: 278.0. Samples: 8062448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:27:05,960][166323] Avg episode reward: [(0, '1269.014')]
[36m[2025-07-02 04:27:10,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8060928. Throughput: 0: 282.2. Samples: 8064288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:27:10,952][166323] Avg episode reward: [(0, '1284.810')]
[36m[2025-07-02 04:27:15,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8060928. Throughput: 0: 285.1. Samples: 8066016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:27:15,959][166323] Avg episode reward: [(0, '1329.803')]
[36m[2025-07-02 04:27:20,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8060928. Throughput: 0: 283.6. Samples: 8066832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:27:20,952][166323] Avg episode reward: [(0, '1363.895')]
[36m[2025-07-02 04:27:25,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8060928. Throughput: 0: 282.6. Samples: 8068464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:27:25,968][166323] Avg episode reward: [(0, '1339.978')]
[36m[2025-07-02 04:27:30,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8060928. Throughput: 0: 285.3. Samples: 8070272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:27:30,997][166323] Avg episode reward: [(0, '1331.202')]
[36m[2025-07-02 04:27:35,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8060928. Throughput: 0: 284.3. Samples: 8071056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:27:35,955][166323] Avg episode reward: [(0, '1360.945')]
[37m[1m[2025-07-02 04:27:36,006][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015736_8060928.pth...
[36m[2025-07-02 04:27:36,010][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015608_7995392.pth
[36m[2025-07-02 04:27:40,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8060928. Throughput: 0: 282.7. Samples: 8072672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:27:40,950][166323] Avg episode reward: [(0, '1297.310')]
[36m[2025-07-02 04:27:45,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8060928. Throughput: 0: 288.5. Samples: 8074576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:27:45,994][166323] Avg episode reward: [(0, '1303.093')]
[36m[2025-07-02 04:27:50,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8060928. Throughput: 0: 288.2. Samples: 8075424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:27:50,981][166323] Avg episode reward: [(0, '1286.455')]
[36m[2025-07-02 04:27:55,954][166323] Fps is (10 sec: 1644.8, 60 sec: 546.1, 300 sec: 333.3). Total num frames: 8077312. Throughput: 0: 286.9. Samples: 8077200. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:27:55,955][166323] Avg episode reward: [(0, '1309.357')]
[36m[2025-07-02 04:28:00,978][166323] Fps is (10 sec: 1638.8, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 8077312. Throughput: 0: 284.7. Samples: 8078832. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:28:00,978][166323] Avg episode reward: [(0, '1344.820')]
[36m[2025-07-02 04:28:05,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8077312. Throughput: 0: 284.6. Samples: 8079648. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:28:05,979][166323] Avg episode reward: [(0, '1274.616')]
[36m[2025-07-02 04:28:10,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8077312. Throughput: 0: 285.5. Samples: 8081312. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:28:10,968][166323] Avg episode reward: [(0, '1294.757')]
[36m[2025-07-02 04:28:15,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8077312. Throughput: 0: 280.8. Samples: 8082896. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:28:15,961][166323] Avg episode reward: [(0, '1319.232')]
[36m[2025-07-02 04:28:20,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8077312. Throughput: 0: 282.0. Samples: 8083744. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:28:20,950][166323] Avg episode reward: [(0, '1302.792')]
[31m[28492644 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[28492644 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[28492645 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:28:25,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8077312. Throughput: 0: 280.9. Samples: 8085312. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:28:25,962][166323] Avg episode reward: [(0, '1276.825')]
[36m[2025-07-02 04:28:30,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8077312. Throughput: 0: 275.2. Samples: 8086960. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:28:30,993][166323] Avg episode reward: [(0, '1265.648')]
[36m[2025-07-02 04:28:35,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8077312. Throughput: 0: 274.1. Samples: 8087760. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:28:35,985][166323] Avg episode reward: [(0, '1245.559')]
[36m[2025-07-02 04:28:40,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8077312. Throughput: 0: 275.2. Samples: 8089584. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:28:40,957][166323] Avg episode reward: [(0, '1211.205')]
[36m[2025-07-02 04:28:45,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8077312. Throughput: 0: 279.5. Samples: 8091408. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:28:45,973][166323] Avg episode reward: [(0, '1218.313')]
[36m[2025-07-02 04:28:50,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8077312. Throughput: 0: 280.6. Samples: 8092272. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:28:50,968][166323] Avg episode reward: [(0, '1137.067')]
[36m[2025-07-02 04:28:55,988][166323] Fps is (10 sec: 1635.9, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 8093696. Throughput: 0: 282.2. Samples: 8094016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:28:55,989][166323] Avg episode reward: [(0, '1185.998')]
[36m[2025-07-02 04:29:01,013][166323] Fps is (10 sec: 1631.1, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8093696. Throughput: 0: 284.1. Samples: 8095696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:29:01,014][166323] Avg episode reward: [(0, '1177.456')]
[36m[2025-07-02 04:29:05,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8093696. Throughput: 0: 284.5. Samples: 8096560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:29:05,991][166323] Avg episode reward: [(0, '1213.199')]
[36m[2025-07-02 04:29:10,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8093696. Throughput: 0: 284.9. Samples: 8098144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:29:10,985][166323] Avg episode reward: [(0, '1296.578')]
[36m[2025-07-02 04:29:16,004][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8093696. Throughput: 0: 289.3. Samples: 8099984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:29:16,004][166323] Avg episode reward: [(0, '1315.174')]
[36m[2025-07-02 04:29:20,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8093696. Throughput: 0: 289.9. Samples: 8100800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:29:20,964][166323] Avg episode reward: [(0, '1280.739')]
[36m[2025-07-02 04:29:25,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8093696. Throughput: 0: 288.3. Samples: 8102560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:29:25,969][166323] Avg episode reward: [(0, '1342.779')]
[36m[2025-07-02 04:29:30,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8093696. Throughput: 0: 282.8. Samples: 8104128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:29:30,959][166323] Avg episode reward: [(0, '1322.510')]
[36m[2025-07-02 04:29:35,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8093696. Throughput: 0: 280.1. Samples: 8104880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:29:35,984][166323] Avg episode reward: [(0, '1307.663')]
[37m[1m[2025-07-02 04:29:36,035][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015800_8093696.pth...
[36m[2025-07-02 04:29:36,039][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015672_8028160.pth
[36m[2025-07-02 04:29:40,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8093696. Throughput: 0: 275.8. Samples: 8106416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:29:40,949][166323] Avg episode reward: [(0, '1278.147')]
[36m[2025-07-02 04:29:45,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8093696. Throughput: 0: 276.7. Samples: 8108128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:29:45,943][166323] Avg episode reward: [(0, '1253.487')]
[36m[2025-07-02 04:29:50,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8093696. Throughput: 0: 277.2. Samples: 8109024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:29:50,959][166323] Avg episode reward: [(0, '1249.523')]
[36m[2025-07-02 04:29:55,962][166323] Fps is (10 sec: 1635.3, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8110080. Throughput: 0: 280.3. Samples: 8110752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:29:55,962][166323] Avg episode reward: [(0, '1251.293')]
[36m[2025-07-02 04:30:00,985][166323] Fps is (10 sec: 1634.1, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8110080. Throughput: 0: 277.8. Samples: 8112480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:30:00,986][166323] Avg episode reward: [(0, '1222.270')]
[31m[28590808 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[28590808 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[28590808 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:30:05,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8110080. Throughput: 0: 278.4. Samples: 8113328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:30:05,965][166323] Avg episode reward: [(0, '1237.789')]
[36m[2025-07-02 04:30:10,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8110080. Throughput: 0: 278.5. Samples: 8115088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:30:10,956][166323] Avg episode reward: [(0, '1259.415')]
[36m[2025-07-02 04:30:15,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8110080. Throughput: 0: 282.9. Samples: 8116864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:30:15,972][166323] Avg episode reward: [(0, '1272.428')]
[36m[2025-07-02 04:30:20,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8110080. Throughput: 0: 287.1. Samples: 8117792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:30:20,958][166323] Avg episode reward: [(0, '1304.718')]
[36m[2025-07-02 04:30:25,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8110080. Throughput: 0: 291.8. Samples: 8119552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:30:25,962][166323] Avg episode reward: [(0, '1297.265')]
[36m[2025-07-02 04:30:30,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8110080. Throughput: 0: 289.3. Samples: 8121152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:30:30,962][166323] Avg episode reward: [(0, '1285.422')]
[36m[2025-07-02 04:30:35,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8110080. Throughput: 0: 289.4. Samples: 8122048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:30:35,968][166323] Avg episode reward: [(0, '1285.728')]
[36m[2025-07-02 04:30:40,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8110080. Throughput: 0: 292.0. Samples: 8123888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:30:40,949][166323] Avg episode reward: [(0, '1283.558')]
[36m[2025-07-02 04:30:45,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8110080. Throughput: 0: 290.3. Samples: 8125536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:30:45,966][166323] Avg episode reward: [(0, '1246.471')]
[36m[2025-07-02 04:30:50,978][166323] Fps is (10 sec: 1633.7, 60 sec: 546.0, 300 sec: 333.2). Total num frames: 8126464. Throughput: 0: 291.1. Samples: 8126432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:30:50,978][166323] Avg episode reward: [(0, '1229.803')]
[36m[2025-07-02 04:30:55,965][166323] Fps is (10 sec: 1638.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8126464. Throughput: 0: 287.9. Samples: 8128048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:30:55,965][166323] Avg episode reward: [(0, '1170.354')]
[36m[2025-07-02 04:31:01,003][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8126464. Throughput: 0: 283.9. Samples: 8129648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:31:01,003][166323] Avg episode reward: [(0, '1237.026')]
[36m[2025-07-02 04:31:05,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8126464. Throughput: 0: 280.7. Samples: 8130432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:31:05,985][166323] Avg episode reward: [(0, '1221.541')]
[36m[2025-07-02 04:31:10,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8126464. Throughput: 0: 279.0. Samples: 8132112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:31:10,985][166323] Avg episode reward: [(0, '1217.532')]
[36m[2025-07-02 04:31:15,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8126464. Throughput: 0: 279.7. Samples: 8133744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:31:15,975][166323] Avg episode reward: [(0, '1234.130')]
[36m[2025-07-02 04:31:20,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8126464. Throughput: 0: 278.3. Samples: 8134576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:31:20,982][166323] Avg episode reward: [(0, '1264.047')]
[36m[2025-07-02 04:31:25,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8126464. Throughput: 0: 274.7. Samples: 8136256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:31:25,976][166323] Avg episode reward: [(0, '1302.211')]
[36m[2025-07-02 04:31:30,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8126464. Throughput: 0: 271.4. Samples: 8137760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:31:30,999][166323] Avg episode reward: [(0, '1294.892')]
[36m[2025-07-02 04:31:35,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8126464. Throughput: 0: 270.7. Samples: 8138608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:31:35,960][166323] Avg episode reward: [(0, '1258.442')]
[37m[1m[2025-07-02 04:31:36,044][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015864_8126464.pth...
[36m[2025-07-02 04:31:36,051][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015736_8060928.pth
[36m[2025-07-02 04:31:40,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8126464. Throughput: 0: 273.5. Samples: 8140352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:31:40,951][166323] Avg episode reward: [(0, '1224.207')]
[36m[2025-07-02 04:31:45,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8126464. Throughput: 0: 274.7. Samples: 8142000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:31:45,963][166323] Avg episode reward: [(0, '1181.522')]
[36m[2025-07-02 04:31:50,976][166323] Fps is (10 sec: 1634.4, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 8142848. Throughput: 0: 276.7. Samples: 8142880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:31:50,976][166323] Avg episode reward: [(0, '1241.775')]
[36m[2025-07-02 04:31:55,963][166323] Fps is (10 sec: 1638.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8142848. Throughput: 0: 280.3. Samples: 8144720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:31:55,963][166323] Avg episode reward: [(0, '1230.255')]
[36m[2025-07-02 04:32:00,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8142848. Throughput: 0: 280.1. Samples: 8146352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:32:00,989][166323] Avg episode reward: [(0, '1220.931')]
[36m[2025-07-02 04:32:06,010][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 8142848. Throughput: 0: 278.6. Samples: 8147120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:32:06,010][166323] Avg episode reward: [(0, '1226.809')]
[36m[2025-07-02 04:32:10,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8142848. Throughput: 0: 277.2. Samples: 8148736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:32:10,999][166323] Avg episode reward: [(0, '1292.846')]
[36m[2025-07-02 04:32:15,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8142848. Throughput: 0: 280.6. Samples: 8150384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:32:15,987][166323] Avg episode reward: [(0, '1327.731')]
[36m[2025-07-02 04:32:20,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8142848. Throughput: 0: 280.4. Samples: 8151232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:32:20,979][166323] Avg episode reward: [(0, '1341.932')]
[36m[2025-07-02 04:32:25,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8142848. Throughput: 0: 277.9. Samples: 8152864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:32:25,969][166323] Avg episode reward: [(0, '1322.396')]
[36m[2025-07-02 04:32:30,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8142848. Throughput: 0: 279.7. Samples: 8154592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:32:30,990][166323] Avg episode reward: [(0, '1321.472')]
[36m[2025-07-02 04:32:35,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8142848. Throughput: 0: 279.4. Samples: 8155456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:32:35,991][166323] Avg episode reward: [(0, '1364.230')]
[36m[2025-07-02 04:32:40,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8142848. Throughput: 0: 272.7. Samples: 8156992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:32:40,966][166323] Avg episode reward: [(0, '1355.837')]
[36m[2025-07-02 04:32:45,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8142848. Throughput: 0: 272.2. Samples: 8158592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:32:45,948][166323] Avg episode reward: [(0, '1368.172')]
[31m[28755311 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[28755311 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[28755312 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:32:50,985][166323] Fps is (10 sec: 1635.2, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8159232. Throughput: 0: 271.1. Samples: 8159312. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:32:50,985][166323] Avg episode reward: [(0, '1312.248')]
[36m[2025-07-02 04:32:55,995][166323] Fps is (10 sec: 1630.7, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8159232. Throughput: 0: 273.1. Samples: 8161024. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:32:55,995][166323] Avg episode reward: [(0, '1252.671')]
[36m[2025-07-02 04:33:01,004][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8159232. Throughput: 0: 274.0. Samples: 8162720. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:33:01,004][166323] Avg episode reward: [(0, '1223.414')]
[36m[2025-07-02 04:33:05,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8159232. Throughput: 0: 274.1. Samples: 8163568. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:33:05,978][166323] Avg episode reward: [(0, '1205.571')]
[36m[2025-07-02 04:33:10,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8159232. Throughput: 0: 276.6. Samples: 8165312. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:33:10,965][166323] Avg episode reward: [(0, '1220.427')]
[36m[2025-07-02 04:33:15,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8159232. Throughput: 0: 274.8. Samples: 8166944. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:33:15,943][166323] Avg episode reward: [(0, '1173.706')]
[36m[2025-07-02 04:33:20,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8159232. Throughput: 0: 276.4. Samples: 8167888. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:33:20,973][166323] Avg episode reward: [(0, '1185.728')]
[36m[2025-07-02 04:33:25,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8159232. Throughput: 0: 277.9. Samples: 8169504. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:33:25,984][166323] Avg episode reward: [(0, '1224.424')]
[36m[2025-07-02 04:33:30,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8159232. Throughput: 0: 280.0. Samples: 8171200. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:33:30,983][166323] Avg episode reward: [(0, '1268.860')]
[33m[28800833 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[28800833 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.86962890625
[33mCrash Rate: 0.11328125
[33mTimeout Rate: 0.01708984375 (navigation_task.py:265)
[33m[28800833 ms][navigation_task] - WARNING : 
[33mSuccesses: 1781
[33mCrashes : 232
[33mTimeouts: 35 (navigation_task.py:268)
[36m[2025-07-02 04:33:35,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8159232. Throughput: 0: 282.0. Samples: 8172000. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:33:35,973][166323] Avg episode reward: [(0, '1279.794')]
[37m[1m[2025-07-02 04:33:36,036][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015928_8159232.pth...
[36m[2025-07-02 04:33:36,040][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015800_8093696.pth
[36m[2025-07-02 04:33:40,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8159232. Throughput: 0: 281.6. Samples: 8173696. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:33:40,991][166323] Avg episode reward: [(0, '1300.393')]
[36m[2025-07-02 04:33:45,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8159232. Throughput: 0: 283.3. Samples: 8175456. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:33:45,964][166323] Avg episode reward: [(0, '1248.627')]
[36m[2025-07-02 04:33:50,958][166323] Fps is (10 sec: 1643.8, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8175616. Throughput: 0: 282.8. Samples: 8176288. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 04:33:50,958][166323] Avg episode reward: [(0, '1278.983')]
[31m[28823839 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[28823839 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[28823839 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:33:55,990][166323] Fps is (10 sec: 1634.1, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8175616. Throughput: 0: 279.7. Samples: 8177904. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 04:33:55,990][166323] Avg episode reward: [(0, '1283.441')]
[36m[2025-07-02 04:34:00,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8175616. Throughput: 0: 281.1. Samples: 8179600. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 04:34:00,967][166323] Avg episode reward: [(0, '1248.263')]
[36m[2025-07-02 04:34:05,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8175616. Throughput: 0: 279.1. Samples: 8180448. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 04:34:05,968][166323] Avg episode reward: [(0, '1206.300')]
[36m[2025-07-02 04:34:10,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8175616. Throughput: 0: 279.4. Samples: 8182080. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 04:34:10,990][166323] Avg episode reward: [(0, '1216.531')]
[36m[2025-07-02 04:34:15,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 8175616. Throughput: 0: 276.2. Samples: 8183632. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 04:34:15,995][166323] Avg episode reward: [(0, '1240.563')]
[36m[2025-07-02 04:34:20,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8175616. Throughput: 0: 276.2. Samples: 8184432. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 04:34:20,979][166323] Avg episode reward: [(0, '1206.464')]
[31m[28853543 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[28853543 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[28853543 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:34:25,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8175616. Throughput: 0: 277.4. Samples: 8186176. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 04:34:25,981][166323] Avg episode reward: [(0, '1145.766')]
[36m[2025-07-02 04:34:30,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8175616. Throughput: 0: 276.3. Samples: 8187888. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 04:34:30,965][166323] Avg episode reward: [(0, '1191.439')]
[36m[2025-07-02 04:34:35,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8175616. Throughput: 0: 278.7. Samples: 8188832. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 04:34:35,964][166323] Avg episode reward: [(0, '1221.557')]
[36m[2025-07-02 04:34:40,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8175616. Throughput: 0: 280.7. Samples: 8190528. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 04:34:40,970][166323] Avg episode reward: [(0, '1212.402')]
[36m[2025-07-02 04:34:45,998][166323] Fps is (10 sec: 1632.9, 60 sec: 545.8, 300 sec: 333.2). Total num frames: 8192000. Throughput: 0: 280.7. Samples: 8192240. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 04:34:45,998][166323] Avg episode reward: [(0, '1206.375')]
[36m[2025-07-02 04:34:50,975][166323] Fps is (10 sec: 1637.5, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8192000. Throughput: 0: 280.8. Samples: 8193088. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 04:34:50,975][166323] Avg episode reward: [(0, '1178.233')]
[36m[2025-07-02 04:34:55,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8192000. Throughput: 0: 284.0. Samples: 8194848. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 04:34:55,951][166323] Avg episode reward: [(0, '1259.682')]
[36m[2025-07-02 04:35:00,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8192000. Throughput: 0: 285.7. Samples: 8196480. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 04:35:00,971][166323] Avg episode reward: [(0, '1258.126')]
[36m[2025-07-02 04:35:05,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8192000. Throughput: 0: 286.1. Samples: 8197296. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 04:35:05,945][166323] Avg episode reward: [(0, '1288.770')]
[36m[2025-07-02 04:35:10,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8192000. Throughput: 0: 285.6. Samples: 8199024. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 04:35:10,961][166323] Avg episode reward: [(0, '1267.801')]
[36m[2025-07-02 04:35:15,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8192000. Throughput: 0: 281.7. Samples: 8200560. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 04:35:15,946][166323] Avg episode reward: [(0, '1245.344')]
[36m[2025-07-02 04:35:20,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8192000. Throughput: 0: 279.1. Samples: 8201392. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 04:35:20,971][166323] Avg episode reward: [(0, '1248.882')]
[36m[2025-07-02 04:35:25,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8192000. Throughput: 0: 277.6. Samples: 8203024. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 04:35:25,986][166323] Avg episode reward: [(0, '1239.851')]
[36m[2025-07-02 04:35:30,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8192000. Throughput: 0: 275.7. Samples: 8204640. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 04:35:30,980][166323] Avg episode reward: [(0, '1178.334')]
[31m[28921640 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[28921640 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[28921641 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:35:35,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8192000. Throughput: 0: 275.1. Samples: 8205472. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 04:35:35,994][166323] Avg episode reward: [(0, '1208.119')]
[37m[1m[2025-07-02 04:35:36,050][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015992_8192000.pth...
[36m[2025-07-02 04:35:36,056][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015864_8126464.pth
[36m[2025-07-02 04:35:40,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8192000. Throughput: 0: 273.1. Samples: 8207136. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 04:35:40,951][166323] Avg episode reward: [(0, '1181.392')]
[36m[2025-07-02 04:35:45,952][166323] Fps is (10 sec: 1645.3, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8208384. Throughput: 0: 271.0. Samples: 8208672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:35:45,962][166323] Avg episode reward: [(0, '1188.032')]
[36m[2025-07-02 04:35:50,975][166323] Fps is (10 sec: 1634.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8208384. Throughput: 0: 272.9. Samples: 8209584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:35:50,975][166323] Avg episode reward: [(0, '1135.141')]
[31m[28939806 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[28939807 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[28939808 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:35:55,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8208384. Throughput: 0: 271.5. Samples: 8211248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:35:55,989][166323] Avg episode reward: [(0, '1117.192')]
[36m[2025-07-02 04:36:00,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8208384. Throughput: 0: 276.7. Samples: 8213024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:36:00,987][166323] Avg episode reward: [(0, '1150.141')]
[36m[2025-07-02 04:36:05,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8208384. Throughput: 0: 274.9. Samples: 8213760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:36:05,958][166323] Avg episode reward: [(0, '1195.060')]
[36m[2025-07-02 04:36:10,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8208384. Throughput: 0: 274.7. Samples: 8215376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:36:10,959][166323] Avg episode reward: [(0, '1247.018')]
[36m[2025-07-02 04:36:15,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8208384. Throughput: 0: 274.6. Samples: 8216992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:36:15,969][166323] Avg episode reward: [(0, '1230.314')]
[31m[28966866 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[28966867 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[28966867 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:36:20,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8208384. Throughput: 0: 273.3. Samples: 8217760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:36:20,957][166323] Avg episode reward: [(0, '1259.227')]
[36m[2025-07-02 04:36:25,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8208384. Throughput: 0: 272.6. Samples: 8219408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:36:25,964][166323] Avg episode reward: [(0, '1334.383')]
[36m[2025-07-02 04:36:30,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8208384. Throughput: 0: 278.6. Samples: 8221216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:36:30,978][166323] Avg episode reward: [(0, '1403.387')]
[31m[28979607 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[28979608 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[28979608 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:36:35,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8208384. Throughput: 0: 278.1. Samples: 8222096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:36:35,962][166323] Avg episode reward: [(0, '1346.065')]
[36m[2025-07-02 04:36:40,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8208384. Throughput: 0: 278.9. Samples: 8223792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:36:40,972][166323] Avg episode reward: [(0, '1327.194')]
[36m[2025-07-02 04:36:45,978][166323] Fps is (10 sec: 1635.7, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8224768. Throughput: 0: 274.5. Samples: 8225376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:36:45,978][166323] Avg episode reward: [(0, '1302.866')]
[36m[2025-07-02 04:36:50,989][166323] Fps is (10 sec: 1635.5, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8224768. Throughput: 0: 274.3. Samples: 8226112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:36:50,989][166323] Avg episode reward: [(0, '1263.989')]
[36m[2025-07-02 04:36:55,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8224768. Throughput: 0: 275.4. Samples: 8227776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:36:55,978][166323] Avg episode reward: [(0, '1240.927')]
[36m[2025-07-02 04:37:00,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8224768. Throughput: 0: 276.2. Samples: 8229424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:37:00,980][166323] Avg episode reward: [(0, '1233.285')]
[36m[2025-07-02 04:37:05,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8224768. Throughput: 0: 278.2. Samples: 8230288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:37:05,992][166323] Avg episode reward: [(0, '1229.620')]
[36m[2025-07-02 04:37:10,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8224768. Throughput: 0: 278.3. Samples: 8231936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:37:10,980][166323] Avg episode reward: [(0, '1194.714')]
[31m[29024202 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[29024202 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[29024202 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:37:15,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8224768. Throughput: 0: 272.2. Samples: 8233456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:37:15,946][166323] Avg episode reward: [(0, '1219.175')]
[36m[2025-07-02 04:37:20,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8224768. Throughput: 0: 273.7. Samples: 8234416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:37:20,968][166323] Avg episode reward: [(0, '1212.088')]
[31m[29029683 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[29029683 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[29029683 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:37:25,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8224768. Throughput: 0: 274.1. Samples: 8236128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:37:25,977][166323] Avg episode reward: [(0, '1247.568')]
[36m[2025-07-02 04:37:30,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8224768. Throughput: 0: 276.1. Samples: 8237792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:37:30,953][166323] Avg episode reward: [(0, '1258.509')]
[36m[2025-07-02 04:37:35,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8224768. Throughput: 0: 280.0. Samples: 8238704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:37:35,969][166323] Avg episode reward: [(0, '1255.542')]
[37m[1m[2025-07-02 04:37:36,019][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000016056_8224768.pth...
[36m[2025-07-02 04:37:36,023][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015928_8159232.pth
[36m[2025-07-02 04:37:40,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8224768. Throughput: 0: 282.6. Samples: 8240496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:37:40,983][166323] Avg episode reward: [(0, '1274.009')]
[36m[2025-07-02 04:37:45,954][166323] Fps is (10 sec: 1640.7, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8241152. Throughput: 0: 282.8. Samples: 8242144. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 04:37:45,954][166323] Avg episode reward: [(0, '1284.276')]
[36m[2025-07-02 04:37:50,979][166323] Fps is (10 sec: 1638.9, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8241152. Throughput: 0: 281.0. Samples: 8242928. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 04:37:50,979][166323] Avg episode reward: [(0, '1302.686')]
[36m[2025-07-02 04:37:55,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8241152. Throughput: 0: 282.1. Samples: 8244624. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 04:37:55,956][166323] Avg episode reward: [(0, '1319.714')]
[36m[2025-07-02 04:38:00,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8241152. Throughput: 0: 287.6. Samples: 8246400. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 04:38:00,948][166323] Avg episode reward: [(0, '1272.290')]
[36m[2025-07-02 04:38:05,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8241152. Throughput: 0: 286.6. Samples: 8247312. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 04:38:05,960][166323] Avg episode reward: [(0, '1321.681')]
[36m[2025-07-02 04:38:10,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8241152. Throughput: 0: 288.8. Samples: 8249120. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 04:38:10,961][166323] Avg episode reward: [(0, '1312.148')]
[36m[2025-07-02 04:38:15,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8241152. Throughput: 0: 291.8. Samples: 8250928. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 04:38:15,970][166323] Avg episode reward: [(0, '1332.590')]
[36m[2025-07-02 04:38:20,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8241152. Throughput: 0: 291.5. Samples: 8251824. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 04:38:20,979][166323] Avg episode reward: [(0, '1329.722')]
[36m[2025-07-02 04:38:25,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8241152. Throughput: 0: 287.0. Samples: 8253408. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 04:38:25,971][166323] Avg episode reward: [(0, '1358.446')]
[36m[2025-07-02 04:38:30,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8241152. Throughput: 0: 284.7. Samples: 8254960. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 04:38:30,972][166323] Avg episode reward: [(0, '1333.184')]
[36m[2025-07-02 04:38:35,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8241152. Throughput: 0: 286.1. Samples: 8255792. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 04:38:35,945][166323] Avg episode reward: [(0, '1305.083')]
[36m[2025-07-02 04:38:40,949][166323] Fps is (10 sec: 1642.1, 60 sec: 546.4, 300 sec: 333.2). Total num frames: 8257536. Throughput: 0: 282.7. Samples: 8257344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:38:40,950][166323] Avg episode reward: [(0, '1278.166')]
[36m[2025-07-02 04:38:45,967][166323] Fps is (10 sec: 1634.8, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8257536. Throughput: 0: 277.9. Samples: 8258912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:38:45,967][166323] Avg episode reward: [(0, '1245.523')]
[36m[2025-07-02 04:38:50,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8257536. Throughput: 0: 277.4. Samples: 8259792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:38:50,955][166323] Avg episode reward: [(0, '1219.671')]
[31m[29121954 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[29121954 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[29121955 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:38:55,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8257536. Throughput: 0: 276.8. Samples: 8261584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:38:55,985][166323] Avg episode reward: [(0, '1168.077')]
[36m[2025-07-02 04:39:00,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8257536. Throughput: 0: 272.4. Samples: 8263184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:39:00,969][166323] Avg episode reward: [(0, '1159.333')]
[36m[2025-07-02 04:39:05,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8257536. Throughput: 0: 269.0. Samples: 8263920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:39:05,944][166323] Avg episode reward: [(0, '1191.914')]
[36m[2025-07-02 04:39:10,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8257536. Throughput: 0: 273.9. Samples: 8265728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:39:10,957][166323] Avg episode reward: [(0, '1229.616')]
[36m[2025-07-02 04:39:15,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8257536. Throughput: 0: 275.5. Samples: 8267360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:39:15,986][166323] Avg episode reward: [(0, '1182.208')]
[36m[2025-07-02 04:39:20,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8257536. Throughput: 0: 274.7. Samples: 8268160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:39:20,963][166323] Avg episode reward: [(0, '1248.524')]
[36m[2025-07-02 04:39:25,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8257536. Throughput: 0: 279.2. Samples: 8269920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:39:25,987][166323] Avg episode reward: [(0, '1297.819')]
[36m[2025-07-02 04:39:30,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8257536. Throughput: 0: 284.0. Samples: 8271696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:39:30,974][166323] Avg episode reward: [(0, '1299.448')]
[36m[2025-07-02 04:39:35,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8257536. Throughput: 0: 282.8. Samples: 8272528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:39:35,989][166323] Avg episode reward: [(0, '1293.201')]
[37m[1m[2025-07-02 04:39:36,041][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000016120_8257536.pth...
[36m[2025-07-02 04:39:36,045][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000015992_8192000.pth
[36m[2025-07-02 04:39:40,967][166323] Fps is (10 sec: 1639.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8273920. Throughput: 0: 280.3. Samples: 8274192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:39:40,967][166323] Avg episode reward: [(0, '1263.828')]
[36m[2025-07-02 04:39:45,952][166323] Fps is (10 sec: 1644.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8273920. Throughput: 0: 280.3. Samples: 8275792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:39:45,952][166323] Avg episode reward: [(0, '1263.853')]
[36m[2025-07-02 04:39:50,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8273920. Throughput: 0: 282.1. Samples: 8276624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:39:50,975][166323] Avg episode reward: [(0, '1264.867')]
[36m[2025-07-02 04:39:55,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8273920. Throughput: 0: 279.1. Samples: 8278288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:39:55,961][166323] Avg episode reward: [(0, '1257.234')]
[36m[2025-07-02 04:40:00,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8273920. Throughput: 0: 281.7. Samples: 8280032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:40:00,977][166323] Avg episode reward: [(0, '1235.577')]
[36m[2025-07-02 04:40:05,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8273920. Throughput: 0: 283.0. Samples: 8280896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:40:05,965][166323] Avg episode reward: [(0, '1259.274')]
[36m[2025-07-02 04:40:11,011][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 8273920. Throughput: 0: 277.2. Samples: 8282400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:40:11,011][166323] Avg episode reward: [(0, '1270.088')]
[31m[29201304 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[29201305 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[29201305 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:40:15,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8273920. Throughput: 0: 277.5. Samples: 8284176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:40:15,953][166323] Avg episode reward: [(0, '1280.350')]
[36m[2025-07-02 04:40:20,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8273920. Throughput: 0: 278.1. Samples: 8285040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:40:20,981][166323] Avg episode reward: [(0, '1243.213')]
[36m[2025-07-02 04:40:25,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8273920. Throughput: 0: 276.2. Samples: 8286624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:40:25,972][166323] Avg episode reward: [(0, '1227.292')]
[36m[2025-07-02 04:40:30,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8273920. Throughput: 0: 276.1. Samples: 8288224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:40:30,974][166323] Avg episode reward: [(0, '1198.982')]
[31m[29221076 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[29221077 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[29221077 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:40:35,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8273920. Throughput: 0: 276.1. Samples: 8289056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:40:35,999][166323] Avg episode reward: [(0, '1139.478')]
[36m[2025-07-02 04:40:41,002][166323] Fps is (10 sec: 1633.8, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 8290304. Throughput: 0: 273.2. Samples: 8290592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:40:41,003][166323] Avg episode reward: [(0, '1150.524')]
[36m[2025-07-02 04:40:45,989][166323] Fps is (10 sec: 1640.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8290304. Throughput: 0: 274.4. Samples: 8292384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:40:45,989][166323] Avg episode reward: [(0, '1186.028')]
[36m[2025-07-02 04:40:50,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8290304. Throughput: 0: 273.3. Samples: 8293200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:40:50,983][166323] Avg episode reward: [(0, '1164.643')]
[36m[2025-07-02 04:40:56,009][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 8290304. Throughput: 0: 277.3. Samples: 8294880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:40:56,009][166323] Avg episode reward: [(0, '1144.870')]
[31m[29245266 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[29245266 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[29245267 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:41:00,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8290304. Throughput: 0: 276.3. Samples: 8296608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:41:00,947][166323] Avg episode reward: [(0, '1112.077')]
[36m[2025-07-02 04:41:05,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8290304. Throughput: 0: 273.7. Samples: 8297360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:41:05,991][166323] Avg episode reward: [(0, '1136.455')]
[36m[2025-07-02 04:41:10,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8290304. Throughput: 0: 273.8. Samples: 8298944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:41:10,960][166323] Avg episode reward: [(0, '1078.036')]
[31m[29261851 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[29261852 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[29261852 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:41:16,009][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 8290304. Throughput: 0: 272.1. Samples: 8300480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:41:16,010][166323] Avg episode reward: [(0, '1081.516')]
[36m[2025-07-02 04:41:20,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8290304. Throughput: 0: 269.9. Samples: 8301200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:41:20,985][166323] Avg episode reward: [(0, '1132.418')]
[36m[2025-07-02 04:41:25,995][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8290304. Throughput: 0: 271.7. Samples: 8302816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:41:25,996][166323] Avg episode reward: [(0, '1177.011')]
[36m[2025-07-02 04:41:31,006][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8290304. Throughput: 0: 265.1. Samples: 8304320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:41:31,006][166323] Avg episode reward: [(0, '1216.871')]
[36m[2025-07-02 04:41:35,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8290304. Throughput: 0: 264.5. Samples: 8305104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:41:35,987][166323] Avg episode reward: [(0, '1276.321')]
[37m[1m[2025-07-02 04:41:36,038][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000016184_8290304.pth...
[36m[2025-07-02 04:41:36,042][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000016056_8224768.pth
[36m[2025-07-02 04:41:41,003][166323] Fps is (10 sec: 1638.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8306688. Throughput: 0: 263.1. Samples: 8306720. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 04:41:41,003][166323] Avg episode reward: [(0, '1252.344')]
[36m[2025-07-02 04:41:45,995][166323] Fps is (10 sec: 1637.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8306688. Throughput: 0: 265.7. Samples: 8308576. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 04:41:45,995][166323] Avg episode reward: [(0, '1342.047')]
[31m[29296258 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[29296258 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[29296258 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:41:50,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8306688. Throughput: 0: 268.2. Samples: 8309424. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 04:41:50,973][166323] Avg episode reward: [(0, '1291.785')]
[36m[2025-07-02 04:41:55,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8306688. Throughput: 0: 272.9. Samples: 8311232. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 04:41:55,990][166323] Avg episode reward: [(0, '1254.711')]
[36m[2025-07-02 04:42:00,976][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8306688. Throughput: 0: 276.1. Samples: 8312896. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 04:42:00,977][166323] Avg episode reward: [(0, '1262.527')]
[36m[2025-07-02 04:42:05,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8306688. Throughput: 0: 275.9. Samples: 8313616. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 04:42:05,990][166323] Avg episode reward: [(0, '1251.932')]
[36m[2025-07-02 04:42:10,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8306688. Throughput: 0: 278.1. Samples: 8315328. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 04:42:10,990][166323] Avg episode reward: [(0, '1251.176')]
[36m[2025-07-02 04:42:15,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8306688. Throughput: 0: 282.8. Samples: 8317040. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 04:42:15,991][166323] Avg episode reward: [(0, '1228.901')]
[36m[2025-07-02 04:42:20,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8306688. Throughput: 0: 283.8. Samples: 8317872. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 04:42:20,973][166323] Avg episode reward: [(0, '1239.241')]
[36m[2025-07-02 04:42:25,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8306688. Throughput: 0: 282.8. Samples: 8319440. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 04:42:25,976][166323] Avg episode reward: [(0, '1236.914')]
[36m[2025-07-02 04:42:30,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8306688. Throughput: 0: 275.8. Samples: 8320976. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 04:42:30,954][166323] Avg episode reward: [(0, '1279.090')]
[36m[2025-07-02 04:42:35,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8306688. Throughput: 0: 274.9. Samples: 8321792. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 04:42:35,956][166323] Avg episode reward: [(0, '1245.954')]
[36m[2025-07-02 04:42:40,960][166323] Fps is (10 sec: 1637.3, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8323072. Throughput: 0: 269.3. Samples: 8323344. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 04:42:40,960][166323] Avg episode reward: [(0, '1187.454')]
[36m[2025-07-02 04:42:45,997][166323] Fps is (10 sec: 1631.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8323072. Throughput: 0: 271.9. Samples: 8325136. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 04:42:45,998][166323] Avg episode reward: [(0, '1193.699')]
[36m[2025-07-02 04:42:50,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8323072. Throughput: 0: 273.4. Samples: 8325920. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 04:42:50,991][166323] Avg episode reward: [(0, '1140.109')]
[36m[2025-07-02 04:42:55,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8323072. Throughput: 0: 272.6. Samples: 8327584. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 04:42:55,951][166323] Avg episode reward: [(0, '1144.320')]
[36m[2025-07-02 04:43:00,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8323072. Throughput: 0: 272.6. Samples: 8329296. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 04:43:00,949][166323] Avg episode reward: [(0, '1181.813')]
[36m[2025-07-02 04:43:05,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8323072. Throughput: 0: 271.5. Samples: 8330080. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 04:43:05,945][166323] Avg episode reward: [(0, '1176.662')]
[36m[2025-07-02 04:43:10,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8323072. Throughput: 0: 273.8. Samples: 8331760. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 04:43:10,978][166323] Avg episode reward: [(0, '1228.101')]
[36m[2025-07-02 04:43:15,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8323072. Throughput: 0: 279.1. Samples: 8333536. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 04:43:15,955][166323] Avg episode reward: [(0, '1300.255')]
[36m[2025-07-02 04:43:20,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8323072. Throughput: 0: 279.3. Samples: 8334368. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 04:43:20,984][166323] Avg episode reward: [(0, '1238.992')]
[36m[2025-07-02 04:43:25,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8323072. Throughput: 0: 283.1. Samples: 8336080. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 04:43:25,946][166323] Avg episode reward: [(0, '1343.750')]
[36m[2025-07-02 04:43:30,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8323072. Throughput: 0: 279.6. Samples: 8337712. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 04:43:30,974][166323] Avg episode reward: [(0, '1344.445')]
[36m[2025-07-02 04:43:35,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 8323072. Throughput: 0: 280.9. Samples: 8338560. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 04:43:35,997][166323] Avg episode reward: [(0, '1306.484')]
[37m[1m[2025-07-02 04:43:36,053][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000016248_8323072.pth...
[36m[2025-07-02 04:43:36,058][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000016120_8257536.pth
[36m[2025-07-02 04:43:40,946][166323] Fps is (10 sec: 1643.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8339456. Throughput: 0: 280.6. Samples: 8340208. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 04:43:40,946][166323] Avg episode reward: [(0, '1311.952')]
[36m[2025-07-02 04:43:45,988][166323] Fps is (10 sec: 1639.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8339456. Throughput: 0: 281.4. Samples: 8341968. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 04:43:45,988][166323] Avg episode reward: [(0, '1245.526')]
[36m[2025-07-02 04:43:50,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8339456. Throughput: 0: 285.0. Samples: 8342912. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 04:43:50,969][166323] Avg episode reward: [(0, '1291.134')]
[36m[2025-07-02 04:43:55,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8339456. Throughput: 0: 284.2. Samples: 8344544. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 04:43:55,964][166323] Avg episode reward: [(0, '1216.041')]
[36m[2025-07-02 04:44:00,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 8339456. Throughput: 0: 281.3. Samples: 8346208. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 04:44:00,996][166323] Avg episode reward: [(0, '1176.384')]
[36m[2025-07-02 04:44:05,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8339456. Throughput: 0: 281.5. Samples: 8347024. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 04:44:05,950][166323] Avg episode reward: [(0, '1195.340')]
[36m[2025-07-02 04:44:10,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8339456. Throughput: 0: 280.5. Samples: 8348704. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 04:44:10,958][166323] Avg episode reward: [(0, '1168.562')]
[36m[2025-07-02 04:44:16,031][166323] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 277.6). Total num frames: 8339456. Throughput: 0: 279.1. Samples: 8350288. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 04:44:16,031][166323] Avg episode reward: [(0, '1160.838')]
[36m[2025-07-02 04:44:20,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8339456. Throughput: 0: 279.0. Samples: 8351104. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 04:44:20,956][166323] Avg episode reward: [(0, '1197.337')]
[36m[2025-07-02 04:44:25,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8339456. Throughput: 0: 279.7. Samples: 8352800. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 04:44:25,962][166323] Avg episode reward: [(0, '1196.629')]
[36m[2025-07-02 04:44:30,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8339456. Throughput: 0: 276.7. Samples: 8354416. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 04:44:30,970][166323] Avg episode reward: [(0, '1270.001')]
[36m[2025-07-02 04:44:36,009][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 8339456. Throughput: 0: 273.2. Samples: 8355216. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 04:44:36,010][166323] Avg episode reward: [(0, '1306.347')]
[36m[2025-07-02 04:44:40,954][166323] Fps is (10 sec: 1641.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8355840. Throughput: 0: 274.2. Samples: 8356880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:44:40,955][166323] Avg episode reward: [(0, '1310.155')]
[36m[2025-07-02 04:44:45,955][166323] Fps is (10 sec: 1647.3, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8355840. Throughput: 0: 273.3. Samples: 8358496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:44:45,955][166323] Avg episode reward: [(0, '1276.725')]
[31m[29476308 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[29476308 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[29476308 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:44:50,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8355840. Throughput: 0: 274.5. Samples: 8359376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:44:50,946][166323] Avg episode reward: [(0, '1286.970')]
[33m[29481578 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[29481578 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.85888671875
[33mCrash Rate: 0.12841796875
[33mTimeout Rate: 0.0126953125 (navigation_task.py:265)
[33m[29481578 ms][navigation_task] - WARNING : 
[33mSuccesses: 1759
[33mCrashes : 263
[33mTimeouts: 26 (navigation_task.py:268)
[36m[2025-07-02 04:44:55,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8355840. Throughput: 0: 275.4. Samples: 8361104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:44:55,978][166323] Avg episode reward: [(0, '1256.107')]
[36m[2025-07-02 04:45:00,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8355840. Throughput: 0: 282.0. Samples: 8362960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:45:00,964][166323] Avg episode reward: [(0, '1238.241')]
[36m[2025-07-02 04:45:05,978][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8355840. Throughput: 0: 282.5. Samples: 8363824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:45:05,978][166323] Avg episode reward: [(0, '1212.078')]
[36m[2025-07-02 04:45:10,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8355840. Throughput: 0: 282.3. Samples: 8365504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:45:10,965][166323] Avg episode reward: [(0, '1186.643')]
[36m[2025-07-02 04:45:15,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 8355840. Throughput: 0: 284.6. Samples: 8367216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:45:15,951][166323] Avg episode reward: [(0, '1224.813')]
[36m[2025-07-02 04:45:20,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8355840. Throughput: 0: 286.0. Samples: 8368080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:45:20,987][166323] Avg episode reward: [(0, '1268.346')]
[36m[2025-07-02 04:45:25,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8355840. Throughput: 0: 285.3. Samples: 8369728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:45:25,986][166323] Avg episode reward: [(0, '1216.451')]
[36m[2025-07-02 04:45:30,995][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8355840. Throughput: 0: 284.9. Samples: 8371328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:45:30,996][166323] Avg episode reward: [(0, '1130.810')]
[31m[29519651 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[29519652 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[29519652 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[29523586 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[29523587 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[29523587 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:45:35,989][166323] Fps is (10 sec: 1637.9, 60 sec: 546.3, 300 sec: 277.7). Total num frames: 8372224. Throughput: 0: 283.5. Samples: 8372144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:45:35,989][166323] Avg episode reward: [(0, '1129.604')]
[37m[1m[2025-07-02 04:45:36,039][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000016344_8372224.pth...
[36m[2025-07-02 04:45:36,043][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000016184_8290304.pth
[36m[2025-07-02 04:45:40,963][166323] Fps is (10 sec: 1643.7, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8372224. Throughput: 0: 283.8. Samples: 8373872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:45:40,963][166323] Avg episode reward: [(0, '1081.583')]
[36m[2025-07-02 04:45:45,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8372224. Throughput: 0: 279.8. Samples: 8375552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:45:45,963][166323] Avg episode reward: [(0, '1125.619')]
[36m[2025-07-02 04:45:50,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8372224. Throughput: 0: 277.7. Samples: 8376320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:45:50,968][166323] Avg episode reward: [(0, '1049.997')]
[36m[2025-07-02 04:45:55,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8372224. Throughput: 0: 279.4. Samples: 8378080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:45:55,979][166323] Avg episode reward: [(0, '1074.006')]
[36m[2025-07-02 04:46:00,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8372224. Throughput: 0: 276.5. Samples: 8379664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:46:00,975][166323] Avg episode reward: [(0, '1153.945')]
[36m[2025-07-02 04:46:05,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8372224. Throughput: 0: 275.2. Samples: 8380464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:46:05,980][166323] Avg episode reward: [(0, '1170.286')]
[36m[2025-07-02 04:46:10,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8372224. Throughput: 0: 276.9. Samples: 8382192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:46:10,992][166323] Avg episode reward: [(0, '1253.703')]
[36m[2025-07-02 04:46:15,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8372224. Throughput: 0: 274.9. Samples: 8383696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:46:15,982][166323] Avg episode reward: [(0, '1253.288')]
[36m[2025-07-02 04:46:20,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8372224. Throughput: 0: 276.6. Samples: 8384592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:46:20,992][166323] Avg episode reward: [(0, '1254.077')]
[36m[2025-07-02 04:46:25,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.8). Total num frames: 8372224. Throughput: 0: 273.9. Samples: 8386192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:46:25,945][166323] Avg episode reward: [(0, '1263.293')]
[36m[2025-07-02 04:46:30,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8372224. Throughput: 0: 272.9. Samples: 8387840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:46:30,984][166323] Avg episode reward: [(0, '1247.978')]
[36m[2025-07-02 04:46:35,969][166323] Fps is (10 sec: 1634.5, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8388608. Throughput: 0: 273.8. Samples: 8388640. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:46:35,969][166323] Avg episode reward: [(0, '1262.595')]
[36m[2025-07-02 04:46:41,025][166323] Fps is (10 sec: 1631.8, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 8388608. Throughput: 0: 268.9. Samples: 8390192. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:46:41,025][166323] Avg episode reward: [(0, '1245.200')]
[31m[29590065 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[29590065 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[29590065 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:46:45,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8388608. Throughput: 0: 271.6. Samples: 8391888. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:46:45,988][166323] Avg episode reward: [(0, '1205.317')]
[36m[2025-07-02 04:46:50,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8388608. Throughput: 0: 273.2. Samples: 8392752. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:46:50,964][166323] Avg episode reward: [(0, '1257.730')]
[36m[2025-07-02 04:46:55,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8388608. Throughput: 0: 271.6. Samples: 8394416. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:46:55,994][166323] Avg episode reward: [(0, '1285.865')]
[36m[2025-07-02 04:47:01,003][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8388608. Throughput: 0: 274.7. Samples: 8396064. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:47:01,003][166323] Avg episode reward: [(0, '1283.848')]
[36m[2025-07-02 04:47:05,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8388608. Throughput: 0: 273.9. Samples: 8396912. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:47:05,967][166323] Avg episode reward: [(0, '1349.678')]
[31m[29615951 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[29615951 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[29615952 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:47:10,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8388608. Throughput: 0: 276.8. Samples: 8398656. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:47:10,970][166323] Avg episode reward: [(0, '1282.989')]
[36m[2025-07-02 04:47:15,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8388608. Throughput: 0: 277.0. Samples: 8400304. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:47:15,977][166323] Avg episode reward: [(0, '1308.752')]
[36m[2025-07-02 04:47:20,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8388608. Throughput: 0: 277.6. Samples: 8401136. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:47:20,989][166323] Avg episode reward: [(0, '1314.813')]
[36m[2025-07-02 04:47:25,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8388608. Throughput: 0: 279.9. Samples: 8402768. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:47:25,957][166323] Avg episode reward: [(0, '1274.881')]
[36m[2025-07-02 04:47:30,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8388608. Throughput: 0: 282.0. Samples: 8404576. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 04:47:30,980][166323] Avg episode reward: [(0, '1265.471')]
[31m[29640542 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[29640542 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[29640542 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:47:35,945][166323] Fps is (10 sec: 1640.4, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8404992. Throughput: 0: 279.9. Samples: 8405344. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:47:35,945][166323] Avg episode reward: [(0, '1263.396')]
[37m[1m[2025-07-02 04:47:35,996][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000016408_8404992.pth...
[36m[2025-07-02 04:47:36,000][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000016248_8323072.pth
[36m[2025-07-02 04:47:40,965][166323] Fps is (10 sec: 1640.8, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8404992. Throughput: 0: 280.0. Samples: 8407008. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:47:40,965][166323] Avg episode reward: [(0, '1275.708')]
[36m[2025-07-02 04:47:45,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8404992. Throughput: 0: 278.2. Samples: 8408576. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:47:45,979][166323] Avg episode reward: [(0, '1315.234')]
[36m[2025-07-02 04:47:50,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8404992. Throughput: 0: 277.5. Samples: 8409392. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:47:50,947][166323] Avg episode reward: [(0, '1263.248')]
[36m[2025-07-02 04:47:55,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8404992. Throughput: 0: 280.8. Samples: 8411296. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:47:55,980][166323] Avg episode reward: [(0, '1232.036')]
[36m[2025-07-02 04:48:00,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8404992. Throughput: 0: 278.2. Samples: 8412816. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:48:00,952][166323] Avg episode reward: [(0, '1276.081')]
[36m[2025-07-02 04:48:05,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8404992. Throughput: 0: 277.5. Samples: 8413616. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:48:05,963][166323] Avg episode reward: [(0, '1286.724')]
[36m[2025-07-02 04:48:10,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8404992. Throughput: 0: 278.3. Samples: 8415296. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:48:10,970][166323] Avg episode reward: [(0, '1255.415')]
[36m[2025-07-02 04:48:15,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8404992. Throughput: 0: 274.8. Samples: 8416944. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:48:15,991][166323] Avg episode reward: [(0, '1270.655')]
[36m[2025-07-02 04:48:20,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8404992. Throughput: 0: 275.0. Samples: 8417728. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:48:20,985][166323] Avg episode reward: [(0, '1268.543')]
[36m[2025-07-02 04:48:25,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8404992. Throughput: 0: 274.9. Samples: 8419376. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:48:25,962][166323] Avg episode reward: [(0, '1325.384')]
[36m[2025-07-02 04:48:30,997][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8404992. Throughput: 0: 277.2. Samples: 8421056. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:48:30,997][166323] Avg episode reward: [(0, '1325.893')]
[36m[2025-07-02 04:48:35,969][166323] Fps is (10 sec: 1637.1, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8421376. Throughput: 0: 275.4. Samples: 8421792. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:48:35,970][166323] Avg episode reward: [(0, '1311.969')]
[36m[2025-07-02 04:48:40,959][166323] Fps is (10 sec: 1644.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8421376. Throughput: 0: 268.9. Samples: 8423392. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:48:40,960][166323] Avg episode reward: [(0, '1311.073')]
[36m[2025-07-02 04:48:45,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8421376. Throughput: 0: 270.4. Samples: 8424992. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:48:45,987][166323] Avg episode reward: [(0, '1265.335')]
[36m[2025-07-02 04:48:50,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8421376. Throughput: 0: 269.8. Samples: 8425760. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:48:50,966][166323] Avg episode reward: [(0, '1268.558')]
[36m[2025-07-02 04:48:55,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8421376. Throughput: 0: 267.8. Samples: 8427344. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:48:55,961][166323] Avg episode reward: [(0, '1309.696')]
[36m[2025-07-02 04:49:00,978][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8421376. Throughput: 0: 272.8. Samples: 8429216. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:49:00,978][166323] Avg episode reward: [(0, '1272.591')]
[36m[2025-07-02 04:49:05,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8421376. Throughput: 0: 273.1. Samples: 8430016. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:49:05,972][166323] Avg episode reward: [(0, '1238.581')]
[36m[2025-07-02 04:49:11,000][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8421376. Throughput: 0: 274.3. Samples: 8431728. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:49:11,001][166323] Avg episode reward: [(0, '1248.686')]
[31m[29740572 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[29740572 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[29740573 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:49:15,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8421376. Throughput: 0: 273.7. Samples: 8433360. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:49:15,946][166323] Avg episode reward: [(0, '1261.511')]
[36m[2025-07-02 04:49:20,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8421376. Throughput: 0: 276.2. Samples: 8434224. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:49:20,978][166323] Avg episode reward: [(0, '1288.446')]
[36m[2025-07-02 04:49:26,001][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8421376. Throughput: 0: 277.8. Samples: 8435904. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:49:26,001][166323] Avg episode reward: [(0, '1284.989')]
[36m[2025-07-02 04:49:30,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8421376. Throughput: 0: 277.9. Samples: 8437488. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 04:49:30,960][166323] Avg episode reward: [(0, '1217.397')]
[36m[2025-07-02 04:49:35,946][166323] Fps is (10 sec: 1647.5, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8437760. Throughput: 0: 278.9. Samples: 8438304. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 04:49:35,946][166323] Avg episode reward: [(0, '1246.162')]
[37m[1m[2025-07-02 04:49:36,003][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000016472_8437760.pth...
[36m[2025-07-02 04:49:36,007][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000016344_8372224.pth
[36m[2025-07-02 04:49:40,971][166323] Fps is (10 sec: 1636.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8437760. Throughput: 0: 281.9. Samples: 8440032. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 04:49:40,971][166323] Avg episode reward: [(0, '1318.597')]
[36m[2025-07-02 04:49:45,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8437760. Throughput: 0: 276.2. Samples: 8441648. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 04:49:45,989][166323] Avg episode reward: [(0, '1336.587')]
[36m[2025-07-02 04:49:50,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8437760. Throughput: 0: 277.1. Samples: 8442480. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 04:49:50,957][166323] Avg episode reward: [(0, '1293.190')]
[36m[2025-07-02 04:49:55,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8437760. Throughput: 0: 280.0. Samples: 8444320. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 04:49:55,973][166323] Avg episode reward: [(0, '1285.581')]
[36m[2025-07-02 04:50:00,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8437760. Throughput: 0: 278.6. Samples: 8445904. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 04:50:00,969][166323] Avg episode reward: [(0, '1294.915')]
[36m[2025-07-02 04:50:05,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8437760. Throughput: 0: 277.5. Samples: 8446704. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 04:50:05,948][166323] Avg episode reward: [(0, '1296.446')]
[31m[29794810 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[29794810 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[29794810 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:50:10,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8437760. Throughput: 0: 276.9. Samples: 8448352. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 04:50:10,952][166323] Avg episode reward: [(0, '1311.452')]
[36m[2025-07-02 04:50:15,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8437760. Throughput: 0: 281.3. Samples: 8450144. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 04:50:15,959][166323] Avg episode reward: [(0, '1284.896')]
[36m[2025-07-02 04:50:20,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8437760. Throughput: 0: 281.8. Samples: 8450992. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 04:50:20,965][166323] Avg episode reward: [(0, '1252.801')]
[36m[2025-07-02 04:50:25,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8437760. Throughput: 0: 280.4. Samples: 8452656. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 04:50:25,989][166323] Avg episode reward: [(0, '1298.397')]
[36m[2025-07-02 04:50:30,962][166323] Fps is (10 sec: 1638.8, 60 sec: 546.1, 300 sec: 277.7). Total num frames: 8454144. Throughput: 0: 278.9. Samples: 8454192. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 04:50:30,962][166323] Avg episode reward: [(0, '1330.812')]
[31m[29820609 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[29820609 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[29820609 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:50:35,978][166323] Fps is (10 sec: 1640.2, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8454144. Throughput: 0: 278.6. Samples: 8455024. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 04:50:35,978][166323] Avg episode reward: [(0, '1261.109')]
[36m[2025-07-02 04:50:40,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8454144. Throughput: 0: 275.6. Samples: 8456720. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 04:50:40,957][166323] Avg episode reward: [(0, '1318.856')]
[36m[2025-07-02 04:50:45,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8454144. Throughput: 0: 279.3. Samples: 8458480. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 04:50:45,989][166323] Avg episode reward: [(0, '1325.042')]
[36m[2025-07-02 04:50:50,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8454144. Throughput: 0: 279.6. Samples: 8459296. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 04:50:50,980][166323] Avg episode reward: [(0, '1280.051')]
[36m[2025-07-02 04:50:55,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8454144. Throughput: 0: 281.1. Samples: 8461008. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 04:50:55,967][166323] Avg episode reward: [(0, '1317.325')]
[36m[2025-07-02 04:51:01,004][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8454144. Throughput: 0: 277.0. Samples: 8462624. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 04:51:01,005][166323] Avg episode reward: [(0, '1343.033')]
[36m[2025-07-02 04:51:05,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8454144. Throughput: 0: 275.9. Samples: 8463408. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 04:51:05,961][166323] Avg episode reward: [(0, '1345.196')]
[36m[2025-07-02 04:51:10,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8454144. Throughput: 0: 274.6. Samples: 8465008. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 04:51:10,976][166323] Avg episode reward: [(0, '1359.474')]
[36m[2025-07-02 04:51:15,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8454144. Throughput: 0: 278.7. Samples: 8466736. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 04:51:15,973][166323] Avg episode reward: [(0, '1356.062')]
[36m[2025-07-02 04:51:20,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8454144. Throughput: 0: 280.5. Samples: 8467648. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 04:51:20,990][166323] Avg episode reward: [(0, '1381.025')]
[36m[2025-07-02 04:51:25,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8454144. Throughput: 0: 276.5. Samples: 8469168. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 04:51:25,971][166323] Avg episode reward: [(0, '1382.644')]
[36m[2025-07-02 04:51:30,996][166323] Fps is (10 sec: 1637.3, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8470528. Throughput: 0: 270.5. Samples: 8470656. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:51:30,996][166323] Avg episode reward: [(0, '1315.341')]
[36m[2025-07-02 04:51:35,994][166323] Fps is (10 sec: 1634.5, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8470528. Throughput: 0: 269.4. Samples: 8471424. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:51:35,995][166323] Avg episode reward: [(0, '1304.674')]
[37m[1m[2025-07-02 04:51:36,083][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000016536_8470528.pth...
[36m[2025-07-02 04:51:36,088][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000016408_8404992.pth
[36m[2025-07-02 04:51:40,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8470528. Throughput: 0: 266.7. Samples: 8473008. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:51:40,965][166323] Avg episode reward: [(0, '1270.354')]
[36m[2025-07-02 04:51:45,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8470528. Throughput: 0: 268.3. Samples: 8474688. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:51:45,964][166323] Avg episode reward: [(0, '1238.390')]
[36m[2025-07-02 04:51:51,004][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8470528. Throughput: 0: 268.5. Samples: 8475504. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:51:51,004][166323] Avg episode reward: [(0, '1232.950')]
[36m[2025-07-02 04:51:55,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8470528. Throughput: 0: 269.5. Samples: 8477136. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:51:55,979][166323] Avg episode reward: [(0, '1198.051')]
[36m[2025-07-02 04:52:00,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8470528. Throughput: 0: 270.8. Samples: 8478928. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:52:00,989][166323] Avg episode reward: [(0, '1195.434')]
[36m[2025-07-02 04:52:05,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8470528. Throughput: 0: 269.5. Samples: 8479776. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:52:05,996][166323] Avg episode reward: [(0, '1265.554')]
[36m[2025-07-02 04:52:10,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8470528. Throughput: 0: 275.4. Samples: 8481568. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:52:10,997][166323] Avg episode reward: [(0, '1235.824')]
[36m[2025-07-02 04:52:15,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8470528. Throughput: 0: 280.3. Samples: 8483264. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:52:15,969][166323] Avg episode reward: [(0, '1237.720')]
[36m[2025-07-02 04:52:20,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8470528. Throughput: 0: 282.5. Samples: 8484128. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:52:20,960][166323] Avg episode reward: [(0, '1266.317')]
[31m[29929859 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[29929860 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[29929860 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:52:25,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8470528. Throughput: 0: 281.3. Samples: 8485664. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 04:52:25,964][166323] Avg episode reward: [(0, '1275.261')]
[36m[2025-07-02 04:52:30,952][166323] Fps is (10 sec: 1639.6, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8486912. Throughput: 0: 276.0. Samples: 8487104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:52:30,952][166323] Avg episode reward: [(0, '1307.577')]
[36m[2025-07-02 04:52:35,989][166323] Fps is (10 sec: 1634.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8486912. Throughput: 0: 277.1. Samples: 8487968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:52:35,990][166323] Avg episode reward: [(0, '1280.128')]
[36m[2025-07-02 04:52:40,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8486912. Throughput: 0: 280.6. Samples: 8489760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:52:40,967][166323] Avg episode reward: [(0, '1253.738')]
[36m[2025-07-02 04:52:45,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8486912. Throughput: 0: 279.9. Samples: 8491520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:52:45,974][166323] Avg episode reward: [(0, '1292.031')]
[31m[29956661 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[29956661 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[29956661 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:52:50,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8486912. Throughput: 0: 283.1. Samples: 8492512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:52:50,984][166323] Avg episode reward: [(0, '1275.814')]
[36m[2025-07-02 04:52:55,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8486912. Throughput: 0: 278.5. Samples: 8494096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:52:55,973][166323] Avg episode reward: [(0, '1290.120')]
[36m[2025-07-02 04:53:00,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8486912. Throughput: 0: 278.8. Samples: 8495808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:53:00,957][166323] Avg episode reward: [(0, '1261.264')]
[31m[29970200 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[29970200 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[29970200 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:53:05,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8486912. Throughput: 0: 276.1. Samples: 8496560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:53:05,992][166323] Avg episode reward: [(0, '1254.791')]
[36m[2025-07-02 04:53:10,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8486912. Throughput: 0: 279.2. Samples: 8498224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:53:10,952][166323] Avg episode reward: [(0, '1224.138')]
[36m[2025-07-02 04:53:16,000][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8486912. Throughput: 0: 285.6. Samples: 8499968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:53:16,001][166323] Avg episode reward: [(0, '1264.092')]
[36m[2025-07-02 04:53:20,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8486912. Throughput: 0: 283.9. Samples: 8500736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:53:20,967][166323] Avg episode reward: [(0, '1236.343')]
[36m[2025-07-02 04:53:25,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8486912. Throughput: 0: 279.9. Samples: 8502352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:53:25,954][166323] Avg episode reward: [(0, '1211.489')]
[36m[2025-07-02 04:53:30,969][166323] Fps is (10 sec: 1638.1, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8503296. Throughput: 0: 277.7. Samples: 8504016. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 04:53:30,969][166323] Avg episode reward: [(0, '1186.592')]
[36m[2025-07-02 04:53:35,974][166323] Fps is (10 sec: 1635.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8503296. Throughput: 0: 273.5. Samples: 8504816. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 04:53:35,974][166323] Avg episode reward: [(0, '1207.365')]
[37m[1m[2025-07-02 04:53:36,065][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000016600_8503296.pth...
[36m[2025-07-02 04:53:36,071][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000016472_8437760.pth
[36m[2025-07-02 04:53:40,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8503296. Throughput: 0: 273.9. Samples: 8506416. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 04:53:40,946][166323] Avg episode reward: [(0, '1228.449')]
[36m[2025-07-02 04:53:45,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8503296. Throughput: 0: 271.6. Samples: 8508032. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 04:53:45,962][166323] Avg episode reward: [(0, '1276.369')]
[36m[2025-07-02 04:53:50,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8503296. Throughput: 0: 273.7. Samples: 8508864. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 04:53:50,949][166323] Avg episode reward: [(0, '1291.351')]
[36m[2025-07-02 04:53:55,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8503296. Throughput: 0: 271.0. Samples: 8510432. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 04:53:55,992][166323] Avg episode reward: [(0, '1243.291')]
[36m[2025-07-02 04:54:00,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8503296. Throughput: 0: 273.6. Samples: 8512272. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 04:54:00,977][166323] Avg episode reward: [(0, '1259.658')]
[31m[30032958 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[30032958 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[30032959 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:54:05,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8503296. Throughput: 0: 274.5. Samples: 8513088. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 04:54:05,969][166323] Avg episode reward: [(0, '1238.351')]
[36m[2025-07-02 04:54:11,013][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 8503296. Throughput: 0: 277.7. Samples: 8514864. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 04:54:11,013][166323] Avg episode reward: [(0, '1207.347')]
[36m[2025-07-02 04:54:15,997][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8503296. Throughput: 0: 276.8. Samples: 8516480. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 04:54:15,997][166323] Avg episode reward: [(0, '1246.757')]
[36m[2025-07-02 04:54:20,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8503296. Throughput: 0: 276.8. Samples: 8517264. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 04:54:20,946][166323] Avg episode reward: [(0, '1237.363')]
[36m[2025-07-02 04:54:25,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8503296. Throughput: 0: 278.9. Samples: 8518976. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 04:54:25,977][166323] Avg episode reward: [(0, '1264.600')]
[36m[2025-07-02 04:54:30,972][166323] Fps is (10 sec: 1634.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8519680. Throughput: 0: 281.2. Samples: 8520688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:54:30,973][166323] Avg episode reward: [(0, '1309.593')]
[36m[2025-07-02 04:54:35,992][166323] Fps is (10 sec: 1635.9, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8519680. Throughput: 0: 282.4. Samples: 8521584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:54:35,993][166323] Avg episode reward: [(0, '1319.213')]
[36m[2025-07-02 04:54:40,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8519680. Throughput: 0: 286.3. Samples: 8523312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:54:40,984][166323] Avg episode reward: [(0, '1339.457')]
[36m[2025-07-02 04:54:45,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8519680. Throughput: 0: 282.0. Samples: 8524960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:54:45,964][166323] Avg episode reward: [(0, '1331.284')]
[36m[2025-07-02 04:54:51,018][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 8519680. Throughput: 0: 280.9. Samples: 8525744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:54:51,018][166323] Avg episode reward: [(0, '1279.657')]
[36m[2025-07-02 04:54:55,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8519680. Throughput: 0: 278.2. Samples: 8527376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:54:55,992][166323] Avg episode reward: [(0, '1297.574')]
[36m[2025-07-02 04:55:00,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8519680. Throughput: 0: 278.8. Samples: 8529024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:55:00,984][166323] Avg episode reward: [(0, '1293.887')]
[36m[2025-07-02 04:55:05,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8519680. Throughput: 0: 281.0. Samples: 8529920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:55:05,989][166323] Avg episode reward: [(0, '1335.239')]
[36m[2025-07-02 04:55:10,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8519680. Throughput: 0: 282.0. Samples: 8531664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:55:10,965][166323] Avg episode reward: [(0, '1322.725')]
[36m[2025-07-02 04:55:15,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8519680. Throughput: 0: 279.4. Samples: 8533264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:55:15,981][166323] Avg episode reward: [(0, '1369.790')]
[36m[2025-07-02 04:55:20,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8519680. Throughput: 0: 277.0. Samples: 8534048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:55:20,984][166323] Avg episode reward: [(0, '1380.961')]
[36m[2025-07-02 04:55:25,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.1). Total num frames: 8519680. Throughput: 0: 274.5. Samples: 8535664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:55:25,975][166323] Avg episode reward: [(0, '1425.333')]
[36m[2025-07-02 04:55:30,989][166323] Fps is (10 sec: 1637.7, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8536064. Throughput: 0: 272.9. Samples: 8537248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:55:30,989][166323] Avg episode reward: [(0, '1372.156')]
[36m[2025-07-02 04:55:35,988][166323] Fps is (10 sec: 1636.3, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8536064. Throughput: 0: 274.0. Samples: 8538064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:55:35,988][166323] Avg episode reward: [(0, '1281.445')]
[37m[1m[2025-07-02 04:55:36,038][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000016664_8536064.pth...
[36m[2025-07-02 04:55:36,042][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000016536_8470528.pth
[36m[2025-07-02 04:55:40,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8536064. Throughput: 0: 277.0. Samples: 8539840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:55:40,989][166323] Avg episode reward: [(0, '1229.954')]
[36m[2025-07-02 04:55:45,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8536064. Throughput: 0: 277.1. Samples: 8541488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:55:45,969][166323] Avg episode reward: [(0, '1232.754')]
[36m[2025-07-02 04:55:50,997][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8536064. Throughput: 0: 274.1. Samples: 8542256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:55:50,997][166323] Avg episode reward: [(0, '1225.347')]
[36m[2025-07-02 04:55:55,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8536064. Throughput: 0: 270.4. Samples: 8543840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:55:55,998][166323] Avg episode reward: [(0, '1260.210')]
[36m[2025-07-02 04:56:00,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8536064. Throughput: 0: 271.5. Samples: 8545472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:56:00,954][166323] Avg episode reward: [(0, '1234.155')]
[36m[2025-07-02 04:56:05,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8536064. Throughput: 0: 271.6. Samples: 8546272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:56:05,994][166323] Avg episode reward: [(0, '1264.219')]
[36m[2025-07-02 04:56:10,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8536064. Throughput: 0: 273.6. Samples: 8547968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:56:10,946][166323] Avg episode reward: [(0, '1323.039')]
[36m[2025-07-02 04:56:15,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8536064. Throughput: 0: 272.3. Samples: 8549488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:56:15,947][166323] Avg episode reward: [(0, '1371.654')]
[36m[2025-07-02 04:56:20,997][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8536064. Throughput: 0: 272.7. Samples: 8550336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:56:20,998][166323] Avg episode reward: [(0, '1376.604')]
[36m[2025-07-02 04:56:25,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 8536064. Throughput: 0: 269.0. Samples: 8551936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:56:25,963][166323] Avg episode reward: [(0, '1330.588')]
[33m[30175717 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[30175717 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.87451171875
[33mCrash Rate: 0.11181640625
[33mTimeout Rate: 0.013671875 (navigation_task.py:265)
[33m[30175717 ms][navigation_task] - WARNING : 
[33mSuccesses: 1791
[33mCrashes : 229
[33mTimeouts: 28 (navigation_task.py:268)
[36m[2025-07-02 04:56:30,961][166323] Fps is (10 sec: 1644.5, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8552448. Throughput: 0: 267.1. Samples: 8553504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:56:30,961][166323] Avg episode reward: [(0, '1268.425')]
[36m[2025-07-02 04:56:35,955][166323] Fps is (10 sec: 1639.6, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8552448. Throughput: 0: 266.6. Samples: 8554240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:56:35,955][166323] Avg episode reward: [(0, '1259.080')]
[36m[2025-07-02 04:56:40,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8552448. Throughput: 0: 268.3. Samples: 8555904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:56:40,962][166323] Avg episode reward: [(0, '1304.868')]
[36m[2025-07-02 04:56:45,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8552448. Throughput: 0: 268.7. Samples: 8557568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:56:45,970][166323] Avg episode reward: [(0, '1249.638')]
[36m[2025-07-02 04:56:50,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8552448. Throughput: 0: 267.9. Samples: 8558320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:56:50,969][166323] Avg episode reward: [(0, '1238.031')]
[36m[2025-07-02 04:56:55,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8552448. Throughput: 0: 265.9. Samples: 8559936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:56:55,952][166323] Avg episode reward: [(0, '1238.434')]
[36m[2025-07-02 04:57:00,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8552448. Throughput: 0: 268.2. Samples: 8561568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:57:00,987][166323] Avg episode reward: [(0, '1258.180')]
[36m[2025-07-02 04:57:05,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8552448. Throughput: 0: 267.6. Samples: 8562368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:57:05,967][166323] Avg episode reward: [(0, '1295.205')]
[36m[2025-07-02 04:57:10,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8552448. Throughput: 0: 268.0. Samples: 8564000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:57:10,982][166323] Avg episode reward: [(0, '1331.629')]
[36m[2025-07-02 04:57:15,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8552448. Throughput: 0: 269.6. Samples: 8565632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:57:15,951][166323] Avg episode reward: [(0, '1278.523')]
[36m[2025-07-02 04:57:20,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8552448. Throughput: 0: 272.2. Samples: 8566496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:57:20,978][166323] Avg episode reward: [(0, '1289.450')]
[36m[2025-07-02 04:57:25,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 8552448. Throughput: 0: 270.5. Samples: 8568080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:57:25,973][166323] Avg episode reward: [(0, '1242.300')]
[31m[30235351 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[30235353 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[30235353 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:57:30,964][166323] Fps is (10 sec: 1640.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8568832. Throughput: 0: 272.0. Samples: 8569808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:57:30,964][166323] Avg episode reward: [(0, '1257.574')]
[36m[2025-07-02 04:57:36,004][166323] Fps is (10 sec: 1633.4, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 8568832. Throughput: 0: 274.6. Samples: 8570688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:57:36,004][166323] Avg episode reward: [(0, '1263.113')]
[37m[1m[2025-07-02 04:57:36,078][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000016728_8568832.pth...
[36m[2025-07-02 04:57:36,082][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000016600_8503296.pth
[36m[2025-07-02 04:57:40,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8568832. Throughput: 0: 273.7. Samples: 8572256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:57:40,971][166323] Avg episode reward: [(0, '1273.063')]
[36m[2025-07-02 04:57:45,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8568832. Throughput: 0: 274.4. Samples: 8573904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:57:45,945][166323] Avg episode reward: [(0, '1303.144')]
[36m[2025-07-02 04:57:50,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8568832. Throughput: 0: 274.4. Samples: 8574720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:57:50,990][166323] Avg episode reward: [(0, '1345.321')]
[36m[2025-07-02 04:57:55,977][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8568832. Throughput: 0: 273.4. Samples: 8576304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:57:55,977][166323] Avg episode reward: [(0, '1344.188')]
[36m[2025-07-02 04:58:00,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8568832. Throughput: 0: 275.4. Samples: 8578032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:58:00,971][166323] Avg episode reward: [(0, '1405.803')]
[36m[2025-07-02 04:58:05,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8568832. Throughput: 0: 273.6. Samples: 8578800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:58:05,944][166323] Avg episode reward: [(0, '1407.468')]
[36m[2025-07-02 04:58:10,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8568832. Throughput: 0: 276.8. Samples: 8580528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:58:10,950][166323] Avg episode reward: [(0, '1370.173')]
[36m[2025-07-02 04:58:16,010][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 8568832. Throughput: 0: 274.9. Samples: 8582192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:58:16,010][166323] Avg episode reward: [(0, '1388.623')]
[36m[2025-07-02 04:58:20,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8568832. Throughput: 0: 273.4. Samples: 8582976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:58:20,957][166323] Avg episode reward: [(0, '1342.701')]
[31m[30289944 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[30289945 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[30289945 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:58:25,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 8568832. Throughput: 0: 276.7. Samples: 8584704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:58:25,960][166323] Avg episode reward: [(0, '1249.515')]
[36m[2025-07-02 04:58:30,985][166323] Fps is (10 sec: 1633.8, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8585216. Throughput: 0: 277.4. Samples: 8586400. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 04:58:30,985][166323] Avg episode reward: [(0, '1241.664')]
[36m[2025-07-02 04:58:35,981][166323] Fps is (10 sec: 1634.9, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8585216. Throughput: 0: 278.1. Samples: 8587232. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 04:58:35,981][166323] Avg episode reward: [(0, '1247.438')]
[36m[2025-07-02 04:58:40,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8585216. Throughput: 0: 280.2. Samples: 8588912. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 04:58:40,966][166323] Avg episode reward: [(0, '1270.758')]
[36m[2025-07-02 04:58:45,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8585216. Throughput: 0: 279.7. Samples: 8590624. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 04:58:45,986][166323] Avg episode reward: [(0, '1263.368')]
[36m[2025-07-02 04:58:51,012][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8585216. Throughput: 0: 279.0. Samples: 8591376. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 04:58:51,012][166323] Avg episode reward: [(0, '1264.989')]
[36m[2025-07-02 04:58:55,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8585216. Throughput: 0: 277.7. Samples: 8593024. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 04:58:55,956][166323] Avg episode reward: [(0, '1304.666')]
[36m[2025-07-02 04:59:00,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8585216. Throughput: 0: 278.7. Samples: 8594720. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 04:59:00,964][166323] Avg episode reward: [(0, '1382.843')]
[36m[2025-07-02 04:59:06,012][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 8585216. Throughput: 0: 280.5. Samples: 8595616. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 04:59:06,012][166323] Avg episode reward: [(0, '1308.506')]
[36m[2025-07-02 04:59:10,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 8585216. Throughput: 0: 278.9. Samples: 8597264. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 04:59:11,000][166323] Avg episode reward: [(0, '1276.824')]
[36m[2025-07-02 04:59:15,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8585216. Throughput: 0: 277.9. Samples: 8598896. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 04:59:15,952][166323] Avg episode reward: [(0, '1264.037')]
[36m[2025-07-02 04:59:20,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8585216. Throughput: 0: 277.3. Samples: 8599712. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 04:59:20,986][166323] Avg episode reward: [(0, '1269.362')]
[36m[2025-07-02 04:59:26,001][166323] Fps is (10 sec: 1630.3, 60 sec: 545.8, 300 sec: 277.7). Total num frames: 8601600. Throughput: 0: 278.5. Samples: 8601456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:59:26,002][166323] Avg episode reward: [(0, '1233.838')]
[36m[2025-07-02 04:59:30,949][166323] Fps is (10 sec: 1644.5, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8601600. Throughput: 0: 278.3. Samples: 8603136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:59:30,950][166323] Avg episode reward: [(0, '1225.154')]
[36m[2025-07-02 04:59:35,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8601600. Throughput: 0: 279.8. Samples: 8603952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:59:35,952][166323] Avg episode reward: [(0, '1258.202')]
[37m[1m[2025-07-02 04:59:36,019][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000016792_8601600.pth...
[36m[2025-07-02 04:59:36,023][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000016664_8536064.pth
[36m[2025-07-02 04:59:40,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8601600. Throughput: 0: 282.2. Samples: 8605728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:59:40,968][166323] Avg episode reward: [(0, '1277.726')]
[31m[30373221 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[30373221 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[30373221 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 04:59:45,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 8601600. Throughput: 0: 280.9. Samples: 8607360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:59:45,959][166323] Avg episode reward: [(0, '1311.006')]
[36m[2025-07-02 04:59:50,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8601600. Throughput: 0: 278.9. Samples: 8608160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:59:50,986][166323] Avg episode reward: [(0, '1306.675')]
[36m[2025-07-02 04:59:55,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8601600. Throughput: 0: 280.1. Samples: 8609856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 04:59:55,960][166323] Avg episode reward: [(0, '1284.765')]
[36m[2025-07-02 05:00:00,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8601600. Throughput: 0: 281.2. Samples: 8611552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:00:00,957][166323] Avg episode reward: [(0, '1292.809')]
[36m[2025-07-02 05:00:05,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 8601600. Throughput: 0: 281.8. Samples: 8612384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:00:05,949][166323] Avg episode reward: [(0, '1318.214')]
[36m[2025-07-02 05:00:10,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8601600. Throughput: 0: 279.8. Samples: 8614032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:00:10,952][166323] Avg episode reward: [(0, '1324.841')]
[36m[2025-07-02 05:00:15,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8601600. Throughput: 0: 282.3. Samples: 8615840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:00:15,951][166323] Avg episode reward: [(0, '1330.072')]
[36m[2025-07-02 05:00:20,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8601600. Throughput: 0: 284.3. Samples: 8616752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:00:20,972][166323] Avg episode reward: [(0, '1284.635')]
[36m[2025-07-02 05:00:25,987][166323] Fps is (10 sec: 1632.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8617984. Throughput: 0: 281.5. Samples: 8618400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:00:25,987][166323] Avg episode reward: [(0, '1303.635')]
[36m[2025-07-02 05:00:30,964][166323] Fps is (10 sec: 1639.7, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8617984. Throughput: 0: 284.1. Samples: 8620144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:00:30,964][166323] Avg episode reward: [(0, '1344.084')]
[36m[2025-07-02 05:00:35,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8617984. Throughput: 0: 284.1. Samples: 8620944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:00:35,988][166323] Avg episode reward: [(0, '1335.741')]
[31m[30428570 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[30428570 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[30428570 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:00:40,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8617984. Throughput: 0: 283.7. Samples: 8622624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:00:40,967][166323] Avg episode reward: [(0, '1285.236')]
[36m[2025-07-02 05:00:45,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8617984. Throughput: 0: 281.9. Samples: 8624240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:00:45,958][166323] Avg episode reward: [(0, '1265.784')]
[36m[2025-07-02 05:00:50,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8617984. Throughput: 0: 282.6. Samples: 8625104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:00:50,957][166323] Avg episode reward: [(0, '1305.213')]
[31m[30441688 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[30441688 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[30441689 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:00:55,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8617984. Throughput: 0: 281.9. Samples: 8626720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:00:55,964][166323] Avg episode reward: [(0, '1323.203')]
[36m[2025-07-02 05:01:00,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8617984. Throughput: 0: 278.0. Samples: 8628352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:01:00,957][166323] Avg episode reward: [(0, '1311.217')]
[36m[2025-07-02 05:01:05,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8617984. Throughput: 0: 275.0. Samples: 8629120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:01:05,954][166323] Avg episode reward: [(0, '1272.989')]
[36m[2025-07-02 05:01:10,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8617984. Throughput: 0: 276.0. Samples: 8630816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:01:10,976][166323] Avg episode reward: [(0, '1262.005')]
[36m[2025-07-02 05:01:15,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8617984. Throughput: 0: 275.0. Samples: 8632528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:01:15,993][166323] Avg episode reward: [(0, '1321.889')]
[31m[30465787 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[30465788 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[30465788 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[30466809 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[30466810 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[30466810 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:01:20,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8617984. Throughput: 0: 276.8. Samples: 8633392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:01:20,956][166323] Avg episode reward: [(0, '1360.688')]
[36m[2025-07-02 05:01:25,966][166323] Fps is (10 sec: 1642.7, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8634368. Throughput: 0: 275.2. Samples: 8635008. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:01:25,966][166323] Avg episode reward: [(0, '1353.907')]
[36m[2025-07-02 05:01:30,967][166323] Fps is (10 sec: 1636.5, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8634368. Throughput: 0: 278.3. Samples: 8636768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:01:30,968][166323] Avg episode reward: [(0, '1331.042')]
[36m[2025-07-02 05:01:35,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8634368. Throughput: 0: 275.1. Samples: 8637488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:01:35,980][166323] Avg episode reward: [(0, '1317.256')]
[37m[1m[2025-07-02 05:01:36,054][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000016856_8634368.pth...
[36m[2025-07-02 05:01:36,061][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000016728_8568832.pth
[31m[30488838 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[30488838 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[30488838 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:01:40,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8634368. Throughput: 0: 278.1. Samples: 8639232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:01:40,959][166323] Avg episode reward: [(0, '1334.491')]
[36m[2025-07-02 05:01:45,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8634368. Throughput: 0: 277.7. Samples: 8640848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:01:45,950][166323] Avg episode reward: [(0, '1360.692')]
[36m[2025-07-02 05:01:50,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8634368. Throughput: 0: 279.1. Samples: 8641680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:01:50,963][166323] Avg episode reward: [(0, '1297.800')]
[36m[2025-07-02 05:01:55,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8634368. Throughput: 0: 277.4. Samples: 8643296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:01:55,962][166323] Avg episode reward: [(0, '1325.663')]
[36m[2025-07-02 05:02:00,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8634368. Throughput: 0: 277.8. Samples: 8645024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:02:00,980][166323] Avg episode reward: [(0, '1338.316')]
[36m[2025-07-02 05:02:05,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8634368. Throughput: 0: 274.9. Samples: 8645760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:02:05,947][166323] Avg episode reward: [(0, '1351.468')]
[36m[2025-07-02 05:02:10,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8634368. Throughput: 0: 273.3. Samples: 8647312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:02:10,989][166323] Avg episode reward: [(0, '1328.225')]
[36m[2025-07-02 05:02:15,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8634368. Throughput: 0: 275.2. Samples: 8649152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:02:15,976][166323] Avg episode reward: [(0, '1312.004')]
[36m[2025-07-02 05:02:20,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8634368. Throughput: 0: 277.7. Samples: 8649984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:02:20,975][166323] Avg episode reward: [(0, '1289.386')]
[31m[30529666 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[30529666 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[30529666 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:02:25,980][166323] Fps is (10 sec: 1637.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8650752. Throughput: 0: 276.1. Samples: 8651664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:02:25,981][166323] Avg episode reward: [(0, '1343.948')]
[36m[2025-07-02 05:02:30,970][166323] Fps is (10 sec: 1639.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8650752. Throughput: 0: 281.5. Samples: 8653520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:02:30,970][166323] Avg episode reward: [(0, '1342.689')]
[36m[2025-07-02 05:02:35,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8650752. Throughput: 0: 280.5. Samples: 8654304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:02:35,969][166323] Avg episode reward: [(0, '1331.562')]
[36m[2025-07-02 05:02:40,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8650752. Throughput: 0: 280.4. Samples: 8655920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:02:40,975][166323] Avg episode reward: [(0, '1313.518')]
[36m[2025-07-02 05:02:45,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8650752. Throughput: 0: 279.6. Samples: 8657600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:02:45,951][166323] Avg episode reward: [(0, '1320.241')]
[36m[2025-07-02 05:02:50,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8650752. Throughput: 0: 281.9. Samples: 8658448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:02:50,954][166323] Avg episode reward: [(0, '1321.264')]
[36m[2025-07-02 05:02:55,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8650752. Throughput: 0: 284.7. Samples: 8660128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:02:56,000][166323] Avg episode reward: [(0, '1346.846')]
[36m[2025-07-02 05:03:00,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8650752. Throughput: 0: 279.1. Samples: 8661712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:03:00,972][166323] Avg episode reward: [(0, '1288.020')]
[36m[2025-07-02 05:03:05,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8650752. Throughput: 0: 280.6. Samples: 8662608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:03:05,971][166323] Avg episode reward: [(0, '1243.603')]
[36m[2025-07-02 05:03:10,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8650752. Throughput: 0: 280.1. Samples: 8664272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:03:10,986][166323] Avg episode reward: [(0, '1263.025')]
[36m[2025-07-02 05:03:15,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8650752. Throughput: 0: 276.9. Samples: 8665984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:03:15,979][166323] Avg episode reward: [(0, '1221.068')]
[36m[2025-07-02 05:03:20,954][166323] Fps is (10 sec: 1643.5, 60 sec: 546.3, 300 sec: 333.2). Total num frames: 8667136. Throughput: 0: 279.6. Samples: 8666880. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 05:03:20,955][166323] Avg episode reward: [(0, '1190.320')]
[36m[2025-07-02 05:03:25,946][166323] Fps is (10 sec: 1643.7, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8667136. Throughput: 0: 281.1. Samples: 8668560. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 05:03:25,947][166323] Avg episode reward: [(0, '1172.161')]
[36m[2025-07-02 05:03:30,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8667136. Throughput: 0: 281.0. Samples: 8670256. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 05:03:30,991][166323] Avg episode reward: [(0, '1193.684')]
[36m[2025-07-02 05:03:35,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8667136. Throughput: 0: 281.6. Samples: 8671120. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 05:03:35,947][166323] Avg episode reward: [(0, '1199.619')]
[37m[1m[2025-07-02 05:03:36,006][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000016920_8667136.pth...
[36m[2025-07-02 05:03:36,010][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000016792_8601600.pth
[36m[2025-07-02 05:03:40,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8667136. Throughput: 0: 280.4. Samples: 8672736. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 05:03:40,962][166323] Avg episode reward: [(0, '1233.590')]
[36m[2025-07-02 05:03:45,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8667136. Throughput: 0: 279.5. Samples: 8674288. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 05:03:45,962][166323] Avg episode reward: [(0, '1213.367')]
[36m[2025-07-02 05:03:50,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8667136. Throughput: 0: 277.9. Samples: 8675120. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 05:03:50,996][166323] Avg episode reward: [(0, '1242.638')]
[36m[2025-07-02 05:03:55,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8667136. Throughput: 0: 277.9. Samples: 8676768. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 05:03:55,945][166323] Avg episode reward: [(0, '1269.540')]
[36m[2025-07-02 05:04:00,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8667136. Throughput: 0: 277.4. Samples: 8678464. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 05:04:00,969][166323] Avg episode reward: [(0, '1249.742')]
[36m[2025-07-02 05:04:05,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8667136. Throughput: 0: 275.2. Samples: 8679264. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 05:04:05,949][166323] Avg episode reward: [(0, '1246.157')]
[36m[2025-07-02 05:04:10,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8667136. Throughput: 0: 273.4. Samples: 8680864. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 05:04:10,947][166323] Avg episode reward: [(0, '1243.119')]
[36m[2025-07-02 05:04:15,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8667136. Throughput: 0: 271.1. Samples: 8682448. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 05:04:15,969][166323] Avg episode reward: [(0, '1272.944')]
[36m[2025-07-02 05:04:20,951][166323] Fps is (10 sec: 1637.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8683520. Throughput: 0: 270.5. Samples: 8683296. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 05:04:20,951][166323] Avg episode reward: [(0, '1281.235')]
[36m[2025-07-02 05:04:25,980][166323] Fps is (10 sec: 1636.6, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8683520. Throughput: 0: 270.1. Samples: 8684896. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 05:04:25,980][166323] Avg episode reward: [(0, '1271.851')]
[36m[2025-07-02 05:04:30,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8683520. Throughput: 0: 274.9. Samples: 8686656. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 05:04:30,958][166323] Avg episode reward: [(0, '1320.006')]
[36m[2025-07-02 05:04:35,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8683520. Throughput: 0: 274.2. Samples: 8687456. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 05:04:35,985][166323] Avg episode reward: [(0, '1329.285')]
[36m[2025-07-02 05:04:40,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8683520. Throughput: 0: 272.7. Samples: 8689040. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 05:04:40,951][166323] Avg episode reward: [(0, '1349.656')]
[36m[2025-07-02 05:04:45,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8683520. Throughput: 0: 272.9. Samples: 8690752. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 05:04:45,989][166323] Avg episode reward: [(0, '1430.035')]
[36m[2025-07-02 05:04:50,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8683520. Throughput: 0: 275.6. Samples: 8691664. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 05:04:50,945][166323] Avg episode reward: [(0, '1339.214')]
[36m[2025-07-02 05:04:55,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8683520. Throughput: 0: 279.0. Samples: 8693424. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 05:04:55,957][166323] Avg episode reward: [(0, '1342.241')]
[36m[2025-07-02 05:05:00,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8683520. Throughput: 0: 280.9. Samples: 8695088. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 05:05:00,974][166323] Avg episode reward: [(0, '1368.427')]
[36m[2025-07-02 05:05:05,978][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8683520. Throughput: 0: 280.0. Samples: 8695904. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 05:05:05,978][166323] Avg episode reward: [(0, '1324.686')]
[36m[2025-07-02 05:05:10,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8683520. Throughput: 0: 285.1. Samples: 8697728. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 05:05:10,982][166323] Avg episode reward: [(0, '1369.320')]
[36m[2025-07-02 05:05:15,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8683520. Throughput: 0: 285.5. Samples: 8699504. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 05:05:15,962][166323] Avg episode reward: [(0, '1342.420')]
[36m[2025-07-02 05:05:20,956][166323] Fps is (10 sec: 1642.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8699904. Throughput: 0: 286.8. Samples: 8700352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:05:20,957][166323] Avg episode reward: [(0, '1271.245')]
[36m[2025-07-02 05:05:25,968][166323] Fps is (10 sec: 1637.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8699904. Throughput: 0: 287.2. Samples: 8701968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:05:25,968][166323] Avg episode reward: [(0, '1291.302')]
[36m[2025-07-02 05:05:30,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8699904. Throughput: 0: 283.6. Samples: 8703504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:05:30,950][166323] Avg episode reward: [(0, '1251.374')]
[36m[2025-07-02 05:05:35,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8699904. Throughput: 0: 282.9. Samples: 8704400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:05:35,957][166323] Avg episode reward: [(0, '1279.757')]
[37m[1m[2025-07-02 05:05:36,008][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000016984_8699904.pth...
[36m[2025-07-02 05:05:36,012][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000016856_8634368.pth
[36m[2025-07-02 05:05:40,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8699904. Throughput: 0: 280.8. Samples: 8706064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:05:40,963][166323] Avg episode reward: [(0, '1245.144')]
[36m[2025-07-02 05:05:45,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8699904. Throughput: 0: 282.8. Samples: 8707808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:05:45,950][166323] Avg episode reward: [(0, '1241.605')]
[36m[2025-07-02 05:05:50,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8699904. Throughput: 0: 280.9. Samples: 8708544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:05:50,970][166323] Avg episode reward: [(0, '1278.167')]
[36m[2025-07-02 05:05:55,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8699904. Throughput: 0: 274.4. Samples: 8710080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:05:55,993][166323] Avg episode reward: [(0, '1265.713')]
[36m[2025-07-02 05:06:00,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8699904. Throughput: 0: 272.3. Samples: 8711760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:06:00,967][166323] Avg episode reward: [(0, '1261.099')]
[36m[2025-07-02 05:06:05,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8699904. Throughput: 0: 272.7. Samples: 8712624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:06:05,957][166323] Avg episode reward: [(0, '1259.829')]
[36m[2025-07-02 05:06:10,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8699904. Throughput: 0: 275.5. Samples: 8714368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:06:10,976][166323] Avg episode reward: [(0, '1195.696')]
[36m[2025-07-02 05:06:15,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8699904. Throughput: 0: 277.3. Samples: 8715984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:06:15,953][166323] Avg episode reward: [(0, '1206.680')]
[36m[2025-07-02 05:06:20,977][166323] Fps is (10 sec: 1638.1, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8716288. Throughput: 0: 275.4. Samples: 8716800. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 05:06:20,978][166323] Avg episode reward: [(0, '1131.267')]
[36m[2025-07-02 05:06:25,963][166323] Fps is (10 sec: 1636.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8716288. Throughput: 0: 275.9. Samples: 8718480. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 05:06:25,963][166323] Avg episode reward: [(0, '1122.418')]
[36m[2025-07-02 05:06:30,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8716288. Throughput: 0: 273.8. Samples: 8720128. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 05:06:30,950][166323] Avg episode reward: [(0, '1182.335')]
[36m[2025-07-02 05:06:35,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8716288. Throughput: 0: 274.4. Samples: 8720896. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 05:06:35,978][166323] Avg episode reward: [(0, '1176.155')]
[36m[2025-07-02 05:06:40,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8716288. Throughput: 0: 279.3. Samples: 8722640. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 05:06:40,967][166323] Avg episode reward: [(0, '1220.830')]
[36m[2025-07-02 05:06:45,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8716288. Throughput: 0: 281.3. Samples: 8724416. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 05:06:45,962][166323] Avg episode reward: [(0, '1284.309')]
[36m[2025-07-02 05:06:50,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8716288. Throughput: 0: 280.7. Samples: 8725264. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 05:06:50,988][166323] Avg episode reward: [(0, '1265.604')]
[36m[2025-07-02 05:06:56,007][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8716288. Throughput: 0: 280.7. Samples: 8727008. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 05:06:56,007][166323] Avg episode reward: [(0, '1338.359')]
[36m[2025-07-02 05:07:00,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8716288. Throughput: 0: 282.3. Samples: 8728688. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 05:07:00,955][166323] Avg episode reward: [(0, '1339.514')]
[36m[2025-07-02 05:07:05,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8716288. Throughput: 0: 282.9. Samples: 8729536. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 05:07:05,990][166323] Avg episode reward: [(0, '1318.432')]
[36m[2025-07-02 05:07:10,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8716288. Throughput: 0: 283.0. Samples: 8731216. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 05:07:10,970][166323] Avg episode reward: [(0, '1341.674')]
[36m[2025-07-02 05:07:15,953][166323] Fps is (10 sec: 1644.4, 60 sec: 546.1, 300 sec: 333.3). Total num frames: 8732672. Throughput: 0: 285.5. Samples: 8732976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:07:15,953][166323] Avg episode reward: [(0, '1275.508')]
[36m[2025-07-02 05:07:20,997][166323] Fps is (10 sec: 1634.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8732672. Throughput: 0: 288.2. Samples: 8733872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:07:20,997][166323] Avg episode reward: [(0, '1261.709')]
[36m[2025-07-02 05:07:25,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8732672. Throughput: 0: 284.0. Samples: 8735424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:07:25,988][166323] Avg episode reward: [(0, '1277.548')]
[36m[2025-07-02 05:07:30,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8732672. Throughput: 0: 282.3. Samples: 8737120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:07:30,960][166323] Avg episode reward: [(0, '1304.581')]
[36m[2025-07-02 05:07:35,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8732672. Throughput: 0: 282.4. Samples: 8737968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:07:35,968][166323] Avg episode reward: [(0, '1272.385')]
[37m[1m[2025-07-02 05:07:36,026][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000017048_8732672.pth...
[36m[2025-07-02 05:07:36,031][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000016920_8667136.pth
[36m[2025-07-02 05:07:40,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8732672. Throughput: 0: 281.5. Samples: 8739664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:07:40,962][166323] Avg episode reward: [(0, '1299.409')]
[36m[2025-07-02 05:07:45,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8732672. Throughput: 0: 282.1. Samples: 8741392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:07:45,982][166323] Avg episode reward: [(0, '1272.390')]
[36m[2025-07-02 05:07:50,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8732672. Throughput: 0: 282.9. Samples: 8742256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:07:50,954][166323] Avg episode reward: [(0, '1276.543')]
[36m[2025-07-02 05:07:55,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8732672. Throughput: 0: 281.3. Samples: 8743872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:07:55,968][166323] Avg episode reward: [(0, '1323.862')]
[36m[2025-07-02 05:08:00,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8732672. Throughput: 0: 278.0. Samples: 8745488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:08:00,965][166323] Avg episode reward: [(0, '1309.474')]
[33m[30870834 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[30870834 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.8877501487731934
[33mCrash Rate: 0.10200097411870956
[33mTimeout Rate: 0.010248902253806591 (navigation_task.py:265)
[33m[30870834 ms][navigation_task] - WARNING : 
[33mSuccesses: 1819
[33mCrashes : 209
[33mTimeouts: 21 (navigation_task.py:268)
[36m[2025-07-02 05:08:05,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8732672. Throughput: 0: 276.6. Samples: 8746304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:08:05,946][166323] Avg episode reward: [(0, '1312.204')]
[36m[2025-07-02 05:08:10,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8732672. Throughput: 0: 279.6. Samples: 8748000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:08:10,959][166323] Avg episode reward: [(0, '1313.417')]
[36m[2025-07-02 05:08:15,966][166323] Fps is (10 sec: 1635.1, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8749056. Throughput: 0: 276.9. Samples: 8749584. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 05:08:15,967][166323] Avg episode reward: [(0, '1309.600')]
[36m[2025-07-02 05:08:20,987][166323] Fps is (10 sec: 1633.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8749056. Throughput: 0: 277.6. Samples: 8750464. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 05:08:20,987][166323] Avg episode reward: [(0, '1256.799')]
[36m[2025-07-02 05:08:25,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8749056. Throughput: 0: 276.5. Samples: 8752112. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 05:08:25,987][166323] Avg episode reward: [(0, '1278.052')]
[36m[2025-07-02 05:08:30,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8749056. Throughput: 0: 273.1. Samples: 8753680. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 05:08:30,971][166323] Avg episode reward: [(0, '1271.077')]
[31m[30900312 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[30900312 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[30900313 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:08:35,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8749056. Throughput: 0: 272.2. Samples: 8754512. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 05:08:35,983][166323] Avg episode reward: [(0, '1166.542')]
[36m[2025-07-02 05:08:40,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8749056. Throughput: 0: 272.7. Samples: 8756144. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 05:08:40,967][166323] Avg episode reward: [(0, '1150.641')]
[36m[2025-07-02 05:08:45,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8749056. Throughput: 0: 272.3. Samples: 8757744. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 05:08:45,967][166323] Avg episode reward: [(0, '1188.225')]
[36m[2025-07-02 05:08:50,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8749056. Throughput: 0: 272.5. Samples: 8758576. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 05:08:50,976][166323] Avg episode reward: [(0, '1226.867')]
[36m[2025-07-02 05:08:55,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8749056. Throughput: 0: 271.7. Samples: 8760224. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 05:08:55,950][166323] Avg episode reward: [(0, '1290.267')]
[36m[2025-07-02 05:09:00,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8749056. Throughput: 0: 273.9. Samples: 8761904. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 05:09:00,944][166323] Avg episode reward: [(0, '1282.422')]
[36m[2025-07-02 05:09:05,980][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8749056. Throughput: 0: 273.1. Samples: 8762752. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 05:09:05,981][166323] Avg episode reward: [(0, '1269.934')]
[36m[2025-07-02 05:09:10,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8749056. Throughput: 0: 273.1. Samples: 8764400. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 05:09:10,984][166323] Avg episode reward: [(0, '1323.453')]
[36m[2025-07-02 05:09:15,948][166323] Fps is (10 sec: 1643.7, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8765440. Throughput: 0: 274.6. Samples: 8766032. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 05:09:15,948][166323] Avg episode reward: [(0, '1343.393')]
[36m[2025-07-02 05:09:20,971][166323] Fps is (10 sec: 1640.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8765440. Throughput: 0: 274.6. Samples: 8766864. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 05:09:20,971][166323] Avg episode reward: [(0, '1271.785')]
[36m[2025-07-02 05:09:25,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8765440. Throughput: 0: 273.7. Samples: 8768464. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 05:09:25,984][166323] Avg episode reward: [(0, '1258.130')]
[36m[2025-07-02 05:09:30,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8765440. Throughput: 0: 275.5. Samples: 8770144. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 05:09:30,978][166323] Avg episode reward: [(0, '1256.843')]
[36m[2025-07-02 05:09:35,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8765440. Throughput: 0: 275.6. Samples: 8770976. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 05:09:35,964][166323] Avg episode reward: [(0, '1277.445')]
[37m[1m[2025-07-02 05:09:36,025][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000017112_8765440.pth...
[36m[2025-07-02 05:09:36,033][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000016984_8699904.pth
[36m[2025-07-02 05:09:40,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8765440. Throughput: 0: 271.9. Samples: 8772464. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 05:09:40,963][166323] Avg episode reward: [(0, '1243.703')]
[36m[2025-07-02 05:09:45,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8765440. Throughput: 0: 273.9. Samples: 8774240. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 05:09:45,984][166323] Avg episode reward: [(0, '1303.792')]
[36m[2025-07-02 05:09:50,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8765440. Throughput: 0: 274.1. Samples: 8775088. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 05:09:50,981][166323] Avg episode reward: [(0, '1301.603')]
[36m[2025-07-02 05:09:55,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8765440. Throughput: 0: 272.5. Samples: 8776656. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 05:09:55,963][166323] Avg episode reward: [(0, '1339.677')]
[36m[2025-07-02 05:10:00,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8765440. Throughput: 0: 275.8. Samples: 8778448. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 05:10:00,964][166323] Avg episode reward: [(0, '1331.557')]
[36m[2025-07-02 05:10:05,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8765440. Throughput: 0: 274.5. Samples: 8779216. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 05:10:05,968][166323] Avg episode reward: [(0, '1353.534')]
[36m[2025-07-02 05:10:10,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8765440. Throughput: 0: 275.9. Samples: 8780880. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 05:10:10,987][166323] Avg episode reward: [(0, '1342.104')]
[36m[2025-07-02 05:10:15,949][166323] Fps is (10 sec: 1641.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8781824. Throughput: 0: 273.6. Samples: 8782448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:10:15,950][166323] Avg episode reward: [(0, '1343.204')]
[36m[2025-07-02 05:10:20,960][166323] Fps is (10 sec: 1642.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8781824. Throughput: 0: 273.8. Samples: 8783296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:10:20,960][166323] Avg episode reward: [(0, '1340.052')]
[36m[2025-07-02 05:10:26,012][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 8781824. Throughput: 0: 276.0. Samples: 8784896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:10:26,013][166323] Avg episode reward: [(0, '1307.682')]
[36m[2025-07-02 05:10:30,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8781824. Throughput: 0: 270.6. Samples: 8786416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:10:30,980][166323] Avg episode reward: [(0, '1292.935')]
[36m[2025-07-02 05:10:35,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8781824. Throughput: 0: 272.3. Samples: 8787344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:10:35,984][166323] Avg episode reward: [(0, '1283.613')]
[36m[2025-07-02 05:10:40,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8781824. Throughput: 0: 277.4. Samples: 8789136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:10:40,944][166323] Avg episode reward: [(0, '1296.703')]
[36m[2025-07-02 05:10:45,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8781824. Throughput: 0: 275.5. Samples: 8790848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:10:45,975][166323] Avg episode reward: [(0, '1260.522')]
[36m[2025-07-02 05:10:50,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8781824. Throughput: 0: 274.5. Samples: 8791568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:10:50,968][166323] Avg episode reward: [(0, '1262.465')]
[36m[2025-07-02 05:10:55,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8781824. Throughput: 0: 274.9. Samples: 8793248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:10:55,970][166323] Avg episode reward: [(0, '1250.775')]
[36m[2025-07-02 05:11:00,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8781824. Throughput: 0: 277.4. Samples: 8794944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:11:00,992][166323] Avg episode reward: [(0, '1299.105')]
[36m[2025-07-02 05:11:05,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8781824. Throughput: 0: 277.9. Samples: 8795808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:11:05,984][166323] Avg episode reward: [(0, '1285.627')]
[36m[2025-07-02 05:11:10,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8781824. Throughput: 0: 278.8. Samples: 8797424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:11:10,946][166323] Avg episode reward: [(0, '1311.897')]
[36m[2025-07-02 05:11:15,946][166323] Fps is (10 sec: 1644.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8798208. Throughput: 0: 279.7. Samples: 8798992. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 05:11:15,946][166323] Avg episode reward: [(0, '1258.743')]
[36m[2025-07-02 05:11:20,958][166323] Fps is (10 sec: 1636.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8798208. Throughput: 0: 277.5. Samples: 8799824. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 05:11:20,959][166323] Avg episode reward: [(0, '1294.433')]
[36m[2025-07-02 05:11:25,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8798208. Throughput: 0: 273.0. Samples: 8801424. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 05:11:25,959][166323] Avg episode reward: [(0, '1334.406')]
[36m[2025-07-02 05:11:30,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8798208. Throughput: 0: 269.3. Samples: 8802960. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 05:11:30,952][166323] Avg episode reward: [(0, '1347.321')]
[36m[2025-07-02 05:11:35,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8798208. Throughput: 0: 271.6. Samples: 8803792. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 05:11:35,979][166323] Avg episode reward: [(0, '1309.719')]
[37m[1m[2025-07-02 05:11:36,046][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000017176_8798208.pth...
[36m[2025-07-02 05:11:36,050][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000017048_8732672.pth
[36m[2025-07-02 05:11:40,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8798208. Throughput: 0: 268.6. Samples: 8805328. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 05:11:40,944][166323] Avg episode reward: [(0, '1313.609')]
[31m[31089934 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[31089934 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[31089934 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:11:45,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8798208. Throughput: 0: 268.0. Samples: 8807008. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 05:11:46,000][166323] Avg episode reward: [(0, '1253.513')]
[36m[2025-07-02 05:11:50,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 8798208. Throughput: 0: 268.3. Samples: 8807872. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 05:11:50,946][166323] Avg episode reward: [(0, '1325.084')]
[36m[2025-07-02 05:11:55,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8798208. Throughput: 0: 269.5. Samples: 8809552. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 05:11:55,955][166323] Avg episode reward: [(0, '1260.655')]
[36m[2025-07-02 05:12:00,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8798208. Throughput: 0: 269.4. Samples: 8811120. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 05:12:00,958][166323] Avg episode reward: [(0, '1224.610')]
[36m[2025-07-02 05:12:05,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8798208. Throughput: 0: 269.9. Samples: 8811968. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 05:12:05,952][166323] Avg episode reward: [(0, '1228.383')]
[36m[2025-07-02 05:12:10,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 8798208. Throughput: 0: 270.4. Samples: 8813600. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 05:12:10,992][166323] Avg episode reward: [(0, '1225.262')]
[36m[2025-07-02 05:12:16,000][166323] Fps is (10 sec: 1630.6, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 8814592. Throughput: 0: 271.7. Samples: 8815200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:12:16,001][166323] Avg episode reward: [(0, '1278.595')]
[36m[2025-07-02 05:12:20,988][166323] Fps is (10 sec: 1638.9, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8814592. Throughput: 0: 270.9. Samples: 8815984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:12:20,989][166323] Avg episode reward: [(0, '1326.355')]
[36m[2025-07-02 05:12:25,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8814592. Throughput: 0: 274.0. Samples: 8817664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:12:25,959][166323] Avg episode reward: [(0, '1332.851')]
[31m[31137475 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[31137475 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[31137475 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:12:30,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8814592. Throughput: 0: 274.4. Samples: 8819344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:12:30,961][166323] Avg episode reward: [(0, '1300.550')]
[36m[2025-07-02 05:12:35,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8814592. Throughput: 0: 272.6. Samples: 8820144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:12:35,961][166323] Avg episode reward: [(0, '1324.233')]
[36m[2025-07-02 05:12:40,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8814592. Throughput: 0: 268.7. Samples: 8821648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:12:40,965][166323] Avg episode reward: [(0, '1325.099')]
[36m[2025-07-02 05:12:45,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8814592. Throughput: 0: 272.7. Samples: 8823392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:12:45,952][166323] Avg episode reward: [(0, '1309.552')]
[36m[2025-07-02 05:12:50,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8814592. Throughput: 0: 271.9. Samples: 8824208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:12:50,963][166323] Avg episode reward: [(0, '1324.139')]
[36m[2025-07-02 05:12:55,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8814592. Throughput: 0: 270.1. Samples: 8825744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:12:55,955][166323] Avg episode reward: [(0, '1267.292')]
[36m[2025-07-02 05:13:00,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8814592. Throughput: 0: 267.3. Samples: 8827216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:13:00,962][166323] Avg episode reward: [(0, '1240.716')]
[36m[2025-07-02 05:13:05,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8814592. Throughput: 0: 268.6. Samples: 8828064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:13:05,967][166323] Avg episode reward: [(0, '1257.925')]
[36m[2025-07-02 05:13:10,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 222.2). Total num frames: 8814592. Throughput: 0: 265.3. Samples: 8829600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:13:10,946][166323] Avg episode reward: [(0, '1199.784')]
[36m[2025-07-02 05:13:15,953][166323] Fps is (10 sec: 1640.7, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8830976. Throughput: 0: 262.4. Samples: 8831152. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 05:13:15,953][166323] Avg episode reward: [(0, '1186.780')]
[36m[2025-07-02 05:13:20,978][166323] Fps is (10 sec: 1633.1, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8830976. Throughput: 0: 263.4. Samples: 8832000. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 05:13:20,978][166323] Avg episode reward: [(0, '1144.940')]
[36m[2025-07-02 05:13:26,003][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8830976. Throughput: 0: 267.2. Samples: 8833680. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 05:13:26,003][166323] Avg episode reward: [(0, '1129.740')]
[36m[2025-07-02 05:13:30,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8830976. Throughput: 0: 268.0. Samples: 8835456. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 05:13:30,974][166323] Avg episode reward: [(0, '1169.078')]
[36m[2025-07-02 05:13:36,002][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8830976. Throughput: 0: 267.5. Samples: 8836256. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 05:13:36,002][166323] Avg episode reward: [(0, '1207.708')]
[37m[1m[2025-07-02 05:13:36,057][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000017240_8830976.pth...
[36m[2025-07-02 05:13:36,061][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000017112_8765440.pth
[36m[2025-07-02 05:13:40,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8830976. Throughput: 0: 270.9. Samples: 8837936. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 05:13:40,953][166323] Avg episode reward: [(0, '1262.725')]
[36m[2025-07-02 05:13:45,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8830976. Throughput: 0: 272.9. Samples: 8839504. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 05:13:45,986][166323] Avg episode reward: [(0, '1269.843')]
[36m[2025-07-02 05:13:51,003][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 8830976. Throughput: 0: 269.6. Samples: 8840208. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 05:13:51,004][166323] Avg episode reward: [(0, '1310.481')]
[36m[2025-07-02 05:13:55,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8830976. Throughput: 0: 269.8. Samples: 8841744. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 05:13:55,962][166323] Avg episode reward: [(0, '1266.080')]
[36m[2025-07-02 05:14:00,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8830976. Throughput: 0: 271.2. Samples: 8843360. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 05:14:00,961][166323] Avg episode reward: [(0, '1253.525')]
[36m[2025-07-02 05:14:05,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8830976. Throughput: 0: 272.4. Samples: 8844256. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 05:14:05,973][166323] Avg episode reward: [(0, '1219.841')]
[36m[2025-07-02 05:14:10,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 8830976. Throughput: 0: 270.5. Samples: 8845840. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 05:14:10,963][166323] Avg episode reward: [(0, '1124.147')]
[36m[2025-07-02 05:14:15,992][166323] Fps is (10 sec: 1635.1, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8847360. Throughput: 0: 266.6. Samples: 8847456. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 05:14:15,993][166323] Avg episode reward: [(0, '1113.930')]
[36m[2025-07-02 05:14:20,983][166323] Fps is (10 sec: 1635.1, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8847360. Throughput: 0: 266.4. Samples: 8848240. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 05:14:20,983][166323] Avg episode reward: [(0, '1136.870')]
[36m[2025-07-02 05:14:25,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8847360. Throughput: 0: 265.1. Samples: 8849872. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 05:14:25,984][166323] Avg episode reward: [(0, '1125.394')]
[36m[2025-07-02 05:14:30,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8847360. Throughput: 0: 267.7. Samples: 8851552. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 05:14:30,992][166323] Avg episode reward: [(0, '1171.268')]
[36m[2025-07-02 05:14:35,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8847360. Throughput: 0: 268.6. Samples: 8852288. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 05:14:35,983][166323] Avg episode reward: [(0, '1153.879')]
[36m[2025-07-02 05:14:40,979][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8847360. Throughput: 0: 273.3. Samples: 8854048. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 05:14:40,980][166323] Avg episode reward: [(0, '1133.607')]
[36m[2025-07-02 05:14:45,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8847360. Throughput: 0: 274.6. Samples: 8855712. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 05:14:45,947][166323] Avg episode reward: [(0, '1191.502')]
[36m[2025-07-02 05:14:50,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8847360. Throughput: 0: 273.2. Samples: 8856544. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 05:14:50,951][166323] Avg episode reward: [(0, '1169.069')]
[36m[2025-07-02 05:14:55,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8847360. Throughput: 0: 274.5. Samples: 8858192. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 05:14:55,958][166323] Avg episode reward: [(0, '1203.613')]
[36m[2025-07-02 05:15:00,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8847360. Throughput: 0: 272.5. Samples: 8859712. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 05:15:00,976][166323] Avg episode reward: [(0, '1204.038')]
[36m[2025-07-02 05:15:05,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8847360. Throughput: 0: 272.1. Samples: 8860480. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 05:15:05,970][166323] Avg episode reward: [(0, '1153.583')]
[36m[2025-07-02 05:15:10,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 8847360. Throughput: 0: 273.2. Samples: 8862160. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 05:15:10,956][166323] Avg episode reward: [(0, '1235.624')]
[36m[2025-07-02 05:15:15,946][166323] Fps is (10 sec: 1642.4, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8863744. Throughput: 0: 273.0. Samples: 8863824. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 05:15:15,946][166323] Avg episode reward: [(0, '1232.951')]
[36m[2025-07-02 05:15:20,978][166323] Fps is (10 sec: 1634.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8863744. Throughput: 0: 277.0. Samples: 8864752. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 05:15:20,978][166323] Avg episode reward: [(0, '1263.727')]
[36m[2025-07-02 05:15:25,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8863744. Throughput: 0: 273.3. Samples: 8866336. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 05:15:25,947][166323] Avg episode reward: [(0, '1257.127')]
[36m[2025-07-02 05:15:30,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8863744. Throughput: 0: 272.5. Samples: 8867984. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 05:15:30,973][166323] Avg episode reward: [(0, '1242.419')]
[31m[31322768 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[31322769 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[31322770 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:15:35,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8863744. Throughput: 0: 271.2. Samples: 8868752. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 05:15:35,970][166323] Avg episode reward: [(0, '1220.341')]
[37m[1m[2025-07-02 05:15:36,053][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000017304_8863744.pth...
[36m[2025-07-02 05:15:36,061][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000017176_8798208.pth
[36m[2025-07-02 05:15:40,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8863744. Throughput: 0: 270.5. Samples: 8870368. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 05:15:40,965][166323] Avg episode reward: [(0, '1283.633')]
[36m[2025-07-02 05:15:45,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8863744. Throughput: 0: 274.3. Samples: 8872048. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 05:15:45,944][166323] Avg episode reward: [(0, '1294.347')]
[36m[2025-07-02 05:15:50,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8863744. Throughput: 0: 273.7. Samples: 8872800. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 05:15:50,981][166323] Avg episode reward: [(0, '1277.401')]
[36m[2025-07-02 05:15:56,007][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 8863744. Throughput: 0: 272.4. Samples: 8874432. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 05:15:56,007][166323] Avg episode reward: [(0, '1257.162')]
[36m[2025-07-02 05:16:00,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8863744. Throughput: 0: 270.7. Samples: 8876016. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 05:16:00,984][166323] Avg episode reward: [(0, '1298.575')]
[36m[2025-07-02 05:16:05,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8863744. Throughput: 0: 266.3. Samples: 8876736. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 05:16:05,982][166323] Avg episode reward: [(0, '1291.350')]
[36m[2025-07-02 05:16:10,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 8863744. Throughput: 0: 266.1. Samples: 8878320. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 05:16:10,986][166323] Avg episode reward: [(0, '1334.061')]
[31m[31363861 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[31363862 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[31363862 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:16:15,992][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 222.1). Total num frames: 8863744. Throughput: 0: 264.4. Samples: 8879888. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 05:16:15,993][166323] Avg episode reward: [(0, '1351.163')]
[36m[2025-07-02 05:16:21,000][166323] Fps is (10 sec: 1636.1, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8880128. Throughput: 0: 262.6. Samples: 8880576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:16:21,000][166323] Avg episode reward: [(0, '1310.096')]
[36m[2025-07-02 05:16:25,946][166323] Fps is (10 sec: 1646.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8880128. Throughput: 0: 267.1. Samples: 8882384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:16:25,947][166323] Avg episode reward: [(0, '1324.845')]
[36m[2025-07-02 05:16:30,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8880128. Throughput: 0: 265.3. Samples: 8884000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:16:30,999][166323] Avg episode reward: [(0, '1324.604')]
[36m[2025-07-02 05:16:35,997][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 8880128. Throughput: 0: 266.9. Samples: 8884816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:16:35,998][166323] Avg episode reward: [(0, '1222.422')]
[36m[2025-07-02 05:16:40,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8880128. Throughput: 0: 267.7. Samples: 8886464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:16:40,954][166323] Avg episode reward: [(0, '1217.668')]
[36m[2025-07-02 05:16:45,997][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 8880128. Throughput: 0: 267.7. Samples: 8888064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:16:45,997][166323] Avg episode reward: [(0, '1230.541')]
[36m[2025-07-02 05:16:50,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8880128. Throughput: 0: 269.1. Samples: 8888848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:16:50,990][166323] Avg episode reward: [(0, '1213.182')]
[36m[2025-07-02 05:16:55,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8880128. Throughput: 0: 271.4. Samples: 8890528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:16:55,966][166323] Avg episode reward: [(0, '1192.334')]
[36m[2025-07-02 05:17:00,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8880128. Throughput: 0: 275.3. Samples: 8892272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:17:00,980][166323] Avg episode reward: [(0, '1204.310')]
[36m[2025-07-02 05:17:06,009][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8880128. Throughput: 0: 276.6. Samples: 8893024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:17:06,009][166323] Avg episode reward: [(0, '1257.906')]
[36m[2025-07-02 05:17:10,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 8880128. Throughput: 0: 272.9. Samples: 8894672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:17:10,969][166323] Avg episode reward: [(0, '1290.925')]
[36m[2025-07-02 05:17:15,995][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 8880128. Throughput: 0: 274.5. Samples: 8896352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:17:15,996][166323] Avg episode reward: [(0, '1238.536')]
[36m[2025-07-02 05:17:20,951][166323] Fps is (10 sec: 1641.4, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8896512. Throughput: 0: 275.5. Samples: 8897200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:17:20,951][166323] Avg episode reward: [(0, '1262.880')]
[36m[2025-07-02 05:17:25,963][166323] Fps is (10 sec: 1643.7, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8896512. Throughput: 0: 274.4. Samples: 8898816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:17:25,963][166323] Avg episode reward: [(0, '1267.909')]
[36m[2025-07-02 05:17:30,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8896512. Throughput: 0: 276.1. Samples: 8900480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:17:30,972][166323] Avg episode reward: [(0, '1268.922')]
[36m[2025-07-02 05:17:35,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8896512. Throughput: 0: 277.3. Samples: 8901312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:17:35,944][166323] Avg episode reward: [(0, '1251.528')]
[37m[1m[2025-07-02 05:17:35,994][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000017368_8896512.pth...
[36m[2025-07-02 05:17:35,999][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000017240_8830976.pth
[36m[2025-07-02 05:17:40,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8896512. Throughput: 0: 278.4. Samples: 8903056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:17:40,969][166323] Avg episode reward: [(0, '1262.950')]
[36m[2025-07-02 05:17:45,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8896512. Throughput: 0: 273.9. Samples: 8904592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:17:45,952][166323] Avg episode reward: [(0, '1247.450')]
[36m[2025-07-02 05:17:50,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8896512. Throughput: 0: 274.5. Samples: 8905360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:17:50,952][166323] Avg episode reward: [(0, '1333.047')]
[36m[2025-07-02 05:17:55,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8896512. Throughput: 0: 274.0. Samples: 8907008. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:17:55,984][166323] Avg episode reward: [(0, '1252.965')]
[36m[2025-07-02 05:18:00,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8896512. Throughput: 0: 275.3. Samples: 8908736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:18:00,982][166323] Avg episode reward: [(0, '1262.546')]
[36m[2025-07-02 05:18:05,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8896512. Throughput: 0: 273.3. Samples: 8909504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:18:05,974][166323] Avg episode reward: [(0, '1314.956')]
[36m[2025-07-02 05:18:10,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 8896512. Throughput: 0: 277.2. Samples: 8911296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:18:10,991][166323] Avg episode reward: [(0, '1341.461')]
[36m[2025-07-02 05:18:16,005][166323] Fps is (10 sec: 1633.4, 60 sec: 546.0, 300 sec: 277.7). Total num frames: 8912896. Throughput: 0: 276.4. Samples: 8912928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:18:16,005][166323] Avg episode reward: [(0, '1325.197')]
[36m[2025-07-02 05:18:20,985][166323] Fps is (10 sec: 1639.3, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8912896. Throughput: 0: 274.6. Samples: 8913680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:18:20,986][166323] Avg episode reward: [(0, '1280.859')]
[36m[2025-07-02 05:18:25,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8912896. Throughput: 0: 271.8. Samples: 8915296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:18:25,996][166323] Avg episode reward: [(0, '1265.535')]
[36m[2025-07-02 05:18:30,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8912896. Throughput: 0: 272.9. Samples: 8916880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:18:30,982][166323] Avg episode reward: [(0, '1331.061')]
[36m[2025-07-02 05:18:35,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8912896. Throughput: 0: 274.1. Samples: 8917696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:18:35,951][166323] Avg episode reward: [(0, '1346.263')]
[36m[2025-07-02 05:18:40,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8912896. Throughput: 0: 273.8. Samples: 8919328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:18:40,979][166323] Avg episode reward: [(0, '1287.938')]
[36m[2025-07-02 05:18:45,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8912896. Throughput: 0: 269.0. Samples: 8920832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:18:45,947][166323] Avg episode reward: [(0, '1294.177')]
[36m[2025-07-02 05:18:51,017][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 8912896. Throughput: 0: 268.9. Samples: 8921616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:18:51,017][166323] Avg episode reward: [(0, '1286.290')]
[36m[2025-07-02 05:18:55,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8912896. Throughput: 0: 264.3. Samples: 8923184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:18:55,977][166323] Avg episode reward: [(0, '1349.357')]
[36m[2025-07-02 05:19:00,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8912896. Throughput: 0: 263.0. Samples: 8924752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:19:00,964][166323] Avg episode reward: [(0, '1353.156')]
[36m[2025-07-02 05:19:05,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8912896. Throughput: 0: 264.3. Samples: 8925568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:19:05,962][166323] Avg episode reward: [(0, '1349.516')]
[36m[2025-07-02 05:19:10,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 8912896. Throughput: 0: 263.9. Samples: 8927168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:19:10,991][166323] Avg episode reward: [(0, '1324.311')]
[36m[2025-07-02 05:19:15,960][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 222.2). Total num frames: 8912896. Throughput: 0: 265.0. Samples: 8928800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:19:15,960][166323] Avg episode reward: [(0, '1325.550')]
[36m[2025-07-02 05:19:20,963][166323] Fps is (10 sec: 1642.9, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8929280. Throughput: 0: 264.8. Samples: 8929616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:19:20,964][166323] Avg episode reward: [(0, '1350.343')]
[36m[2025-07-02 05:19:25,978][166323] Fps is (10 sec: 1635.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8929280. Throughput: 0: 268.1. Samples: 8931392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:19:25,978][166323] Avg episode reward: [(0, '1293.222')]
[31m[31557761 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[31557761 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[31557761 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:19:30,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8929280. Throughput: 0: 272.1. Samples: 8933088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:19:30,984][166323] Avg episode reward: [(0, '1266.543')]
[36m[2025-07-02 05:19:35,998][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8929280. Throughput: 0: 273.2. Samples: 8933904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:19:35,998][166323] Avg episode reward: [(0, '1259.804')]
[37m[1m[2025-07-02 05:19:36,066][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000017432_8929280.pth...
[36m[2025-07-02 05:19:36,070][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000017304_8863744.pth
[36m[2025-07-02 05:19:41,008][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 8929280. Throughput: 0: 272.9. Samples: 8935472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:19:41,009][166323] Avg episode reward: [(0, '1268.822')]
[36m[2025-07-02 05:19:45,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8929280. Throughput: 0: 274.6. Samples: 8937104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:19:45,952][166323] Avg episode reward: [(0, '1243.629')]
[33m[31575720 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[31575720 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.869140625
[33mCrash Rate: 0.1142578125
[33mTimeout Rate: 0.0166015625 (navigation_task.py:265)
[33m[31575720 ms][navigation_task] - WARNING : 
[33mSuccesses: 1780
[33mCrashes : 234
[33mTimeouts: 34 (navigation_task.py:268)
[36m[2025-07-02 05:19:50,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8929280. Throughput: 0: 274.5. Samples: 8937920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:19:50,964][166323] Avg episode reward: [(0, '1204.391')]
[36m[2025-07-02 05:19:55,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8929280. Throughput: 0: 275.5. Samples: 8939552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:19:55,949][166323] Avg episode reward: [(0, '1260.050')]
[36m[2025-07-02 05:20:00,998][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8929280. Throughput: 0: 276.7. Samples: 8941264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:20:00,999][166323] Avg episode reward: [(0, '1285.200')]
[36m[2025-07-02 05:20:05,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8929280. Throughput: 0: 277.9. Samples: 8942128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:20:05,991][166323] Avg episode reward: [(0, '1302.800')]
[36m[2025-07-02 05:20:10,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 222.2). Total num frames: 8929280. Throughput: 0: 275.0. Samples: 8943760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:20:10,948][166323] Avg episode reward: [(0, '1280.507')]
[36m[2025-07-02 05:20:15,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 8929280. Throughput: 0: 276.0. Samples: 8945504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:20:15,973][166323] Avg episode reward: [(0, '1245.287')]
[36m[2025-07-02 05:20:20,966][166323] Fps is (10 sec: 1635.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8945664. Throughput: 0: 277.2. Samples: 8946368. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 05:20:20,966][166323] Avg episode reward: [(0, '1315.512')]
[36m[2025-07-02 05:20:25,956][166323] Fps is (10 sec: 1641.1, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8945664. Throughput: 0: 279.1. Samples: 8948016. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 05:20:25,957][166323] Avg episode reward: [(0, '1333.472')]
[36m[2025-07-02 05:20:31,006][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8945664. Throughput: 0: 278.8. Samples: 8949664. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 05:20:31,006][166323] Avg episode reward: [(0, '1323.793')]
[36m[2025-07-02 05:20:35,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8945664. Throughput: 0: 282.5. Samples: 8950640. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 05:20:35,985][166323] Avg episode reward: [(0, '1286.141')]
[36m[2025-07-02 05:20:40,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8945664. Throughput: 0: 283.9. Samples: 8952336. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 05:20:40,975][166323] Avg episode reward: [(0, '1245.960')]
[36m[2025-07-02 05:20:45,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8945664. Throughput: 0: 280.3. Samples: 8953872. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 05:20:45,975][166323] Avg episode reward: [(0, '1323.697')]
[36m[2025-07-02 05:20:51,019][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 8945664. Throughput: 0: 277.9. Samples: 8954640. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 05:20:51,020][166323] Avg episode reward: [(0, '1316.219')]
[36m[2025-07-02 05:20:55,977][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8945664. Throughput: 0: 277.9. Samples: 8956272. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 05:20:55,977][166323] Avg episode reward: [(0, '1302.552')]
[36m[2025-07-02 05:21:00,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8945664. Throughput: 0: 276.2. Samples: 8957936. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 05:21:00,976][166323] Avg episode reward: [(0, '1290.876')]
[36m[2025-07-02 05:21:05,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8945664. Throughput: 0: 275.9. Samples: 8958784. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 05:21:05,970][166323] Avg episode reward: [(0, '1317.399')]
[36m[2025-07-02 05:21:10,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8945664. Throughput: 0: 277.4. Samples: 8960496. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 05:21:10,953][166323] Avg episode reward: [(0, '1294.550')]
[36m[2025-07-02 05:21:15,983][166323] Fps is (10 sec: 1636.3, 60 sec: 546.0, 300 sec: 277.7). Total num frames: 8962048. Throughput: 0: 278.2. Samples: 8962176. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 05:21:15,983][166323] Avg episode reward: [(0, '1329.817')]
[36m[2025-07-02 05:21:21,004][166323] Fps is (10 sec: 1629.9, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 8962048. Throughput: 0: 276.5. Samples: 8963088. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 05:21:21,005][166323] Avg episode reward: [(0, '1298.531')]
[36m[2025-07-02 05:21:25,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8962048. Throughput: 0: 277.1. Samples: 8964800. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 05:21:25,955][166323] Avg episode reward: [(0, '1288.406')]
[36m[2025-07-02 05:21:30,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8962048. Throughput: 0: 279.4. Samples: 8966448. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 05:21:30,988][166323] Avg episode reward: [(0, '1232.401')]
[36m[2025-07-02 05:21:35,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8962048. Throughput: 0: 283.6. Samples: 8967392. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 05:21:35,983][166323] Avg episode reward: [(0, '1272.241')]
[37m[1m[2025-07-02 05:21:36,066][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000017496_8962048.pth...
[36m[2025-07-02 05:21:36,073][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000017368_8896512.pth
[36m[2025-07-02 05:21:40,997][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8962048. Throughput: 0: 283.3. Samples: 8969024. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 05:21:40,997][166323] Avg episode reward: [(0, '1266.758')]
[36m[2025-07-02 05:21:45,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8962048. Throughput: 0: 281.9. Samples: 8970624. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 05:21:45,980][166323] Avg episode reward: [(0, '1272.620')]
[36m[2025-07-02 05:21:50,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8962048. Throughput: 0: 282.6. Samples: 8971504. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 05:21:50,979][166323] Avg episode reward: [(0, '1309.657')]
[36m[2025-07-02 05:21:55,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8962048. Throughput: 0: 282.6. Samples: 8973216. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 05:21:55,969][166323] Avg episode reward: [(0, '1291.644')]
[36m[2025-07-02 05:22:00,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8962048. Throughput: 0: 283.1. Samples: 8974912. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 05:22:00,968][166323] Avg episode reward: [(0, '1297.140')]
[36m[2025-07-02 05:22:05,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8962048. Throughput: 0: 279.3. Samples: 8975648. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 05:22:05,979][166323] Avg episode reward: [(0, '1278.882')]
[36m[2025-07-02 05:22:11,012][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 8962048. Throughput: 0: 277.0. Samples: 8977280. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 05:22:11,012][166323] Avg episode reward: [(0, '1234.091')]
[36m[2025-07-02 05:22:15,981][166323] Fps is (10 sec: 1638.1, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8978432. Throughput: 0: 275.2. Samples: 8978832. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 05:22:15,981][166323] Avg episode reward: [(0, '1271.607')]
[36m[2025-07-02 05:22:20,969][166323] Fps is (10 sec: 1645.5, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8978432. Throughput: 0: 272.1. Samples: 8979632. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 05:22:20,969][166323] Avg episode reward: [(0, '1261.981')]
[36m[2025-07-02 05:22:25,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8978432. Throughput: 0: 274.0. Samples: 8981344. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 05:22:25,967][166323] Avg episode reward: [(0, '1276.401')]
[31m[31735465 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[31735465 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[31735465 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:22:30,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8978432. Throughput: 0: 274.8. Samples: 8982992. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 05:22:30,985][166323] Avg episode reward: [(0, '1236.715')]
[36m[2025-07-02 05:22:35,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8978432. Throughput: 0: 276.2. Samples: 8983936. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 05:22:35,983][166323] Avg episode reward: [(0, '1264.452')]
[36m[2025-07-02 05:22:40,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 8978432. Throughput: 0: 276.4. Samples: 8985648. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 05:22:40,945][166323] Avg episode reward: [(0, '1299.081')]
[36m[2025-07-02 05:22:45,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8978432. Throughput: 0: 274.7. Samples: 8987280. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 05:22:45,984][166323] Avg episode reward: [(0, '1305.190')]
[36m[2025-07-02 05:22:51,000][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8978432. Throughput: 0: 277.6. Samples: 8988144. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 05:22:51,000][166323] Avg episode reward: [(0, '1267.697')]
[31m[31760896 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[31760896 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[31760897 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:22:55,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8978432. Throughput: 0: 276.0. Samples: 8989696. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 05:22:55,991][166323] Avg episode reward: [(0, '1255.436')]
[36m[2025-07-02 05:23:00,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8978432. Throughput: 0: 276.9. Samples: 8991296. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 05:23:00,991][166323] Avg episode reward: [(0, '1274.305')]
[36m[2025-07-02 05:23:05,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8978432. Throughput: 0: 277.6. Samples: 8992128. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 05:23:05,978][166323] Avg episode reward: [(0, '1266.927')]
[36m[2025-07-02 05:23:10,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 222.2). Total num frames: 8978432. Throughput: 0: 276.4. Samples: 8993776. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 05:23:10,945][166323] Avg episode reward: [(0, '1271.004')]
[36m[2025-07-02 05:23:15,961][166323] Fps is (10 sec: 1641.2, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8994816. Throughput: 0: 277.5. Samples: 8995472. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 05:23:15,961][166323] Avg episode reward: [(0, '1253.286')]
[36m[2025-07-02 05:23:20,979][166323] Fps is (10 sec: 1632.8, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 8994816. Throughput: 0: 275.6. Samples: 8996336. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 05:23:20,979][166323] Avg episode reward: [(0, '1265.307')]
[36m[2025-07-02 05:23:25,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8994816. Throughput: 0: 274.1. Samples: 8997984. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 05:23:25,956][166323] Avg episode reward: [(0, '1239.561')]
[36m[2025-07-02 05:23:30,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8994816. Throughput: 0: 273.4. Samples: 8999584. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 05:23:30,985][166323] Avg episode reward: [(0, '1263.009')]
[36m[2025-07-02 05:23:35,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8994816. Throughput: 0: 272.6. Samples: 9000400. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 05:23:35,959][166323] Avg episode reward: [(0, '1260.543')]
[37m[1m[2025-07-02 05:23:36,010][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000017560_8994816.pth...
[36m[2025-07-02 05:23:36,014][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000017432_8929280.pth
[36m[2025-07-02 05:23:40,971][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 8994816. Throughput: 0: 274.6. Samples: 9002048. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 05:23:40,971][166323] Avg episode reward: [(0, '1288.138')]
[36m[2025-07-02 05:23:45,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8994816. Throughput: 0: 276.1. Samples: 9003712. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 05:23:45,963][166323] Avg episode reward: [(0, '1259.287')]
[36m[2025-07-02 05:23:50,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8994816. Throughput: 0: 275.9. Samples: 9004544. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 05:23:50,976][166323] Avg episode reward: [(0, '1257.755')]
[36m[2025-07-02 05:23:55,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 8994816. Throughput: 0: 273.9. Samples: 9006112. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 05:23:55,989][166323] Avg episode reward: [(0, '1241.024')]
[36m[2025-07-02 05:24:00,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8994816. Throughput: 0: 273.5. Samples: 9007776. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 05:24:00,956][166323] Avg episode reward: [(0, '1222.342')]
[31m[31831983 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[31831983 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[31831983 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[31832212 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[31832213 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[31832213 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:24:05,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 8994816. Throughput: 0: 272.5. Samples: 9008592. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 05:24:05,952][166323] Avg episode reward: [(0, '1182.625')]
[36m[2025-07-02 05:24:11,018][166323] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 277.6). Total num frames: 8994816. Throughput: 0: 277.3. Samples: 9010480. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 05:24:11,018][166323] Avg episode reward: [(0, '1138.576')]
[36m[2025-07-02 05:24:15,997][166323] Fps is (10 sec: 1631.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9011200. Throughput: 0: 276.5. Samples: 9012032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:24:15,997][166323] Avg episode reward: [(0, '1229.336')]
[36m[2025-07-02 05:24:20,955][166323] Fps is (10 sec: 1648.8, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9011200. Throughput: 0: 277.0. Samples: 9012864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:24:20,955][166323] Avg episode reward: [(0, '1261.144')]
[36m[2025-07-02 05:24:25,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9011200. Throughput: 0: 275.6. Samples: 9014448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:24:25,971][166323] Avg episode reward: [(0, '1241.050')]
[36m[2025-07-02 05:24:30,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9011200. Throughput: 0: 273.5. Samples: 9016016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:24:30,948][166323] Avg episode reward: [(0, '1311.218')]
[36m[2025-07-02 05:24:35,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9011200. Throughput: 0: 272.0. Samples: 9016784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:24:35,970][166323] Avg episode reward: [(0, '1304.110')]
[36m[2025-07-02 05:24:40,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9011200. Throughput: 0: 274.6. Samples: 9018464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:24:40,971][166323] Avg episode reward: [(0, '1332.354')]
[36m[2025-07-02 05:24:45,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9011200. Throughput: 0: 271.8. Samples: 9020016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:24:45,985][166323] Avg episode reward: [(0, '1372.929')]
[36m[2025-07-02 05:24:50,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9011200. Throughput: 0: 270.4. Samples: 9020768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:24:50,975][166323] Avg episode reward: [(0, '1382.724')]
[36m[2025-07-02 05:24:55,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9011200. Throughput: 0: 267.0. Samples: 9022480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:24:55,955][166323] Avg episode reward: [(0, '1378.146')]
[36m[2025-07-02 05:25:00,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9011200. Throughput: 0: 266.9. Samples: 9024032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:25:00,966][166323] Avg episode reward: [(0, '1369.465')]
[36m[2025-07-02 05:25:05,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9011200. Throughput: 0: 267.7. Samples: 9024912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:25:05,955][166323] Avg episode reward: [(0, '1328.438')]
[36m[2025-07-02 05:25:10,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9011200. Throughput: 0: 266.4. Samples: 9026432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:25:10,960][166323] Avg episode reward: [(0, '1365.639')]
[36m[2025-07-02 05:25:15,968][166323] Fps is (10 sec: 1636.1, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9027584. Throughput: 0: 265.5. Samples: 9027968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:25:15,969][166323] Avg episode reward: [(0, '1401.132')]
[36m[2025-07-02 05:25:20,991][166323] Fps is (10 sec: 1633.3, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9027584. Throughput: 0: 265.5. Samples: 9028736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:25:20,991][166323] Avg episode reward: [(0, '1376.040')]
[36m[2025-07-02 05:25:25,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9027584. Throughput: 0: 264.6. Samples: 9030368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:25:25,955][166323] Avg episode reward: [(0, '1315.823')]
[36m[2025-07-02 05:25:30,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9027584. Throughput: 0: 266.9. Samples: 9032016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:25:30,948][166323] Avg episode reward: [(0, '1236.438')]
[36m[2025-07-02 05:25:35,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9027584. Throughput: 0: 268.9. Samples: 9032864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:25:35,953][166323] Avg episode reward: [(0, '1264.146')]
[37m[1m[2025-07-02 05:25:36,006][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000017624_9027584.pth...
[36m[2025-07-02 05:25:36,010][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000017496_8962048.pth
[36m[2025-07-02 05:25:40,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9027584. Throughput: 0: 267.5. Samples: 9034528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:25:40,993][166323] Avg episode reward: [(0, '1217.225')]
[36m[2025-07-02 05:25:45,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9027584. Throughput: 0: 267.6. Samples: 9036080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:25:45,992][166323] Avg episode reward: [(0, '1175.243')]
[36m[2025-07-02 05:25:50,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9027584. Throughput: 0: 265.4. Samples: 9036864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:25:50,989][166323] Avg episode reward: [(0, '1172.117')]
[36m[2025-07-02 05:25:55,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9027584. Throughput: 0: 268.8. Samples: 9038528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:25:55,966][166323] Avg episode reward: [(0, '1217.249')]
[31m[31945609 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[31945609 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[31945609 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:26:00,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9027584. Throughput: 0: 270.8. Samples: 9040160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:26:00,988][166323] Avg episode reward: [(0, '1220.451')]
[36m[2025-07-02 05:26:05,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9027584. Throughput: 0: 273.9. Samples: 9041056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:26:05,978][166323] Avg episode reward: [(0, '1252.733')]
[36m[2025-07-02 05:26:10,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 9027584. Throughput: 0: 275.1. Samples: 9042752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:26:10,977][166323] Avg episode reward: [(0, '1227.656')]
[31m[31960902 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[31960903 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[31960903 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:26:15,989][166323] Fps is (10 sec: 1636.5, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9043968. Throughput: 0: 274.2. Samples: 9044368. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 05:26:15,989][166323] Avg episode reward: [(0, '1245.315')]
[36m[2025-07-02 05:26:20,958][166323] Fps is (10 sec: 1641.4, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9043968. Throughput: 0: 273.4. Samples: 9045168. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 05:26:20,959][166323] Avg episode reward: [(0, '1276.752')]
[36m[2025-07-02 05:26:25,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9043968. Throughput: 0: 270.4. Samples: 9046688. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 05:26:25,964][166323] Avg episode reward: [(0, '1291.240')]
[36m[2025-07-02 05:26:30,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9043968. Throughput: 0: 273.7. Samples: 9048384. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 05:26:30,952][166323] Avg episode reward: [(0, '1243.519')]
[36m[2025-07-02 05:26:35,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9043968. Throughput: 0: 273.1. Samples: 9049152. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 05:26:35,982][166323] Avg episode reward: [(0, '1223.251')]
[36m[2025-07-02 05:26:41,008][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9043968. Throughput: 0: 273.2. Samples: 9050832. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 05:26:41,009][166323] Avg episode reward: [(0, '1205.050')]
[36m[2025-07-02 05:26:46,001][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9043968. Throughput: 0: 272.3. Samples: 9052416. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 05:26:46,001][166323] Avg episode reward: [(0, '1218.645')]
[36m[2025-07-02 05:26:50,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9043968. Throughput: 0: 270.5. Samples: 9053232. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 05:26:50,999][166323] Avg episode reward: [(0, '1214.315')]
[31m[31999565 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[31999565 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[31999566 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:26:55,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9043968. Throughput: 0: 269.9. Samples: 9054896. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 05:26:55,974][166323] Avg episode reward: [(0, '1191.905')]
[36m[2025-07-02 05:27:00,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9043968. Throughput: 0: 273.6. Samples: 9056672. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 05:27:00,953][166323] Avg episode reward: [(0, '1212.426')]
[36m[2025-07-02 05:27:05,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9043968. Throughput: 0: 273.4. Samples: 9057472. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 05:27:05,957][166323] Avg episode reward: [(0, '1244.649')]
[36m[2025-07-02 05:27:10,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 9043968. Throughput: 0: 273.4. Samples: 9058992. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 05:27:10,973][166323] Avg episode reward: [(0, '1263.498')]
[36m[2025-07-02 05:27:15,951][166323] Fps is (10 sec: 1639.3, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9060352. Throughput: 0: 269.9. Samples: 9060528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:27:15,951][166323] Avg episode reward: [(0, '1308.842')]
[36m[2025-07-02 05:27:20,948][166323] Fps is (10 sec: 1642.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9060352. Throughput: 0: 271.1. Samples: 9061344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:27:20,948][166323] Avg episode reward: [(0, '1326.735')]
[36m[2025-07-02 05:27:25,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9060352. Throughput: 0: 271.9. Samples: 9063056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:27:25,974][166323] Avg episode reward: [(0, '1361.603')]
[36m[2025-07-02 05:27:30,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9060352. Throughput: 0: 270.9. Samples: 9064592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:27:30,949][166323] Avg episode reward: [(0, '1386.139')]
[36m[2025-07-02 05:27:35,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 9060352. Throughput: 0: 271.6. Samples: 9065456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:27:35,999][166323] Avg episode reward: [(0, '1378.243')]
[37m[1m[2025-07-02 05:27:36,083][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000017688_9060352.pth...
[36m[2025-07-02 05:27:36,088][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000017560_8994816.pth
[36m[2025-07-02 05:27:40,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9060352. Throughput: 0: 271.4. Samples: 9067104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:27:40,949][166323] Avg episode reward: [(0, '1381.615')]
[31m[32050349 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[32050349 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[32050350 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:27:45,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9060352. Throughput: 0: 266.6. Samples: 9068672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:27:45,966][166323] Avg episode reward: [(0, '1367.818')]
[31m[32055742 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[32055743 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[32055743 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:27:50,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9060352. Throughput: 0: 267.7. Samples: 9069520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:27:50,957][166323] Avg episode reward: [(0, '1333.362')]
[36m[2025-07-02 05:27:55,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9060352. Throughput: 0: 275.6. Samples: 9071392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:27:55,970][166323] Avg episode reward: [(0, '1324.476')]
[36m[2025-07-02 05:28:00,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9060352. Throughput: 0: 275.7. Samples: 9072944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:28:00,990][166323] Avg episode reward: [(0, '1295.802')]
[36m[2025-07-02 05:28:05,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9060352. Throughput: 0: 276.2. Samples: 9073776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:28:05,958][166323] Avg episode reward: [(0, '1236.242')]
[36m[2025-07-02 05:28:10,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 9060352. Throughput: 0: 274.8. Samples: 9075424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:28:10,989][166323] Avg episode reward: [(0, '1188.092')]
[31m[32082459 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[32082459 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[32082459 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:28:15,974][166323] Fps is (10 sec: 1635.7, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9076736. Throughput: 0: 276.5. Samples: 9077040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:28:15,974][166323] Avg episode reward: [(0, '1172.715')]
[36m[2025-07-02 05:28:20,985][166323] Fps is (10 sec: 1638.9, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9076736. Throughput: 0: 276.7. Samples: 9077904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:28:20,986][166323] Avg episode reward: [(0, '1189.610')]
[36m[2025-07-02 05:28:25,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9076736. Throughput: 0: 277.6. Samples: 9079600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:28:25,958][166323] Avg episode reward: [(0, '1136.906')]
[36m[2025-07-02 05:28:31,008][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 9076736. Throughput: 0: 280.6. Samples: 9081312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:28:31,009][166323] Avg episode reward: [(0, '1147.689')]
[36m[2025-07-02 05:28:35,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9076736. Throughput: 0: 281.0. Samples: 9082160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:28:35,944][166323] Avg episode reward: [(0, '1189.386')]
[36m[2025-07-02 05:28:40,980][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9076736. Throughput: 0: 279.1. Samples: 9083952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:28:40,980][166323] Avg episode reward: [(0, '1221.197')]
[36m[2025-07-02 05:28:45,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9076736. Throughput: 0: 281.8. Samples: 9085616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:28:45,958][166323] Avg episode reward: [(0, '1200.345')]
[36m[2025-07-02 05:28:50,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9076736. Throughput: 0: 281.3. Samples: 9086432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:28:50,949][166323] Avg episode reward: [(0, '1223.719')]
[36m[2025-07-02 05:28:55,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9076736. Throughput: 0: 281.7. Samples: 9088096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:28:55,978][166323] Avg episode reward: [(0, '1225.069')]
[36m[2025-07-02 05:29:00,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9076736. Throughput: 0: 279.0. Samples: 9089600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:29:00,993][166323] Avg episode reward: [(0, '1190.686')]
[36m[2025-07-02 05:29:06,001][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9076736. Throughput: 0: 275.1. Samples: 9090288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:29:06,002][166323] Avg episode reward: [(0, '1150.932')]
[36m[2025-07-02 05:29:10,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 9076736. Throughput: 0: 272.0. Samples: 9091840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:29:10,960][166323] Avg episode reward: [(0, '1152.943')]
[36m[2025-07-02 05:29:15,995][166323] Fps is (10 sec: 1639.5, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9093120. Throughput: 0: 265.7. Samples: 9093264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:29:15,995][166323] Avg episode reward: [(0, '1224.717')]
[36m[2025-07-02 05:29:21,013][166323] Fps is (10 sec: 1629.8, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9093120. Throughput: 0: 262.0. Samples: 9093968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:29:21,013][166323] Avg episode reward: [(0, '1213.116')]
[36m[2025-07-02 05:29:25,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9093120. Throughput: 0: 257.8. Samples: 9095552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:29:25,971][166323] Avg episode reward: [(0, '1203.811')]
[36m[2025-07-02 05:29:30,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9093120. Throughput: 0: 256.3. Samples: 9097152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:29:30,965][166323] Avg episode reward: [(0, '1257.047')]
[36m[2025-07-02 05:29:35,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9093120. Throughput: 0: 255.2. Samples: 9097920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:29:35,962][166323] Avg episode reward: [(0, '1308.206')]
[37m[1m[2025-07-02 05:29:36,015][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000017752_9093120.pth...
[36m[2025-07-02 05:29:36,022][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000017624_9027584.pth
[36m[2025-07-02 05:29:40,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9093120. Throughput: 0: 256.4. Samples: 9099632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:29:40,972][166323] Avg episode reward: [(0, '1366.318')]
[36m[2025-07-02 05:29:45,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9093120. Throughput: 0: 260.8. Samples: 9101328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:29:45,959][166323] Avg episode reward: [(0, '1272.418')]
[36m[2025-07-02 05:29:50,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9093120. Throughput: 0: 263.4. Samples: 9102128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:29:50,960][166323] Avg episode reward: [(0, '1275.473')]
[36m[2025-07-02 05:29:55,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9093120. Throughput: 0: 264.0. Samples: 9103728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:29:55,984][166323] Avg episode reward: [(0, '1290.525')]
[36m[2025-07-02 05:30:01,003][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 9093120. Throughput: 0: 268.4. Samples: 9105344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:30:01,004][166323] Avg episode reward: [(0, '1248.266')]
[36m[2025-07-02 05:30:05,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9093120. Throughput: 0: 270.8. Samples: 9106144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:30:05,984][166323] Avg episode reward: [(0, '1146.521')]
[36m[2025-07-02 05:30:10,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 9093120. Throughput: 0: 273.3. Samples: 9107856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:30:10,995][166323] Avg episode reward: [(0, '1189.526')]
[36m[2025-07-02 05:30:16,020][166323] Fps is (10 sec: 1632.4, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9109504. Throughput: 0: 273.1. Samples: 9109456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:30:16,021][166323] Avg episode reward: [(0, '1208.965')]
[36m[2025-07-02 05:30:20,960][166323] Fps is (10 sec: 1644.1, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9109504. Throughput: 0: 272.0. Samples: 9110160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:30:20,960][166323] Avg episode reward: [(0, '1254.959')]
[36m[2025-07-02 05:30:25,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9109504. Throughput: 0: 270.8. Samples: 9111824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:30:25,989][166323] Avg episode reward: [(0, '1243.325')]
[36m[2025-07-02 05:30:31,009][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 9109504. Throughput: 0: 268.9. Samples: 9113440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:30:31,009][166323] Avg episode reward: [(0, '1254.899')]
[36m[2025-07-02 05:30:35,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9109504. Throughput: 0: 268.6. Samples: 9114224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:30:35,999][166323] Avg episode reward: [(0, '1331.970')]
[36m[2025-07-02 05:30:40,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9109504. Throughput: 0: 272.4. Samples: 9115984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:30:40,977][166323] Avg episode reward: [(0, '1406.855')]
[36m[2025-07-02 05:30:45,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9109504. Throughput: 0: 271.3. Samples: 9117536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:30:45,948][166323] Avg episode reward: [(0, '1387.260')]
[36m[2025-07-02 05:30:50,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9109504. Throughput: 0: 270.4. Samples: 9118304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:30:50,955][166323] Avg episode reward: [(0, '1377.766')]
[36m[2025-07-02 05:30:55,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9109504. Throughput: 0: 270.7. Samples: 9120032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:30:55,975][166323] Avg episode reward: [(0, '1419.784')]
[36m[2025-07-02 05:31:01,012][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9109504. Throughput: 0: 275.2. Samples: 9121840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:31:01,013][166323] Avg episode reward: [(0, '1373.839')]
[36m[2025-07-02 05:31:05,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9109504. Throughput: 0: 276.8. Samples: 9122624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:31:05,988][166323] Avg episode reward: [(0, '1342.877')]
[36m[2025-07-02 05:31:10,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 9109504. Throughput: 0: 277.4. Samples: 9124304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:31:10,980][166323] Avg episode reward: [(0, '1317.924')]
[36m[2025-07-02 05:31:15,990][166323] Fps is (10 sec: 1638.1, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9125888. Throughput: 0: 277.1. Samples: 9125904. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 05:31:15,990][166323] Avg episode reward: [(0, '1265.019')]
[36m[2025-07-02 05:31:20,988][166323] Fps is (10 sec: 1637.1, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9125888. Throughput: 0: 277.8. Samples: 9126720. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 05:31:20,988][166323] Avg episode reward: [(0, '1275.123')]
[36m[2025-07-02 05:31:25,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9125888. Throughput: 0: 274.2. Samples: 9128320. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 05:31:25,974][166323] Avg episode reward: [(0, '1268.437')]
[33m[32279218 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[32279218 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.8662109375
[33mCrash Rate: 0.11181640625
[33mTimeout Rate: 0.02197265625 (navigation_task.py:265)
[33m[32279218 ms][navigation_task] - WARNING : 
[33mSuccesses: 1774
[33mCrashes : 229
[33mTimeouts: 45 (navigation_task.py:268)
[36m[2025-07-02 05:31:30,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9125888. Throughput: 0: 275.2. Samples: 9129920. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 05:31:30,949][166323] Avg episode reward: [(0, '1236.658')]
[36m[2025-07-02 05:31:35,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9125888. Throughput: 0: 278.8. Samples: 9130848. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 05:31:35,952][166323] Avg episode reward: [(0, '1238.067')]
[37m[1m[2025-07-02 05:31:36,008][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000017816_9125888.pth...
[36m[2025-07-02 05:31:36,012][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000017688_9060352.pth
[36m[2025-07-02 05:31:40,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9125888. Throughput: 0: 276.2. Samples: 9132464. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 05:31:40,981][166323] Avg episode reward: [(0, '1260.154')]
[36m[2025-07-02 05:31:45,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9125888. Throughput: 0: 273.4. Samples: 9134128. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 05:31:45,956][166323] Avg episode reward: [(0, '1183.504')]
[36m[2025-07-02 05:31:50,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9125888. Throughput: 0: 273.4. Samples: 9134928. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 05:31:50,994][166323] Avg episode reward: [(0, '1238.703')]
[36m[2025-07-02 05:31:55,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9125888. Throughput: 0: 272.4. Samples: 9136560. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 05:31:55,975][166323] Avg episode reward: [(0, '1237.372')]
[36m[2025-07-02 05:32:00,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9125888. Throughput: 0: 273.6. Samples: 9138208. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 05:32:00,955][166323] Avg episode reward: [(0, '1224.535')]
[36m[2025-07-02 05:32:05,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9125888. Throughput: 0: 275.8. Samples: 9139120. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 05:32:05,956][166323] Avg episode reward: [(0, '1223.506')]
[36m[2025-07-02 05:32:11,012][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 9125888. Throughput: 0: 276.4. Samples: 9140768. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 05:32:11,012][166323] Avg episode reward: [(0, '1209.505')]
[36m[2025-07-02 05:32:15,969][166323] Fps is (10 sec: 1636.1, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9142272. Throughput: 0: 275.4. Samples: 9142320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:32:15,969][166323] Avg episode reward: [(0, '1201.188')]
[36m[2025-07-02 05:32:20,960][166323] Fps is (10 sec: 1647.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9142272. Throughput: 0: 272.7. Samples: 9143120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:32:20,960][166323] Avg episode reward: [(0, '1240.365')]
[36m[2025-07-02 05:32:25,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9142272. Throughput: 0: 275.3. Samples: 9144848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:32:25,960][166323] Avg episode reward: [(0, '1215.000')]
[36m[2025-07-02 05:32:30,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9142272. Throughput: 0: 275.7. Samples: 9146544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:32:30,987][166323] Avg episode reward: [(0, '1210.130')]
[36m[2025-07-02 05:32:35,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9142272. Throughput: 0: 275.3. Samples: 9147312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:32:35,982][166323] Avg episode reward: [(0, '1222.460')]
[36m[2025-07-02 05:32:40,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9142272. Throughput: 0: 275.0. Samples: 9148928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:32:40,952][166323] Avg episode reward: [(0, '1194.863')]
[36m[2025-07-02 05:32:46,004][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 9142272. Throughput: 0: 271.0. Samples: 9150416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:32:46,004][166323] Avg episode reward: [(0, '1223.684')]
[36m[2025-07-02 05:32:50,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9142272. Throughput: 0: 267.0. Samples: 9151136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:32:50,953][166323] Avg episode reward: [(0, '1234.247')]
[36m[2025-07-02 05:32:55,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9142272. Throughput: 0: 268.5. Samples: 9152832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:32:55,947][166323] Avg episode reward: [(0, '1243.907')]
[36m[2025-07-02 05:33:00,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9142272. Throughput: 0: 268.2. Samples: 9154384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:33:00,955][166323] Avg episode reward: [(0, '1280.731')]
[36m[2025-07-02 05:33:05,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9142272. Throughput: 0: 268.6. Samples: 9155216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:33:05,989][166323] Avg episode reward: [(0, '1302.953')]
[36m[2025-07-02 05:33:10,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 9142272. Throughput: 0: 266.6. Samples: 9156848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:33:10,977][166323] Avg episode reward: [(0, '1288.011')]
[36m[2025-07-02 05:33:15,963][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 222.2). Total num frames: 9142272. Throughput: 0: 262.5. Samples: 9158352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:33:15,964][166323] Avg episode reward: [(0, '1238.831')]
[36m[2025-07-02 05:33:20,950][166323] Fps is (10 sec: 1642.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9158656. Throughput: 0: 264.4. Samples: 9159200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:33:20,950][166323] Avg episode reward: [(0, '1293.140')]
[31m[32394118 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[32394118 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[32394118 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:33:25,952][166323] Fps is (10 sec: 1640.3, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9158656. Throughput: 0: 263.1. Samples: 9160768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:33:25,952][166323] Avg episode reward: [(0, '1327.554')]
[36m[2025-07-02 05:33:31,014][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 9158656. Throughput: 0: 267.7. Samples: 9162464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:33:31,015][166323] Avg episode reward: [(0, '1327.172')]
[36m[2025-07-02 05:33:35,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9158656. Throughput: 0: 268.6. Samples: 9163232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:33:35,994][166323] Avg episode reward: [(0, '1353.558')]
[37m[1m[2025-07-02 05:33:36,052][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000017880_9158656.pth...
[36m[2025-07-02 05:33:36,056][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000017752_9093120.pth
[36m[2025-07-02 05:33:40,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9158656. Throughput: 0: 268.0. Samples: 9164896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:33:40,967][166323] Avg episode reward: [(0, '1340.952')]
[36m[2025-07-02 05:33:45,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.6). Total num frames: 9158656. Throughput: 0: 269.6. Samples: 9166528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:33:45,999][166323] Avg episode reward: [(0, '1322.390')]
[36m[2025-07-02 05:33:50,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9158656. Throughput: 0: 270.8. Samples: 9167392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:33:50,946][166323] Avg episode reward: [(0, '1330.042')]
[36m[2025-07-02 05:33:55,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9158656. Throughput: 0: 271.1. Samples: 9169040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:33:55,954][166323] Avg episode reward: [(0, '1333.779')]
[36m[2025-07-02 05:34:00,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9158656. Throughput: 0: 272.6. Samples: 9170624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:34:00,987][166323] Avg episode reward: [(0, '1299.984')]
[36m[2025-07-02 05:34:05,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9158656. Throughput: 0: 272.3. Samples: 9171456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:34:05,952][166323] Avg episode reward: [(0, '1285.058')]
[36m[2025-07-02 05:34:10,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 9158656. Throughput: 0: 273.0. Samples: 9173056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:34:10,958][166323] Avg episode reward: [(0, '1250.829')]
[36m[2025-07-02 05:34:15,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 9158656. Throughput: 0: 273.4. Samples: 9174752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:34:15,959][166323] Avg episode reward: [(0, '1285.577')]
[31m[32445973 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[32445973 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[32445973 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:34:20,991][166323] Fps is (10 sec: 1633.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9175040. Throughput: 0: 273.8. Samples: 9175552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:34:20,991][166323] Avg episode reward: [(0, '1246.749')]
[36m[2025-07-02 05:34:25,945][166323] Fps is (10 sec: 1640.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9175040. Throughput: 0: 271.4. Samples: 9177104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:34:25,945][166323] Avg episode reward: [(0, '1245.697')]
[36m[2025-07-02 05:34:30,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9175040. Throughput: 0: 272.1. Samples: 9178768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:34:30,982][166323] Avg episode reward: [(0, '1214.957')]
[36m[2025-07-02 05:34:36,001][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9175040. Throughput: 0: 272.4. Samples: 9179664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:34:36,002][166323] Avg episode reward: [(0, '1249.551')]
[36m[2025-07-02 05:34:41,004][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9175040. Throughput: 0: 269.6. Samples: 9181184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:34:41,004][166323] Avg episode reward: [(0, '1268.432')]
[36m[2025-07-02 05:34:45,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9175040. Throughput: 0: 270.9. Samples: 9182816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:34:45,984][166323] Avg episode reward: [(0, '1251.430')]
[36m[2025-07-02 05:34:50,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9175040. Throughput: 0: 272.7. Samples: 9183728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:34:50,957][166323] Avg episode reward: [(0, '1259.876')]
[36m[2025-07-02 05:34:55,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9175040. Throughput: 0: 273.3. Samples: 9185360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:34:55,984][166323] Avg episode reward: [(0, '1246.760')]
[36m[2025-07-02 05:35:00,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9175040. Throughput: 0: 273.3. Samples: 9187056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:35:00,982][166323] Avg episode reward: [(0, '1338.631')]
[36m[2025-07-02 05:35:05,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9175040. Throughput: 0: 275.2. Samples: 9187936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:35:05,993][166323] Avg episode reward: [(0, '1335.395')]
[36m[2025-07-02 05:35:10,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.2). Total num frames: 9175040. Throughput: 0: 275.3. Samples: 9189504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:35:10,991][166323] Avg episode reward: [(0, '1316.447')]
[36m[2025-07-02 05:35:16,005][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 9175040. Throughput: 0: 273.3. Samples: 9191072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:35:16,005][166323] Avg episode reward: [(0, '1285.524')]
[36m[2025-07-02 05:35:20,977][166323] Fps is (10 sec: 1640.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9191424. Throughput: 0: 269.7. Samples: 9191792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:35:20,978][166323] Avg episode reward: [(0, '1266.946')]
[36m[2025-07-02 05:35:25,947][166323] Fps is (10 sec: 1647.9, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 9191424. Throughput: 0: 271.6. Samples: 9193392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:35:25,947][166323] Avg episode reward: [(0, '1259.224')]
[36m[2025-07-02 05:35:30,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9191424. Throughput: 0: 271.1. Samples: 9195008. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:35:30,961][166323] Avg episode reward: [(0, '1278.639')]
[36m[2025-07-02 05:35:35,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9191424. Throughput: 0: 269.2. Samples: 9195840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:35:35,957][166323] Avg episode reward: [(0, '1256.701')]
[37m[1m[2025-07-02 05:35:36,015][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000017944_9191424.pth...
[36m[2025-07-02 05:35:36,019][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000017816_9125888.pth
[36m[2025-07-02 05:35:40,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9191424. Throughput: 0: 271.1. Samples: 9197552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:35:40,952][166323] Avg episode reward: [(0, '1246.408')]
[36m[2025-07-02 05:35:46,017][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 9191424. Throughput: 0: 267.2. Samples: 9199088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:35:46,018][166323] Avg episode reward: [(0, '1194.561')]
[36m[2025-07-02 05:35:50,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9191424. Throughput: 0: 264.2. Samples: 9199824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:35:50,989][166323] Avg episode reward: [(0, '1189.956')]
[36m[2025-07-02 05:35:55,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 9191424. Throughput: 0: 266.2. Samples: 9201472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:35:55,952][166323] Avg episode reward: [(0, '1211.271')]
[36m[2025-07-02 05:36:00,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9191424. Throughput: 0: 269.3. Samples: 9203184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:36:00,987][166323] Avg episode reward: [(0, '1210.643')]
[36m[2025-07-02 05:36:05,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9191424. Throughput: 0: 271.8. Samples: 9204016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:36:05,951][166323] Avg episode reward: [(0, '1234.766')]
[36m[2025-07-02 05:36:10,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 9191424. Throughput: 0: 270.7. Samples: 9205584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:36:10,984][166323] Avg episode reward: [(0, '1256.558')]
[36m[2025-07-02 05:36:15,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 9191424. Throughput: 0: 273.0. Samples: 9207296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:36:15,978][166323] Avg episode reward: [(0, '1226.527')]
[36m[2025-07-02 05:36:20,975][166323] Fps is (10 sec: 1639.9, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9207808. Throughput: 0: 270.8. Samples: 9208032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:36:20,975][166323] Avg episode reward: [(0, '1246.184')]
[36m[2025-07-02 05:36:25,963][166323] Fps is (10 sec: 1640.9, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9207808. Throughput: 0: 268.7. Samples: 9209648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:36:25,963][166323] Avg episode reward: [(0, '1258.086')]
[36m[2025-07-02 05:36:30,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9207808. Throughput: 0: 274.3. Samples: 9211424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:36:30,990][166323] Avg episode reward: [(0, '1212.348')]
[36m[2025-07-02 05:36:35,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9207808. Throughput: 0: 276.4. Samples: 9212256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:36:35,967][166323] Avg episode reward: [(0, '1204.251')]
[36m[2025-07-02 05:36:40,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9207808. Throughput: 0: 275.9. Samples: 9213888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:36:40,949][166323] Avg episode reward: [(0, '1204.573')]
[36m[2025-07-02 05:36:45,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9207808. Throughput: 0: 271.8. Samples: 9215408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:36:45,962][166323] Avg episode reward: [(0, '1150.706')]
[36m[2025-07-02 05:36:50,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9207808. Throughput: 0: 273.0. Samples: 9216304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:36:50,961][166323] Avg episode reward: [(0, '1270.251')]
[36m[2025-07-02 05:36:55,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9207808. Throughput: 0: 275.7. Samples: 9217984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:36:55,965][166323] Avg episode reward: [(0, '1266.540')]
[36m[2025-07-02 05:37:00,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9207808. Throughput: 0: 272.3. Samples: 9219552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:37:00,991][166323] Avg episode reward: [(0, '1266.717')]
[36m[2025-07-02 05:37:05,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 9207808. Throughput: 0: 274.7. Samples: 9220400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:37:06,000][166323] Avg episode reward: [(0, '1306.347')]
[36m[2025-07-02 05:37:10,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 9207808. Throughput: 0: 274.4. Samples: 9222000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:37:10,971][166323] Avg episode reward: [(0, '1292.029')]
[36m[2025-07-02 05:37:15,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.1). Total num frames: 9207808. Throughput: 0: 270.7. Samples: 9223600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:37:15,975][166323] Avg episode reward: [(0, '1290.215')]
[36m[2025-07-02 05:37:20,981][166323] Fps is (10 sec: 1636.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9224192. Throughput: 0: 269.4. Samples: 9224384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:37:20,982][166323] Avg episode reward: [(0, '1357.720')]
[36m[2025-07-02 05:37:25,963][166323] Fps is (10 sec: 1640.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9224192. Throughput: 0: 270.8. Samples: 9226080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:37:25,963][166323] Avg episode reward: [(0, '1286.317')]
[36m[2025-07-02 05:37:30,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9224192. Throughput: 0: 274.8. Samples: 9227776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:37:30,961][166323] Avg episode reward: [(0, '1290.033')]
[36m[2025-07-02 05:37:35,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9224192. Throughput: 0: 274.8. Samples: 9228672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:37:35,970][166323] Avg episode reward: [(0, '1268.532')]
[37m[1m[2025-07-02 05:37:36,028][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018008_9224192.pth...
[36m[2025-07-02 05:37:36,035][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000017880_9158656.pth
[36m[2025-07-02 05:37:40,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9224192. Throughput: 0: 276.0. Samples: 9230400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:37:40,957][166323] Avg episode reward: [(0, '1239.754')]
[36m[2025-07-02 05:37:45,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9224192. Throughput: 0: 278.2. Samples: 9232064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:37:45,974][166323] Avg episode reward: [(0, '1241.043')]
[36m[2025-07-02 05:37:50,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9224192. Throughput: 0: 276.9. Samples: 9232848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:37:50,959][166323] Avg episode reward: [(0, '1242.561')]
[36m[2025-07-02 05:37:55,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9224192. Throughput: 0: 276.9. Samples: 9234464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:37:55,982][166323] Avg episode reward: [(0, '1244.676')]
[36m[2025-07-02 05:38:00,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9224192. Throughput: 0: 276.9. Samples: 9236064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:38:00,991][166323] Avg episode reward: [(0, '1218.258')]
[36m[2025-07-02 05:38:05,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9224192. Throughput: 0: 278.2. Samples: 9236896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:38:05,952][166323] Avg episode reward: [(0, '1261.771')]
[36m[2025-07-02 05:38:10,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9224192. Throughput: 0: 283.1. Samples: 9238816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:38:10,955][166323] Avg episode reward: [(0, '1233.759')]
[36m[2025-07-02 05:38:15,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 9224192. Throughput: 0: 280.9. Samples: 9240416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:38:15,958][166323] Avg episode reward: [(0, '1286.797')]
[36m[2025-07-02 05:38:20,944][166323] Fps is (10 sec: 1640.1, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9240576. Throughput: 0: 276.4. Samples: 9241104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:38:20,945][166323] Avg episode reward: [(0, '1260.029')]
[36m[2025-07-02 05:38:25,963][166323] Fps is (10 sec: 1637.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9240576. Throughput: 0: 273.4. Samples: 9242704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:38:25,963][166323] Avg episode reward: [(0, '1221.069')]
[31m[32697741 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[32697741 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[32697742 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:38:30,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9240576. Throughput: 0: 275.2. Samples: 9244448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:38:30,969][166323] Avg episode reward: [(0, '1185.722')]
[36m[2025-07-02 05:38:35,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9240576. Throughput: 0: 276.5. Samples: 9245296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:38:35,980][166323] Avg episode reward: [(0, '1255.249')]
[36m[2025-07-02 05:38:40,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9240576. Throughput: 0: 277.2. Samples: 9246928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:38:40,950][166323] Avg episode reward: [(0, '1239.301')]
[36m[2025-07-02 05:38:45,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9240576. Throughput: 0: 277.1. Samples: 9248528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:38:45,975][166323] Avg episode reward: [(0, '1264.729')]
[36m[2025-07-02 05:38:50,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9240576. Throughput: 0: 276.3. Samples: 9249328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:38:50,952][166323] Avg episode reward: [(0, '1247.506')]
[36m[2025-07-02 05:38:55,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9240576. Throughput: 0: 270.5. Samples: 9250992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:38:55,976][166323] Avg episode reward: [(0, '1283.216')]
[36m[2025-07-02 05:39:00,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9240576. Throughput: 0: 269.2. Samples: 9252528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:39:00,944][166323] Avg episode reward: [(0, '1293.135')]
[31m[32734410 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[32734410 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[32734410 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:39:05,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9240576. Throughput: 0: 272.3. Samples: 9253360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:39:05,950][166323] Avg episode reward: [(0, '1294.301')]
[36m[2025-07-02 05:39:10,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9240576. Throughput: 0: 274.3. Samples: 9255056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:39:10,995][166323] Avg episode reward: [(0, '1256.458')]
[36m[2025-07-02 05:39:16,011][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 222.1). Total num frames: 9240576. Throughput: 0: 273.2. Samples: 9256752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:39:16,011][166323] Avg episode reward: [(0, '1331.088')]
[31m[32745831 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[32745831 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[32745832 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:39:21,004][166323] Fps is (10 sec: 1636.8, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 9256960. Throughput: 0: 272.2. Samples: 9257552. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 05:39:21,005][166323] Avg episode reward: [(0, '1277.693')]
[36m[2025-07-02 05:39:25,982][166323] Fps is (10 sec: 1643.1, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9256960. Throughput: 0: 273.6. Samples: 9259248. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 05:39:25,982][166323] Avg episode reward: [(0, '1267.661')]
[36m[2025-07-02 05:39:30,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9256960. Throughput: 0: 275.5. Samples: 9260928. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 05:39:30,985][166323] Avg episode reward: [(0, '1249.168')]
[36m[2025-07-02 05:39:35,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9256960. Throughput: 0: 279.3. Samples: 9261904. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 05:39:35,980][166323] Avg episode reward: [(0, '1176.279')]
[37m[1m[2025-07-02 05:39:36,031][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018072_9256960.pth...
[36m[2025-07-02 05:39:36,035][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000017944_9191424.pth
[36m[2025-07-02 05:39:40,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9256960. Throughput: 0: 280.6. Samples: 9263616. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 05:39:40,959][166323] Avg episode reward: [(0, '1196.388')]
[36m[2025-07-02 05:39:45,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9256960. Throughput: 0: 282.8. Samples: 9265264. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 05:39:45,974][166323] Avg episode reward: [(0, '1149.999')]
[36m[2025-07-02 05:39:50,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9256960. Throughput: 0: 282.0. Samples: 9266064. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 05:39:50,994][166323] Avg episode reward: [(0, '1117.448')]
[31m[32780955 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[32780955 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[32780956 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:39:55,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9256960. Throughput: 0: 280.4. Samples: 9267664. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 05:39:55,951][166323] Avg episode reward: [(0, '1162.497')]
[36m[2025-07-02 05:40:01,024][166323] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 277.7). Total num frames: 9256960. Throughput: 0: 278.0. Samples: 9269264. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 05:40:01,025][166323] Avg episode reward: [(0, '1162.938')]
[36m[2025-07-02 05:40:06,000][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 9256960. Throughput: 0: 278.8. Samples: 9270096. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 05:40:06,000][166323] Avg episode reward: [(0, '1149.979')]
[36m[2025-07-02 05:40:11,006][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9256960. Throughput: 0: 277.9. Samples: 9271760. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 05:40:11,006][166323] Avg episode reward: [(0, '1166.658')]
[36m[2025-07-02 05:40:15,964][166323] Fps is (10 sec: 1644.4, 60 sec: 546.6, 300 sec: 277.7). Total num frames: 9273344. Throughput: 0: 278.2. Samples: 9273440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:40:15,964][166323] Avg episode reward: [(0, '1230.009')]
[36m[2025-07-02 05:40:20,985][166323] Fps is (10 sec: 1641.7, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9273344. Throughput: 0: 275.9. Samples: 9274320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:40:20,985][166323] Avg episode reward: [(0, '1171.792')]
[36m[2025-07-02 05:40:25,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9273344. Throughput: 0: 273.8. Samples: 9275936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:40:25,958][166323] Avg episode reward: [(0, '1220.336')]
[36m[2025-07-02 05:40:30,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9273344. Throughput: 0: 272.5. Samples: 9277520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:40:30,944][166323] Avg episode reward: [(0, '1192.340')]
[36m[2025-07-02 05:40:35,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9273344. Throughput: 0: 271.1. Samples: 9278256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:40:35,961][166323] Avg episode reward: [(0, '1222.692')]
[36m[2025-07-02 05:40:40,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9273344. Throughput: 0: 268.6. Samples: 9279760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:40:40,977][166323] Avg episode reward: [(0, '1192.228')]
[36m[2025-07-02 05:40:45,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9273344. Throughput: 0: 273.4. Samples: 9281552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:40:45,972][166323] Avg episode reward: [(0, '1193.378')]
[36m[2025-07-02 05:40:50,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9273344. Throughput: 0: 273.6. Samples: 9282400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:40:50,973][166323] Avg episode reward: [(0, '1211.276')]
[36m[2025-07-02 05:40:55,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9273344. Throughput: 0: 272.7. Samples: 9284016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:40:55,950][166323] Avg episode reward: [(0, '1302.780')]
[36m[2025-07-02 05:41:00,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9273344. Throughput: 0: 271.2. Samples: 9285648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:41:00,986][166323] Avg episode reward: [(0, '1272.582')]
[36m[2025-07-02 05:41:05,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9273344. Throughput: 0: 267.6. Samples: 9286352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:41:05,945][166323] Avg episode reward: [(0, '1302.106')]
[36m[2025-07-02 05:41:10,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9273344. Throughput: 0: 270.3. Samples: 9288096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:41:10,952][166323] Avg episode reward: [(0, '1300.438')]
[36m[2025-07-02 05:41:15,963][166323] Fps is (10 sec: 1635.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9289728. Throughput: 0: 271.2. Samples: 9289728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:41:15,963][166323] Avg episode reward: [(0, '1299.625')]
[36m[2025-07-02 05:41:20,974][166323] Fps is (10 sec: 1634.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9289728. Throughput: 0: 272.3. Samples: 9290512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:41:20,975][166323] Avg episode reward: [(0, '1270.306')]
[36m[2025-07-02 05:41:25,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9289728. Throughput: 0: 276.1. Samples: 9292176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:41:25,945][166323] Avg episode reward: [(0, '1238.913')]
[31m[32877749 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[32877750 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[32877750 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:41:30,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9289728. Throughput: 0: 271.9. Samples: 9293792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:41:30,981][166323] Avg episode reward: [(0, '1205.348')]
[36m[2025-07-02 05:41:35,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9289728. Throughput: 0: 271.5. Samples: 9294624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:41:35,995][166323] Avg episode reward: [(0, '1180.439')]
[37m[1m[2025-07-02 05:41:36,049][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018136_9289728.pth...
[36m[2025-07-02 05:41:36,053][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018008_9224192.pth
[36m[2025-07-02 05:41:40,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9289728. Throughput: 0: 270.8. Samples: 9296208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:41:40,979][166323] Avg episode reward: [(0, '1189.795')]
[36m[2025-07-02 05:41:45,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9289728. Throughput: 0: 270.3. Samples: 9297808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:41:45,968][166323] Avg episode reward: [(0, '1198.738')]
[31m[32898918 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[32898918 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[32898919 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:41:50,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9289728. Throughput: 0: 272.3. Samples: 9298608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:41:50,960][166323] Avg episode reward: [(0, '1200.429')]
[36m[2025-07-02 05:41:55,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9289728. Throughput: 0: 270.5. Samples: 9300272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:41:55,969][166323] Avg episode reward: [(0, '1158.172')]
[36m[2025-07-02 05:42:00,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9289728. Throughput: 0: 269.2. Samples: 9301840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:42:00,962][166323] Avg episode reward: [(0, '1134.485')]
[36m[2025-07-02 05:42:05,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9289728. Throughput: 0: 272.1. Samples: 9302752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:42:05,952][166323] Avg episode reward: [(0, '1145.155')]
[36m[2025-07-02 05:42:10,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9289728. Throughput: 0: 268.4. Samples: 9304256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:42:10,955][166323] Avg episode reward: [(0, '1174.811')]
[36m[2025-07-02 05:42:15,978][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 222.2). Total num frames: 9289728. Throughput: 0: 269.5. Samples: 9305920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:42:15,978][166323] Avg episode reward: [(0, '1127.211')]
[36m[2025-07-02 05:42:20,986][166323] Fps is (10 sec: 1633.2, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9306112. Throughput: 0: 271.3. Samples: 9306832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:42:20,987][166323] Avg episode reward: [(0, '1142.815')]
[36m[2025-07-02 05:42:26,000][166323] Fps is (10 sec: 1634.7, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 9306112. Throughput: 0: 273.6. Samples: 9308528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:42:26,001][166323] Avg episode reward: [(0, '1199.103')]
[36m[2025-07-02 05:42:30,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9306112. Throughput: 0: 275.5. Samples: 9310208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:42:30,972][166323] Avg episode reward: [(0, '1265.624')]
[36m[2025-07-02 05:42:35,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9306112. Throughput: 0: 278.3. Samples: 9311136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:42:35,977][166323] Avg episode reward: [(0, '1295.119')]
[36m[2025-07-02 05:42:40,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9306112. Throughput: 0: 278.5. Samples: 9312800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:42:40,949][166323] Avg episode reward: [(0, '1332.900')]
[36m[2025-07-02 05:42:45,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9306112. Throughput: 0: 276.2. Samples: 9314272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:42:45,974][166323] Avg episode reward: [(0, '1280.776')]
[36m[2025-07-02 05:42:50,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9306112. Throughput: 0: 274.7. Samples: 9315120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:42:50,973][166323] Avg episode reward: [(0, '1312.185')]
[36m[2025-07-02 05:42:55,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9306112. Throughput: 0: 279.2. Samples: 9316816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:42:55,947][166323] Avg episode reward: [(0, '1328.603')]
[36m[2025-07-02 05:43:00,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9306112. Throughput: 0: 277.4. Samples: 9318400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:43:00,971][166323] Avg episode reward: [(0, '1366.944')]
[33m[32971355 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[32971355 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.8525390625
[33mCrash Rate: 0.13330078125
[33mTimeout Rate: 0.01416015625 (navigation_task.py:265)
[33m[32971356 ms][navigation_task] - WARNING : 
[33mSuccesses: 1746
[33mCrashes : 273
[33mTimeouts: 29 (navigation_task.py:268)
[36m[2025-07-02 05:43:05,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9306112. Throughput: 0: 276.0. Samples: 9319248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:43:05,977][166323] Avg episode reward: [(0, '1321.040')]
[36m[2025-07-02 05:43:10,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9306112. Throughput: 0: 276.5. Samples: 9320960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:43:10,968][166323] Avg episode reward: [(0, '1218.961')]
[36m[2025-07-02 05:43:15,966][166323] Fps is (10 sec: 1640.0, 60 sec: 546.2, 300 sec: 277.7). Total num frames: 9322496. Throughput: 0: 275.9. Samples: 9322624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:43:15,966][166323] Avg episode reward: [(0, '1252.549')]
[36m[2025-07-02 05:43:20,984][166323] Fps is (10 sec: 1635.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9322496. Throughput: 0: 274.4. Samples: 9323488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:43:20,984][166323] Avg episode reward: [(0, '1257.557')]
[36m[2025-07-02 05:43:25,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9322496. Throughput: 0: 271.6. Samples: 9325024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:43:25,949][166323] Avg episode reward: [(0, '1271.226')]
[36m[2025-07-02 05:43:30,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9322496. Throughput: 0: 276.2. Samples: 9326704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:43:30,991][166323] Avg episode reward: [(0, '1254.550')]
[36m[2025-07-02 05:43:35,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9322496. Throughput: 0: 275.6. Samples: 9327520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:43:35,973][166323] Avg episode reward: [(0, '1264.394')]
[37m[1m[2025-07-02 05:43:36,033][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018200_9322496.pth...
[36m[2025-07-02 05:43:36,037][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018072_9256960.pth
[36m[2025-07-02 05:43:40,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9322496. Throughput: 0: 272.7. Samples: 9329088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:43:40,949][166323] Avg episode reward: [(0, '1226.591')]
[36m[2025-07-02 05:43:45,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9322496. Throughput: 0: 277.8. Samples: 9330896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:43:45,959][166323] Avg episode reward: [(0, '1273.136')]
[36m[2025-07-02 05:43:50,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9322496. Throughput: 0: 277.4. Samples: 9331728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:43:50,963][166323] Avg episode reward: [(0, '1251.769')]
[36m[2025-07-02 05:43:55,975][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9322496. Throughput: 0: 278.7. Samples: 9333504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:43:55,975][166323] Avg episode reward: [(0, '1299.999')]
[36m[2025-07-02 05:44:00,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9322496. Throughput: 0: 279.5. Samples: 9335200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:44:00,957][166323] Avg episode reward: [(0, '1318.279')]
[36m[2025-07-02 05:44:05,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9322496. Throughput: 0: 279.6. Samples: 9336064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:44:05,970][166323] Avg episode reward: [(0, '1270.489')]
[36m[2025-07-02 05:44:11,000][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9322496. Throughput: 0: 281.3. Samples: 9337696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:44:11,000][166323] Avg episode reward: [(0, '1269.537')]
[36m[2025-07-02 05:44:15,945][166323] Fps is (10 sec: 1642.5, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 9338880. Throughput: 0: 279.7. Samples: 9339280. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 05:44:15,945][166323] Avg episode reward: [(0, '1316.680')]
[36m[2025-07-02 05:44:20,959][166323] Fps is (10 sec: 1645.1, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9338880. Throughput: 0: 279.2. Samples: 9340080. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 05:44:20,959][166323] Avg episode reward: [(0, '1322.675')]
[36m[2025-07-02 05:44:25,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9338880. Throughput: 0: 281.0. Samples: 9341744. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 05:44:25,987][166323] Avg episode reward: [(0, '1304.280')]
[36m[2025-07-02 05:44:30,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9338880. Throughput: 0: 278.9. Samples: 9343456. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 05:44:30,993][166323] Avg episode reward: [(0, '1264.706')]
[36m[2025-07-02 05:44:35,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9338880. Throughput: 0: 279.4. Samples: 9344304. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 05:44:35,978][166323] Avg episode reward: [(0, '1230.921')]
[36m[2025-07-02 05:44:40,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9338880. Throughput: 0: 275.8. Samples: 9345920. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 05:44:40,990][166323] Avg episode reward: [(0, '1213.301')]
[36m[2025-07-02 05:44:45,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9338880. Throughput: 0: 273.5. Samples: 9347504. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 05:44:45,945][166323] Avg episode reward: [(0, '1216.807')]
[36m[2025-07-02 05:44:51,010][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 9338880. Throughput: 0: 272.5. Samples: 9348336. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 05:44:51,010][166323] Avg episode reward: [(0, '1168.071')]
[31m[33080112 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[33080113 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[33080113 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:44:55,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9338880. Throughput: 0: 272.1. Samples: 9349936. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 05:44:55,990][166323] Avg episode reward: [(0, '1167.253')]
[36m[2025-07-02 05:45:00,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9338880. Throughput: 0: 273.3. Samples: 9351584. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 05:45:00,968][166323] Avg episode reward: [(0, '1218.121')]
[36m[2025-07-02 05:45:05,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9338880. Throughput: 0: 274.4. Samples: 9352432. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 05:45:05,972][166323] Avg episode reward: [(0, '1221.332')]
[36m[2025-07-02 05:45:10,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 9338880. Throughput: 0: 275.7. Samples: 9354144. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 05:45:10,962][166323] Avg episode reward: [(0, '1193.803')]
[36m[2025-07-02 05:45:15,980][166323] Fps is (10 sec: 1637.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9355264. Throughput: 0: 273.5. Samples: 9355760. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 05:45:15,980][166323] Avg episode reward: [(0, '1244.511')]
[36m[2025-07-02 05:45:20,945][166323] Fps is (10 sec: 1641.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9355264. Throughput: 0: 272.9. Samples: 9356576. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 05:45:20,946][166323] Avg episode reward: [(0, '1284.755')]
[36m[2025-07-02 05:45:25,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9355264. Throughput: 0: 276.0. Samples: 9358336. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 05:45:25,981][166323] Avg episode reward: [(0, '1355.851')]
[36m[2025-07-02 05:45:31,020][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 9355264. Throughput: 0: 276.5. Samples: 9359968. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 05:45:31,020][166323] Avg episode reward: [(0, '1356.243')]
[36m[2025-07-02 05:45:35,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9355264. Throughput: 0: 275.2. Samples: 9360704. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 05:45:35,953][166323] Avg episode reward: [(0, '1372.490')]
[37m[1m[2025-07-02 05:45:36,011][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018264_9355264.pth...
[36m[2025-07-02 05:45:36,015][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018136_9289728.pth
[36m[2025-07-02 05:45:40,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9355264. Throughput: 0: 275.2. Samples: 9362320. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 05:45:40,986][166323] Avg episode reward: [(0, '1298.495')]
[36m[2025-07-02 05:45:45,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9355264. Throughput: 0: 275.9. Samples: 9364000. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 05:45:45,968][166323] Avg episode reward: [(0, '1346.908')]
[36m[2025-07-02 05:45:51,003][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.6). Total num frames: 9355264. Throughput: 0: 273.9. Samples: 9364768. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 05:45:51,003][166323] Avg episode reward: [(0, '1349.550')]
[36m[2025-07-02 05:45:55,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9355264. Throughput: 0: 274.1. Samples: 9366480. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 05:45:55,974][166323] Avg episode reward: [(0, '1346.531')]
[36m[2025-07-02 05:46:00,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9355264. Throughput: 0: 277.0. Samples: 9368224. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 05:46:00,969][166323] Avg episode reward: [(0, '1317.153')]
[36m[2025-07-02 05:46:05,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9355264. Throughput: 0: 276.5. Samples: 9369024. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 05:46:05,973][166323] Avg episode reward: [(0, '1356.357')]
[36m[2025-07-02 05:46:10,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 9355264. Throughput: 0: 273.2. Samples: 9370624. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 05:46:10,955][166323] Avg episode reward: [(0, '1333.774')]
[36m[2025-07-02 05:46:15,951][166323] Fps is (10 sec: 1641.9, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9371648. Throughput: 0: 271.7. Samples: 9372176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:46:15,952][166323] Avg episode reward: [(0, '1392.332')]
[36m[2025-07-02 05:46:21,019][166323] Fps is (10 sec: 1627.9, 60 sec: 272.7, 300 sec: 277.6). Total num frames: 9371648. Throughput: 0: 274.1. Samples: 9373056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:46:21,019][166323] Avg episode reward: [(0, '1341.893')]
[36m[2025-07-02 05:46:25,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9371648. Throughput: 0: 273.2. Samples: 9374608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:46:25,965][166323] Avg episode reward: [(0, '1328.909')]
[36m[2025-07-02 05:46:30,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9371648. Throughput: 0: 274.0. Samples: 9376336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:46:30,996][166323] Avg episode reward: [(0, '1317.929')]
[36m[2025-07-02 05:46:35,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9371648. Throughput: 0: 276.6. Samples: 9377200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:46:35,948][166323] Avg episode reward: [(0, '1335.496')]
[31m[33188626 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[33188627 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[33188627 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:46:40,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9371648. Throughput: 0: 274.6. Samples: 9378832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:46:40,964][166323] Avg episode reward: [(0, '1302.995')]
[36m[2025-07-02 05:46:45,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9371648. Throughput: 0: 271.1. Samples: 9380416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:46:45,948][166323] Avg episode reward: [(0, '1294.661')]
[36m[2025-07-02 05:46:51,004][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9371648. Throughput: 0: 273.2. Samples: 9381328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:46:51,004][166323] Avg episode reward: [(0, '1322.083')]
[36m[2025-07-02 05:46:55,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9371648. Throughput: 0: 273.6. Samples: 9382944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:46:55,986][166323] Avg episode reward: [(0, '1351.744')]
[36m[2025-07-02 05:47:00,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9371648. Throughput: 0: 278.3. Samples: 9384704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:47:00,963][166323] Avg episode reward: [(0, '1302.756')]
[36m[2025-07-02 05:47:06,008][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 9371648. Throughput: 0: 276.3. Samples: 9385488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:47:06,008][166323] Avg episode reward: [(0, '1327.850')]
[36m[2025-07-02 05:47:10,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9371648. Throughput: 0: 277.8. Samples: 9387104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:47:10,952][166323] Avg episode reward: [(0, '1318.140')]
[36m[2025-07-02 05:47:15,971][166323] Fps is (10 sec: 1644.4, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9388032. Throughput: 0: 274.6. Samples: 9388688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:47:15,971][166323] Avg episode reward: [(0, '1342.692')]
[36m[2025-07-02 05:47:20,972][166323] Fps is (10 sec: 1635.2, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9388032. Throughput: 0: 273.3. Samples: 9389504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:47:20,972][166323] Avg episode reward: [(0, '1328.910')]
[36m[2025-07-02 05:47:25,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9388032. Throughput: 0: 274.6. Samples: 9391184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:47:25,947][166323] Avg episode reward: [(0, '1309.312')]
[36m[2025-07-02 05:47:31,000][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9388032. Throughput: 0: 276.3. Samples: 9392864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:47:31,000][166323] Avg episode reward: [(0, '1293.285')]
[31m[33243969 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[33243969 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[33243969 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:47:35,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9388032. Throughput: 0: 274.1. Samples: 9393648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:47:35,954][166323] Avg episode reward: [(0, '1313.321')]
[37m[1m[2025-07-02 05:47:36,036][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018328_9388032.pth...
[36m[2025-07-02 05:47:36,041][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018200_9322496.pth
[36m[2025-07-02 05:47:40,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9388032. Throughput: 0: 276.0. Samples: 9395360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:47:40,981][166323] Avg episode reward: [(0, '1284.936')]
[36m[2025-07-02 05:47:45,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9388032. Throughput: 0: 272.0. Samples: 9396944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:47:45,957][166323] Avg episode reward: [(0, '1239.917')]
[36m[2025-07-02 05:47:50,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9388032. Throughput: 0: 271.2. Samples: 9397680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:47:50,967][166323] Avg episode reward: [(0, '1214.610')]
[36m[2025-07-02 05:47:55,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9388032. Throughput: 0: 274.5. Samples: 9399456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:47:55,957][166323] Avg episode reward: [(0, '1248.604')]
[36m[2025-07-02 05:48:00,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9388032. Throughput: 0: 278.1. Samples: 9401200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:48:00,959][166323] Avg episode reward: [(0, '1250.554')]
[36m[2025-07-02 05:48:05,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9388032. Throughput: 0: 279.5. Samples: 9402080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:48:05,960][166323] Avg episode reward: [(0, '1262.308')]
[36m[2025-07-02 05:48:10,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 9388032. Throughput: 0: 279.8. Samples: 9403776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:48:10,953][166323] Avg episode reward: [(0, '1240.982')]
[36m[2025-07-02 05:48:15,973][166323] Fps is (10 sec: 1636.3, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9404416. Throughput: 0: 278.6. Samples: 9405392. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 05:48:15,973][166323] Avg episode reward: [(0, '1283.317')]
[36m[2025-07-02 05:48:20,985][166323] Fps is (10 sec: 1633.1, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9404416. Throughput: 0: 278.2. Samples: 9406176. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 05:48:20,986][166323] Avg episode reward: [(0, '1301.675')]
[36m[2025-07-02 05:48:25,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9404416. Throughput: 0: 277.9. Samples: 9407872. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 05:48:25,991][166323] Avg episode reward: [(0, '1263.562')]
[31m[33299343 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[33299343 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[33299343 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:48:31,002][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9404416. Throughput: 0: 281.3. Samples: 9409616. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 05:48:31,003][166323] Avg episode reward: [(0, '1295.958')]
[36m[2025-07-02 05:48:35,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9404416. Throughput: 0: 284.5. Samples: 9410480. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 05:48:35,955][166323] Avg episode reward: [(0, '1261.357')]
[36m[2025-07-02 05:48:41,009][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 9404416. Throughput: 0: 280.9. Samples: 9412112. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 05:48:41,009][166323] Avg episode reward: [(0, '1290.234')]
[36m[2025-07-02 05:48:45,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9404416. Throughput: 0: 277.2. Samples: 9413680. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 05:48:45,987][166323] Avg episode reward: [(0, '1282.113')]
[36m[2025-07-02 05:48:50,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9404416. Throughput: 0: 277.3. Samples: 9414560. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 05:48:50,961][166323] Avg episode reward: [(0, '1222.648')]
[36m[2025-07-02 05:48:55,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9404416. Throughput: 0: 276.5. Samples: 9416224. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 05:48:55,976][166323] Avg episode reward: [(0, '1237.371')]
[36m[2025-07-02 05:49:00,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9404416. Throughput: 0: 280.0. Samples: 9417984. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 05:49:00,947][166323] Avg episode reward: [(0, '1246.564')]
[36m[2025-07-02 05:49:05,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9404416. Throughput: 0: 282.1. Samples: 9418864. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 05:49:05,967][166323] Avg episode reward: [(0, '1259.181')]
[36m[2025-07-02 05:49:10,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 9404416. Throughput: 0: 278.7. Samples: 9420416. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 05:49:10,994][166323] Avg episode reward: [(0, '1249.303')]
[36m[2025-07-02 05:49:15,969][166323] Fps is (10 sec: 1637.9, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9420800. Throughput: 0: 273.3. Samples: 9421904. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 05:49:15,970][166323] Avg episode reward: [(0, '1216.126')]
[36m[2025-07-02 05:49:20,962][166323] Fps is (10 sec: 1643.6, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9420800. Throughput: 0: 272.3. Samples: 9422736. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 05:49:20,962][166323] Avg episode reward: [(0, '1230.395')]
[36m[2025-07-02 05:49:25,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9420800. Throughput: 0: 271.0. Samples: 9424288. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 05:49:25,945][166323] Avg episode reward: [(0, '1266.242')]
[36m[2025-07-02 05:49:30,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9420800. Throughput: 0: 270.3. Samples: 9425840. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 05:49:30,967][166323] Avg episode reward: [(0, '1260.402')]
[36m[2025-07-02 05:49:35,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9420800. Throughput: 0: 269.6. Samples: 9426688. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 05:49:35,954][166323] Avg episode reward: [(0, '1286.889')]
[37m[1m[2025-07-02 05:49:36,011][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018392_9420800.pth...
[36m[2025-07-02 05:49:36,015][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018264_9355264.pth
[36m[2025-07-02 05:49:40,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9420800. Throughput: 0: 270.6. Samples: 9428400. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 05:49:40,966][166323] Avg episode reward: [(0, '1303.136')]
[36m[2025-07-02 05:49:45,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9420800. Throughput: 0: 268.2. Samples: 9430064. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 05:49:45,984][166323] Avg episode reward: [(0, '1280.272')]
[36m[2025-07-02 05:49:50,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9420800. Throughput: 0: 266.5. Samples: 9430864. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 05:49:50,991][166323] Avg episode reward: [(0, '1301.008')]
[36m[2025-07-02 05:49:55,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9420800. Throughput: 0: 266.5. Samples: 9432400. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 05:49:55,969][166323] Avg episode reward: [(0, '1348.626')]
[36m[2025-07-02 05:50:00,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9420800. Throughput: 0: 266.8. Samples: 9433904. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 05:50:00,955][166323] Avg episode reward: [(0, '1309.989')]
[36m[2025-07-02 05:50:05,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9420800. Throughput: 0: 265.4. Samples: 9434688. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 05:50:05,990][166323] Avg episode reward: [(0, '1299.415')]
[36m[2025-07-02 05:50:10,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 9420800. Throughput: 0: 267.2. Samples: 9436320. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 05:50:10,970][166323] Avg episode reward: [(0, '1314.906')]
[36m[2025-07-02 05:50:15,973][166323] Fps is (10 sec: 1641.1, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9437184. Throughput: 0: 267.3. Samples: 9437872. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 05:50:15,973][166323] Avg episode reward: [(0, '1220.500')]
[36m[2025-07-02 05:50:20,964][166323] Fps is (10 sec: 1639.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9437184. Throughput: 0: 267.7. Samples: 9438736. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 05:50:20,964][166323] Avg episode reward: [(0, '1217.008')]
[36m[2025-07-02 05:50:25,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.8). Total num frames: 9437184. Throughput: 0: 268.5. Samples: 9440480. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 05:50:25,962][166323] Avg episode reward: [(0, '1250.991')]
[36m[2025-07-02 05:50:30,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9437184. Throughput: 0: 269.6. Samples: 9442192. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 05:50:30,967][166323] Avg episode reward: [(0, '1251.266')]
[36m[2025-07-02 05:50:35,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9437184. Throughput: 0: 269.0. Samples: 9442960. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 05:50:35,959][166323] Avg episode reward: [(0, '1283.408')]
[36m[2025-07-02 05:50:40,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9437184. Throughput: 0: 274.5. Samples: 9444752. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 05:50:40,968][166323] Avg episode reward: [(0, '1293.982')]
[36m[2025-07-02 05:50:45,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9437184. Throughput: 0: 276.2. Samples: 9446336. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 05:50:45,970][166323] Avg episode reward: [(0, '1235.942')]
[36m[2025-07-02 05:50:50,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9437184. Throughput: 0: 277.1. Samples: 9447152. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 05:50:50,963][166323] Avg episode reward: [(0, '1345.507')]
[36m[2025-07-02 05:50:55,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9437184. Throughput: 0: 278.9. Samples: 9448864. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 05:50:55,955][166323] Avg episode reward: [(0, '1372.685')]
[36m[2025-07-02 05:51:00,980][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9437184. Throughput: 0: 281.9. Samples: 9450560. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 05:51:00,981][166323] Avg episode reward: [(0, '1361.243')]
[31m[33450337 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[33450337 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[33450337 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:51:05,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9437184. Throughput: 0: 280.5. Samples: 9451360. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 05:51:05,967][166323] Avg episode reward: [(0, '1314.337')]
[36m[2025-07-02 05:51:10,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 9437184. Throughput: 0: 279.9. Samples: 9453072. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 05:51:10,944][166323] Avg episode reward: [(0, '1272.812')]
[36m[2025-07-02 05:51:15,945][166323] Fps is (10 sec: 1641.8, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 9453568. Throughput: 0: 280.0. Samples: 9454784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:51:15,946][166323] Avg episode reward: [(0, '1281.721')]
[36m[2025-07-02 05:51:20,948][166323] Fps is (10 sec: 1637.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9453568. Throughput: 0: 278.1. Samples: 9455472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:51:20,948][166323] Avg episode reward: [(0, '1323.856')]
[36m[2025-07-02 05:51:25,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9453568. Throughput: 0: 275.9. Samples: 9457168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:51:25,970][166323] Avg episode reward: [(0, '1303.099')]
[36m[2025-07-02 05:51:30,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9453568. Throughput: 0: 277.9. Samples: 9458848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:51:30,988][166323] Avg episode reward: [(0, '1286.335')]
[36m[2025-07-02 05:51:35,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9453568. Throughput: 0: 279.0. Samples: 9459712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:51:35,983][166323] Avg episode reward: [(0, '1265.703')]
[37m[1m[2025-07-02 05:51:36,060][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018456_9453568.pth...
[36m[2025-07-02 05:51:36,064][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018328_9388032.pth
[36m[2025-07-02 05:51:40,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 9453568. Throughput: 0: 276.3. Samples: 9461312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:51:40,999][166323] Avg episode reward: [(0, '1291.049')]
[36m[2025-07-02 05:51:45,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 9453568. Throughput: 0: 274.0. Samples: 9462880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:51:45,943][166323] Avg episode reward: [(0, '1342.750')]
[36m[2025-07-02 05:51:50,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9453568. Throughput: 0: 275.9. Samples: 9463776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:51:50,967][166323] Avg episode reward: [(0, '1358.599')]
[36m[2025-07-02 05:51:55,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9453568. Throughput: 0: 275.4. Samples: 9465472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:51:55,970][166323] Avg episode reward: [(0, '1297.053')]
[36m[2025-07-02 05:52:00,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 9453568. Throughput: 0: 274.5. Samples: 9467136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:52:00,947][166323] Avg episode reward: [(0, '1297.690')]
[36m[2025-07-02 05:52:05,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9453568. Throughput: 0: 278.7. Samples: 9468016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:52:05,949][166323] Avg episode reward: [(0, '1292.351')]
[36m[2025-07-02 05:52:10,964][166323] Fps is (10 sec: 1635.6, 60 sec: 546.0, 300 sec: 277.7). Total num frames: 9469952. Throughput: 0: 280.2. Samples: 9469776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:52:10,964][166323] Avg episode reward: [(0, '1271.636')]
[36m[2025-07-02 05:52:15,952][166323] Fps is (10 sec: 1638.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9469952. Throughput: 0: 280.0. Samples: 9471440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:52:15,952][166323] Avg episode reward: [(0, '1248.775')]
[36m[2025-07-02 05:52:20,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9469952. Throughput: 0: 277.2. Samples: 9472176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:52:20,946][166323] Avg episode reward: [(0, '1193.680')]
[36m[2025-07-02 05:52:26,015][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9469952. Throughput: 0: 277.6. Samples: 9473808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:52:26,015][166323] Avg episode reward: [(0, '1231.574')]
[36m[2025-07-02 05:52:30,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9469952. Throughput: 0: 279.4. Samples: 9475456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:52:30,949][166323] Avg episode reward: [(0, '1243.803')]
[36m[2025-07-02 05:52:35,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9469952. Throughput: 0: 277.6. Samples: 9476272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:52:35,982][166323] Avg episode reward: [(0, '1282.470')]
[36m[2025-07-02 05:52:40,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9469952. Throughput: 0: 274.9. Samples: 9477840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:52:40,960][166323] Avg episode reward: [(0, '1255.774')]
[36m[2025-07-02 05:52:45,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9469952. Throughput: 0: 274.6. Samples: 9479504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:52:45,982][166323] Avg episode reward: [(0, '1291.807')]
[36m[2025-07-02 05:52:50,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9469952. Throughput: 0: 272.9. Samples: 9480304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:52:50,978][166323] Avg episode reward: [(0, '1282.069')]
[36m[2025-07-02 05:52:55,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9469952. Throughput: 0: 269.1. Samples: 9481888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:52:55,974][166323] Avg episode reward: [(0, '1272.098')]
[36m[2025-07-02 05:53:00,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9469952. Throughput: 0: 266.8. Samples: 9483456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:53:00,994][166323] Avg episode reward: [(0, '1256.535')]
[36m[2025-07-02 05:53:05,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9469952. Throughput: 0: 266.8. Samples: 9484192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:53:05,991][166323] Avg episode reward: [(0, '1229.809')]
[36m[2025-07-02 05:53:11,000][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 222.1). Total num frames: 9469952. Throughput: 0: 266.8. Samples: 9485808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:53:11,000][166323] Avg episode reward: [(0, '1233.449')]
[36m[2025-07-02 05:53:15,956][166323] Fps is (10 sec: 1644.1, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9486336. Throughput: 0: 265.2. Samples: 9487392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:53:15,956][166323] Avg episode reward: [(0, '1221.518')]
[36m[2025-07-02 05:53:20,987][166323] Fps is (10 sec: 1640.4, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9486336. Throughput: 0: 266.6. Samples: 9488272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:53:20,987][166323] Avg episode reward: [(0, '1218.820')]
[36m[2025-07-02 05:53:25,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 9486336. Throughput: 0: 269.9. Samples: 9489984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:53:25,950][166323] Avg episode reward: [(0, '1158.769')]
[36m[2025-07-02 05:53:30,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9486336. Throughput: 0: 269.3. Samples: 9491616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:53:30,963][166323] Avg episode reward: [(0, '1210.745')]
[36m[2025-07-02 05:53:35,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9486336. Throughput: 0: 269.9. Samples: 9492448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:53:35,977][166323] Avg episode reward: [(0, '1185.538')]
[37m[1m[2025-07-02 05:53:36,033][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018520_9486336.pth...
[36m[2025-07-02 05:53:36,039][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018392_9420800.pth
[36m[2025-07-02 05:53:40,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9486336. Throughput: 0: 272.2. Samples: 9494144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:53:40,997][166323] Avg episode reward: [(0, '1243.057')]
[36m[2025-07-02 05:53:45,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9486336. Throughput: 0: 275.9. Samples: 9495872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:53:45,989][166323] Avg episode reward: [(0, '1207.135')]
[31m[33618613 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[33618614 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[33618614 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:53:50,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9486336. Throughput: 0: 277.6. Samples: 9496672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:53:50,951][166323] Avg episode reward: [(0, '1173.060')]
[36m[2025-07-02 05:53:55,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9486336. Throughput: 0: 276.1. Samples: 9498224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:53:55,965][166323] Avg episode reward: [(0, '1176.362')]
[36m[2025-07-02 05:54:00,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9486336. Throughput: 0: 279.1. Samples: 9499952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:54:00,951][166323] Avg episode reward: [(0, '1245.703')]
[36m[2025-07-02 05:54:06,007][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9486336. Throughput: 0: 278.6. Samples: 9500816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:54:06,007][166323] Avg episode reward: [(0, '1282.452')]
[36m[2025-07-02 05:54:10,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 9486336. Throughput: 0: 275.4. Samples: 9502384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:54:10,970][166323] Avg episode reward: [(0, '1303.865')]
[36m[2025-07-02 05:54:15,988][166323] Fps is (10 sec: 1641.5, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9502720. Throughput: 0: 274.0. Samples: 9503952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:54:15,988][166323] Avg episode reward: [(0, '1245.035')]
[36m[2025-07-02 05:54:21,019][166323] Fps is (10 sec: 1630.2, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 9502720. Throughput: 0: 273.9. Samples: 9504784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:54:21,020][166323] Avg episode reward: [(0, '1258.995')]
[36m[2025-07-02 05:54:25,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9502720. Throughput: 0: 271.1. Samples: 9506336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:54:25,961][166323] Avg episode reward: [(0, '1285.220')]
[36m[2025-07-02 05:54:30,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9502720. Throughput: 0: 271.4. Samples: 9508080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:54:30,973][166323] Avg episode reward: [(0, '1242.399')]
[31m[33661473 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[33661474 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[33661474 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:54:35,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9502720. Throughput: 0: 270.2. Samples: 9508832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:54:35,946][166323] Avg episode reward: [(0, '1257.096')]
[36m[2025-07-02 05:54:40,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9502720. Throughput: 0: 274.0. Samples: 9510560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:54:40,990][166323] Avg episode reward: [(0, '1191.132')]
[33m[33671799 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[33671799 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.87353515625
[33mCrash Rate: 0.11181640625
[33mTimeout Rate: 0.0146484375 (navigation_task.py:265)
[33m[33671799 ms][navigation_task] - WARNING : 
[33mSuccesses: 1789
[33mCrashes : 229
[33mTimeouts: 30 (navigation_task.py:268)
[36m[2025-07-02 05:54:45,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9502720. Throughput: 0: 273.9. Samples: 9512288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:54:45,992][166323] Avg episode reward: [(0, '1199.314')]
[36m[2025-07-02 05:54:50,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9502720. Throughput: 0: 272.4. Samples: 9513056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:54:50,946][166323] Avg episode reward: [(0, '1285.373')]
[31m[33680588 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[33680588 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[33680588 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:54:55,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9502720. Throughput: 0: 274.5. Samples: 9514736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:54:55,976][166323] Avg episode reward: [(0, '1162.919')]
[36m[2025-07-02 05:55:00,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9502720. Throughput: 0: 275.8. Samples: 9516352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:55:00,956][166323] Avg episode reward: [(0, '1197.376')]
[36m[2025-07-02 05:55:05,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9502720. Throughput: 0: 274.4. Samples: 9517120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:55:05,974][166323] Avg episode reward: [(0, '1146.752')]
[36m[2025-07-02 05:55:10,943][166323] Fps is (10 sec: 1640.5, 60 sec: 546.4, 300 sec: 277.7). Total num frames: 9519104. Throughput: 0: 280.3. Samples: 9518944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:55:10,943][166323] Avg episode reward: [(0, '1145.159')]
[36m[2025-07-02 05:55:15,958][166323] Fps is (10 sec: 1641.1, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9519104. Throughput: 0: 279.9. Samples: 9520672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:55:15,958][166323] Avg episode reward: [(0, '1164.509')]
[36m[2025-07-02 05:55:20,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9519104. Throughput: 0: 283.5. Samples: 9521600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:55:20,988][166323] Avg episode reward: [(0, '1090.515')]
[36m[2025-07-02 05:55:25,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9519104. Throughput: 0: 280.5. Samples: 9523168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:55:25,943][166323] Avg episode reward: [(0, '1167.270')]
[36m[2025-07-02 05:55:30,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9519104. Throughput: 0: 279.3. Samples: 9524848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:55:30,961][166323] Avg episode reward: [(0, '1186.685')]
[36m[2025-07-02 05:55:35,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 9519104. Throughput: 0: 278.8. Samples: 9525616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:55:35,996][166323] Avg episode reward: [(0, '1205.744')]
[37m[1m[2025-07-02 05:55:36,061][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018584_9519104.pth...
[36m[2025-07-02 05:55:36,065][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018456_9453568.pth
[36m[2025-07-02 05:55:40,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9519104. Throughput: 0: 276.3. Samples: 9527168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:55:40,971][166323] Avg episode reward: [(0, '1253.451')]
[36m[2025-07-02 05:55:45,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9519104. Throughput: 0: 281.5. Samples: 9529024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:55:45,972][166323] Avg episode reward: [(0, '1233.735')]
[36m[2025-07-02 05:55:50,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9519104. Throughput: 0: 283.3. Samples: 9529872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:55:50,984][166323] Avg episode reward: [(0, '1246.884')]
[36m[2025-07-02 05:55:55,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9519104. Throughput: 0: 277.0. Samples: 9531424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:55:55,990][166323] Avg episode reward: [(0, '1290.847')]
[36m[2025-07-02 05:56:00,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9519104. Throughput: 0: 274.0. Samples: 9533008. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:56:00,986][166323] Avg episode reward: [(0, '1311.134')]
[36m[2025-07-02 05:56:05,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9519104. Throughput: 0: 271.6. Samples: 9533808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:56:05,944][166323] Avg episode reward: [(0, '1314.849')]
[36m[2025-07-02 05:56:10,945][166323] Fps is (10 sec: 1645.1, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9535488. Throughput: 0: 272.3. Samples: 9535424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:56:10,945][166323] Avg episode reward: [(0, '1353.237')]
[31m[33762859 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[33762859 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[33762860 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:56:15,986][166323] Fps is (10 sec: 1631.6, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9535488. Throughput: 0: 271.5. Samples: 9537072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:56:15,986][166323] Avg episode reward: [(0, '1286.870')]
[31m[33764655 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[33764656 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[33764656 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:56:20,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9535488. Throughput: 0: 274.6. Samples: 9537968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:56:20,971][166323] Avg episode reward: [(0, '1291.583')]
[36m[2025-07-02 05:56:26,026][166323] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 277.7). Total num frames: 9535488. Throughput: 0: 276.3. Samples: 9539616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:56:26,026][166323] Avg episode reward: [(0, '1276.657')]
[36m[2025-07-02 05:56:30,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9535488. Throughput: 0: 273.2. Samples: 9541312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:56:30,944][166323] Avg episode reward: [(0, '1252.928')]
[36m[2025-07-02 05:56:36,005][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9535488. Throughput: 0: 271.9. Samples: 9542112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:56:36,005][166323] Avg episode reward: [(0, '1168.743')]
[36m[2025-07-02 05:56:40,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 9535488. Throughput: 0: 275.9. Samples: 9543840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:56:40,999][166323] Avg episode reward: [(0, '1217.588')]
[36m[2025-07-02 05:56:45,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9535488. Throughput: 0: 275.3. Samples: 9545392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:56:45,970][166323] Avg episode reward: [(0, '1137.592')]
[36m[2025-07-02 05:56:50,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9535488. Throughput: 0: 274.5. Samples: 9546160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:56:50,944][166323] Avg episode reward: [(0, '1230.649')]
[36m[2025-07-02 05:56:55,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9535488. Throughput: 0: 276.5. Samples: 9547872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:56:55,972][166323] Avg episode reward: [(0, '1207.642')]
[36m[2025-07-02 05:57:00,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9535488. Throughput: 0: 274.8. Samples: 9549440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:57:00,985][166323] Avg episode reward: [(0, '1244.527')]
[36m[2025-07-02 05:57:05,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 9535488. Throughput: 0: 273.4. Samples: 9550272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:57:05,968][166323] Avg episode reward: [(0, '1197.750')]
[36m[2025-07-02 05:57:10,987][166323] Fps is (10 sec: 1638.1, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9551872. Throughput: 0: 273.3. Samples: 9551904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:57:10,987][166323] Avg episode reward: [(0, '1245.622')]
[36m[2025-07-02 05:57:15,946][166323] Fps is (10 sec: 1642.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9551872. Throughput: 0: 270.9. Samples: 9553504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:57:15,946][166323] Avg episode reward: [(0, '1231.569')]
[36m[2025-07-02 05:57:20,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9551872. Throughput: 0: 271.8. Samples: 9554336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:57:20,977][166323] Avg episode reward: [(0, '1283.401')]
[36m[2025-07-02 05:57:26,003][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.6). Total num frames: 9551872. Throughput: 0: 270.6. Samples: 9556016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:57:26,003][166323] Avg episode reward: [(0, '1253.939')]
[36m[2025-07-02 05:57:30,977][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9551872. Throughput: 0: 275.5. Samples: 9557792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:57:30,978][166323] Avg episode reward: [(0, '1271.468')]
[36m[2025-07-02 05:57:35,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9551872. Throughput: 0: 276.4. Samples: 9558608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:57:35,986][166323] Avg episode reward: [(0, '1254.877')]
[37m[1m[2025-07-02 05:57:36,051][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018648_9551872.pth...
[36m[2025-07-02 05:57:36,057][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018520_9486336.pth
[36m[2025-07-02 05:57:41,008][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9551872. Throughput: 0: 275.3. Samples: 9560272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:57:41,008][166323] Avg episode reward: [(0, '1276.470')]
[36m[2025-07-02 05:57:45,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9551872. Throughput: 0: 274.3. Samples: 9561776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:57:45,965][166323] Avg episode reward: [(0, '1229.269')]
[36m[2025-07-02 05:57:50,974][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9551872. Throughput: 0: 272.3. Samples: 9562528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:57:50,975][166323] Avg episode reward: [(0, '1235.165')]
[36m[2025-07-02 05:57:56,000][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9551872. Throughput: 0: 272.3. Samples: 9564160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:57:56,000][166323] Avg episode reward: [(0, '1202.871')]
[36m[2025-07-02 05:58:00,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9551872. Throughput: 0: 274.7. Samples: 9565872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:58:00,972][166323] Avg episode reward: [(0, '1179.557')]
[36m[2025-07-02 05:58:05,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9551872. Throughput: 0: 275.2. Samples: 9566720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:58:05,982][166323] Avg episode reward: [(0, '1162.417')]
[36m[2025-07-02 05:58:11,017][166323] Fps is (10 sec: 1631.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 9568256. Throughput: 0: 273.7. Samples: 9568336. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 05:58:11,017][166323] Avg episode reward: [(0, '1179.676')]
[36m[2025-07-02 05:58:15,968][166323] Fps is (10 sec: 1640.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9568256. Throughput: 0: 271.7. Samples: 9570016. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 05:58:15,969][166323] Avg episode reward: [(0, '1194.038')]
[36m[2025-07-02 05:58:20,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9568256. Throughput: 0: 272.2. Samples: 9570848. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 05:58:20,945][166323] Avg episode reward: [(0, '1274.269')]
[31m[33891795 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[33891796 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[33891796 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:58:25,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9568256. Throughput: 0: 271.3. Samples: 9572464. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 05:58:25,954][166323] Avg episode reward: [(0, '1243.823')]
[36m[2025-07-02 05:58:30,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9568256. Throughput: 0: 277.0. Samples: 9574240. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 05:58:30,965][166323] Avg episode reward: [(0, '1301.603')]
[36m[2025-07-02 05:58:36,014][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9568256. Throughput: 0: 276.7. Samples: 9574992. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 05:58:36,015][166323] Avg episode reward: [(0, '1356.268')]
[36m[2025-07-02 05:58:40,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9568256. Throughput: 0: 274.2. Samples: 9576496. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 05:58:40,993][166323] Avg episode reward: [(0, '1345.428')]
[36m[2025-07-02 05:58:45,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9568256. Throughput: 0: 273.3. Samples: 9578176. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 05:58:45,989][166323] Avg episode reward: [(0, '1354.887')]
[36m[2025-07-02 05:58:50,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9568256. Throughput: 0: 272.5. Samples: 9578976. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 05:58:50,963][166323] Avg episode reward: [(0, '1364.746')]
[36m[2025-07-02 05:58:55,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.6). Total num frames: 9568256. Throughput: 0: 275.0. Samples: 9580704. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 05:58:55,999][166323] Avg episode reward: [(0, '1359.696')]
[36m[2025-07-02 05:59:00,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9568256. Throughput: 0: 273.7. Samples: 9582336. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 05:59:00,985][166323] Avg episode reward: [(0, '1354.970')]
[31m[33934167 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[33934167 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[33934167 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 05:59:05,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9568256. Throughput: 0: 273.3. Samples: 9583152. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 05:59:05,961][166323] Avg episode reward: [(0, '1330.647')]
[36m[2025-07-02 05:59:10,998][166323] Fps is (10 sec: 1636.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9584640. Throughput: 0: 271.4. Samples: 9584688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:59:10,999][166323] Avg episode reward: [(0, '1348.134')]
[36m[2025-07-02 05:59:15,982][166323] Fps is (10 sec: 1634.9, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9584640. Throughput: 0: 268.3. Samples: 9586320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:59:15,982][166323] Avg episode reward: [(0, '1363.001')]
[36m[2025-07-02 05:59:20,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9584640. Throughput: 0: 273.7. Samples: 9587296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:59:20,964][166323] Avg episode reward: [(0, '1367.781')]
[36m[2025-07-02 05:59:25,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9584640. Throughput: 0: 278.0. Samples: 9588992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:59:25,943][166323] Avg episode reward: [(0, '1270.628')]
[36m[2025-07-02 05:59:30,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 9584640. Throughput: 0: 280.1. Samples: 9590784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:59:30,999][166323] Avg episode reward: [(0, '1308.497')]
[36m[2025-07-02 05:59:35,995][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9584640. Throughput: 0: 279.3. Samples: 9591552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:59:35,996][166323] Avg episode reward: [(0, '1322.358')]
[37m[1m[2025-07-02 05:59:36,067][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018712_9584640.pth...
[36m[2025-07-02 05:59:36,074][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018584_9519104.pth
[36m[2025-07-02 05:59:40,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9584640. Throughput: 0: 278.9. Samples: 9593248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:59:40,976][166323] Avg episode reward: [(0, '1276.049')]
[36m[2025-07-02 05:59:45,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9584640. Throughput: 0: 281.0. Samples: 9594976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:59:45,962][166323] Avg episode reward: [(0, '1275.002')]
[36m[2025-07-02 05:59:50,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9584640. Throughput: 0: 282.5. Samples: 9595872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:59:50,982][166323] Avg episode reward: [(0, '1256.201')]
[36m[2025-07-02 05:59:55,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9584640. Throughput: 0: 287.2. Samples: 9597600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 05:59:55,953][166323] Avg episode reward: [(0, '1288.016')]
[36m[2025-07-02 06:00:00,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9584640. Throughput: 0: 287.6. Samples: 9599264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:00:00,992][166323] Avg episode reward: [(0, '1264.077')]
[36m[2025-07-02 06:00:05,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 9584640. Throughput: 0: 284.8. Samples: 9600112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:00:05,972][166323] Avg episode reward: [(0, '1295.923')]
[36m[2025-07-02 06:00:10,961][166323] Fps is (10 sec: 1643.5, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9601024. Throughput: 0: 285.4. Samples: 9601840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:00:10,961][166323] Avg episode reward: [(0, '1332.365')]
[36m[2025-07-02 06:00:15,954][166323] Fps is (10 sec: 1641.4, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9601024. Throughput: 0: 281.2. Samples: 9603424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:00:15,954][166323] Avg episode reward: [(0, '1349.689')]
[36m[2025-07-02 06:00:20,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9601024. Throughput: 0: 282.2. Samples: 9604240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:00:20,959][166323] Avg episode reward: [(0, '1341.814')]
[36m[2025-07-02 06:00:25,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9601024. Throughput: 0: 281.7. Samples: 9605920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:00:25,961][166323] Avg episode reward: [(0, '1377.736')]
[36m[2025-07-02 06:00:31,016][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9601024. Throughput: 0: 277.4. Samples: 9607472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:00:31,017][166323] Avg episode reward: [(0, '1360.709')]
[36m[2025-07-02 06:00:35,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9601024. Throughput: 0: 276.6. Samples: 9608320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:00:35,980][166323] Avg episode reward: [(0, '1415.427')]
[36m[2025-07-02 06:00:41,009][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9601024. Throughput: 0: 272.0. Samples: 9609856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:00:41,010][166323] Avg episode reward: [(0, '1344.383')]
[36m[2025-07-02 06:00:45,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9601024. Throughput: 0: 270.1. Samples: 9611408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:00:45,960][166323] Avg episode reward: [(0, '1317.355')]
[36m[2025-07-02 06:00:50,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9601024. Throughput: 0: 268.8. Samples: 9612208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:00:50,968][166323] Avg episode reward: [(0, '1378.521')]
[36m[2025-07-02 06:00:55,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9601024. Throughput: 0: 264.8. Samples: 9613760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:00:55,968][166323] Avg episode reward: [(0, '1373.792')]
[36m[2025-07-02 06:01:00,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9601024. Throughput: 0: 267.9. Samples: 9615488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:01:00,984][166323] Avg episode reward: [(0, '1323.582')]
[36m[2025-07-02 06:01:05,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.1). Total num frames: 9601024. Throughput: 0: 269.4. Samples: 9616368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:01:05,970][166323] Avg episode reward: [(0, '1363.613')]
[31m[34054937 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[34054938 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[34054938 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:01:10,981][166323] Fps is (10 sec: 1638.7, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9617408. Throughput: 0: 268.7. Samples: 9618016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:01:10,981][166323] Avg episode reward: [(0, '1327.984')]
[36m[2025-07-02 06:01:15,988][166323] Fps is (10 sec: 1635.4, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9617408. Throughput: 0: 273.9. Samples: 9619792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:01:15,998][166323] Avg episode reward: [(0, '1329.041')]
[36m[2025-07-02 06:01:21,000][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9617408. Throughput: 0: 274.0. Samples: 9620656. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:01:21,001][166323] Avg episode reward: [(0, '1355.032')]
[31m[34073954 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[34073955 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[34073955 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:01:25,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9617408. Throughput: 0: 276.5. Samples: 9622288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:01:25,977][166323] Avg episode reward: [(0, '1263.411')]
[36m[2025-07-02 06:01:30,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9617408. Throughput: 0: 280.5. Samples: 9624032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:01:30,964][166323] Avg episode reward: [(0, '1289.769')]
[36m[2025-07-02 06:01:35,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9617408. Throughput: 0: 281.0. Samples: 9624848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:01:35,955][166323] Avg episode reward: [(0, '1275.317')]
[37m[1m[2025-07-02 06:01:36,047][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018776_9617408.pth...
[36m[2025-07-02 06:01:36,055][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018648_9551872.pth
[36m[2025-07-02 06:01:40,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9617408. Throughput: 0: 282.3. Samples: 9626464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:01:40,977][166323] Avg episode reward: [(0, '1258.165')]
[36m[2025-07-02 06:01:45,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 9617408. Throughput: 0: 280.8. Samples: 9628128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:01:45,995][166323] Avg episode reward: [(0, '1229.569')]
[36m[2025-07-02 06:01:50,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9617408. Throughput: 0: 277.7. Samples: 9628864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:01:50,972][166323] Avg episode reward: [(0, '1189.847')]
[36m[2025-07-02 06:01:55,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9617408. Throughput: 0: 283.6. Samples: 9630768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:01:55,943][166323] Avg episode reward: [(0, '1194.269')]
[36m[2025-07-02 06:02:00,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9617408. Throughput: 0: 280.6. Samples: 9632416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:02:00,976][166323] Avg episode reward: [(0, '1199.509')]
[36m[2025-07-02 06:02:05,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 9617408. Throughput: 0: 277.2. Samples: 9633120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:02:05,957][166323] Avg episode reward: [(0, '1217.235')]
[31m[34115485 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[34115485 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[34115485 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:02:10,977][166323] Fps is (10 sec: 1638.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9633792. Throughput: 0: 274.5. Samples: 9634640. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 06:02:10,978][166323] Avg episode reward: [(0, '1216.166')]
[36m[2025-07-02 06:02:15,956][166323] Fps is (10 sec: 1638.6, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9633792. Throughput: 0: 273.8. Samples: 9636352. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 06:02:15,956][166323] Avg episode reward: [(0, '1183.889')]
[36m[2025-07-02 06:02:20,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9633792. Throughput: 0: 275.4. Samples: 9637248. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 06:02:20,986][166323] Avg episode reward: [(0, '1231.773')]
[36m[2025-07-02 06:02:25,995][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9633792. Throughput: 0: 279.7. Samples: 9639056. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 06:02:25,995][166323] Avg episode reward: [(0, '1218.534')]
[36m[2025-07-02 06:02:30,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9633792. Throughput: 0: 278.3. Samples: 9640640. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 06:02:30,948][166323] Avg episode reward: [(0, '1254.443')]
[36m[2025-07-02 06:02:36,010][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 9633792. Throughput: 0: 281.0. Samples: 9641520. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 06:02:36,011][166323] Avg episode reward: [(0, '1226.152')]
[36m[2025-07-02 06:02:40,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9633792. Throughput: 0: 277.3. Samples: 9643248. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 06:02:40,947][166323] Avg episode reward: [(0, '1278.427')]
[36m[2025-07-02 06:02:45,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9633792. Throughput: 0: 276.8. Samples: 9644864. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 06:02:45,946][166323] Avg episode reward: [(0, '1312.750')]
[36m[2025-07-02 06:02:50,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9633792. Throughput: 0: 279.1. Samples: 9645680. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 06:02:50,953][166323] Avg episode reward: [(0, '1321.098')]
[36m[2025-07-02 06:02:55,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9633792. Throughput: 0: 278.1. Samples: 9647152. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 06:02:55,963][166323] Avg episode reward: [(0, '1338.198')]
[36m[2025-07-02 06:03:00,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9633792. Throughput: 0: 277.2. Samples: 9648832. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 06:03:00,973][166323] Avg episode reward: [(0, '1321.599')]
[36m[2025-07-02 06:03:05,983][166323] Fps is (10 sec: 1635.2, 60 sec: 545.9, 300 sec: 277.7). Total num frames: 9650176. Throughput: 0: 275.6. Samples: 9649648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:03:05,983][166323] Avg episode reward: [(0, '1351.701')]
[36m[2025-07-02 06:03:11,029][166323] Fps is (10 sec: 1629.2, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 9650176. Throughput: 0: 270.7. Samples: 9651248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:03:11,030][166323] Avg episode reward: [(0, '1353.452')]
[36m[2025-07-02 06:03:15,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9650176. Throughput: 0: 270.8. Samples: 9652832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:03:15,967][166323] Avg episode reward: [(0, '1351.905')]
[36m[2025-07-02 06:03:20,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9650176. Throughput: 0: 269.5. Samples: 9653632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:03:20,953][166323] Avg episode reward: [(0, '1284.604')]
[36m[2025-07-02 06:03:25,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9650176. Throughput: 0: 266.5. Samples: 9655248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:03:25,971][166323] Avg episode reward: [(0, '1269.032')]
[31m[34197515 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[34197516 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[34197516 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:03:30,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9650176. Throughput: 0: 264.3. Samples: 9656768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:03:30,992][166323] Avg episode reward: [(0, '1291.232')]
[36m[2025-07-02 06:03:35,995][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9650176. Throughput: 0: 265.7. Samples: 9657648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:03:35,996][166323] Avg episode reward: [(0, '1316.741')]
[37m[1m[2025-07-02 06:03:36,061][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018840_9650176.pth...
[36m[2025-07-02 06:03:36,068][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018712_9584640.pth
[36m[2025-07-02 06:03:40,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9650176. Throughput: 0: 268.9. Samples: 9659248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:03:40,952][166323] Avg episode reward: [(0, '1281.256')]
[36m[2025-07-02 06:03:45,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9650176. Throughput: 0: 267.1. Samples: 9660848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:03:45,959][166323] Avg episode reward: [(0, '1238.150')]
[36m[2025-07-02 06:03:50,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9650176. Throughput: 0: 267.0. Samples: 9661664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:03:50,990][166323] Avg episode reward: [(0, '1260.079')]
[36m[2025-07-02 06:03:55,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9650176. Throughput: 0: 266.8. Samples: 9663232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:03:55,951][166323] Avg episode reward: [(0, '1290.659')]
[36m[2025-07-02 06:04:00,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9650176. Throughput: 0: 269.9. Samples: 9664976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:04:00,969][166323] Avg episode reward: [(0, '1334.745')]
[36m[2025-07-02 06:04:05,957][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 222.2). Total num frames: 9650176. Throughput: 0: 268.8. Samples: 9665728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:04:05,958][166323] Avg episode reward: [(0, '1329.679')]
[36m[2025-07-02 06:04:11,021][166323] Fps is (10 sec: 1629.9, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9666560. Throughput: 0: 268.9. Samples: 9667360. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 06:04:11,021][166323] Avg episode reward: [(0, '1325.542')]
[36m[2025-07-02 06:04:15,999][166323] Fps is (10 sec: 1631.5, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9666560. Throughput: 0: 271.6. Samples: 9668992. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 06:04:16,000][166323] Avg episode reward: [(0, '1290.056')]
[36m[2025-07-02 06:04:20,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9666560. Throughput: 0: 270.9. Samples: 9669824. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 06:04:20,946][166323] Avg episode reward: [(0, '1317.608')]
[36m[2025-07-02 06:04:25,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9666560. Throughput: 0: 273.2. Samples: 9671552. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 06:04:25,993][166323] Avg episode reward: [(0, '1234.222')]
[36m[2025-07-02 06:04:30,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9666560. Throughput: 0: 274.7. Samples: 9673216. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 06:04:30,983][166323] Avg episode reward: [(0, '1274.889')]
[36m[2025-07-02 06:04:35,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9666560. Throughput: 0: 277.0. Samples: 9674128. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 06:04:35,979][166323] Avg episode reward: [(0, '1267.147')]
[36m[2025-07-02 06:04:40,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9666560. Throughput: 0: 279.2. Samples: 9675808. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 06:04:40,993][166323] Avg episode reward: [(0, '1277.778')]
[36m[2025-07-02 06:04:45,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9666560. Throughput: 0: 279.3. Samples: 9677536. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 06:04:45,945][166323] Avg episode reward: [(0, '1240.313')]
[36m[2025-07-02 06:04:50,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9666560. Throughput: 0: 281.9. Samples: 9678416. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 06:04:50,962][166323] Avg episode reward: [(0, '1252.652')]
[36m[2025-07-02 06:04:55,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9666560. Throughput: 0: 281.8. Samples: 9680032. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 06:04:55,984][166323] Avg episode reward: [(0, '1297.199')]
[36m[2025-07-02 06:05:00,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9666560. Throughput: 0: 282.6. Samples: 9681696. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 06:05:00,958][166323] Avg episode reward: [(0, '1249.978')]
[33m[2025-07-02 06:05:05,263][166323] KL-divergence is very high: 100.4611
[36m[2025-07-02 06:05:05,961][166323] Fps is (10 sec: 1642.3, 60 sec: 546.1, 300 sec: 277.7). Total num frames: 9682944. Throughput: 0: 281.9. Samples: 9682512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:05:05,961][166323] Avg episode reward: [(0, '1192.415')]
[36m[2025-07-02 06:05:10,990][166323] Fps is (10 sec: 1633.2, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9682944. Throughput: 0: 279.1. Samples: 9684112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:05:10,990][166323] Avg episode reward: [(0, '1177.824')]
[36m[2025-07-02 06:05:15,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9682944. Throughput: 0: 281.4. Samples: 9685872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:05:15,965][166323] Avg episode reward: [(0, '1182.017')]
[31m[34309348 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[34309349 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[34309349 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:05:20,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 9682944. Throughput: 0: 279.4. Samples: 9686704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:05:20,995][166323] Avg episode reward: [(0, '1109.236')]
[36m[2025-07-02 06:05:26,007][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9682944. Throughput: 0: 279.0. Samples: 9688368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:05:26,007][166323] Avg episode reward: [(0, '1087.597')]
[36m[2025-07-02 06:05:30,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9682944. Throughput: 0: 276.6. Samples: 9689984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:05:30,957][166323] Avg episode reward: [(0, '1089.548')]
[36m[2025-07-02 06:05:36,004][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9682944. Throughput: 0: 275.7. Samples: 9690832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:05:36,004][166323] Avg episode reward: [(0, '1217.422')]
[31m[34324585 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[34324586 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[34324586 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[37m[1m[2025-07-02 06:05:36,081][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018904_9682944.pth...
[36m[2025-07-02 06:05:36,085][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018776_9617408.pth
[36m[2025-07-02 06:05:40,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9682944. Throughput: 0: 275.4. Samples: 9692416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:05:40,951][166323] Avg episode reward: [(0, '1151.916')]
[36m[2025-07-02 06:05:45,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9682944. Throughput: 0: 271.8. Samples: 9693936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:05:45,989][166323] Avg episode reward: [(0, '1147.534')]
[36m[2025-07-02 06:05:50,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9682944. Throughput: 0: 271.2. Samples: 9694720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:05:50,969][166323] Avg episode reward: [(0, '1176.013')]
[36m[2025-07-02 06:05:55,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9682944. Throughput: 0: 271.7. Samples: 9696336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:05:55,982][166323] Avg episode reward: [(0, '1274.042')]
[36m[2025-07-02 06:06:00,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9682944. Throughput: 0: 266.8. Samples: 9697872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:06:00,951][166323] Avg episode reward: [(0, '1276.354')]
[36m[2025-07-02 06:06:05,995][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 222.1). Total num frames: 9682944. Throughput: 0: 265.2. Samples: 9698640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:06:05,996][166323] Avg episode reward: [(0, '1278.340')]
[36m[2025-07-02 06:06:10,966][166323] Fps is (10 sec: 1635.9, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9699328. Throughput: 0: 264.4. Samples: 9700256. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 06:06:10,966][166323] Avg episode reward: [(0, '1289.596')]
[36m[2025-07-02 06:06:16,003][166323] Fps is (10 sec: 1637.2, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9699328. Throughput: 0: 264.3. Samples: 9701888. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 06:06:16,003][166323] Avg episode reward: [(0, '1272.998')]
[31m[34365273 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[34365273 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[34365274 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[33m[34365274 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[34365275 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.8603515625
[33mCrash Rate: 0.12548828125
[33mTimeout Rate: 0.01416015625 (navigation_task.py:265)
[33m[34365275 ms][navigation_task] - WARNING : 
[33mSuccesses: 1762
[33mCrashes : 257
[33mTimeouts: 29 (navigation_task.py:268)
[36m[2025-07-02 06:06:20,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9699328. Throughput: 0: 265.6. Samples: 9702768. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 06:06:20,946][166323] Avg episode reward: [(0, '1238.003')]
[36m[2025-07-02 06:06:25,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9699328. Throughput: 0: 263.9. Samples: 9704304. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 06:06:25,999][166323] Avg episode reward: [(0, '1237.160')]
[36m[2025-07-02 06:06:30,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9699328. Throughput: 0: 262.5. Samples: 9705744. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 06:06:30,965][166323] Avg episode reward: [(0, '1213.939')]
[36m[2025-07-02 06:06:35,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9699328. Throughput: 0: 263.8. Samples: 9706592. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 06:06:35,977][166323] Avg episode reward: [(0, '1189.187')]
[36m[2025-07-02 06:06:40,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9699328. Throughput: 0: 267.3. Samples: 9708368. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 06:06:40,996][166323] Avg episode reward: [(0, '1177.420')]
[36m[2025-07-02 06:06:45,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9699328. Throughput: 0: 270.7. Samples: 9710064. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 06:06:45,987][166323] Avg episode reward: [(0, '1201.388')]
[36m[2025-07-02 06:06:50,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9699328. Throughput: 0: 272.1. Samples: 9710880. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 06:06:50,981][166323] Avg episode reward: [(0, '1234.831')]
[36m[2025-07-02 06:06:55,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9699328. Throughput: 0: 272.3. Samples: 9712512. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 06:06:55,979][166323] Avg episode reward: [(0, '1265.688')]
[36m[2025-07-02 06:07:01,024][166323] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 277.6). Total num frames: 9699328. Throughput: 0: 269.4. Samples: 9714016. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 06:07:01,025][166323] Avg episode reward: [(0, '1265.555')]
[36m[2025-07-02 06:07:05,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 9699328. Throughput: 0: 270.1. Samples: 9714928. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 06:07:05,964][166323] Avg episode reward: [(0, '1264.568')]
[36m[2025-07-02 06:07:10,959][166323] Fps is (10 sec: 1649.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9715712. Throughput: 0: 276.2. Samples: 9716720. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 06:07:10,959][166323] Avg episode reward: [(0, '1257.296')]
[36m[2025-07-02 06:07:15,965][166323] Fps is (10 sec: 1638.2, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9715712. Throughput: 0: 280.5. Samples: 9718368. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 06:07:15,966][166323] Avg episode reward: [(0, '1246.289')]
[36m[2025-07-02 06:07:20,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9715712. Throughput: 0: 278.7. Samples: 9719136. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 06:07:20,993][166323] Avg episode reward: [(0, '1252.678')]
[36m[2025-07-02 06:07:26,008][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 9715712. Throughput: 0: 276.2. Samples: 9720800. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 06:07:26,009][166323] Avg episode reward: [(0, '1251.370')]
[36m[2025-07-02 06:07:30,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9715712. Throughput: 0: 274.5. Samples: 9722416. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 06:07:30,992][166323] Avg episode reward: [(0, '1255.844')]
[36m[2025-07-02 06:07:35,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9715712. Throughput: 0: 273.5. Samples: 9723184. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 06:07:35,967][166323] Avg episode reward: [(0, '1254.136')]
[37m[1m[2025-07-02 06:07:36,025][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018968_9715712.pth...
[36m[2025-07-02 06:07:36,030][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018840_9650176.pth
[36m[2025-07-02 06:07:40,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9715712. Throughput: 0: 277.4. Samples: 9724992. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 06:07:40,961][166323] Avg episode reward: [(0, '1287.078')]
[36m[2025-07-02 06:07:45,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9715712. Throughput: 0: 280.2. Samples: 9726608. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 06:07:45,961][166323] Avg episode reward: [(0, '1288.074')]
[36m[2025-07-02 06:07:50,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9715712. Throughput: 0: 279.1. Samples: 9727488. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 06:07:50,959][166323] Avg episode reward: [(0, '1242.049')]
[36m[2025-07-02 06:07:55,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9715712. Throughput: 0: 275.5. Samples: 9729120. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 06:07:55,974][166323] Avg episode reward: [(0, '1270.537')]
[31m[34469314 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[34469315 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[34469315 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:08:00,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.1). Total num frames: 9715712. Throughput: 0: 274.3. Samples: 9730720. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 06:08:00,994][166323] Avg episode reward: [(0, '1254.846')]
[36m[2025-07-02 06:08:05,987][166323] Fps is (10 sec: 1636.2, 60 sec: 545.9, 300 sec: 277.7). Total num frames: 9732096. Throughput: 0: 275.2. Samples: 9731520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:08:05,988][166323] Avg episode reward: [(0, '1240.845')]
[36m[2025-07-02 06:08:10,960][166323] Fps is (10 sec: 1644.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9732096. Throughput: 0: 273.7. Samples: 9733104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:08:10,960][166323] Avg episode reward: [(0, '1280.654')]
[36m[2025-07-02 06:08:15,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9732096. Throughput: 0: 275.6. Samples: 9734816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:08:15,988][166323] Avg episode reward: [(0, '1234.481')]
[36m[2025-07-02 06:08:20,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9732096. Throughput: 0: 278.5. Samples: 9735712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:08:20,950][166323] Avg episode reward: [(0, '1248.432')]
[36m[2025-07-02 06:08:25,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9732096. Throughput: 0: 276.6. Samples: 9737440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:08:25,972][166323] Avg episode reward: [(0, '1293.137')]
[36m[2025-07-02 06:08:30,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9732096. Throughput: 0: 277.9. Samples: 9739120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:08:30,983][166323] Avg episode reward: [(0, '1335.451')]
[36m[2025-07-02 06:08:35,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9732096. Throughput: 0: 278.0. Samples: 9740000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:08:35,965][166323] Avg episode reward: [(0, '1367.996')]
[36m[2025-07-02 06:08:40,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9732096. Throughput: 0: 278.5. Samples: 9741648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:08:40,962][166323] Avg episode reward: [(0, '1360.476')]
[36m[2025-07-02 06:08:45,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9732096. Throughput: 0: 279.2. Samples: 9743280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:08:45,981][166323] Avg episode reward: [(0, '1335.849')]
[36m[2025-07-02 06:08:50,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9732096. Throughput: 0: 277.8. Samples: 9744016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:08:50,975][166323] Avg episode reward: [(0, '1347.722')]
[36m[2025-07-02 06:08:55,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9732096. Throughput: 0: 283.1. Samples: 9745840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:08:55,946][166323] Avg episode reward: [(0, '1324.556')]
[36m[2025-07-02 06:09:00,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9732096. Throughput: 0: 282.8. Samples: 9747536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:09:00,963][166323] Avg episode reward: [(0, '1271.524')]
[36m[2025-07-02 06:09:06,008][166323] Fps is (10 sec: 1628.4, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9748480. Throughput: 0: 282.7. Samples: 9748448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:09:06,008][166323] Avg episode reward: [(0, '1274.541')]
[36m[2025-07-02 06:09:10,964][166323] Fps is (10 sec: 1638.3, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9748480. Throughput: 0: 280.2. Samples: 9750048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:09:10,964][166323] Avg episode reward: [(0, '1251.309')]
[36m[2025-07-02 06:09:15,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9748480. Throughput: 0: 277.0. Samples: 9751584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:09:15,981][166323] Avg episode reward: [(0, '1191.091')]
[36m[2025-07-02 06:09:20,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9748480. Throughput: 0: 276.5. Samples: 9752448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:09:20,986][166323] Avg episode reward: [(0, '1196.453')]
[36m[2025-07-02 06:09:25,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9748480. Throughput: 0: 276.2. Samples: 9754080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:09:25,970][166323] Avg episode reward: [(0, '1208.908')]
[36m[2025-07-02 06:09:31,014][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9748480. Throughput: 0: 276.4. Samples: 9755728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:09:31,014][166323] Avg episode reward: [(0, '1160.094')]
[31m[34564473 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[34564474 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[34564474 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:09:35,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9748480. Throughput: 0: 277.8. Samples: 9756512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:09:35,962][166323] Avg episode reward: [(0, '1122.935')]
[37m[1m[2025-07-02 06:09:36,012][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019032_9748480.pth...
[36m[2025-07-02 06:09:36,016][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018904_9682944.pth
[36m[2025-07-02 06:09:40,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9748480. Throughput: 0: 278.9. Samples: 9758400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:09:40,988][166323] Avg episode reward: [(0, '1118.338')]
[36m[2025-07-02 06:09:45,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9748480. Throughput: 0: 276.3. Samples: 9759968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:09:45,951][166323] Avg episode reward: [(0, '1133.597')]
[36m[2025-07-02 06:09:50,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9748480. Throughput: 0: 273.9. Samples: 9760768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:09:50,986][166323] Avg episode reward: [(0, '1089.860')]
[36m[2025-07-02 06:09:55,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9748480. Throughput: 0: 275.2. Samples: 9762432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:09:55,959][166323] Avg episode reward: [(0, '1093.287')]
[36m[2025-07-02 06:10:00,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 9748480. Throughput: 0: 277.2. Samples: 9764064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:10:00,996][166323] Avg episode reward: [(0, '1152.823')]
[36m[2025-07-02 06:10:05,958][166323] Fps is (10 sec: 1638.5, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9764864. Throughput: 0: 276.1. Samples: 9764864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:10:05,959][166323] Avg episode reward: [(0, '1112.904')]
[36m[2025-07-02 06:10:10,971][166323] Fps is (10 sec: 1642.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9764864. Throughput: 0: 275.9. Samples: 9766496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:10:10,971][166323] Avg episode reward: [(0, '1131.768')]
[31m[34601244 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[34601244 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[34601245 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:10:15,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9764864. Throughput: 0: 276.9. Samples: 9768176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:10:15,970][166323] Avg episode reward: [(0, '1152.238')]
[31m[34609051 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[34609052 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[34609052 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:10:20,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9764864. Throughput: 0: 276.9. Samples: 9768976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:10:20,973][166323] Avg episode reward: [(0, '1190.691')]
[36m[2025-07-02 06:10:25,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9764864. Throughput: 0: 274.6. Samples: 9770752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:10:25,977][166323] Avg episode reward: [(0, '1236.061')]
[36m[2025-07-02 06:10:30,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9764864. Throughput: 0: 276.8. Samples: 9772432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:10:30,983][166323] Avg episode reward: [(0, '1245.017')]
[31m[34624384 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[34624384 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[34624384 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:10:35,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9764864. Throughput: 0: 277.3. Samples: 9773248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:10:35,992][166323] Avg episode reward: [(0, '1253.781')]
[36m[2025-07-02 06:10:40,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9764864. Throughput: 0: 274.1. Samples: 9774768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:10:40,966][166323] Avg episode reward: [(0, '1281.437')]
[36m[2025-07-02 06:10:45,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9764864. Throughput: 0: 275.8. Samples: 9776464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:10:45,953][166323] Avg episode reward: [(0, '1248.496')]
[36m[2025-07-02 06:10:50,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9764864. Throughput: 0: 274.9. Samples: 9777232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:10:50,947][166323] Avg episode reward: [(0, '1242.552')]
[36m[2025-07-02 06:10:55,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9764864. Throughput: 0: 277.7. Samples: 9778992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:10:55,972][166323] Avg episode reward: [(0, '1246.825')]
[36m[2025-07-02 06:11:00,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9764864. Throughput: 0: 275.6. Samples: 9780576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:11:00,970][166323] Avg episode reward: [(0, '1182.758')]
[36m[2025-07-02 06:11:05,998][166323] Fps is (10 sec: 1634.1, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9781248. Throughput: 0: 273.3. Samples: 9781280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:11:05,998][166323] Avg episode reward: [(0, '1179.375')]
[36m[2025-07-02 06:11:10,980][166323] Fps is (10 sec: 1636.7, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9781248. Throughput: 0: 271.3. Samples: 9782960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:11:10,981][166323] Avg episode reward: [(0, '1136.994')]
[36m[2025-07-02 06:11:15,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9781248. Throughput: 0: 270.4. Samples: 9784592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:11:15,952][166323] Avg episode reward: [(0, '1156.466')]
[36m[2025-07-02 06:11:20,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9781248. Throughput: 0: 271.2. Samples: 9785440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:11:20,955][166323] Avg episode reward: [(0, '1170.821')]
[36m[2025-07-02 06:11:25,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9781248. Throughput: 0: 274.0. Samples: 9787104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:11:25,986][166323] Avg episode reward: [(0, '1184.163')]
[36m[2025-07-02 06:11:30,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9781248. Throughput: 0: 274.6. Samples: 9788832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:11:30,994][166323] Avg episode reward: [(0, '1178.834')]
[36m[2025-07-02 06:11:35,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9781248. Throughput: 0: 274.5. Samples: 9789584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:11:35,951][166323] Avg episode reward: [(0, '1220.051')]
[37m[1m[2025-07-02 06:11:36,009][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019096_9781248.pth...
[36m[2025-07-02 06:11:36,018][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000018968_9715712.pth
[36m[2025-07-02 06:11:40,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9781248. Throughput: 0: 271.5. Samples: 9791216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:11:40,997][166323] Avg episode reward: [(0, '1156.683')]
[36m[2025-07-02 06:11:45,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9781248. Throughput: 0: 272.2. Samples: 9792832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:11:45,989][166323] Avg episode reward: [(0, '1206.716')]
[36m[2025-07-02 06:11:50,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9781248. Throughput: 0: 273.0. Samples: 9793552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:11:50,957][166323] Avg episode reward: [(0, '1220.158')]
[36m[2025-07-02 06:11:55,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9781248. Throughput: 0: 271.3. Samples: 9795168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:11:55,986][166323] Avg episode reward: [(0, '1156.455')]
[36m[2025-07-02 06:12:00,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9781248. Throughput: 0: 268.7. Samples: 9796688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:12:00,969][166323] Avg episode reward: [(0, '1172.602')]
[36m[2025-07-02 06:12:05,986][166323] Fps is (10 sec: 1638.3, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9797632. Throughput: 0: 269.0. Samples: 9797552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:12:05,987][166323] Avg episode reward: [(0, '1201.318')]
[36m[2025-07-02 06:12:10,952][166323] Fps is (10 sec: 1641.2, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9797632. Throughput: 0: 269.4. Samples: 9799216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:12:10,952][166323] Avg episode reward: [(0, '1144.438')]
[36m[2025-07-02 06:12:15,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9797632. Throughput: 0: 269.7. Samples: 9800960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:12:15,955][166323] Avg episode reward: [(0, '1202.641')]
[36m[2025-07-02 06:12:20,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9797632. Throughput: 0: 270.8. Samples: 9801776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:12:20,971][166323] Avg episode reward: [(0, '1266.382')]
[36m[2025-07-02 06:12:25,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9797632. Throughput: 0: 268.1. Samples: 9803280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:12:25,987][166323] Avg episode reward: [(0, '1279.896')]
[36m[2025-07-02 06:12:30,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9797632. Throughput: 0: 267.4. Samples: 9804864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:12:30,991][166323] Avg episode reward: [(0, '1306.088')]
[36m[2025-07-02 06:12:35,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9797632. Throughput: 0: 270.0. Samples: 9805712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:12:35,987][166323] Avg episode reward: [(0, '1277.307')]
[36m[2025-07-02 06:12:40,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9797632. Throughput: 0: 271.1. Samples: 9807360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:12:40,953][166323] Avg episode reward: [(0, '1349.936')]
[36m[2025-07-02 06:12:45,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9797632. Throughput: 0: 274.7. Samples: 9809056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:12:45,991][166323] Avg episode reward: [(0, '1405.070')]
[36m[2025-07-02 06:12:50,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9797632. Throughput: 0: 274.3. Samples: 9809888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:12:50,960][166323] Avg episode reward: [(0, '1352.257')]
[36m[2025-07-02 06:12:55,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9797632. Throughput: 0: 273.1. Samples: 9811504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:12:55,945][166323] Avg episode reward: [(0, '1327.171')]
[36m[2025-07-02 06:13:01,010][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 9797632. Throughput: 0: 267.4. Samples: 9813008. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:13:01,010][166323] Avg episode reward: [(0, '1303.432')]
[36m[2025-07-02 06:13:05,950][166323] Fps is (10 sec: 1637.5, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9814016. Throughput: 0: 267.1. Samples: 9813792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:13:05,951][166323] Avg episode reward: [(0, '1280.449')]
[36m[2025-07-02 06:13:10,973][166323] Fps is (10 sec: 1644.5, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9814016. Throughput: 0: 272.1. Samples: 9815520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:13:10,973][166323] Avg episode reward: [(0, '1301.889')]
[36m[2025-07-02 06:13:15,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9814016. Throughput: 0: 275.5. Samples: 9817248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:13:15,947][166323] Avg episode reward: [(0, '1180.297')]
[36m[2025-07-02 06:13:20,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9814016. Throughput: 0: 274.5. Samples: 9818064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:13:20,993][166323] Avg episode reward: [(0, '1206.564')]
[36m[2025-07-02 06:13:25,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9814016. Throughput: 0: 272.1. Samples: 9819616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:13:25,998][166323] Avg episode reward: [(0, '1207.226')]
[36m[2025-07-02 06:13:30,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9814016. Throughput: 0: 269.6. Samples: 9821184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:13:30,981][166323] Avg episode reward: [(0, '1189.238')]
[36m[2025-07-02 06:13:35,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9814016. Throughput: 0: 267.2. Samples: 9821920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:13:35,996][166323] Avg episode reward: [(0, '1225.571')]
[37m[1m[2025-07-02 06:13:36,047][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019160_9814016.pth...
[36m[2025-07-02 06:13:36,051][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019032_9748480.pth
[31m[34807310 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[34807311 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[34807311 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:13:40,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9814016. Throughput: 0: 268.0. Samples: 9823568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:13:40,958][166323] Avg episode reward: [(0, '1247.424')]
[36m[2025-07-02 06:13:45,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9814016. Throughput: 0: 273.2. Samples: 9825296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:13:45,983][166323] Avg episode reward: [(0, '1217.069')]
[36m[2025-07-02 06:13:50,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9814016. Throughput: 0: 274.3. Samples: 9826144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:13:50,974][166323] Avg episode reward: [(0, '1310.692')]
[31m[34819540 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[34819541 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[34819541 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:13:55,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9814016. Throughput: 0: 269.9. Samples: 9827664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:13:55,970][166323] Avg episode reward: [(0, '1313.940')]
[36m[2025-07-02 06:14:00,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 222.2). Total num frames: 9814016. Throughput: 0: 268.7. Samples: 9829344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:14:00,965][166323] Avg episode reward: [(0, '1287.584')]
[36m[2025-07-02 06:14:05,944][166323] Fps is (10 sec: 1642.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9830400. Throughput: 0: 267.7. Samples: 9830096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:14:05,945][166323] Avg episode reward: [(0, '1242.370')]
[36m[2025-07-02 06:14:10,947][166323] Fps is (10 sec: 1641.3, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9830400. Throughput: 0: 268.0. Samples: 9831664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:14:10,947][166323] Avg episode reward: [(0, '1259.687')]
[36m[2025-07-02 06:14:15,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9830400. Throughput: 0: 268.8. Samples: 9833280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:14:15,982][166323] Avg episode reward: [(0, '1238.838')]
[36m[2025-07-02 06:14:20,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9830400. Throughput: 0: 271.3. Samples: 9834128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:14:20,990][166323] Avg episode reward: [(0, '1212.316')]
[36m[2025-07-02 06:14:25,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9830400. Throughput: 0: 270.8. Samples: 9835760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:14:25,974][166323] Avg episode reward: [(0, '1215.894')]
[36m[2025-07-02 06:14:30,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9830400. Throughput: 0: 269.8. Samples: 9837440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:14:30,993][166323] Avg episode reward: [(0, '1213.595')]
[36m[2025-07-02 06:14:35,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9830400. Throughput: 0: 267.3. Samples: 9838176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:14:35,983][166323] Avg episode reward: [(0, '1219.904')]
[36m[2025-07-02 06:14:40,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9830400. Throughput: 0: 270.5. Samples: 9839840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:14:40,980][166323] Avg episode reward: [(0, '1287.714')]
[36m[2025-07-02 06:14:45,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9830400. Throughput: 0: 268.7. Samples: 9841440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:14:45,983][166323] Avg episode reward: [(0, '1260.225')]
[36m[2025-07-02 06:14:50,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9830400. Throughput: 0: 270.1. Samples: 9842256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:14:50,961][166323] Avg episode reward: [(0, '1254.115')]
[36m[2025-07-02 06:14:56,018][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 9830400. Throughput: 0: 269.8. Samples: 9843824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:14:56,018][166323] Avg episode reward: [(0, '1244.229')]
[36m[2025-07-02 06:15:00,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.1). Total num frames: 9830400. Throughput: 0: 270.0. Samples: 9845424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:15:00,967][166323] Avg episode reward: [(0, '1306.092')]
[36m[2025-07-02 06:15:06,009][166323] Fps is (10 sec: 1639.8, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 9846784. Throughput: 0: 268.3. Samples: 9846208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:15:06,010][166323] Avg episode reward: [(0, '1276.024')]
[36m[2025-07-02 06:15:10,990][166323] Fps is (10 sec: 1634.6, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9846784. Throughput: 0: 270.1. Samples: 9847920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:15:10,990][166323] Avg episode reward: [(0, '1244.001')]
[36m[2025-07-02 06:15:15,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9846784. Throughput: 0: 271.7. Samples: 9849664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:15:15,980][166323] Avg episode reward: [(0, '1158.755')]
[36m[2025-07-02 06:15:20,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9846784. Throughput: 0: 273.9. Samples: 9850496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:15:20,965][166323] Avg episode reward: [(0, '1191.144')]
[36m[2025-07-02 06:15:25,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9846784. Throughput: 0: 272.6. Samples: 9852112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:15:25,993][166323] Avg episode reward: [(0, '1223.464')]
[31m[34918904 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[34918904 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[34918904 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:15:30,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9846784. Throughput: 0: 272.8. Samples: 9853712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:15:30,965][166323] Avg episode reward: [(0, '1222.772')]
[36m[2025-07-02 06:15:36,006][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9846784. Throughput: 0: 271.7. Samples: 9854496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:15:36,007][166323] Avg episode reward: [(0, '1220.479')]
[37m[1m[2025-07-02 06:15:36,015][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019224_9846784.pth...
[36m[2025-07-02 06:15:36,023][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019096_9781248.pth
[36m[2025-07-02 06:15:40,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9846784. Throughput: 0: 272.3. Samples: 9856064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:15:40,967][166323] Avg episode reward: [(0, '1233.243')]
[36m[2025-07-02 06:15:45,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9846784. Throughput: 0: 272.3. Samples: 9857680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:15:45,969][166323] Avg episode reward: [(0, '1270.942')]
[36m[2025-07-02 06:15:50,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9846784. Throughput: 0: 273.5. Samples: 9858496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:15:50,945][166323] Avg episode reward: [(0, '1333.231')]
[36m[2025-07-02 06:15:55,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9846784. Throughput: 0: 274.9. Samples: 9860288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:15:55,974][166323] Avg episode reward: [(0, '1230.945')]
[36m[2025-07-02 06:16:00,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 9846784. Throughput: 0: 275.6. Samples: 9862064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:16:00,973][166323] Avg episode reward: [(0, '1191.669')]
[36m[2025-07-02 06:16:05,987][166323] Fps is (10 sec: 1636.1, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9863168. Throughput: 0: 276.1. Samples: 9862928. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 06:16:05,988][166323] Avg episode reward: [(0, '1153.909')]
[31m[34954722 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[34954722 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[34954722 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:16:10,958][166323] Fps is (10 sec: 1640.9, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9863168. Throughput: 0: 278.6. Samples: 9864640. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 06:16:10,958][166323] Avg episode reward: [(0, '1121.573')]
[36m[2025-07-02 06:16:15,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9863168. Throughput: 0: 280.0. Samples: 9866320. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 06:16:15,990][166323] Avg episode reward: [(0, '1102.009')]
[36m[2025-07-02 06:16:20,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9863168. Throughput: 0: 280.6. Samples: 9867120. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 06:16:20,991][166323] Avg episode reward: [(0, '1159.479')]
[36m[2025-07-02 06:16:25,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9863168. Throughput: 0: 279.7. Samples: 9868656. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 06:16:25,992][166323] Avg episode reward: [(0, '1170.211')]
[36m[2025-07-02 06:16:30,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9863168. Throughput: 0: 282.0. Samples: 9870368. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 06:16:30,959][166323] Avg episode reward: [(0, '1273.500')]
[31m[34982867 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[34982867 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[34982867 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:16:35,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9863168. Throughput: 0: 280.9. Samples: 9871136. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 06:16:35,946][166323] Avg episode reward: [(0, '1231.210')]
[36m[2025-07-02 06:16:40,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9863168. Throughput: 0: 278.3. Samples: 9872816. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 06:16:40,990][166323] Avg episode reward: [(0, '1226.376')]
[36m[2025-07-02 06:16:45,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9863168. Throughput: 0: 276.5. Samples: 9874512. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 06:16:45,992][166323] Avg episode reward: [(0, '1269.319')]
[36m[2025-07-02 06:16:50,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9863168. Throughput: 0: 277.5. Samples: 9875408. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 06:16:50,956][166323] Avg episode reward: [(0, '1207.270')]
[36m[2025-07-02 06:16:55,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9863168. Throughput: 0: 278.3. Samples: 9877168. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 06:16:55,966][166323] Avg episode reward: [(0, '1210.910')]
[36m[2025-07-02 06:17:00,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 9863168. Throughput: 0: 278.4. Samples: 9878848. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 06:17:00,987][166323] Avg episode reward: [(0, '1191.359')]
[36m[2025-07-02 06:17:05,971][166323] Fps is (10 sec: 1637.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9879552. Throughput: 0: 278.9. Samples: 9879664. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:17:05,972][166323] Avg episode reward: [(0, '1250.754')]
[36m[2025-07-02 06:17:11,002][166323] Fps is (10 sec: 1635.9, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9879552. Throughput: 0: 281.9. Samples: 9881344. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:17:11,002][166323] Avg episode reward: [(0, '1262.277')]
[36m[2025-07-02 06:17:16,005][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9879552. Throughput: 0: 279.9. Samples: 9882976. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:17:16,005][166323] Avg episode reward: [(0, '1304.334')]
[36m[2025-07-02 06:17:20,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9879552. Throughput: 0: 280.0. Samples: 9883744. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:17:20,975][166323] Avg episode reward: [(0, '1275.149')]
[36m[2025-07-02 06:17:25,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9879552. Throughput: 0: 281.1. Samples: 9885456. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:17:25,949][166323] Avg episode reward: [(0, '1285.836')]
[36m[2025-07-02 06:17:30,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9879552. Throughput: 0: 282.8. Samples: 9887232. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:17:30,974][166323] Avg episode reward: [(0, '1230.282')]
[36m[2025-07-02 06:17:36,023][166323] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 277.6). Total num frames: 9879552. Throughput: 0: 283.0. Samples: 9888160. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:17:36,024][166323] Avg episode reward: [(0, '1230.555')]
[37m[1m[2025-07-02 06:17:36,030][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019288_9879552.pth...
[36m[2025-07-02 06:17:36,038][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019160_9814016.pth
[31m[35045968 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[35045968 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[35045969 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:17:40,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9879552. Throughput: 0: 279.7. Samples: 9889760. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:17:40,991][166323] Avg episode reward: [(0, '1184.050')]
[36m[2025-07-02 06:17:45,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9879552. Throughput: 0: 276.6. Samples: 9891296. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:17:45,994][166323] Avg episode reward: [(0, '1234.653')]
[33m[35054981 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[35054981 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.85107421875
[33mCrash Rate: 0.13232421875
[33mTimeout Rate: 0.0166015625 (navigation_task.py:265)
[33m[35054982 ms][navigation_task] - WARNING : 
[33mSuccesses: 1743
[33mCrashes : 271
[33mTimeouts: 34 (navigation_task.py:268)
[31m[35059262 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[35059262 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[35059262 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:17:50,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9879552. Throughput: 0: 276.4. Samples: 9892096. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:17:50,945][166323] Avg episode reward: [(0, '1231.226')]
[36m[2025-07-02 06:17:55,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9879552. Throughput: 0: 275.1. Samples: 9893712. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:17:55,959][166323] Avg episode reward: [(0, '1228.986')]
[36m[2025-07-02 06:18:00,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.1). Total num frames: 9879552. Throughput: 0: 274.4. Samples: 9895312. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:18:00,965][166323] Avg episode reward: [(0, '1243.622')]
[36m[2025-07-02 06:18:05,976][166323] Fps is (10 sec: 1635.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9895936. Throughput: 0: 274.8. Samples: 9896112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:18:05,976][166323] Avg episode reward: [(0, '1292.549')]
[36m[2025-07-02 06:18:10,969][166323] Fps is (10 sec: 1637.7, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9895936. Throughput: 0: 274.7. Samples: 9897824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:18:10,970][166323] Avg episode reward: [(0, '1238.527')]
[36m[2025-07-02 06:18:15,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9895936. Throughput: 0: 273.5. Samples: 9899536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:18:15,956][166323] Avg episode reward: [(0, '1252.587')]
[36m[2025-07-02 06:18:20,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9895936. Throughput: 0: 270.7. Samples: 9900320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:18:20,946][166323] Avg episode reward: [(0, '1219.304')]
[36m[2025-07-02 06:18:25,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9895936. Throughput: 0: 267.9. Samples: 9901808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:18:25,954][166323] Avg episode reward: [(0, '1201.285')]
[36m[2025-07-02 06:18:30,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9895936. Throughput: 0: 269.0. Samples: 9903392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:18:30,965][166323] Avg episode reward: [(0, '1187.260')]
[36m[2025-07-02 06:18:35,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9895936. Throughput: 0: 269.0. Samples: 9904208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:18:35,977][166323] Avg episode reward: [(0, '1224.726')]
[36m[2025-07-02 06:18:40,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9895936. Throughput: 0: 269.5. Samples: 9905840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:18:40,962][166323] Avg episode reward: [(0, '1192.370')]
[36m[2025-07-02 06:18:45,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9895936. Throughput: 0: 269.4. Samples: 9907440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:18:45,980][166323] Avg episode reward: [(0, '1236.665')]
[36m[2025-07-02 06:18:50,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9895936. Throughput: 0: 269.4. Samples: 9908240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:18:50,989][166323] Avg episode reward: [(0, '1200.045')]
[36m[2025-07-02 06:18:55,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9895936. Throughput: 0: 272.1. Samples: 9910064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:18:55,949][166323] Avg episode reward: [(0, '1204.043')]
[31m[35124859 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[35124860 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[35124860 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:19:00,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 9895936. Throughput: 0: 269.9. Samples: 9911680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:19:00,948][166323] Avg episode reward: [(0, '1270.432')]
[36m[2025-07-02 06:19:05,979][166323] Fps is (10 sec: 1633.3, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9912320. Throughput: 0: 270.7. Samples: 9912512. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 06:19:05,980][166323] Avg episode reward: [(0, '1292.931')]
[36m[2025-07-02 06:19:10,997][166323] Fps is (10 sec: 1630.4, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9912320. Throughput: 0: 276.7. Samples: 9914272. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 06:19:10,997][166323] Avg episode reward: [(0, '1238.238')]
[36m[2025-07-02 06:19:15,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9912320. Throughput: 0: 276.9. Samples: 9915856. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 06:19:15,981][166323] Avg episode reward: [(0, '1277.680')]
[36m[2025-07-02 06:19:20,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9912320. Throughput: 0: 275.0. Samples: 9916576. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 06:19:20,956][166323] Avg episode reward: [(0, '1276.948')]
[36m[2025-07-02 06:19:25,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9912320. Throughput: 0: 275.7. Samples: 9918240. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 06:19:25,944][166323] Avg episode reward: [(0, '1281.586')]
[36m[2025-07-02 06:19:30,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9912320. Throughput: 0: 275.3. Samples: 9919824. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 06:19:30,970][166323] Avg episode reward: [(0, '1266.834')]
[36m[2025-07-02 06:19:35,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9912320. Throughput: 0: 275.4. Samples: 9920624. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 06:19:35,949][166323] Avg episode reward: [(0, '1280.361')]
[37m[1m[2025-07-02 06:19:36,006][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019352_9912320.pth...
[36m[2025-07-02 06:19:36,012][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019224_9846784.pth
[36m[2025-07-02 06:19:40,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9912320. Throughput: 0: 272.6. Samples: 9922336. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 06:19:40,963][166323] Avg episode reward: [(0, '1258.969')]
[36m[2025-07-02 06:19:45,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9912320. Throughput: 0: 273.3. Samples: 9923984. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 06:19:45,975][166323] Avg episode reward: [(0, '1321.113')]
[36m[2025-07-02 06:19:50,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9912320. Throughput: 0: 273.5. Samples: 9924816. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 06:19:50,965][166323] Avg episode reward: [(0, '1321.586')]
[36m[2025-07-02 06:19:55,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9912320. Throughput: 0: 270.9. Samples: 9926448. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 06:19:55,949][166323] Avg episode reward: [(0, '1281.230')]
[36m[2025-07-02 06:20:00,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 9912320. Throughput: 0: 273.3. Samples: 9928144. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 06:20:00,950][166323] Avg episode reward: [(0, '1337.762')]
[36m[2025-07-02 06:20:05,985][166323] Fps is (10 sec: 1632.4, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9928704. Throughput: 0: 274.0. Samples: 9928912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:20:05,985][166323] Avg episode reward: [(0, '1358.113')]
[36m[2025-07-02 06:20:10,994][166323] Fps is (10 sec: 1631.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9928704. Throughput: 0: 271.0. Samples: 9930448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:20:10,994][166323] Avg episode reward: [(0, '1335.538')]
[36m[2025-07-02 06:20:15,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9928704. Throughput: 0: 272.0. Samples: 9932064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:20:15,977][166323] Avg episode reward: [(0, '1307.420')]
[31m[35208924 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[35208924 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[35208925 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:20:20,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9928704. Throughput: 0: 272.6. Samples: 9932896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:20:20,973][166323] Avg episode reward: [(0, '1239.096')]
[36m[2025-07-02 06:20:25,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 9928704. Throughput: 0: 269.7. Samples: 9934480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:20:25,993][166323] Avg episode reward: [(0, '1243.743')]
[36m[2025-07-02 06:20:30,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 9928704. Throughput: 0: 268.6. Samples: 9936064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:20:30,947][166323] Avg episode reward: [(0, '1229.967')]
[36m[2025-07-02 06:20:35,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9928704. Throughput: 0: 268.4. Samples: 9936896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:20:35,970][166323] Avg episode reward: [(0, '1243.521')]
[31m[35227535 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[35227535 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[35227536 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:20:40,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9928704. Throughput: 0: 269.1. Samples: 9938560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:20:40,952][166323] Avg episode reward: [(0, '1248.595')]
[36m[2025-07-02 06:20:45,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9928704. Throughput: 0: 268.6. Samples: 9940240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:20:45,981][166323] Avg episode reward: [(0, '1253.817')]
[36m[2025-07-02 06:20:50,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9928704. Throughput: 0: 270.4. Samples: 9941072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:20:50,948][166323] Avg episode reward: [(0, '1214.982')]
[36m[2025-07-02 06:20:55,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9928704. Throughput: 0: 274.3. Samples: 9942784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:20:55,962][166323] Avg episode reward: [(0, '1257.173')]
[36m[2025-07-02 06:21:00,979][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.2). Total num frames: 9928704. Throughput: 0: 274.1. Samples: 9944400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:21:00,980][166323] Avg episode reward: [(0, '1239.898')]
[36m[2025-07-02 06:21:05,965][166323] Fps is (10 sec: 1638.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9945088. Throughput: 0: 273.5. Samples: 9945200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:21:05,965][166323] Avg episode reward: [(0, '1330.455')]
[36m[2025-07-02 06:21:10,948][166323] Fps is (10 sec: 1643.6, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9945088. Throughput: 0: 276.2. Samples: 9946896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:21:10,948][166323] Avg episode reward: [(0, '1300.075')]
[36m[2025-07-02 06:21:15,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9945088. Throughput: 0: 275.0. Samples: 9948448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:21:15,972][166323] Avg episode reward: [(0, '1315.961')]
[36m[2025-07-02 06:21:20,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9945088. Throughput: 0: 274.7. Samples: 9949264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:21:20,994][166323] Avg episode reward: [(0, '1275.837')]
[36m[2025-07-02 06:21:25,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9945088. Throughput: 0: 274.1. Samples: 9950896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:21:25,965][166323] Avg episode reward: [(0, '1312.921')]
[36m[2025-07-02 06:21:30,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9945088. Throughput: 0: 274.9. Samples: 9952608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:21:30,970][166323] Avg episode reward: [(0, '1349.312')]
[36m[2025-07-02 06:21:36,021][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 9945088. Throughput: 0: 273.3. Samples: 9953392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:21:36,021][166323] Avg episode reward: [(0, '1335.700')]
[37m[1m[2025-07-02 06:21:36,094][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019416_9945088.pth...
[36m[2025-07-02 06:21:36,098][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019288_9879552.pth
[36m[2025-07-02 06:21:40,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9945088. Throughput: 0: 269.8. Samples: 9954928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:21:40,977][166323] Avg episode reward: [(0, '1276.443')]
[36m[2025-07-02 06:21:45,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9945088. Throughput: 0: 271.0. Samples: 9956592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:21:45,965][166323] Avg episode reward: [(0, '1238.128')]
[36m[2025-07-02 06:21:50,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9945088. Throughput: 0: 271.0. Samples: 9957392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:21:50,948][166323] Avg episode reward: [(0, '1163.266')]
[36m[2025-07-02 06:21:55,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9945088. Throughput: 0: 266.3. Samples: 9958880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:21:55,945][166323] Avg episode reward: [(0, '1176.460')]
[36m[2025-07-02 06:22:00,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 9945088. Throughput: 0: 267.8. Samples: 9960496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:22:00,963][166323] Avg episode reward: [(0, '1154.208')]
[36m[2025-07-02 06:22:05,953][166323] Fps is (10 sec: 1637.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9961472. Throughput: 0: 269.4. Samples: 9961376. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 06:22:05,954][166323] Avg episode reward: [(0, '1091.323')]
[36m[2025-07-02 06:22:10,993][166323] Fps is (10 sec: 1633.5, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9961472. Throughput: 0: 270.4. Samples: 9963072. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 06:22:10,993][166323] Avg episode reward: [(0, '1118.968')]
[36m[2025-07-02 06:22:15,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9961472. Throughput: 0: 269.5. Samples: 9964736. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 06:22:15,974][166323] Avg episode reward: [(0, '1147.034')]
[36m[2025-07-02 06:22:20,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9961472. Throughput: 0: 270.9. Samples: 9965568. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 06:22:20,976][166323] Avg episode reward: [(0, '1182.601')]
[31m[35332337 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[35332338 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[35332338 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:22:25,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9961472. Throughput: 0: 271.6. Samples: 9967152. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 06:22:25,984][166323] Avg episode reward: [(0, '1191.613')]
[36m[2025-07-02 06:22:30,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 9961472. Throughput: 0: 271.4. Samples: 9968800. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 06:22:30,952][166323] Avg episode reward: [(0, '1160.271')]
[36m[2025-07-02 06:22:35,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 9961472. Throughput: 0: 271.6. Samples: 9969616. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 06:22:35,953][166323] Avg episode reward: [(0, '1225.194')]
[36m[2025-07-02 06:22:40,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9961472. Throughput: 0: 274.4. Samples: 9971232. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 06:22:40,957][166323] Avg episode reward: [(0, '1252.443')]
[36m[2025-07-02 06:22:45,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9961472. Throughput: 0: 278.1. Samples: 9973008. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 06:22:45,955][166323] Avg episode reward: [(0, '1222.541')]
[36m[2025-07-02 06:22:51,002][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 9961472. Throughput: 0: 276.0. Samples: 9973808. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 06:22:51,002][166323] Avg episode reward: [(0, '1271.224')]
[36m[2025-07-02 06:22:55,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 9961472. Throughput: 0: 274.8. Samples: 9975440. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 06:22:55,993][166323] Avg episode reward: [(0, '1203.767')]
[36m[2025-07-02 06:23:00,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 9961472. Throughput: 0: 271.2. Samples: 9976944. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 06:23:00,984][166323] Avg episode reward: [(0, '1291.769')]
[36m[2025-07-02 06:23:05,957][166323] Fps is (10 sec: 1644.2, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9977856. Throughput: 0: 270.3. Samples: 9977728. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 06:23:05,958][166323] Avg episode reward: [(0, '1284.740')]
[36m[2025-07-02 06:23:10,963][166323] Fps is (10 sec: 1641.8, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9977856. Throughput: 0: 271.1. Samples: 9979344. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 06:23:10,963][166323] Avg episode reward: [(0, '1210.317')]
[36m[2025-07-02 06:23:15,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9977856. Throughput: 0: 272.5. Samples: 9981072. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 06:23:15,982][166323] Avg episode reward: [(0, '1261.920')]
[36m[2025-07-02 06:23:20,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9977856. Throughput: 0: 274.5. Samples: 9981968. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 06:23:20,944][166323] Avg episode reward: [(0, '1230.847')]
[36m[2025-07-02 06:23:25,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9977856. Throughput: 0: 277.7. Samples: 9983728. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 06:23:25,962][166323] Avg episode reward: [(0, '1237.703')]
[36m[2025-07-02 06:23:30,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9977856. Throughput: 0: 275.4. Samples: 9985408. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 06:23:30,986][166323] Avg episode reward: [(0, '1270.145')]
[36m[2025-07-02 06:23:35,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9977856. Throughput: 0: 277.0. Samples: 9986256. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 06:23:35,947][166323] Avg episode reward: [(0, '1241.865')]
[37m[1m[2025-07-02 06:23:36,020][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019480_9977856.pth...
[36m[2025-07-02 06:23:36,025][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019352_9912320.pth
[36m[2025-07-02 06:23:40,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9977856. Throughput: 0: 275.4. Samples: 9987824. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 06:23:40,958][166323] Avg episode reward: [(0, '1258.591')]
[36m[2025-07-02 06:23:45,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9977856. Throughput: 0: 278.9. Samples: 9989488. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 06:23:45,966][166323] Avg episode reward: [(0, '1311.178')]
[36m[2025-07-02 06:23:50,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9977856. Throughput: 0: 278.9. Samples: 9990288. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 06:23:50,987][166323] Avg episode reward: [(0, '1251.646')]
[36m[2025-07-02 06:23:56,001][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 9977856. Throughput: 0: 278.5. Samples: 9991888. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 06:23:56,001][166323] Avg episode reward: [(0, '1234.019')]
[36m[2025-07-02 06:24:00,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 9977856. Throughput: 0: 278.1. Samples: 9993584. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 06:24:00,967][166323] Avg episode reward: [(0, '1236.654')]
[36m[2025-07-02 06:24:05,945][166323] Fps is (10 sec: 1647.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9994240. Throughput: 0: 275.2. Samples: 9994352. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 06:24:05,945][166323] Avg episode reward: [(0, '1246.896')]
[36m[2025-07-02 06:24:11,000][166323] Fps is (10 sec: 1633.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9994240. Throughput: 0: 273.2. Samples: 9996032. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 06:24:11,000][166323] Avg episode reward: [(0, '1233.952')]
[36m[2025-07-02 06:24:15,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9994240. Throughput: 0: 272.6. Samples: 9997664. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 06:24:15,946][166323] Avg episode reward: [(0, '1232.239')]
[36m[2025-07-02 06:24:20,977][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 9994240. Throughput: 0: 272.9. Samples: 9998544. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 06:24:20,977][166323] Avg episode reward: [(0, '1239.451')]
[36m[2025-07-02 06:24:25,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9994240. Throughput: 0: 273.1. Samples: 10000112. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 06:24:25,951][166323] Avg episode reward: [(0, '1232.588')]
[36m[2025-07-02 06:24:30,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9994240. Throughput: 0: 271.6. Samples: 10001712. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 06:24:30,974][166323] Avg episode reward: [(0, '1295.246')]
[36m[2025-07-02 06:24:35,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 9994240. Throughput: 0: 273.3. Samples: 10002576. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 06:24:35,950][166323] Avg episode reward: [(0, '1278.786')]
[36m[2025-07-02 06:24:40,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9994240. Throughput: 0: 271.9. Samples: 10004112. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 06:24:40,973][166323] Avg episode reward: [(0, '1246.772')]
[36m[2025-07-02 06:24:45,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 9994240. Throughput: 0: 269.4. Samples: 10005712. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 06:24:45,979][166323] Avg episode reward: [(0, '1238.021')]
[36m[2025-07-02 06:24:50,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 9994240. Throughput: 0: 269.4. Samples: 10006480. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 06:24:50,963][166323] Avg episode reward: [(0, '1258.897')]
[36m[2025-07-02 06:24:55,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 9994240. Throughput: 0: 272.3. Samples: 10008272. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 06:24:55,944][166323] Avg episode reward: [(0, '1314.309')]
[31m[35489399 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[35489399 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[35489399 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:25:00,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 9994240. Throughput: 0: 271.2. Samples: 10009872. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 06:25:00,957][166323] Avg episode reward: [(0, '1327.252')]
[36m[2025-07-02 06:25:05,967][166323] Fps is (10 sec: 1634.7, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10010624. Throughput: 0: 269.2. Samples: 10010656. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 06:25:05,967][166323] Avg episode reward: [(0, '1323.005')]
[31m[35495551 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[35495551 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[35495552 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[35497692 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[35497692 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[35497692 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:25:11,005][166323] Fps is (10 sec: 1630.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10010624. Throughput: 0: 272.0. Samples: 10012368. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 06:25:11,015][166323] Avg episode reward: [(0, '1305.410')]
[36m[2025-07-02 06:25:15,974][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10010624. Throughput: 0: 272.0. Samples: 10013952. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 06:25:15,974][166323] Avg episode reward: [(0, '1338.503')]
[36m[2025-07-02 06:25:20,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10010624. Throughput: 0: 272.5. Samples: 10014848. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 06:25:20,983][166323] Avg episode reward: [(0, '1270.674')]
[36m[2025-07-02 06:25:25,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10010624. Throughput: 0: 274.5. Samples: 10016464. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 06:25:25,954][166323] Avg episode reward: [(0, '1264.845')]
[36m[2025-07-02 06:25:30,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10010624. Throughput: 0: 275.3. Samples: 10018096. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 06:25:30,956][166323] Avg episode reward: [(0, '1241.666')]
[36m[2025-07-02 06:25:35,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10010624. Throughput: 0: 278.0. Samples: 10018992. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 06:25:35,974][166323] Avg episode reward: [(0, '1227.034')]
[37m[1m[2025-07-02 06:25:36,059][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019544_10010624.pth...
[36m[2025-07-02 06:25:36,065][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019416_9945088.pth
[36m[2025-07-02 06:25:40,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10010624. Throughput: 0: 277.5. Samples: 10020768. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 06:25:40,970][166323] Avg episode reward: [(0, '1253.372')]
[31m[35531740 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[35531741 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[35531741 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:25:45,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10010624. Throughput: 0: 277.9. Samples: 10022384. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 06:25:45,984][166323] Avg episode reward: [(0, '1233.968')]
[36m[2025-07-02 06:25:50,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10010624. Throughput: 0: 276.6. Samples: 10023104. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 06:25:50,978][166323] Avg episode reward: [(0, '1265.293')]
[36m[2025-07-02 06:25:55,976][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10010624. Throughput: 0: 276.4. Samples: 10024800. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 06:25:55,976][166323] Avg episode reward: [(0, '1333.510')]
[36m[2025-07-02 06:26:00,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 10010624. Throughput: 0: 277.2. Samples: 10026432. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 06:26:00,995][166323] Avg episode reward: [(0, '1294.922')]
[31m[35550652 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[35550653 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[35550653 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:26:05,954][166323] Fps is (10 sec: 1641.9, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10027008. Throughput: 0: 274.7. Samples: 10027200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:26:05,955][166323] Avg episode reward: [(0, '1325.110')]
[31m[35556602 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[35556602 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[35556603 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:26:10,968][166323] Fps is (10 sec: 1642.8, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10027008. Throughput: 0: 275.5. Samples: 10028864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:26:10,968][166323] Avg episode reward: [(0, '1289.833')]
[36m[2025-07-02 06:26:16,009][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10027008. Throughput: 0: 273.8. Samples: 10030432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:26:16,009][166323] Avg episode reward: [(0, '1264.103')]
[36m[2025-07-02 06:26:20,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10027008. Throughput: 0: 271.4. Samples: 10031200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:26:20,948][166323] Avg episode reward: [(0, '1297.668')]
[36m[2025-07-02 06:26:25,983][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10027008. Throughput: 0: 267.3. Samples: 10032800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:26:25,983][166323] Avg episode reward: [(0, '1309.580')]
[36m[2025-07-02 06:26:30,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10027008. Throughput: 0: 268.5. Samples: 10034464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:26:30,968][166323] Avg episode reward: [(0, '1285.281')]
[36m[2025-07-02 06:26:36,001][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10027008. Throughput: 0: 269.7. Samples: 10035248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:26:36,001][166323] Avg episode reward: [(0, '1335.911')]
[36m[2025-07-02 06:26:40,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10027008. Throughput: 0: 265.7. Samples: 10036752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:26:40,953][166323] Avg episode reward: [(0, '1301.602')]
[36m[2025-07-02 06:26:45,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10027008. Throughput: 0: 266.6. Samples: 10038416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:26:45,946][166323] Avg episode reward: [(0, '1315.544')]
[36m[2025-07-02 06:26:50,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10027008. Throughput: 0: 268.8. Samples: 10039296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:26:50,955][166323] Avg episode reward: [(0, '1270.148')]
[36m[2025-07-02 06:26:56,000][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10027008. Throughput: 0: 266.5. Samples: 10040864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:26:56,000][166323] Avg episode reward: [(0, '1260.993')]
[36m[2025-07-02 06:27:00,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.1). Total num frames: 10027008. Throughput: 0: 269.1. Samples: 10042528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:27:00,964][166323] Avg episode reward: [(0, '1217.830')]
[36m[2025-07-02 06:27:05,962][166323] Fps is (10 sec: 1644.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10043392. Throughput: 0: 268.4. Samples: 10043280. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 06:27:05,962][166323] Avg episode reward: [(0, '1223.922')]
[36m[2025-07-02 06:27:10,955][166323] Fps is (10 sec: 1639.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10043392. Throughput: 0: 272.9. Samples: 10045072. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 06:27:10,955][166323] Avg episode reward: [(0, '1230.434')]
[36m[2025-07-02 06:27:15,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10043392. Throughput: 0: 273.5. Samples: 10046768. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 06:27:15,950][166323] Avg episode reward: [(0, '1258.715')]
[36m[2025-07-02 06:27:20,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10043392. Throughput: 0: 275.5. Samples: 10047632. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 06:27:20,956][166323] Avg episode reward: [(0, '1266.335')]
[36m[2025-07-02 06:27:25,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10043392. Throughput: 0: 279.1. Samples: 10049312. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 06:27:25,955][166323] Avg episode reward: [(0, '1294.071')]
[36m[2025-07-02 06:27:30,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10043392. Throughput: 0: 276.4. Samples: 10050864. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 06:27:30,975][166323] Avg episode reward: [(0, '1267.606')]
[36m[2025-07-02 06:27:35,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10043392. Throughput: 0: 274.4. Samples: 10051648. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 06:27:35,968][166323] Avg episode reward: [(0, '1299.510')]
[37m[1m[2025-07-02 06:27:36,020][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019608_10043392.pth...
[36m[2025-07-02 06:27:36,024][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019480_9977856.pth
[36m[2025-07-02 06:27:40,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10043392. Throughput: 0: 276.5. Samples: 10053296. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 06:27:40,966][166323] Avg episode reward: [(0, '1276.733')]
[36m[2025-07-02 06:27:45,979][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10043392. Throughput: 0: 276.9. Samples: 10054992. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 06:27:45,979][166323] Avg episode reward: [(0, '1261.378')]
[36m[2025-07-02 06:27:50,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10043392. Throughput: 0: 280.4. Samples: 10055904. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 06:27:50,980][166323] Avg episode reward: [(0, '1241.314')]
[36m[2025-07-02 06:27:55,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10043392. Throughput: 0: 276.5. Samples: 10057520. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 06:27:55,983][166323] Avg episode reward: [(0, '1248.166')]
[36m[2025-07-02 06:28:00,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 10043392. Throughput: 0: 275.2. Samples: 10059152. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 06:28:00,953][166323] Avg episode reward: [(0, '1275.961')]
[36m[2025-07-02 06:28:05,957][166323] Fps is (10 sec: 1642.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10059776. Throughput: 0: 273.4. Samples: 10059936. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 06:28:05,957][166323] Avg episode reward: [(0, '1294.996')]
[36m[2025-07-02 06:28:10,971][166323] Fps is (10 sec: 1635.5, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10059776. Throughput: 0: 273.3. Samples: 10061616. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 06:28:10,971][166323] Avg episode reward: [(0, '1309.898')]
[36m[2025-07-02 06:28:15,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10059776. Throughput: 0: 279.7. Samples: 10063456. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 06:28:16,001][166323] Avg episode reward: [(0, '1281.133')]
[36m[2025-07-02 06:28:20,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10059776. Throughput: 0: 281.8. Samples: 10064336. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 06:28:20,993][166323] Avg episode reward: [(0, '1301.166')]
[36m[2025-07-02 06:28:25,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10059776. Throughput: 0: 281.6. Samples: 10065968. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 06:28:25,960][166323] Avg episode reward: [(0, '1257.230')]
[36m[2025-07-02 06:28:30,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10059776. Throughput: 0: 282.4. Samples: 10067696. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 06:28:30,961][166323] Avg episode reward: [(0, '1285.095')]
[36m[2025-07-02 06:28:35,998][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10059776. Throughput: 0: 279.4. Samples: 10068480. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 06:28:35,998][166323] Avg episode reward: [(0, '1285.784')]
[31m[35707554 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[35707555 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[35707556 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:28:40,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10059776. Throughput: 0: 280.7. Samples: 10070144. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 06:28:40,951][166323] Avg episode reward: [(0, '1279.849')]
[36m[2025-07-02 06:28:45,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10059776. Throughput: 0: 281.7. Samples: 10071840. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 06:28:45,988][166323] Avg episode reward: [(0, '1293.384')]
[36m[2025-07-02 06:28:51,004][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10059776. Throughput: 0: 281.7. Samples: 10072624. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 06:28:51,004][166323] Avg episode reward: [(0, '1324.421')]
[36m[2025-07-02 06:28:56,000][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10059776. Throughput: 0: 281.8. Samples: 10074304. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 06:28:56,000][166323] Avg episode reward: [(0, '1339.779')]
[36m[2025-07-02 06:29:00,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 10059776. Throughput: 0: 276.3. Samples: 10075888. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 06:29:00,991][166323] Avg episode reward: [(0, '1387.131')]
[36m[2025-07-02 06:29:05,960][166323] Fps is (10 sec: 1645.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10076160. Throughput: 0: 272.2. Samples: 10076576. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:29:05,960][166323] Avg episode reward: [(0, '1346.376')]
[36m[2025-07-02 06:29:10,959][166323] Fps is (10 sec: 1643.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10076160. Throughput: 0: 273.8. Samples: 10078288. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:29:10,960][166323] Avg episode reward: [(0, '1332.453')]
[36m[2025-07-02 06:29:15,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10076160. Throughput: 0: 270.1. Samples: 10079856. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:29:15,983][166323] Avg episode reward: [(0, '1346.649')]
[36m[2025-07-02 06:29:20,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10076160. Throughput: 0: 272.3. Samples: 10080720. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:29:20,946][166323] Avg episode reward: [(0, '1323.856')]
[33m[35751815 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[35751815 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.86328125
[33mCrash Rate: 0.12109375
[33mTimeout Rate: 0.015625 (navigation_task.py:265)
[33m[35751815 ms][navigation_task] - WARNING : 
[33mSuccesses: 1768
[33mCrashes : 248
[33mTimeouts: 32 (navigation_task.py:268)
[36m[2025-07-02 06:29:25,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10076160. Throughput: 0: 273.6. Samples: 10082464. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:29:25,982][166323] Avg episode reward: [(0, '1282.283')]
[36m[2025-07-02 06:29:30,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10076160. Throughput: 0: 271.5. Samples: 10084048. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:29:30,952][166323] Avg episode reward: [(0, '1272.372')]
[36m[2025-07-02 06:29:35,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10076160. Throughput: 0: 271.6. Samples: 10084832. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:29:35,958][166323] Avg episode reward: [(0, '1310.842')]
[37m[1m[2025-07-02 06:29:36,007][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019672_10076160.pth...
[36m[2025-07-02 06:29:36,011][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019544_10010624.pth
[36m[2025-07-02 06:29:40,980][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10076160. Throughput: 0: 270.3. Samples: 10086464. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:29:40,980][166323] Avg episode reward: [(0, '1328.752')]
[36m[2025-07-02 06:29:45,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10076160. Throughput: 0: 270.3. Samples: 10088048. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:29:45,974][166323] Avg episode reward: [(0, '1322.380')]
[36m[2025-07-02 06:29:50,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10076160. Throughput: 0: 273.4. Samples: 10088880. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:29:50,969][166323] Avg episode reward: [(0, '1348.869')]
[36m[2025-07-02 06:29:55,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10076160. Throughput: 0: 270.8. Samples: 10090480. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:29:55,989][166323] Avg episode reward: [(0, '1355.674')]
[36m[2025-07-02 06:30:00,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 10076160. Throughput: 0: 270.3. Samples: 10092016. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:30:00,974][166323] Avg episode reward: [(0, '1382.258')]
[36m[2025-07-02 06:30:05,963][166323] Fps is (10 sec: 1642.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10092544. Throughput: 0: 270.8. Samples: 10092912. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 06:30:05,963][166323] Avg episode reward: [(0, '1343.137')]
[31m[35795064 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[35795064 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[35795065 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:30:10,963][166323] Fps is (10 sec: 1640.3, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10092544. Throughput: 0: 270.0. Samples: 10094608. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 06:30:10,963][166323] Avg episode reward: [(0, '1309.024')]
[36m[2025-07-02 06:30:15,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10092544. Throughput: 0: 273.5. Samples: 10096352. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 06:30:15,944][166323] Avg episode reward: [(0, '1287.028')]
[36m[2025-07-02 06:30:20,975][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10092544. Throughput: 0: 275.8. Samples: 10097248. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 06:30:20,975][166323] Avg episode reward: [(0, '1315.580')]
[36m[2025-07-02 06:30:25,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10092544. Throughput: 0: 276.0. Samples: 10098880. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 06:30:25,973][166323] Avg episode reward: [(0, '1290.473')]
[36m[2025-07-02 06:30:30,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10092544. Throughput: 0: 276.0. Samples: 10100464. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 06:30:30,958][166323] Avg episode reward: [(0, '1330.931')]
[36m[2025-07-02 06:30:35,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10092544. Throughput: 0: 275.8. Samples: 10101296. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 06:30:35,982][166323] Avg episode reward: [(0, '1325.726')]
[36m[2025-07-02 06:30:40,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10092544. Throughput: 0: 277.9. Samples: 10102976. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 06:30:40,949][166323] Avg episode reward: [(0, '1305.281')]
[36m[2025-07-02 06:30:45,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10092544. Throughput: 0: 280.3. Samples: 10104624. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 06:30:45,958][166323] Avg episode reward: [(0, '1287.329')]
[36m[2025-07-02 06:30:50,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10092544. Throughput: 0: 280.1. Samples: 10105520. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 06:30:50,972][166323] Avg episode reward: [(0, '1271.391')]
[36m[2025-07-02 06:30:55,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10092544. Throughput: 0: 281.9. Samples: 10107296. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 06:30:55,976][166323] Avg episode reward: [(0, '1296.834')]
[36m[2025-07-02 06:31:00,943][166323] Fps is (10 sec: 1643.2, 60 sec: 546.4, 300 sec: 277.7). Total num frames: 10108928. Throughput: 0: 280.2. Samples: 10108960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:31:00,943][166323] Avg episode reward: [(0, '1301.169')]
[36m[2025-07-02 06:31:05,960][166323] Fps is (10 sec: 1641.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10108928. Throughput: 0: 280.6. Samples: 10109872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:31:05,960][166323] Avg episode reward: [(0, '1257.709')]
[31m[35856578 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[35856578 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[35856578 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:31:10,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10108928. Throughput: 0: 282.4. Samples: 10111584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:31:10,965][166323] Avg episode reward: [(0, '1275.999')]
[36m[2025-07-02 06:31:15,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10108928. Throughput: 0: 283.0. Samples: 10113200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:31:15,956][166323] Avg episode reward: [(0, '1329.620')]
[36m[2025-07-02 06:31:20,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10108928. Throughput: 0: 282.7. Samples: 10114016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:31:20,973][166323] Avg episode reward: [(0, '1268.656')]
[36m[2025-07-02 06:31:25,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10108928. Throughput: 0: 280.5. Samples: 10115600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:31:25,946][166323] Avg episode reward: [(0, '1304.681')]
[31m[35874955 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[35874956 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[35874956 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:31:30,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10108928. Throughput: 0: 278.3. Samples: 10117152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:31:30,967][166323] Avg episode reward: [(0, '1276.336')]
[36m[2025-07-02 06:31:35,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10108928. Throughput: 0: 275.9. Samples: 10117936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:31:35,967][166323] Avg episode reward: [(0, '1274.677')]
[37m[1m[2025-07-02 06:31:36,046][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019736_10108928.pth...
[36m[2025-07-02 06:31:36,051][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019608_10043392.pth
[36m[2025-07-02 06:31:40,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10108928. Throughput: 0: 272.9. Samples: 10119568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:31:40,948][166323] Avg episode reward: [(0, '1312.347')]
[36m[2025-07-02 06:31:45,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10108928. Throughput: 0: 275.1. Samples: 10121344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:31:45,959][166323] Avg episode reward: [(0, '1300.962')]
[36m[2025-07-02 06:31:51,008][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10108928. Throughput: 0: 271.7. Samples: 10122112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:31:51,008][166323] Avg episode reward: [(0, '1292.355')]
[36m[2025-07-02 06:31:55,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10108928. Throughput: 0: 267.6. Samples: 10123632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:31:55,982][166323] Avg episode reward: [(0, '1300.339')]
[36m[2025-07-02 06:32:00,971][166323] Fps is (10 sec: 1644.4, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10125312. Throughput: 0: 269.8. Samples: 10125344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:32:00,971][166323] Avg episode reward: [(0, '1306.024')]
[36m[2025-07-02 06:32:05,983][166323] Fps is (10 sec: 1638.3, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10125312. Throughput: 0: 268.0. Samples: 10126080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:32:05,983][166323] Avg episode reward: [(0, '1316.376')]
[36m[2025-07-02 06:32:10,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10125312. Throughput: 0: 269.1. Samples: 10127712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:32:10,958][166323] Avg episode reward: [(0, '1276.863')]
[36m[2025-07-02 06:32:15,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10125312. Throughput: 0: 269.9. Samples: 10129296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:32:15,962][166323] Avg episode reward: [(0, '1251.308')]
[36m[2025-07-02 06:32:20,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10125312. Throughput: 0: 273.0. Samples: 10130224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:32:20,980][166323] Avg episode reward: [(0, '1193.763')]
[36m[2025-07-02 06:32:25,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 10125312. Throughput: 0: 272.8. Samples: 10131856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:32:25,995][166323] Avg episode reward: [(0, '1192.998')]
[36m[2025-07-02 06:32:30,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10125312. Throughput: 0: 269.6. Samples: 10133472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:32:30,948][166323] Avg episode reward: [(0, '1223.166')]
[36m[2025-07-02 06:32:35,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10125312. Throughput: 0: 269.7. Samples: 10134240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:32:35,982][166323] Avg episode reward: [(0, '1200.553')]
[36m[2025-07-02 06:32:40,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10125312. Throughput: 0: 275.3. Samples: 10136016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:32:40,969][166323] Avg episode reward: [(0, '1227.169')]
[36m[2025-07-02 06:32:45,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10125312. Throughput: 0: 275.5. Samples: 10137744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:32:45,987][166323] Avg episode reward: [(0, '1255.848')]
[36m[2025-07-02 06:32:50,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10125312. Throughput: 0: 277.1. Samples: 10138544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:32:50,971][166323] Avg episode reward: [(0, '1307.294')]
[36m[2025-07-02 06:32:55,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10125312. Throughput: 0: 276.2. Samples: 10140144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:32:55,973][166323] Avg episode reward: [(0, '1359.142')]
[31m[35965678 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[35965678 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[35965678 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:33:00,971][166323] Fps is (10 sec: 1638.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10141696. Throughput: 0: 277.3. Samples: 10141776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:33:00,971][166323] Avg episode reward: [(0, '1367.720')]
[36m[2025-07-02 06:33:05,953][166323] Fps is (10 sec: 1641.7, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10141696. Throughput: 0: 275.4. Samples: 10142608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:33:05,953][166323] Avg episode reward: [(0, '1358.369')]
[36m[2025-07-02 06:33:10,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10141696. Throughput: 0: 277.1. Samples: 10144320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:33:10,983][166323] Avg episode reward: [(0, '1329.989')]
[36m[2025-07-02 06:33:15,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10141696. Throughput: 0: 278.6. Samples: 10146016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:33:15,980][166323] Avg episode reward: [(0, '1333.988')]
[36m[2025-07-02 06:33:20,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10141696. Throughput: 0: 279.5. Samples: 10146816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:33:20,982][166323] Avg episode reward: [(0, '1307.347')]
[36m[2025-07-02 06:33:25,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10141696. Throughput: 0: 276.0. Samples: 10148432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:33:25,953][166323] Avg episode reward: [(0, '1252.691')]
[36m[2025-07-02 06:33:30,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10141696. Throughput: 0: 274.1. Samples: 10150080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:33:30,990][166323] Avg episode reward: [(0, '1172.616')]
[36m[2025-07-02 06:33:35,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10141696. Throughput: 0: 276.3. Samples: 10150976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:33:35,965][166323] Avg episode reward: [(0, '1171.168')]
[37m[1m[2025-07-02 06:33:36,051][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019800_10141696.pth...
[36m[2025-07-02 06:33:36,060][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019672_10076160.pth
[36m[2025-07-02 06:33:40,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10141696. Throughput: 0: 278.4. Samples: 10152672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:33:40,974][166323] Avg episode reward: [(0, '1222.127')]
[36m[2025-07-02 06:33:45,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10141696. Throughput: 0: 276.2. Samples: 10154208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:33:45,989][166323] Avg episode reward: [(0, '1237.510')]
[36m[2025-07-02 06:33:50,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10141696. Throughput: 0: 279.1. Samples: 10155168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:33:50,958][166323] Avg episode reward: [(0, '1186.085')]
[36m[2025-07-02 06:33:56,008][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10141696. Throughput: 0: 277.9. Samples: 10156832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:33:56,008][166323] Avg episode reward: [(0, '1261.146')]
[36m[2025-07-02 06:34:00,959][166323] Fps is (10 sec: 1638.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10158080. Throughput: 0: 277.1. Samples: 10158480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:34:00,959][166323] Avg episode reward: [(0, '1297.310')]
[36m[2025-07-02 06:34:05,996][166323] Fps is (10 sec: 1640.4, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10158080. Throughput: 0: 276.2. Samples: 10159248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:34:05,996][166323] Avg episode reward: [(0, '1306.922')]
[36m[2025-07-02 06:34:10,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10158080. Throughput: 0: 276.2. Samples: 10160864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:34:10,969][166323] Avg episode reward: [(0, '1266.810')]
[36m[2025-07-02 06:34:15,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10158080. Throughput: 0: 276.0. Samples: 10162496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:34:15,981][166323] Avg episode reward: [(0, '1268.077')]
[36m[2025-07-02 06:34:20,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10158080. Throughput: 0: 274.5. Samples: 10163328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:34:20,962][166323] Avg episode reward: [(0, '1309.779')]
[36m[2025-07-02 06:34:25,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10158080. Throughput: 0: 272.3. Samples: 10164928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:34:25,985][166323] Avg episode reward: [(0, '1313.931')]
[36m[2025-07-02 06:34:30,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10158080. Throughput: 0: 276.2. Samples: 10166640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:34:30,997][166323] Avg episode reward: [(0, '1278.904')]
[36m[2025-07-02 06:34:35,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10158080. Throughput: 0: 273.7. Samples: 10167488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:34:35,978][166323] Avg episode reward: [(0, '1308.894')]
[36m[2025-07-02 06:34:40,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10158080. Throughput: 0: 272.6. Samples: 10169088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:34:40,962][166323] Avg episode reward: [(0, '1316.330')]
[36m[2025-07-02 06:34:45,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10158080. Throughput: 0: 271.5. Samples: 10170704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:34:45,980][166323] Avg episode reward: [(0, '1305.990')]
[36m[2025-07-02 06:34:50,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10158080. Throughput: 0: 274.3. Samples: 10171584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:34:50,977][166323] Avg episode reward: [(0, '1271.425')]
[36m[2025-07-02 06:34:55,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10158080. Throughput: 0: 275.2. Samples: 10173248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:34:55,964][166323] Avg episode reward: [(0, '1204.934')]
[36m[2025-07-02 06:35:00,944][166323] Fps is (10 sec: 1643.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10174464. Throughput: 0: 272.2. Samples: 10174736. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:35:00,944][166323] Avg episode reward: [(0, '1223.447')]
[36m[2025-07-02 06:35:05,962][166323] Fps is (10 sec: 1638.7, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10174464. Throughput: 0: 270.6. Samples: 10175504. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:35:05,962][166323] Avg episode reward: [(0, '1271.043')]
[36m[2025-07-02 06:35:10,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10174464. Throughput: 0: 272.2. Samples: 10177168. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:35:10,948][166323] Avg episode reward: [(0, '1199.161')]
[36m[2025-07-02 06:35:16,006][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10174464. Throughput: 0: 271.6. Samples: 10178864. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:35:16,007][166323] Avg episode reward: [(0, '1122.245')]
[36m[2025-07-02 06:35:20,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10174464. Throughput: 0: 270.5. Samples: 10179664. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:35:20,984][166323] Avg episode reward: [(0, '1147.857')]
[36m[2025-07-02 06:35:25,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10174464. Throughput: 0: 269.2. Samples: 10181200. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:35:25,959][166323] Avg episode reward: [(0, '1190.948')]
[36m[2025-07-02 06:35:30,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10174464. Throughput: 0: 272.6. Samples: 10182960. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:35:30,946][166323] Avg episode reward: [(0, '1241.182')]
[36m[2025-07-02 06:35:35,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10174464. Throughput: 0: 270.2. Samples: 10183744. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:35:35,987][166323] Avg episode reward: [(0, '1182.086')]
[37m[1m[2025-07-02 06:35:36,041][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019864_10174464.pth...
[36m[2025-07-02 06:35:36,045][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019736_10108928.pth
[36m[2025-07-02 06:35:40,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10174464. Throughput: 0: 269.6. Samples: 10185376. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:35:40,946][166323] Avg episode reward: [(0, '1202.075')]
[36m[2025-07-02 06:35:45,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10174464. Throughput: 0: 274.4. Samples: 10187088. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:35:45,966][166323] Avg episode reward: [(0, '1296.111')]
[36m[2025-07-02 06:35:50,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10174464. Throughput: 0: 275.0. Samples: 10187888. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:35:50,990][166323] Avg episode reward: [(0, '1365.221')]
[36m[2025-07-02 06:35:55,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 10174464. Throughput: 0: 276.1. Samples: 10189600. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:35:55,980][166323] Avg episode reward: [(0, '1372.203')]
[36m[2025-07-02 06:36:01,001][166323] Fps is (10 sec: 1636.6, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 10190848. Throughput: 0: 274.2. Samples: 10191200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:36:01,002][166323] Avg episode reward: [(0, '1324.052')]
[36m[2025-07-02 06:36:05,970][166323] Fps is (10 sec: 1640.1, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10190848. Throughput: 0: 272.1. Samples: 10191904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:36:05,970][166323] Avg episode reward: [(0, '1291.891')]
[36m[2025-07-02 06:36:10,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10190848. Throughput: 0: 274.0. Samples: 10193536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:36:10,982][166323] Avg episode reward: [(0, '1309.999')]
[36m[2025-07-02 06:36:15,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10190848. Throughput: 0: 269.4. Samples: 10195088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:36:15,971][166323] Avg episode reward: [(0, '1318.288')]
[36m[2025-07-02 06:36:20,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10190848. Throughput: 0: 270.2. Samples: 10195904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:36:20,988][166323] Avg episode reward: [(0, '1244.875')]
[36m[2025-07-02 06:36:25,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10190848. Throughput: 0: 271.2. Samples: 10197584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:36:25,961][166323] Avg episode reward: [(0, '1222.244')]
[36m[2025-07-02 06:36:30,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10190848. Throughput: 0: 270.2. Samples: 10199248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:36:30,964][166323] Avg episode reward: [(0, '1209.894')]
[36m[2025-07-02 06:36:35,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10190848. Throughput: 0: 271.4. Samples: 10200096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:36:35,978][166323] Avg episode reward: [(0, '1239.400')]
[36m[2025-07-02 06:36:40,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10190848. Throughput: 0: 270.6. Samples: 10201776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:36:40,981][166323] Avg episode reward: [(0, '1297.500')]
[36m[2025-07-02 06:36:45,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10190848. Throughput: 0: 271.1. Samples: 10203392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:36:45,980][166323] Avg episode reward: [(0, '1241.306')]
[36m[2025-07-02 06:36:50,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10190848. Throughput: 0: 274.0. Samples: 10204240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:36:50,987][166323] Avg episode reward: [(0, '1260.093')]
[36m[2025-07-02 06:36:55,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.1). Total num frames: 10190848. Throughput: 0: 275.2. Samples: 10205920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:36:55,981][166323] Avg episode reward: [(0, '1221.128')]
[36m[2025-07-02 06:37:00,965][166323] Fps is (10 sec: 1641.8, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10207232. Throughput: 0: 277.0. Samples: 10207552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:37:00,965][166323] Avg episode reward: [(0, '1246.891')]
[36m[2025-07-02 06:37:05,955][166323] Fps is (10 sec: 1642.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10207232. Throughput: 0: 279.3. Samples: 10208464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:37:05,955][166323] Avg episode reward: [(0, '1242.547')]
[36m[2025-07-02 06:37:11,003][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10207232. Throughput: 0: 277.8. Samples: 10210096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:37:11,004][166323] Avg episode reward: [(0, '1244.487')]
[36m[2025-07-02 06:37:15,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10207232. Throughput: 0: 274.7. Samples: 10211616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:37:15,991][166323] Avg episode reward: [(0, '1213.221')]
[36m[2025-07-02 06:37:20,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10207232. Throughput: 0: 273.1. Samples: 10212384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:37:20,965][166323] Avg episode reward: [(0, '1250.054')]
[36m[2025-07-02 06:37:25,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10207232. Throughput: 0: 270.2. Samples: 10213936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:37:25,984][166323] Avg episode reward: [(0, '1212.889')]
[31m[36236433 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[36236433 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[36236434 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:37:30,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10207232. Throughput: 0: 269.8. Samples: 10215536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:37:30,984][166323] Avg episode reward: [(0, '1233.941')]
[36m[2025-07-02 06:37:35,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10207232. Throughput: 0: 270.3. Samples: 10216400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:37:35,975][166323] Avg episode reward: [(0, '1207.272')]
[37m[1m[2025-07-02 06:37:36,042][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019928_10207232.pth...
[36m[2025-07-02 06:37:36,048][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019800_10141696.pth
[31m[36247898 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[36247898 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[36247899 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:37:40,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10207232. Throughput: 0: 271.3. Samples: 10218128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:37:40,981][166323] Avg episode reward: [(0, '1178.976')]
[36m[2025-07-02 06:37:45,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10207232. Throughput: 0: 272.4. Samples: 10219808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:37:45,966][166323] Avg episode reward: [(0, '1198.404')]
[36m[2025-07-02 06:37:50,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10207232. Throughput: 0: 270.4. Samples: 10220640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:37:50,978][166323] Avg episode reward: [(0, '1233.240')]
[31m[36259781 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[36259781 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[36259781 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:37:55,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 10207232. Throughput: 0: 270.1. Samples: 10222240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:37:55,969][166323] Avg episode reward: [(0, '1209.896')]
[36m[2025-07-02 06:38:01,012][166323] Fps is (10 sec: 1632.8, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 10223616. Throughput: 0: 272.2. Samples: 10223872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:38:01,012][166323] Avg episode reward: [(0, '1274.868')]
[31m[36273779 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[36273780 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[36273780 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:38:05,983][166323] Fps is (10 sec: 1636.2, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10223616. Throughput: 0: 271.5. Samples: 10224608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:38:05,983][166323] Avg episode reward: [(0, '1257.024')]
[36m[2025-07-02 06:38:10,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10223616. Throughput: 0: 275.2. Samples: 10226320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:38:10,979][166323] Avg episode reward: [(0, '1258.394')]
[36m[2025-07-02 06:38:15,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10223616. Throughput: 0: 272.9. Samples: 10227808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:38:15,958][166323] Avg episode reward: [(0, '1268.366')]
[36m[2025-07-02 06:38:20,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10223616. Throughput: 0: 270.2. Samples: 10228560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:38:20,985][166323] Avg episode reward: [(0, '1263.146')]
[36m[2025-07-02 06:38:25,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10223616. Throughput: 0: 266.7. Samples: 10230128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:38:25,972][166323] Avg episode reward: [(0, '1261.133')]
[36m[2025-07-02 06:38:30,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10223616. Throughput: 0: 268.2. Samples: 10231872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:38:30,946][166323] Avg episode reward: [(0, '1307.280')]
[36m[2025-07-02 06:38:35,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10223616. Throughput: 0: 270.7. Samples: 10232816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:38:35,954][166323] Avg episode reward: [(0, '1257.592')]
[36m[2025-07-02 06:38:40,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10223616. Throughput: 0: 273.9. Samples: 10234560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:38:40,952][166323] Avg episode reward: [(0, '1318.851')]
[36m[2025-07-02 06:38:45,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10223616. Throughput: 0: 272.6. Samples: 10236128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:38:45,976][166323] Avg episode reward: [(0, '1297.735')]
[36m[2025-07-02 06:38:50,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10223616. Throughput: 0: 275.3. Samples: 10236992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:38:50,961][166323] Avg episode reward: [(0, '1303.888')]
[36m[2025-07-02 06:38:56,001][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 10223616. Throughput: 0: 272.2. Samples: 10238576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:38:56,001][166323] Avg episode reward: [(0, '1276.556')]
[36m[2025-07-02 06:39:00,966][166323] Fps is (10 sec: 1637.5, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10240000. Throughput: 0: 273.4. Samples: 10240112. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 06:39:00,967][166323] Avg episode reward: [(0, '1274.871')]
[36m[2025-07-02 06:39:05,985][166323] Fps is (10 sec: 1641.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10240000. Throughput: 0: 277.0. Samples: 10241024. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 06:39:05,986][166323] Avg episode reward: [(0, '1294.229')]
[36m[2025-07-02 06:39:10,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10240000. Throughput: 0: 278.2. Samples: 10242640. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 06:39:10,946][166323] Avg episode reward: [(0, '1312.739')]
[36m[2025-07-02 06:39:15,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10240000. Throughput: 0: 277.7. Samples: 10244368. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 06:39:15,950][166323] Avg episode reward: [(0, '1302.614')]
[36m[2025-07-02 06:39:20,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10240000. Throughput: 0: 276.8. Samples: 10245280. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 06:39:20,988][166323] Avg episode reward: [(0, '1306.200')]
[36m[2025-07-02 06:39:25,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10240000. Throughput: 0: 275.1. Samples: 10246944. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 06:39:25,974][166323] Avg episode reward: [(0, '1309.715')]
[36m[2025-07-02 06:39:30,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10240000. Throughput: 0: 280.2. Samples: 10248736. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 06:39:30,966][166323] Avg episode reward: [(0, '1296.738')]
[36m[2025-07-02 06:39:35,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10240000. Throughput: 0: 277.9. Samples: 10249504. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 06:39:35,984][166323] Avg episode reward: [(0, '1341.099')]
[37m[1m[2025-07-02 06:39:36,041][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019992_10240000.pth...
[36m[2025-07-02 06:39:36,045][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019864_10174464.pth
[36m[2025-07-02 06:39:40,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10240000. Throughput: 0: 282.5. Samples: 10251280. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 06:39:40,974][166323] Avg episode reward: [(0, '1289.771')]
[36m[2025-07-02 06:39:45,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10240000. Throughput: 0: 287.4. Samples: 10253040. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 06:39:45,944][166323] Avg episode reward: [(0, '1266.219')]
[36m[2025-07-02 06:39:50,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10240000. Throughput: 0: 285.6. Samples: 10253872. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 06:39:50,976][166323] Avg episode reward: [(0, '1261.352')]
[36m[2025-07-02 06:39:55,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 222.1). Total num frames: 10240000. Throughput: 0: 286.9. Samples: 10255552. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 06:39:55,958][166323] Avg episode reward: [(0, '1218.108')]
[36m[2025-07-02 06:40:00,956][166323] Fps is (10 sec: 1641.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10256384. Throughput: 0: 280.1. Samples: 10256976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:40:00,956][166323] Avg episode reward: [(0, '1255.675')]
[36m[2025-07-02 06:40:05,981][166323] Fps is (10 sec: 1634.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10256384. Throughput: 0: 279.5. Samples: 10257856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:40:05,981][166323] Avg episode reward: [(0, '1195.778')]
[36m[2025-07-02 06:40:10,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10256384. Throughput: 0: 278.9. Samples: 10259488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:40:10,949][166323] Avg episode reward: [(0, '1219.657')]
[36m[2025-07-02 06:40:15,997][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 10256384. Throughput: 0: 275.7. Samples: 10261152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:40:15,997][166323] Avg episode reward: [(0, '1272.870')]
[36m[2025-07-02 06:40:20,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10256384. Throughput: 0: 276.3. Samples: 10261936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:40:20,985][166323] Avg episode reward: [(0, '1285.758')]
[36m[2025-07-02 06:40:25,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10256384. Throughput: 0: 272.3. Samples: 10263536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:40:25,985][166323] Avg episode reward: [(0, '1246.372')]
[36m[2025-07-02 06:40:31,010][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10256384. Throughput: 0: 266.6. Samples: 10265056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:40:31,010][166323] Avg episode reward: [(0, '1312.075')]
[36m[2025-07-02 06:40:35,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10256384. Throughput: 0: 268.4. Samples: 10265952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:40:35,984][166323] Avg episode reward: [(0, '1309.817')]
[36m[2025-07-02 06:40:40,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10256384. Throughput: 0: 267.3. Samples: 10267584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:40:40,963][166323] Avg episode reward: [(0, '1336.598')]
[36m[2025-07-02 06:40:45,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 10256384. Throughput: 0: 273.2. Samples: 10269280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:40:45,995][166323] Avg episode reward: [(0, '1304.979')]
[36m[2025-07-02 06:40:50,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10256384. Throughput: 0: 271.0. Samples: 10270048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:40:50,964][166323] Avg episode reward: [(0, '1268.282')]
[36m[2025-07-02 06:40:55,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 10256384. Throughput: 0: 269.8. Samples: 10271632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:40:55,960][166323] Avg episode reward: [(0, '1278.672')]
[33m[36448233 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[36448233 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.8638359904289246
[33mCrash Rate: 0.12493899464607239
[33mTimeout Rate: 0.011224987916648388 (navigation_task.py:265)
[33m[36448233 ms][navigation_task] - WARNING : 
[33mSuccesses: 1770
[33mCrashes : 256
[33mTimeouts: 23 (navigation_task.py:268)
[36m[2025-07-02 06:41:00,949][166323] Fps is (10 sec: 1641.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10272768. Throughput: 0: 267.3. Samples: 10273168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:41:00,949][166323] Avg episode reward: [(0, '1303.649')]
[36m[2025-07-02 06:41:05,992][166323] Fps is (10 sec: 1633.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10272768. Throughput: 0: 267.7. Samples: 10273984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:41:05,993][166323] Avg episode reward: [(0, '1261.660')]
[36m[2025-07-02 06:41:10,998][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 10272768. Throughput: 0: 268.0. Samples: 10275600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:41:10,999][166323] Avg episode reward: [(0, '1236.806')]
[36m[2025-07-02 06:41:15,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10272768. Throughput: 0: 269.8. Samples: 10277184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:41:15,955][166323] Avg episode reward: [(0, '1203.709')]
[36m[2025-07-02 06:41:20,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10272768. Throughput: 0: 268.8. Samples: 10278048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:41:20,981][166323] Avg episode reward: [(0, '1253.585')]
[36m[2025-07-02 06:41:25,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10272768. Throughput: 0: 268.4. Samples: 10279664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:41:25,971][166323] Avg episode reward: [(0, '1241.141')]
[36m[2025-07-02 06:41:30,995][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10272768. Throughput: 0: 268.4. Samples: 10281360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:41:30,995][166323] Avg episode reward: [(0, '1210.414')]
[36m[2025-07-02 06:41:35,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10272768. Throughput: 0: 267.3. Samples: 10282080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:41:35,974][166323] Avg episode reward: [(0, '1217.077')]
[37m[1m[2025-07-02 06:41:36,026][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000020056_10272768.pth...
[36m[2025-07-02 06:41:36,029][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019928_10207232.pth
[36m[2025-07-02 06:41:40,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10272768. Throughput: 0: 267.6. Samples: 10283680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:41:40,989][166323] Avg episode reward: [(0, '1164.336')]
[36m[2025-07-02 06:41:45,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10272768. Throughput: 0: 272.8. Samples: 10285456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:41:45,984][166323] Avg episode reward: [(0, '1150.668')]
[31m[36494549 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[36494549 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[36494550 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:41:50,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10272768. Throughput: 0: 273.6. Samples: 10286288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:41:50,967][166323] Avg episode reward: [(0, '1215.037')]
[36m[2025-07-02 06:41:55,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 10272768. Throughput: 0: 275.2. Samples: 10287968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:41:55,944][166323] Avg episode reward: [(0, '1243.763')]
[36m[2025-07-02 06:42:00,971][166323] Fps is (10 sec: 1637.7, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10289152. Throughput: 0: 274.0. Samples: 10289520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:42:00,971][166323] Avg episode reward: [(0, '1211.714')]
[36m[2025-07-02 06:42:05,944][166323] Fps is (10 sec: 1638.4, 60 sec: 273.3, 300 sec: 277.8). Total num frames: 10289152. Throughput: 0: 272.6. Samples: 10290304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:42:05,944][166323] Avg episode reward: [(0, '1251.591')]
[36m[2025-07-02 06:42:10,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10289152. Throughput: 0: 274.4. Samples: 10292016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:42:10,979][166323] Avg episode reward: [(0, '1324.004')]
[36m[2025-07-02 06:42:15,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10289152. Throughput: 0: 273.2. Samples: 10293648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:42:15,978][166323] Avg episode reward: [(0, '1332.857')]
[36m[2025-07-02 06:42:20,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10289152. Throughput: 0: 275.0. Samples: 10294448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:42:20,944][166323] Avg episode reward: [(0, '1357.721')]
[36m[2025-07-02 06:42:25,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10289152. Throughput: 0: 275.3. Samples: 10296064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:42:25,978][166323] Avg episode reward: [(0, '1330.256')]
[31m[36535071 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[36535072 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[36535072 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:42:30,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10289152. Throughput: 0: 276.7. Samples: 10297904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:42:30,966][166323] Avg episode reward: [(0, '1233.380')]
[36m[2025-07-02 06:42:35,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10289152. Throughput: 0: 276.9. Samples: 10298752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:42:35,983][166323] Avg episode reward: [(0, '1238.193')]
[36m[2025-07-02 06:42:41,003][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10289152. Throughput: 0: 279.1. Samples: 10300544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:42:41,003][166323] Avg episode reward: [(0, '1293.612')]
[31m[36553468 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[36553469 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[36553469 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:42:46,017][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10289152. Throughput: 0: 278.8. Samples: 10302080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:42:46,017][166323] Avg episode reward: [(0, '1210.586')]
[36m[2025-07-02 06:42:50,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10289152. Throughput: 0: 281.0. Samples: 10302960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:42:50,985][166323] Avg episode reward: [(0, '1231.730')]
[36m[2025-07-02 06:42:55,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 10289152. Throughput: 0: 279.3. Samples: 10304576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:42:55,944][166323] Avg episode reward: [(0, '1158.580')]
[36m[2025-07-02 06:43:00,979][166323] Fps is (10 sec: 1639.4, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10305536. Throughput: 0: 278.7. Samples: 10306192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:43:00,979][166323] Avg episode reward: [(0, '1145.963')]
[36m[2025-07-02 06:43:05,990][166323] Fps is (10 sec: 1630.8, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10305536. Throughput: 0: 281.7. Samples: 10307136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:43:05,991][166323] Avg episode reward: [(0, '1180.266')]
[36m[2025-07-02 06:43:10,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10305536. Throughput: 0: 286.3. Samples: 10308944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:43:10,961][166323] Avg episode reward: [(0, '1197.278')]
[36m[2025-07-02 06:43:15,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10305536. Throughput: 0: 280.8. Samples: 10310544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:43:15,987][166323] Avg episode reward: [(0, '1188.464')]
[36m[2025-07-02 06:43:20,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10305536. Throughput: 0: 280.9. Samples: 10311392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:43:20,985][166323] Avg episode reward: [(0, '1176.853')]
[36m[2025-07-02 06:43:26,018][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 10305536. Throughput: 0: 280.4. Samples: 10313168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:43:26,019][166323] Avg episode reward: [(0, '1193.617')]
[36m[2025-07-02 06:43:30,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10305536. Throughput: 0: 283.5. Samples: 10314832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:43:30,997][166323] Avg episode reward: [(0, '1271.096')]
[36m[2025-07-02 06:43:36,023][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 10305536. Throughput: 0: 282.8. Samples: 10315696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:43:36,023][166323] Avg episode reward: [(0, '1201.521')]
[37m[1m[2025-07-02 06:43:36,113][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000020120_10305536.pth...
[36m[2025-07-02 06:43:36,118][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000019992_10240000.pth
[31m[36608306 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[36608307 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[36608307 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:43:40,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10305536. Throughput: 0: 284.0. Samples: 10317360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:43:40,961][166323] Avg episode reward: [(0, '1147.851')]
[36m[2025-07-02 06:43:45,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10305536. Throughput: 0: 286.0. Samples: 10319056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:43:45,960][166323] Avg episode reward: [(0, '1134.478')]
[36m[2025-07-02 06:43:50,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10305536. Throughput: 0: 282.7. Samples: 10319856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:43:50,979][166323] Avg episode reward: [(0, '1207.298')]
[36m[2025-07-02 06:43:55,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 10305536. Throughput: 0: 277.7. Samples: 10321440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:43:55,959][166323] Avg episode reward: [(0, '1209.136')]
[36m[2025-07-02 06:44:01,015][166323] Fps is (10 sec: 1632.5, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10321920. Throughput: 0: 276.1. Samples: 10322976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:44:01,015][166323] Avg episode reward: [(0, '1249.635')]
[36m[2025-07-02 06:44:05,980][166323] Fps is (10 sec: 1634.9, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10321920. Throughput: 0: 276.3. Samples: 10323824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:44:05,980][166323] Avg episode reward: [(0, '1223.667')]
[36m[2025-07-02 06:44:11,000][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 10321920. Throughput: 0: 276.4. Samples: 10325600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:44:11,001][166323] Avg episode reward: [(0, '1278.876')]
[36m[2025-07-02 06:44:15,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10321920. Throughput: 0: 277.9. Samples: 10327328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:44:15,962][166323] Avg episode reward: [(0, '1243.654')]
[36m[2025-07-02 06:44:20,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10321920. Throughput: 0: 275.6. Samples: 10328080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:44:20,957][166323] Avg episode reward: [(0, '1235.688')]
[36m[2025-07-02 06:44:25,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10321920. Throughput: 0: 275.1. Samples: 10329744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:44:25,979][166323] Avg episode reward: [(0, '1234.694')]
[36m[2025-07-02 06:44:30,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10321920. Throughput: 0: 274.0. Samples: 10331392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:44:30,975][166323] Avg episode reward: [(0, '1203.716')]
[31m[36663528 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[36663529 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[36663529 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:44:35,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10321920. Throughput: 0: 274.4. Samples: 10332208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:44:35,988][166323] Avg episode reward: [(0, '1173.122')]
[36m[2025-07-02 06:44:40,998][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 10321920. Throughput: 0: 277.1. Samples: 10333920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:44:40,999][166323] Avg episode reward: [(0, '1179.479')]
[36m[2025-07-02 06:44:45,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10321920. Throughput: 0: 280.1. Samples: 10335568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:44:45,966][166323] Avg episode reward: [(0, '1180.638')]
[36m[2025-07-02 06:44:50,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10321920. Throughput: 0: 280.6. Samples: 10336448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:44:50,970][166323] Avg episode reward: [(0, '1261.121')]
[36m[2025-07-02 06:44:55,943][166323] Fps is (10 sec: 1642.0, 60 sec: 546.3, 300 sec: 277.7). Total num frames: 10338304. Throughput: 0: 282.7. Samples: 10338304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:44:55,943][166323] Avg episode reward: [(0, '1269.552')]
[31m[36688966 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[36688966 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[36688966 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:45:00,959][166323] Fps is (10 sec: 1640.3, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10338304. Throughput: 0: 282.0. Samples: 10340016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:45:00,959][166323] Avg episode reward: [(0, '1227.638')]
[36m[2025-07-02 06:45:05,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 10338304. Throughput: 0: 282.4. Samples: 10340800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:45:05,999][166323] Avg episode reward: [(0, '1276.392')]
[31m[36697912 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[36697912 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[36697912 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:45:10,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10338304. Throughput: 0: 283.0. Samples: 10342480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:45:10,980][166323] Avg episode reward: [(0, '1246.529')]
[36m[2025-07-02 06:45:15,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10338304. Throughput: 0: 286.6. Samples: 10344288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:45:15,970][166323] Avg episode reward: [(0, '1206.307')]
[36m[2025-07-02 06:45:20,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10338304. Throughput: 0: 288.1. Samples: 10345168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:45:20,974][166323] Avg episode reward: [(0, '1258.089')]
[31m[36711543 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[36711543 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[36711543 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:45:25,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10338304. Throughput: 0: 287.8. Samples: 10346864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:45:25,968][166323] Avg episode reward: [(0, '1217.493')]
[36m[2025-07-02 06:45:30,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10338304. Throughput: 0: 286.2. Samples: 10348448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:45:30,963][166323] Avg episode reward: [(0, '1195.955')]
[36m[2025-07-02 06:45:35,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10338304. Throughput: 0: 286.6. Samples: 10349344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:45:35,970][166323] Avg episode reward: [(0, '1161.881')]
[37m[1m[2025-07-02 06:45:36,020][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000020184_10338304.pth...
[36m[2025-07-02 06:45:36,024][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000020056_10272768.pth
[36m[2025-07-02 06:45:40,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10338304. Throughput: 0: 283.5. Samples: 10351072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:45:40,973][166323] Avg episode reward: [(0, '1118.195')]
[36m[2025-07-02 06:45:45,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10338304. Throughput: 0: 283.1. Samples: 10352752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:45:45,948][166323] Avg episode reward: [(0, '1133.939')]
[36m[2025-07-02 06:45:50,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10338304. Throughput: 0: 285.7. Samples: 10353648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:45:50,974][166323] Avg episode reward: [(0, '1035.255')]
[36m[2025-07-02 06:45:55,961][166323] Fps is (10 sec: 1636.3, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10354688. Throughput: 0: 284.2. Samples: 10355264. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 06:45:55,961][166323] Avg episode reward: [(0, '1082.629')]
[36m[2025-07-02 06:46:00,958][166323] Fps is (10 sec: 1641.1, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10354688. Throughput: 0: 277.8. Samples: 10356784. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 06:46:00,958][166323] Avg episode reward: [(0, '1172.496')]
[36m[2025-07-02 06:46:05,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10354688. Throughput: 0: 277.1. Samples: 10357632. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 06:46:05,961][166323] Avg episode reward: [(0, '1208.341')]
[36m[2025-07-02 06:46:10,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10354688. Throughput: 0: 277.1. Samples: 10359328. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 06:46:10,953][166323] Avg episode reward: [(0, '1295.490')]
[36m[2025-07-02 06:46:15,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10354688. Throughput: 0: 279.7. Samples: 10361040. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 06:46:15,989][166323] Avg episode reward: [(0, '1323.475')]
[31m[36766696 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[36766696 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[36766697 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:46:20,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10354688. Throughput: 0: 276.2. Samples: 10361776. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 06:46:20,988][166323] Avg episode reward: [(0, '1361.999')]
[36m[2025-07-02 06:46:25,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10354688. Throughput: 0: 274.9. Samples: 10363440. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 06:46:25,966][166323] Avg episode reward: [(0, '1387.261')]
[31m[36777158 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[36777158 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[36777158 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:46:30,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10354688. Throughput: 0: 275.1. Samples: 10365136. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 06:46:30,971][166323] Avg episode reward: [(0, '1330.485')]
[36m[2025-07-02 06:46:36,013][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10354688. Throughput: 0: 275.3. Samples: 10366048. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 06:46:36,014][166323] Avg episode reward: [(0, '1312.415')]
[31m[36785916 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[36785917 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[36785917 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:46:40,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10354688. Throughput: 0: 276.9. Samples: 10367728. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 06:46:40,978][166323] Avg episode reward: [(0, '1290.557')]
[36m[2025-07-02 06:46:45,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10354688. Throughput: 0: 279.1. Samples: 10369344. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 06:46:45,963][166323] Avg episode reward: [(0, '1254.535')]
[31m[36795271 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[36795271 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[36795271 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:46:50,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10354688. Throughput: 0: 280.2. Samples: 10370240. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 06:46:50,950][166323] Avg episode reward: [(0, '1230.996')]
[36m[2025-07-02 06:46:55,980][166323] Fps is (10 sec: 1635.7, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10371072. Throughput: 0: 277.9. Samples: 10371840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:46:55,980][166323] Avg episode reward: [(0, '1198.083')]
[36m[2025-07-02 06:47:01,024][166323] Fps is (10 sec: 1626.3, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 10371072. Throughput: 0: 277.8. Samples: 10373552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:47:01,025][166323] Avg episode reward: [(0, '1212.317')]
[36m[2025-07-02 06:47:05,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10371072. Throughput: 0: 277.1. Samples: 10374240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:47:05,962][166323] Avg episode reward: [(0, '1253.374')]
[36m[2025-07-02 06:47:10,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10371072. Throughput: 0: 276.9. Samples: 10375904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:47:10,987][166323] Avg episode reward: [(0, '1228.496')]
[36m[2025-07-02 06:47:15,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10371072. Throughput: 0: 273.1. Samples: 10377424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:47:15,967][166323] Avg episode reward: [(0, '1247.875')]
[31m[36827053 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[36827053 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[36827053 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:47:20,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10371072. Throughput: 0: 272.1. Samples: 10378272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:47:20,946][166323] Avg episode reward: [(0, '1227.562')]
[36m[2025-07-02 06:47:26,009][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10371072. Throughput: 0: 269.7. Samples: 10379872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:47:26,009][166323] Avg episode reward: [(0, '1253.828')]
[36m[2025-07-02 06:47:30,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10371072. Throughput: 0: 268.8. Samples: 10381440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:47:30,964][166323] Avg episode reward: [(0, '1301.226')]
[36m[2025-07-02 06:47:35,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10371072. Throughput: 0: 266.1. Samples: 10382224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:47:35,985][166323] Avg episode reward: [(0, '1344.001')]
[37m[1m[2025-07-02 06:47:36,035][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000020248_10371072.pth...
[36m[2025-07-02 06:47:36,039][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000020120_10305536.pth
[36m[2025-07-02 06:47:40,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 10371072. Throughput: 0: 266.8. Samples: 10383840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:47:40,958][166323] Avg episode reward: [(0, '1325.125')]
[36m[2025-07-02 06:47:45,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10371072. Throughput: 0: 264.5. Samples: 10385440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:47:45,971][166323] Avg episode reward: [(0, '1365.217')]
[36m[2025-07-02 06:47:50,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10371072. Throughput: 0: 266.8. Samples: 10386240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:47:50,948][166323] Avg episode reward: [(0, '1377.238')]
[36m[2025-07-02 06:47:56,012][166323] Fps is (10 sec: 1631.7, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10387456. Throughput: 0: 264.0. Samples: 10387792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:47:56,012][166323] Avg episode reward: [(0, '1371.705')]
[31m[36867593 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[36867593 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[36867593 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:48:00,982][166323] Fps is (10 sec: 1632.7, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10387456. Throughput: 0: 262.3. Samples: 10389232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:48:00,983][166323] Avg episode reward: [(0, '1409.412')]
[36m[2025-07-02 06:48:05,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10387456. Throughput: 0: 261.3. Samples: 10390032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:48:05,951][166323] Avg episode reward: [(0, '1359.532')]
[31m[36878190 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[36878191 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[36878191 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:48:10,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10387456. Throughput: 0: 264.9. Samples: 10391776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:48:10,953][166323] Avg episode reward: [(0, '1337.400')]
[36m[2025-07-02 06:48:15,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10387456. Throughput: 0: 268.0. Samples: 10393504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:48:15,982][166323] Avg episode reward: [(0, '1276.667')]
[36m[2025-07-02 06:48:20,976][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10387456. Throughput: 0: 270.3. Samples: 10394384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:48:20,976][166323] Avg episode reward: [(0, '1285.303')]
[36m[2025-07-02 06:48:25,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10387456. Throughput: 0: 273.1. Samples: 10396128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:48:25,956][166323] Avg episode reward: [(0, '1275.152')]
[36m[2025-07-02 06:48:30,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 10387456. Throughput: 0: 273.6. Samples: 10397744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:48:30,944][166323] Avg episode reward: [(0, '1292.504')]
[31m[36901959 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[36901959 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[36901960 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:48:35,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10387456. Throughput: 0: 273.3. Samples: 10398544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:48:35,972][166323] Avg episode reward: [(0, '1298.494')]
[36m[2025-07-02 06:48:40,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10387456. Throughput: 0: 273.2. Samples: 10400080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:48:40,988][166323] Avg episode reward: [(0, '1298.948')]
[36m[2025-07-02 06:48:46,010][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10387456. Throughput: 0: 277.2. Samples: 10401712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:48:46,011][166323] Avg episode reward: [(0, '1304.924')]
[36m[2025-07-02 06:48:50,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10387456. Throughput: 0: 279.0. Samples: 10402592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:48:50,963][166323] Avg episode reward: [(0, '1357.179')]
[36m[2025-07-02 06:48:55,976][166323] Fps is (10 sec: 1644.1, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10403840. Throughput: 0: 275.8. Samples: 10404192. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 06:48:55,976][166323] Avg episode reward: [(0, '1328.094')]
[36m[2025-07-02 06:49:01,012][166323] Fps is (10 sec: 1630.3, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10403840. Throughput: 0: 274.7. Samples: 10405872. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 06:49:01,013][166323] Avg episode reward: [(0, '1335.823')]
[36m[2025-07-02 06:49:05,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10403840. Throughput: 0: 274.7. Samples: 10406752. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 06:49:05,992][166323] Avg episode reward: [(0, '1303.981')]
[36m[2025-07-02 06:49:10,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10403840. Throughput: 0: 274.8. Samples: 10408496. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 06:49:10,963][166323] Avg episode reward: [(0, '1315.773')]
[31m[36942607 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[36942607 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[36942607 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:49:15,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10403840. Throughput: 0: 273.7. Samples: 10410064. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 06:49:15,953][166323] Avg episode reward: [(0, '1288.001')]
[36m[2025-07-02 06:49:20,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10403840. Throughput: 0: 275.6. Samples: 10410944. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 06:49:20,970][166323] Avg episode reward: [(0, '1304.101')]
[36m[2025-07-02 06:49:25,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10403840. Throughput: 0: 277.6. Samples: 10412560. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 06:49:25,951][166323] Avg episode reward: [(0, '1289.249')]
[36m[2025-07-02 06:49:30,970][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10403840. Throughput: 0: 277.6. Samples: 10414192. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 06:49:30,970][166323] Avg episode reward: [(0, '1318.487')]
[36m[2025-07-02 06:49:35,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10403840. Throughput: 0: 279.0. Samples: 10415152. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 06:49:35,978][166323] Avg episode reward: [(0, '1269.924')]
[37m[1m[2025-07-02 06:49:36,029][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000020312_10403840.pth...
[36m[2025-07-02 06:49:36,033][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000020184_10338304.pth
[36m[2025-07-02 06:49:40,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10403840. Throughput: 0: 282.8. Samples: 10416912. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 06:49:40,956][166323] Avg episode reward: [(0, '1293.184')]
[36m[2025-07-02 06:49:45,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 10403840. Throughput: 0: 283.8. Samples: 10418624. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 06:49:45,948][166323] Avg episode reward: [(0, '1324.896')]
[36m[2025-07-02 06:49:50,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.1). Total num frames: 10403840. Throughput: 0: 283.2. Samples: 10419488. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 06:49:50,966][166323] Avg episode reward: [(0, '1338.692')]
[36m[2025-07-02 06:49:56,003][166323] Fps is (10 sec: 1629.4, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10420224. Throughput: 0: 281.7. Samples: 10421184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:49:56,003][166323] Avg episode reward: [(0, '1317.033')]
[36m[2025-07-02 06:50:00,950][166323] Fps is (10 sec: 1641.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10420224. Throughput: 0: 282.0. Samples: 10422752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:50:00,950][166323] Avg episode reward: [(0, '1354.556')]
[36m[2025-07-02 06:50:05,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10420224. Throughput: 0: 281.2. Samples: 10423600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:50:05,981][166323] Avg episode reward: [(0, '1299.836')]
[36m[2025-07-02 06:50:10,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10420224. Throughput: 0: 284.1. Samples: 10425344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:50:10,951][166323] Avg episode reward: [(0, '1357.849')]
[36m[2025-07-02 06:50:15,979][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10420224. Throughput: 0: 281.9. Samples: 10426880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:50:15,979][166323] Avg episode reward: [(0, '1287.473')]
[36m[2025-07-02 06:50:20,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10420224. Throughput: 0: 279.5. Samples: 10427728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:50:20,967][166323] Avg episode reward: [(0, '1235.605')]
[31m[37010592 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[37010592 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[37010592 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:50:26,003][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 10420224. Throughput: 0: 276.7. Samples: 10429376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:50:26,003][166323] Avg episode reward: [(0, '1205.742')]
[36m[2025-07-02 06:50:30,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10420224. Throughput: 0: 276.4. Samples: 10431072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:50:30,979][166323] Avg episode reward: [(0, '1222.767')]
[36m[2025-07-02 06:50:35,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10420224. Throughput: 0: 277.0. Samples: 10431952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:50:35,966][166323] Avg episode reward: [(0, '1233.987')]
[36m[2025-07-02 06:50:40,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10420224. Throughput: 0: 277.2. Samples: 10433648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:50:40,970][166323] Avg episode reward: [(0, '1276.305')]
[36m[2025-07-02 06:50:45,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10420224. Throughput: 0: 282.3. Samples: 10435456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:50:45,953][166323] Avg episode reward: [(0, '1248.117')]
[36m[2025-07-02 06:50:51,003][166323] Fps is (10 sec: 1633.0, 60 sec: 545.8, 300 sec: 277.7). Total num frames: 10436608. Throughput: 0: 282.2. Samples: 10436304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:50:51,003][166323] Avg episode reward: [(0, '1305.167')]
[36m[2025-07-02 06:50:55,993][166323] Fps is (10 sec: 1631.9, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10436608. Throughput: 0: 282.0. Samples: 10438048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:50:55,993][166323] Avg episode reward: [(0, '1321.720')]
[36m[2025-07-02 06:51:00,980][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10436608. Throughput: 0: 284.1. Samples: 10439664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:51:00,980][166323] Avg episode reward: [(0, '1285.890')]
[36m[2025-07-02 06:51:05,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10436608. Throughput: 0: 285.1. Samples: 10440560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:51:05,973][166323] Avg episode reward: [(0, '1280.386')]
[36m[2025-07-02 06:51:10,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10436608. Throughput: 0: 284.0. Samples: 10442144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:51:10,955][166323] Avg episode reward: [(0, '1317.260')]
[36m[2025-07-02 06:51:15,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10436608. Throughput: 0: 281.3. Samples: 10443728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:51:15,978][166323] Avg episode reward: [(0, '1312.161')]
[36m[2025-07-02 06:51:20,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10436608. Throughput: 0: 282.3. Samples: 10444656. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:51:20,964][166323] Avg episode reward: [(0, '1298.968')]
[36m[2025-07-02 06:51:25,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10436608. Throughput: 0: 280.7. Samples: 10446272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:51:25,943][166323] Avg episode reward: [(0, '1335.557')]
[36m[2025-07-02 06:51:30,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 10436608. Throughput: 0: 279.9. Samples: 10448048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:51:30,947][166323] Avg episode reward: [(0, '1333.384')]
[36m[2025-07-02 06:51:35,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10436608. Throughput: 0: 279.1. Samples: 10448848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:51:35,944][166323] Avg episode reward: [(0, '1358.692')]
[37m[1m[2025-07-02 06:51:36,002][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000020376_10436608.pth...
[36m[2025-07-02 06:51:36,006][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000020248_10371072.pth
[36m[2025-07-02 06:51:40,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10436608. Throughput: 0: 278.5. Samples: 10450576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:51:40,975][166323] Avg episode reward: [(0, '1308.935')]
[36m[2025-07-02 06:51:45,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10436608. Throughput: 0: 281.0. Samples: 10452304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:51:45,966][166323] Avg episode reward: [(0, '1270.488')]
[36m[2025-07-02 06:51:50,975][166323] Fps is (10 sec: 1638.3, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10452992. Throughput: 0: 277.0. Samples: 10453024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:51:50,975][166323] Avg episode reward: [(0, '1292.030')]
[36m[2025-07-02 06:51:55,959][166323] Fps is (10 sec: 1639.5, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 10452992. Throughput: 0: 277.7. Samples: 10454640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:51:55,959][166323] Avg episode reward: [(0, '1298.361')]
[36m[2025-07-02 06:52:00,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10452992. Throughput: 0: 280.6. Samples: 10456352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:52:00,969][166323] Avg episode reward: [(0, '1281.443')]
[31m[37110991 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[37110992 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[37110992 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:52:05,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10452992. Throughput: 0: 277.5. Samples: 10457152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:52:05,989][166323] Avg episode reward: [(0, '1296.493')]
[36m[2025-07-02 06:52:10,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10452992. Throughput: 0: 278.3. Samples: 10458800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:52:10,964][166323] Avg episode reward: [(0, '1237.462')]
[36m[2025-07-02 06:52:15,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10452992. Throughput: 0: 276.4. Samples: 10460496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:52:15,983][166323] Avg episode reward: [(0, '1309.174')]
[33m[37128998 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[37128998 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.85693359375
[33mCrash Rate: 0.130859375
[33mTimeout Rate: 0.01220703125 (navigation_task.py:265)
[33m[37128998 ms][navigation_task] - WARNING : 
[33mSuccesses: 1755
[33mCrashes : 268
[33mTimeouts: 25 (navigation_task.py:268)
[36m[2025-07-02 06:52:20,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10452992. Throughput: 0: 278.9. Samples: 10461408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:52:20,977][166323] Avg episode reward: [(0, '1293.337')]
[36m[2025-07-02 06:52:25,980][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10452992. Throughput: 0: 274.5. Samples: 10462928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:52:25,980][166323] Avg episode reward: [(0, '1277.449')]
[36m[2025-07-02 06:52:30,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10452992. Throughput: 0: 277.5. Samples: 10464784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:52:30,946][166323] Avg episode reward: [(0, '1242.208')]
[36m[2025-07-02 06:52:35,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10452992. Throughput: 0: 280.1. Samples: 10465632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:52:35,982][166323] Avg episode reward: [(0, '1240.866')]
[36m[2025-07-02 06:52:40,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10452992. Throughput: 0: 280.7. Samples: 10467280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:52:40,991][166323] Avg episode reward: [(0, '1238.301')]
[36m[2025-07-02 06:52:45,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10452992. Throughput: 0: 278.2. Samples: 10468864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:52:45,950][166323] Avg episode reward: [(0, '1287.474')]
[36m[2025-07-02 06:52:50,966][166323] Fps is (10 sec: 1642.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10469376. Throughput: 0: 277.5. Samples: 10469632. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 06:52:50,966][166323] Avg episode reward: [(0, '1303.412')]
[36m[2025-07-02 06:52:55,950][166323] Fps is (10 sec: 1638.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10469376. Throughput: 0: 280.3. Samples: 10471408. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 06:52:55,950][166323] Avg episode reward: [(0, '1273.918')]
[36m[2025-07-02 06:53:00,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10469376. Throughput: 0: 282.9. Samples: 10473216. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 06:53:00,946][166323] Avg episode reward: [(0, '1300.281')]
[36m[2025-07-02 06:53:05,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10469376. Throughput: 0: 281.5. Samples: 10474080. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 06:53:05,994][166323] Avg episode reward: [(0, '1310.070')]
[36m[2025-07-02 06:53:10,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10469376. Throughput: 0: 282.4. Samples: 10475632. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 06:53:10,962][166323] Avg episode reward: [(0, '1312.823')]
[36m[2025-07-02 06:53:15,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10469376. Throughput: 0: 279.8. Samples: 10477376. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 06:53:15,946][166323] Avg episode reward: [(0, '1316.194')]
[36m[2025-07-02 06:53:21,018][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 10469376. Throughput: 0: 278.9. Samples: 10478192. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 06:53:21,018][166323] Avg episode reward: [(0, '1302.631')]
[36m[2025-07-02 06:53:25,995][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 10469376. Throughput: 0: 277.7. Samples: 10479776. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 06:53:25,996][166323] Avg episode reward: [(0, '1280.330')]
[31m[37196587 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[37196587 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[37196587 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:53:30,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10469376. Throughput: 0: 279.2. Samples: 10481440. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 06:53:30,989][166323] Avg episode reward: [(0, '1265.467')]
[36m[2025-07-02 06:53:35,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10469376. Throughput: 0: 280.0. Samples: 10482240. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 06:53:35,989][166323] Avg episode reward: [(0, '1273.913')]
[37m[1m[2025-07-02 06:53:36,040][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000020440_10469376.pth...
[36m[2025-07-02 06:53:36,045][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000020312_10403840.pth
[36m[2025-07-02 06:53:40,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10469376. Throughput: 0: 278.5. Samples: 10483952. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 06:53:40,998][166323] Avg episode reward: [(0, '1269.076')]
[36m[2025-07-02 06:53:46,042][166323] Fps is (10 sec: 1629.7, 60 sec: 545.3, 300 sec: 333.1). Total num frames: 10485760. Throughput: 0: 276.7. Samples: 10485696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:53:46,043][166323] Avg episode reward: [(0, '1284.516')]
[36m[2025-07-02 06:53:50,961][166323] Fps is (10 sec: 1644.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10485760. Throughput: 0: 274.7. Samples: 10486432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:53:50,961][166323] Avg episode reward: [(0, '1225.256')]
[36m[2025-07-02 06:53:55,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10485760. Throughput: 0: 278.4. Samples: 10488160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:53:55,962][166323] Avg episode reward: [(0, '1262.497')]
[36m[2025-07-02 06:54:00,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10485760. Throughput: 0: 277.6. Samples: 10489872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:54:00,956][166323] Avg episode reward: [(0, '1274.110')]
[36m[2025-07-02 06:54:05,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10485760. Throughput: 0: 280.0. Samples: 10490784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:54:05,982][166323] Avg episode reward: [(0, '1307.354')]
[36m[2025-07-02 06:54:11,017][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 10485760. Throughput: 0: 280.0. Samples: 10492384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:54:11,018][166323] Avg episode reward: [(0, '1288.206')]
[36m[2025-07-02 06:54:15,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10485760. Throughput: 0: 275.9. Samples: 10493856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:54:15,994][166323] Avg episode reward: [(0, '1275.384')]
[36m[2025-07-02 06:54:20,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 10485760. Throughput: 0: 276.5. Samples: 10494672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:54:20,950][166323] Avg episode reward: [(0, '1274.518')]
[36m[2025-07-02 06:54:26,011][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10485760. Throughput: 0: 276.9. Samples: 10496416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:54:26,011][166323] Avg episode reward: [(0, '1279.326')]
[36m[2025-07-02 06:54:30,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10485760. Throughput: 0: 272.8. Samples: 10497952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:54:30,964][166323] Avg episode reward: [(0, '1304.268')]
[36m[2025-07-02 06:54:35,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10485760. Throughput: 0: 273.2. Samples: 10498736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:54:35,991][166323] Avg episode reward: [(0, '1329.047')]
[36m[2025-07-02 06:54:40,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10485760. Throughput: 0: 270.8. Samples: 10500352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:54:40,985][166323] Avg episode reward: [(0, '1270.382')]
[36m[2025-07-02 06:54:45,981][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 277.7). Total num frames: 10485760. Throughput: 0: 269.4. Samples: 10502000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:54:45,981][166323] Avg episode reward: [(0, '1254.591')]
[36m[2025-07-02 06:54:51,018][166323] Fps is (10 sec: 1632.9, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 10502144. Throughput: 0: 266.1. Samples: 10502768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:54:51,018][166323] Avg episode reward: [(0, '1307.091')]
[36m[2025-07-02 06:54:56,014][166323] Fps is (10 sec: 1633.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 10502144. Throughput: 0: 266.0. Samples: 10504352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:54:56,015][166323] Avg episode reward: [(0, '1334.984')]
[31m[37287146 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[37287147 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[37287147 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:55:00,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10502144. Throughput: 0: 270.3. Samples: 10506016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:55:00,982][166323] Avg episode reward: [(0, '1320.640')]
[36m[2025-07-02 06:55:05,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10502144. Throughput: 0: 273.1. Samples: 10506960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:55:05,950][166323] Avg episode reward: [(0, '1284.005')]
[36m[2025-07-02 06:55:11,003][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10502144. Throughput: 0: 271.0. Samples: 10508608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:55:11,004][166323] Avg episode reward: [(0, '1264.060')]
[36m[2025-07-02 06:55:16,007][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10502144. Throughput: 0: 272.1. Samples: 10510208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:55:16,007][166323] Avg episode reward: [(0, '1307.414')]
[36m[2025-07-02 06:55:20,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10502144. Throughput: 0: 271.1. Samples: 10510928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:55:20,968][166323] Avg episode reward: [(0, '1318.576')]
[36m[2025-07-02 06:55:26,028][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 10502144. Throughput: 0: 272.4. Samples: 10512624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:55:26,029][166323] Avg episode reward: [(0, '1271.521')]
[36m[2025-07-02 06:55:30,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10502144. Throughput: 0: 272.5. Samples: 10514256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:55:30,960][166323] Avg episode reward: [(0, '1275.886')]
[36m[2025-07-02 06:55:35,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10502144. Throughput: 0: 274.1. Samples: 10515088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:55:35,966][166323] Avg episode reward: [(0, '1289.078')]
[37m[1m[2025-07-02 06:55:36,028][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000020504_10502144.pth...
[36m[2025-07-02 06:55:36,036][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000020376_10436608.pth
[36m[2025-07-02 06:55:40,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10502144. Throughput: 0: 272.9. Samples: 10516624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:55:40,985][166323] Avg episode reward: [(0, '1275.022')]
[36m[2025-07-02 06:55:45,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 10502144. Throughput: 0: 272.5. Samples: 10518272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:55:45,966][166323] Avg episode reward: [(0, '1293.236')]
[36m[2025-07-02 06:55:50,953][166323] Fps is (10 sec: 1643.6, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 10518528. Throughput: 0: 267.7. Samples: 10519008. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:55:50,953][166323] Avg episode reward: [(0, '1274.559')]
[36m[2025-07-02 06:55:55,992][166323] Fps is (10 sec: 1634.1, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10518528. Throughput: 0: 266.7. Samples: 10520608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:55:55,993][166323] Avg episode reward: [(0, '1285.128')]
[36m[2025-07-02 06:56:00,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10518528. Throughput: 0: 272.6. Samples: 10522464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:56:00,962][166323] Avg episode reward: [(0, '1237.935')]
[36m[2025-07-02 06:56:05,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10518528. Throughput: 0: 275.5. Samples: 10523328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:56:05,971][166323] Avg episode reward: [(0, '1226.170')]
[36m[2025-07-02 06:56:10,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10518528. Throughput: 0: 275.1. Samples: 10524992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:56:10,990][166323] Avg episode reward: [(0, '1210.825')]
[36m[2025-07-02 06:56:15,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10518528. Throughput: 0: 277.6. Samples: 10526752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:56:15,976][166323] Avg episode reward: [(0, '1213.651')]
[36m[2025-07-02 06:56:20,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10518528. Throughput: 0: 275.8. Samples: 10527504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:56:20,984][166323] Avg episode reward: [(0, '1210.877')]
[36m[2025-07-02 06:56:25,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 10518528. Throughput: 0: 277.9. Samples: 10529120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:56:25,956][166323] Avg episode reward: [(0, '1199.571')]
[36m[2025-07-02 06:56:31,003][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 10518528. Throughput: 0: 277.5. Samples: 10530768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:56:31,004][166323] Avg episode reward: [(0, '1195.540')]
[36m[2025-07-02 06:56:35,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10518528. Throughput: 0: 280.0. Samples: 10531616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:56:35,989][166323] Avg episode reward: [(0, '1249.153')]
[31m[37388511 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[37388511 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[37388512 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:56:40,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10518528. Throughput: 0: 283.3. Samples: 10533344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:56:40,947][166323] Avg episode reward: [(0, '1289.551')]
[36m[2025-07-02 06:56:45,948][166323] Fps is (10 sec: 1645.1, 60 sec: 546.3, 300 sec: 277.7). Total num frames: 10534912. Throughput: 0: 277.4. Samples: 10534944. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:56:45,948][166323] Avg episode reward: [(0, '1291.109')]
[36m[2025-07-02 06:56:50,970][166323] Fps is (10 sec: 1634.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10534912. Throughput: 0: 275.2. Samples: 10535712. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:56:50,970][166323] Avg episode reward: [(0, '1310.917')]
[36m[2025-07-02 06:56:55,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10534912. Throughput: 0: 270.9. Samples: 10537184. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:56:55,990][166323] Avg episode reward: [(0, '1338.626')]
[36m[2025-07-02 06:57:00,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10534912. Throughput: 0: 268.4. Samples: 10538832. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:57:00,976][166323] Avg episode reward: [(0, '1389.695')]
[31m[37409895 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[37409896 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[37409896 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:57:05,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10534912. Throughput: 0: 269.7. Samples: 10539632. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:57:05,957][166323] Avg episode reward: [(0, '1377.283')]
[31m[37416088 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[37416088 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[37416088 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:57:10,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10534912. Throughput: 0: 269.1. Samples: 10541232. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:57:10,963][166323] Avg episode reward: [(0, '1342.763')]
[36m[2025-07-02 06:57:15,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10534912. Throughput: 0: 269.7. Samples: 10542896. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:57:15,977][166323] Avg episode reward: [(0, '1326.963')]
[36m[2025-07-02 06:57:20,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10534912. Throughput: 0: 269.7. Samples: 10543744. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:57:20,951][166323] Avg episode reward: [(0, '1347.331')]
[36m[2025-07-02 06:57:25,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10534912. Throughput: 0: 270.2. Samples: 10545504. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:57:25,951][166323] Avg episode reward: [(0, '1330.887')]
[36m[2025-07-02 06:57:30,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10534912. Throughput: 0: 271.9. Samples: 10547184. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:57:30,961][166323] Avg episode reward: [(0, '1207.716')]
[36m[2025-07-02 06:57:35,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10534912. Throughput: 0: 274.2. Samples: 10548048. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:57:35,952][166323] Avg episode reward: [(0, '1232.831')]
[37m[1m[2025-07-02 06:57:36,003][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000020568_10534912.pth...
[36m[2025-07-02 06:57:36,007][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000020440_10469376.pth
[36m[2025-07-02 06:57:40,978][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10534912. Throughput: 0: 279.9. Samples: 10549776. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 06:57:40,978][166323] Avg episode reward: [(0, '1229.126')]
[31m[37452593 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[37452594 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[37452594 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:57:45,994][166323] Fps is (10 sec: 1631.5, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10551296. Throughput: 0: 277.6. Samples: 10551328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:57:45,994][166323] Avg episode reward: [(0, '1223.931')]
[36m[2025-07-02 06:57:50,955][166323] Fps is (10 sec: 1642.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10551296. Throughput: 0: 277.3. Samples: 10552112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:57:50,955][166323] Avg episode reward: [(0, '1207.905')]
[36m[2025-07-02 06:57:56,009][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 10551296. Throughput: 0: 279.9. Samples: 10553840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:57:56,009][166323] Avg episode reward: [(0, '1234.153')]
[36m[2025-07-02 06:58:00,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10551296. Throughput: 0: 281.2. Samples: 10555552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:58:00,980][166323] Avg episode reward: [(0, '1221.320')]
[36m[2025-07-02 06:58:05,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10551296. Throughput: 0: 279.8. Samples: 10556336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:58:05,952][166323] Avg episode reward: [(0, '1290.684')]
[36m[2025-07-02 06:58:10,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10551296. Throughput: 0: 276.9. Samples: 10557968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:58:10,963][166323] Avg episode reward: [(0, '1229.983')]
[36m[2025-07-02 06:58:15,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 10551296. Throughput: 0: 274.5. Samples: 10559536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:58:15,957][166323] Avg episode reward: [(0, '1207.407')]
[36m[2025-07-02 06:58:20,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10551296. Throughput: 0: 272.8. Samples: 10560336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:58:20,988][166323] Avg episode reward: [(0, '1289.062')]
[36m[2025-07-02 06:58:25,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10551296. Throughput: 0: 271.0. Samples: 10561968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:58:25,961][166323] Avg episode reward: [(0, '1282.379')]
[36m[2025-07-02 06:58:31,015][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 10551296. Throughput: 0: 272.9. Samples: 10563616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:58:31,015][166323] Avg episode reward: [(0, '1281.237')]
[31m[37504394 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[37504394 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[37504395 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:58:35,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10551296. Throughput: 0: 272.5. Samples: 10564384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:58:35,990][166323] Avg episode reward: [(0, '1279.574')]
[36m[2025-07-02 06:58:40,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 10551296. Throughput: 0: 269.4. Samples: 10565952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:58:40,970][166323] Avg episode reward: [(0, '1312.688')]
[31m[37510236 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[37510236 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[37510236 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:58:45,949][166323] Fps is (10 sec: 1645.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10567680. Throughput: 0: 269.0. Samples: 10567648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:58:45,950][166323] Avg episode reward: [(0, '1299.999')]
[36m[2025-07-02 06:58:51,009][166323] Fps is (10 sec: 1632.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 10567680. Throughput: 0: 268.5. Samples: 10568432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:58:51,009][166323] Avg episode reward: [(0, '1332.703')]
[36m[2025-07-02 06:58:55,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10567680. Throughput: 0: 271.3. Samples: 10570176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:58:55,969][166323] Avg episode reward: [(0, '1278.547')]
[36m[2025-07-02 06:59:00,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10567680. Throughput: 0: 271.9. Samples: 10571776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:59:00,977][166323] Avg episode reward: [(0, '1315.597')]
[36m[2025-07-02 06:59:05,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10567680. Throughput: 0: 272.2. Samples: 10572576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:59:05,961][166323] Avg episode reward: [(0, '1298.084')]
[36m[2025-07-02 06:59:10,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10567680. Throughput: 0: 274.1. Samples: 10574304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:59:10,958][166323] Avg episode reward: [(0, '1319.540')]
[36m[2025-07-02 06:59:15,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10567680. Throughput: 0: 276.6. Samples: 10576048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:59:15,965][166323] Avg episode reward: [(0, '1312.628')]
[31m[37547751 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[37547752 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[37547752 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 06:59:20,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10567680. Throughput: 0: 277.6. Samples: 10576864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:59:20,953][166323] Avg episode reward: [(0, '1326.922')]
[36m[2025-07-02 06:59:25,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10567680. Throughput: 0: 277.4. Samples: 10578432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:59:25,963][166323] Avg episode reward: [(0, '1307.321')]
[36m[2025-07-02 06:59:31,009][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10567680. Throughput: 0: 276.6. Samples: 10580112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:59:31,010][166323] Avg episode reward: [(0, '1283.964')]
[36m[2025-07-02 06:59:35,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10567680. Throughput: 0: 275.9. Samples: 10580832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:59:35,956][166323] Avg episode reward: [(0, '1324.386')]
[37m[1m[2025-07-02 06:59:36,006][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000020632_10567680.pth...
[36m[2025-07-02 06:59:36,010][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000020504_10502144.pth
[36m[2025-07-02 06:59:41,000][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10567680. Throughput: 0: 274.3. Samples: 10582528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 06:59:41,001][166323] Avg episode reward: [(0, '1337.879')]
[36m[2025-07-02 06:59:45,981][166323] Fps is (10 sec: 1634.3, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10584064. Throughput: 0: 273.8. Samples: 10584096. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 06:59:45,981][166323] Avg episode reward: [(0, '1311.204')]
[36m[2025-07-02 06:59:50,962][166323] Fps is (10 sec: 1644.7, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10584064. Throughput: 0: 273.8. Samples: 10584896. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 06:59:50,962][166323] Avg episode reward: [(0, '1285.076')]
[36m[2025-07-02 06:59:55,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10584064. Throughput: 0: 273.8. Samples: 10586624. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 06:59:55,958][166323] Avg episode reward: [(0, '1264.633')]
[36m[2025-07-02 07:00:00,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10584064. Throughput: 0: 268.8. Samples: 10588144. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 07:00:00,959][166323] Avg episode reward: [(0, '1244.289')]
[36m[2025-07-02 07:00:05,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10584064. Throughput: 0: 269.0. Samples: 10588976. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 07:00:05,979][166323] Avg episode reward: [(0, '1214.968')]
[36m[2025-07-02 07:00:10,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10584064. Throughput: 0: 268.6. Samples: 10590528. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 07:00:10,995][166323] Avg episode reward: [(0, '1222.449')]
[31m[37602703 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[37602704 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[37602704 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:00:15,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10584064. Throughput: 0: 269.1. Samples: 10592208. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 07:00:15,953][166323] Avg episode reward: [(0, '1197.926')]
[36m[2025-07-02 07:00:20,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.8). Total num frames: 10584064. Throughput: 0: 272.3. Samples: 10593088. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 07:00:20,967][166323] Avg episode reward: [(0, '1207.108')]
[36m[2025-07-02 07:00:25,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10584064. Throughput: 0: 273.6. Samples: 10594832. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 07:00:25,970][166323] Avg episode reward: [(0, '1187.322')]
[36m[2025-07-02 07:00:30,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10584064. Throughput: 0: 273.4. Samples: 10596400. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 07:00:30,986][166323] Avg episode reward: [(0, '1217.042')]
[36m[2025-07-02 07:00:35,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10584064. Throughput: 0: 273.5. Samples: 10597200. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 07:00:35,950][166323] Avg episode reward: [(0, '1203.660')]
[36m[2025-07-02 07:00:40,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10584064. Throughput: 0: 268.9. Samples: 10598720. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 07:00:40,947][166323] Avg episode reward: [(0, '1187.359')]
[36m[2025-07-02 07:00:45,954][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 222.2). Total num frames: 10584064. Throughput: 0: 271.0. Samples: 10600336. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 07:00:45,954][166323] Avg episode reward: [(0, '1153.351')]
[36m[2025-07-02 07:00:51,008][166323] Fps is (10 sec: 1628.3, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10600448. Throughput: 0: 270.0. Samples: 10601136. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 07:00:51,008][166323] Avg episode reward: [(0, '1172.414')]
[36m[2025-07-02 07:00:55,966][166323] Fps is (10 sec: 1636.3, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10600448. Throughput: 0: 272.2. Samples: 10602768. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 07:00:55,966][166323] Avg episode reward: [(0, '1166.346')]
[36m[2025-07-02 07:01:00,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10600448. Throughput: 0: 268.5. Samples: 10604288. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 07:01:00,946][166323] Avg episode reward: [(0, '1180.229')]
[31m[37650963 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[37650964 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[37650964 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:01:05,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10600448. Throughput: 0: 267.8. Samples: 10605136. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 07:01:05,949][166323] Avg episode reward: [(0, '1202.100')]
[36m[2025-07-02 07:01:10,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10600448. Throughput: 0: 263.8. Samples: 10606704. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 07:01:10,975][166323] Avg episode reward: [(0, '1280.994')]
[36m[2025-07-02 07:01:15,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10600448. Throughput: 0: 266.2. Samples: 10608368. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 07:01:15,950][166323] Avg episode reward: [(0, '1292.639')]
[36m[2025-07-02 07:01:20,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10600448. Throughput: 0: 264.7. Samples: 10609120. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 07:01:20,979][166323] Avg episode reward: [(0, '1261.110')]
[36m[2025-07-02 07:01:25,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10600448. Throughput: 0: 270.0. Samples: 10610880. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 07:01:25,981][166323] Avg episode reward: [(0, '1283.978')]
[36m[2025-07-02 07:01:30,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10600448. Throughput: 0: 270.7. Samples: 10612528. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 07:01:30,988][166323] Avg episode reward: [(0, '1277.648')]
[36m[2025-07-02 07:01:36,018][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 10600448. Throughput: 0: 270.2. Samples: 10613296. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 07:01:36,018][166323] Avg episode reward: [(0, '1289.157')]
[37m[1m[2025-07-02 07:01:36,088][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000020696_10600448.pth...
[36m[2025-07-02 07:01:36,092][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000020568_10534912.pth
[36m[2025-07-02 07:01:40,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 10600448. Throughput: 0: 270.8. Samples: 10614960. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 07:01:40,989][166323] Avg episode reward: [(0, '1274.296')]
[36m[2025-07-02 07:01:45,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 10600448. Throughput: 0: 271.3. Samples: 10616496. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 07:01:45,944][166323] Avg episode reward: [(0, '1258.927')]
[36m[2025-07-02 07:01:50,964][166323] Fps is (10 sec: 1642.5, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10616832. Throughput: 0: 268.4. Samples: 10617216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:01:50,964][166323] Avg episode reward: [(0, '1245.197')]
[36m[2025-07-02 07:01:55,952][166323] Fps is (10 sec: 1637.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10616832. Throughput: 0: 270.0. Samples: 10618848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:01:55,952][166323] Avg episode reward: [(0, '1263.089')]
[36m[2025-07-02 07:02:00,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10616832. Throughput: 0: 271.2. Samples: 10620576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:02:00,958][166323] Avg episode reward: [(0, '1274.793')]
[36m[2025-07-02 07:02:05,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10616832. Throughput: 0: 274.6. Samples: 10621472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:02:05,955][166323] Avg episode reward: [(0, '1272.327')]
[36m[2025-07-02 07:02:10,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10616832. Throughput: 0: 271.2. Samples: 10623088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:02:10,999][166323] Avg episode reward: [(0, '1290.471')]
[31m[37723467 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[37723468 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[37723468 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:02:15,979][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10616832. Throughput: 0: 270.3. Samples: 10624688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:02:15,980][166323] Avg episode reward: [(0, '1279.705')]
[36m[2025-07-02 07:02:20,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10616832. Throughput: 0: 273.7. Samples: 10625600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:02:20,980][166323] Avg episode reward: [(0, '1327.665')]
[36m[2025-07-02 07:02:25,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10616832. Throughput: 0: 273.8. Samples: 10627280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:02:25,993][166323] Avg episode reward: [(0, '1337.661')]
[36m[2025-07-02 07:02:30,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10616832. Throughput: 0: 275.1. Samples: 10628880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:02:30,958][166323] Avg episode reward: [(0, '1319.528')]
[36m[2025-07-02 07:02:35,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10616832. Throughput: 0: 276.8. Samples: 10629680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:02:35,996][166323] Avg episode reward: [(0, '1301.449')]
[36m[2025-07-02 07:02:40,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 10616832. Throughput: 0: 277.2. Samples: 10631328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:02:40,979][166323] Avg episode reward: [(0, '1186.800')]
[31m[37754312 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[37754313 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[37754313 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:02:45,980][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 10616832. Throughput: 0: 276.1. Samples: 10633008. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:02:45,981][166323] Avg episode reward: [(0, '1201.487')]
[36m[2025-07-02 07:02:51,001][166323] Fps is (10 sec: 1634.9, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10633216. Throughput: 0: 272.4. Samples: 10633744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:02:51,001][166323] Avg episode reward: [(0, '1202.472')]
[36m[2025-07-02 07:02:55,975][166323] Fps is (10 sec: 1639.2, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10633216. Throughput: 0: 273.9. Samples: 10635408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:02:55,976][166323] Avg episode reward: [(0, '1226.665')]
[31m[37765756 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[37765757 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[37765757 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:03:00,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10633216. Throughput: 0: 273.3. Samples: 10636976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:03:00,947][166323] Avg episode reward: [(0, '1220.968')]
[36m[2025-07-02 07:03:05,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10633216. Throughput: 0: 271.3. Samples: 10637808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:03:05,982][166323] Avg episode reward: [(0, '1205.586')]
[36m[2025-07-02 07:03:10,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10633216. Throughput: 0: 270.5. Samples: 10639456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:03:10,993][166323] Avg episode reward: [(0, '1315.307')]
[36m[2025-07-02 07:03:15,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10633216. Throughput: 0: 271.1. Samples: 10641088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:03:15,984][166323] Avg episode reward: [(0, '1376.699')]
[36m[2025-07-02 07:03:20,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10633216. Throughput: 0: 271.2. Samples: 10641872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:03:20,951][166323] Avg episode reward: [(0, '1357.423')]
[36m[2025-07-02 07:03:25,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10633216. Throughput: 0: 268.8. Samples: 10643424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:03:25,976][166323] Avg episode reward: [(0, '1371.896')]
[36m[2025-07-02 07:03:30,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10633216. Throughput: 0: 268.1. Samples: 10645072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:03:30,986][166323] Avg episode reward: [(0, '1326.480')]
[36m[2025-07-02 07:03:36,001][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10633216. Throughput: 0: 272.0. Samples: 10645984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:03:36,001][166323] Avg episode reward: [(0, '1332.066')]
[37m[1m[2025-07-02 07:03:36,069][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000020760_10633216.pth...
[36m[2025-07-02 07:03:36,075][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000020632_10567680.pth
[36m[2025-07-02 07:03:40,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.1). Total num frames: 10633216. Throughput: 0: 269.3. Samples: 10647520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:03:40,959][166323] Avg episode reward: [(0, '1335.420')]
[36m[2025-07-02 07:03:45,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 10633216. Throughput: 0: 269.4. Samples: 10649104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:03:45,958][166323] Avg episode reward: [(0, '1303.975')]
[31m[37815252 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[37815252 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[37815253 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:03:51,000][166323] Fps is (10 sec: 1631.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10649600. Throughput: 0: 269.4. Samples: 10649936. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 07:03:51,001][166323] Avg episode reward: [(0, '1286.053')]
[36m[2025-07-02 07:03:55,952][166323] Fps is (10 sec: 1639.2, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10649600. Throughput: 0: 268.7. Samples: 10651536. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 07:03:55,953][166323] Avg episode reward: [(0, '1302.275')]
[36m[2025-07-02 07:04:00,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10649600. Throughput: 0: 270.9. Samples: 10653280. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 07:04:00,981][166323] Avg episode reward: [(0, '1274.993')]
[33m[37830222 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[37830223 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.8701171875
[33mCrash Rate: 0.11767578125
[33mTimeout Rate: 0.01220703125 (navigation_task.py:265)
[33m[37830223 ms][navigation_task] - WARNING : 
[33mSuccesses: 1782
[33mCrashes : 241
[33mTimeouts: 25 (navigation_task.py:268)
[36m[2025-07-02 07:04:05,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10649600. Throughput: 0: 270.1. Samples: 10654032. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 07:04:05,963][166323] Avg episode reward: [(0, '1307.848')]
[36m[2025-07-02 07:04:11,019][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 10649600. Throughput: 0: 272.8. Samples: 10655712. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 07:04:11,019][166323] Avg episode reward: [(0, '1241.546')]
[36m[2025-07-02 07:04:15,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10649600. Throughput: 0: 273.5. Samples: 10657376. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 07:04:15,969][166323] Avg episode reward: [(0, '1210.971')]
[36m[2025-07-02 07:04:20,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10649600. Throughput: 0: 270.3. Samples: 10658144. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 07:04:20,994][166323] Avg episode reward: [(0, '1217.378')]
[36m[2025-07-02 07:04:25,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10649600. Throughput: 0: 270.8. Samples: 10659712. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 07:04:25,988][166323] Avg episode reward: [(0, '1158.118')]
[31m[37855025 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[37855026 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[37855026 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:04:30,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10649600. Throughput: 0: 271.2. Samples: 10661312. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 07:04:30,967][166323] Avg episode reward: [(0, '1082.776')]
[36m[2025-07-02 07:04:35,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10649600. Throughput: 0: 270.8. Samples: 10662112. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 07:04:35,964][166323] Avg episode reward: [(0, '1115.250')]
[36m[2025-07-02 07:04:40,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 10649600. Throughput: 0: 272.6. Samples: 10663808. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 07:04:40,969][166323] Avg episode reward: [(0, '1087.946')]
[36m[2025-07-02 07:04:45,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 10649600. Throughput: 0: 271.1. Samples: 10665472. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 07:04:45,951][166323] Avg episode reward: [(0, '1148.307')]
[36m[2025-07-02 07:04:50,979][166323] Fps is (10 sec: 1636.8, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10665984. Throughput: 0: 272.3. Samples: 10666288. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:04:50,979][166323] Avg episode reward: [(0, '1103.437')]
[36m[2025-07-02 07:04:55,964][166323] Fps is (10 sec: 1636.3, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10665984. Throughput: 0: 273.4. Samples: 10668000. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:04:55,964][166323] Avg episode reward: [(0, '1174.776')]
[36m[2025-07-02 07:05:00,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10665984. Throughput: 0: 273.1. Samples: 10669664. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:05:00,969][166323] Avg episode reward: [(0, '1282.538')]
[36m[2025-07-02 07:05:05,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10665984. Throughput: 0: 275.3. Samples: 10670528. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:05:05,977][166323] Avg episode reward: [(0, '1327.523')]
[36m[2025-07-02 07:05:10,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10665984. Throughput: 0: 276.7. Samples: 10672160. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:05:10,968][166323] Avg episode reward: [(0, '1275.135')]
[36m[2025-07-02 07:05:15,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10665984. Throughput: 0: 277.6. Samples: 10673808. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:05:15,985][166323] Avg episode reward: [(0, '1374.685')]
[36m[2025-07-02 07:05:20,995][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10665984. Throughput: 0: 276.8. Samples: 10674576. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:05:20,995][166323] Avg episode reward: [(0, '1356.310')]
[36m[2025-07-02 07:05:25,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10665984. Throughput: 0: 276.0. Samples: 10676224. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:05:25,948][166323] Avg episode reward: [(0, '1315.078')]
[36m[2025-07-02 07:05:30,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10665984. Throughput: 0: 275.4. Samples: 10677872. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:05:30,981][166323] Avg episode reward: [(0, '1288.317')]
[36m[2025-07-02 07:05:35,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10665984. Throughput: 0: 276.8. Samples: 10678736. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:05:35,946][166323] Avg episode reward: [(0, '1260.863')]
[37m[1m[2025-07-02 07:05:36,002][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000020824_10665984.pth...
[36m[2025-07-02 07:05:36,009][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000020696_10600448.pth
[36m[2025-07-02 07:05:40,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10665984. Throughput: 0: 275.3. Samples: 10680384. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:05:40,945][166323] Avg episode reward: [(0, '1257.975')]
[36m[2025-07-02 07:05:45,979][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.2). Total num frames: 10665984. Throughput: 0: 276.6. Samples: 10682112. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:05:45,979][166323] Avg episode reward: [(0, '1324.825')]
[36m[2025-07-02 07:05:50,974][166323] Fps is (10 sec: 1633.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10682368. Throughput: 0: 274.1. Samples: 10682864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:05:50,975][166323] Avg episode reward: [(0, '1274.385')]
[36m[2025-07-02 07:05:55,946][166323] Fps is (10 sec: 1643.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10682368. Throughput: 0: 273.2. Samples: 10684448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:05:55,947][166323] Avg episode reward: [(0, '1321.501')]
[36m[2025-07-02 07:06:00,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10682368. Throughput: 0: 271.7. Samples: 10686032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:06:00,977][166323] Avg episode reward: [(0, '1327.179')]
[36m[2025-07-02 07:06:05,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10682368. Throughput: 0: 272.1. Samples: 10686816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:06:05,979][166323] Avg episode reward: [(0, '1311.591')]
[36m[2025-07-02 07:06:10,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10682368. Throughput: 0: 277.6. Samples: 10688720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:06:10,960][166323] Avg episode reward: [(0, '1250.320')]
[36m[2025-07-02 07:06:15,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10682368. Throughput: 0: 273.4. Samples: 10690176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:06:15,978][166323] Avg episode reward: [(0, '1208.142')]
[36m[2025-07-02 07:06:20,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10682368. Throughput: 0: 273.7. Samples: 10691056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:06:20,952][166323] Avg episode reward: [(0, '1198.466')]
[31m[37970118 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[37970118 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[37970118 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:06:25,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10682368. Throughput: 0: 275.3. Samples: 10692784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:06:25,991][166323] Avg episode reward: [(0, '1174.030')]
[36m[2025-07-02 07:06:30,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10682368. Throughput: 0: 271.2. Samples: 10694320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:06:30,993][166323] Avg episode reward: [(0, '1191.922')]
[36m[2025-07-02 07:06:35,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10682368. Throughput: 0: 274.3. Samples: 10695200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:06:35,950][166323] Avg episode reward: [(0, '1147.403')]
[36m[2025-07-02 07:06:40,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10682368. Throughput: 0: 274.4. Samples: 10696800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:06:40,959][166323] Avg episode reward: [(0, '1193.640')]
[36m[2025-07-02 07:06:45,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.1). Total num frames: 10682368. Throughput: 0: 274.5. Samples: 10698384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:06:45,982][166323] Avg episode reward: [(0, '1274.599')]
[36m[2025-07-02 07:06:51,017][166323] Fps is (10 sec: 1628.9, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 10698752. Throughput: 0: 276.0. Samples: 10699248. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:06:51,017][166323] Avg episode reward: [(0, '1339.658')]
[36m[2025-07-02 07:06:55,947][166323] Fps is (10 sec: 1644.1, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10698752. Throughput: 0: 269.9. Samples: 10700864. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:06:55,947][166323] Avg episode reward: [(0, '1296.479')]
[36m[2025-07-02 07:07:00,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10698752. Throughput: 0: 274.1. Samples: 10702512. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:07:00,990][166323] Avg episode reward: [(0, '1365.139')]
[36m[2025-07-02 07:07:05,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10698752. Throughput: 0: 272.6. Samples: 10703328. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:07:05,975][166323] Avg episode reward: [(0, '1331.042')]
[36m[2025-07-02 07:07:10,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10698752. Throughput: 0: 276.9. Samples: 10705232. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:07:10,952][166323] Avg episode reward: [(0, '1360.232')]
[36m[2025-07-02 07:07:15,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10698752. Throughput: 0: 276.3. Samples: 10706752. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:07:15,990][166323] Avg episode reward: [(0, '1335.027')]
[36m[2025-07-02 07:07:20,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10698752. Throughput: 0: 274.4. Samples: 10707552. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:07:20,964][166323] Avg episode reward: [(0, '1332.387')]
[36m[2025-07-02 07:07:25,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10698752. Throughput: 0: 275.3. Samples: 10709184. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:07:25,951][166323] Avg episode reward: [(0, '1301.989')]
[36m[2025-07-02 07:07:30,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10698752. Throughput: 0: 274.3. Samples: 10710720. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:07:30,957][166323] Avg episode reward: [(0, '1253.520')]
[36m[2025-07-02 07:07:35,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10698752. Throughput: 0: 272.0. Samples: 10711472. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:07:35,963][166323] Avg episode reward: [(0, '1269.845')]
[37m[1m[2025-07-02 07:07:36,024][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000020888_10698752.pth...
[36m[2025-07-02 07:07:36,028][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000020760_10633216.pth
[36m[2025-07-02 07:07:40,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10698752. Throughput: 0: 272.9. Samples: 10713152. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:07:40,974][166323] Avg episode reward: [(0, '1315.557')]
[36m[2025-07-02 07:07:45,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 10698752. Throughput: 0: 276.3. Samples: 10714944. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:07:45,980][166323] Avg episode reward: [(0, '1308.488')]
[36m[2025-07-02 07:07:50,974][166323] Fps is (10 sec: 1638.3, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10715136. Throughput: 0: 276.6. Samples: 10715776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:07:50,974][166323] Avg episode reward: [(0, '1308.783')]
[36m[2025-07-02 07:07:55,943][166323] Fps is (10 sec: 1644.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10715136. Throughput: 0: 269.9. Samples: 10717376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:07:55,943][166323] Avg episode reward: [(0, '1339.978')]
[36m[2025-07-02 07:08:00,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10715136. Throughput: 0: 274.8. Samples: 10719104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:08:00,945][166323] Avg episode reward: [(0, '1356.406')]
[36m[2025-07-02 07:08:05,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10715136. Throughput: 0: 273.7. Samples: 10719872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:08:05,970][166323] Avg episode reward: [(0, '1435.923')]
[37m[1m[2025-07-02 07:08:06,022][166323] Saving new best policy, reward=1435.923!
[36m[2025-07-02 07:08:10,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10715136. Throughput: 0: 275.6. Samples: 10721584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:08:10,945][166323] Avg episode reward: [(0, '1341.128')]
[36m[2025-07-02 07:08:15,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 10715136. Throughput: 0: 277.4. Samples: 10723216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:08:15,999][166323] Avg episode reward: [(0, '1329.810')]
[36m[2025-07-02 07:08:20,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10715136. Throughput: 0: 279.0. Samples: 10724032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:08:20,986][166323] Avg episode reward: [(0, '1338.038')]
[36m[2025-07-02 07:08:25,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10715136. Throughput: 0: 280.1. Samples: 10725760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:08:25,989][166323] Avg episode reward: [(0, '1327.621')]
[36m[2025-07-02 07:08:30,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10715136. Throughput: 0: 275.8. Samples: 10727360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:08:30,994][166323] Avg episode reward: [(0, '1316.578')]
[36m[2025-07-02 07:08:35,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10715136. Throughput: 0: 275.9. Samples: 10728192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:08:35,977][166323] Avg episode reward: [(0, '1321.306')]
[36m[2025-07-02 07:08:40,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10715136. Throughput: 0: 279.6. Samples: 10729968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:08:40,978][166323] Avg episode reward: [(0, '1305.686')]
[31m[38114456 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[38114457 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[38114457 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:08:45,949][166323] Fps is (10 sec: 1642.9, 60 sec: 546.4, 300 sec: 277.7). Total num frames: 10731520. Throughput: 0: 276.6. Samples: 10731552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:08:45,950][166323] Avg episode reward: [(0, '1280.010')]
[36m[2025-07-02 07:08:50,989][166323] Fps is (10 sec: 1636.7, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10731520. Throughput: 0: 276.1. Samples: 10732304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:08:50,989][166323] Avg episode reward: [(0, '1221.144')]
[36m[2025-07-02 07:08:55,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10731520. Throughput: 0: 276.9. Samples: 10734048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:08:55,962][166323] Avg episode reward: [(0, '1230.678')]
[36m[2025-07-02 07:09:00,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 10731520. Throughput: 0: 277.4. Samples: 10735696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:09:00,996][166323] Avg episode reward: [(0, '1233.010')]
[36m[2025-07-02 07:09:05,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10731520. Throughput: 0: 275.6. Samples: 10736432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:09:05,982][166323] Avg episode reward: [(0, '1138.716')]
[36m[2025-07-02 07:09:10,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10731520. Throughput: 0: 272.4. Samples: 10738016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:09:10,984][166323] Avg episode reward: [(0, '1136.955')]
[31m[38141886 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[38141887 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[38141887 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:09:15,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10731520. Throughput: 0: 272.2. Samples: 10739600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:09:15,964][166323] Avg episode reward: [(0, '1161.704')]
[36m[2025-07-02 07:09:20,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10731520. Throughput: 0: 272.4. Samples: 10740448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:09:20,967][166323] Avg episode reward: [(0, '1232.011')]
[36m[2025-07-02 07:09:25,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10731520. Throughput: 0: 271.9. Samples: 10742208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:09:25,990][166323] Avg episode reward: [(0, '1285.998')]
[36m[2025-07-02 07:09:30,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10731520. Throughput: 0: 272.7. Samples: 10743824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:09:30,946][166323] Avg episode reward: [(0, '1313.116')]
[36m[2025-07-02 07:09:35,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10731520. Throughput: 0: 274.3. Samples: 10744640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:09:35,960][166323] Avg episode reward: [(0, '1311.024')]
[37m[1m[2025-07-02 07:09:36,016][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000020952_10731520.pth...
[36m[2025-07-02 07:09:36,020][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000020824_10665984.pth
[36m[2025-07-02 07:09:40,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10731520. Throughput: 0: 272.6. Samples: 10746320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:09:40,974][166323] Avg episode reward: [(0, '1378.541')]
[36m[2025-07-02 07:09:45,971][166323] Fps is (10 sec: 1636.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10747904. Throughput: 0: 273.6. Samples: 10748000. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 07:09:45,971][166323] Avg episode reward: [(0, '1358.706')]
[36m[2025-07-02 07:09:50,947][166323] Fps is (10 sec: 1642.8, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10747904. Throughput: 0: 276.1. Samples: 10748848. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 07:09:50,947][166323] Avg episode reward: [(0, '1348.435')]
[36m[2025-07-02 07:09:55,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10747904. Throughput: 0: 276.0. Samples: 10750432. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 07:09:55,962][166323] Avg episode reward: [(0, '1372.335')]
[36m[2025-07-02 07:10:00,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10747904. Throughput: 0: 280.5. Samples: 10752224. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 07:10:00,974][166323] Avg episode reward: [(0, '1320.293')]
[36m[2025-07-02 07:10:05,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10747904. Throughput: 0: 278.4. Samples: 10752976. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 07:10:05,968][166323] Avg episode reward: [(0, '1317.589')]
[36m[2025-07-02 07:10:10,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10747904. Throughput: 0: 274.9. Samples: 10754576. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 07:10:10,984][166323] Avg episode reward: [(0, '1335.358')]
[36m[2025-07-02 07:10:15,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10747904. Throughput: 0: 272.9. Samples: 10756112. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 07:10:15,974][166323] Avg episode reward: [(0, '1241.147')]
[36m[2025-07-02 07:10:20,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10747904. Throughput: 0: 271.1. Samples: 10756848. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 07:10:20,990][166323] Avg episode reward: [(0, '1264.884')]
[36m[2025-07-02 07:10:25,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10747904. Throughput: 0: 270.6. Samples: 10758496. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 07:10:25,975][166323] Avg episode reward: [(0, '1258.082')]
[36m[2025-07-02 07:10:30,973][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10747904. Throughput: 0: 267.4. Samples: 10760032. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 07:10:30,973][166323] Avg episode reward: [(0, '1264.065')]
[36m[2025-07-02 07:10:35,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10747904. Throughput: 0: 267.3. Samples: 10760880. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 07:10:35,967][166323] Avg episode reward: [(0, '1277.252')]
[36m[2025-07-02 07:10:40,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10747904. Throughput: 0: 266.9. Samples: 10762448. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 07:10:40,990][166323] Avg episode reward: [(0, '1296.581')]
[36m[2025-07-02 07:10:45,952][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 222.2). Total num frames: 10747904. Throughput: 0: 264.3. Samples: 10764112. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-02 07:10:45,952][166323] Avg episode reward: [(0, '1219.118')]
[31m[38238762 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[38238763 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[38238763 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:10:50,997][166323] Fps is (10 sec: 1637.1, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 10764288. Throughput: 0: 265.4. Samples: 10764928. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 07:10:50,998][166323] Avg episode reward: [(0, '1236.897')]
[36m[2025-07-02 07:10:56,005][166323] Fps is (10 sec: 1629.8, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10764288. Throughput: 0: 263.3. Samples: 10766432. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 07:10:56,005][166323] Avg episode reward: [(0, '1233.847')]
[36m[2025-07-02 07:11:00,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10764288. Throughput: 0: 268.1. Samples: 10768176. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 07:11:00,980][166323] Avg episode reward: [(0, '1187.421')]
[36m[2025-07-02 07:11:05,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10764288. Throughput: 0: 270.1. Samples: 10768992. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 07:11:05,953][166323] Avg episode reward: [(0, '1138.872')]
[36m[2025-07-02 07:11:11,017][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10764288. Throughput: 0: 269.3. Samples: 10770624. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 07:11:11,018][166323] Avg episode reward: [(0, '1150.379')]
[36m[2025-07-02 07:11:15,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10764288. Throughput: 0: 270.6. Samples: 10772208. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 07:11:15,971][166323] Avg episode reward: [(0, '1185.382')]
[36m[2025-07-02 07:11:20,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10764288. Throughput: 0: 272.7. Samples: 10773152. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 07:11:20,974][166323] Avg episode reward: [(0, '1208.792')]
[36m[2025-07-02 07:11:25,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10764288. Throughput: 0: 278.2. Samples: 10774960. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 07:11:25,959][166323] Avg episode reward: [(0, '1248.299')]
[36m[2025-07-02 07:11:30,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10764288. Throughput: 0: 276.2. Samples: 10776544. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 07:11:30,971][166323] Avg episode reward: [(0, '1252.603')]
[36m[2025-07-02 07:11:35,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10764288. Throughput: 0: 276.3. Samples: 10777360. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 07:11:35,984][166323] Avg episode reward: [(0, '1257.762')]
[37m[1m[2025-07-02 07:11:36,051][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021016_10764288.pth...
[36m[2025-07-02 07:11:36,055][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000020888_10698752.pth
[36m[2025-07-02 07:11:40,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10764288. Throughput: 0: 279.1. Samples: 10778992. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 07:11:40,999][166323] Avg episode reward: [(0, '1250.716')]
[36m[2025-07-02 07:11:45,989][166323] Fps is (10 sec: 1637.6, 60 sec: 545.8, 300 sec: 277.7). Total num frames: 10780672. Throughput: 0: 278.3. Samples: 10780704. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:11:45,989][166323] Avg episode reward: [(0, '1288.399')]
[36m[2025-07-02 07:11:50,945][166323] Fps is (10 sec: 1647.2, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10780672. Throughput: 0: 279.5. Samples: 10781568. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:11:50,946][166323] Avg episode reward: [(0, '1254.504')]
[36m[2025-07-02 07:11:55,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10780672. Throughput: 0: 280.8. Samples: 10783248. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:11:55,981][166323] Avg episode reward: [(0, '1254.521')]
[36m[2025-07-02 07:12:00,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10780672. Throughput: 0: 281.2. Samples: 10784864. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:12:00,982][166323] Avg episode reward: [(0, '1213.385')]
[36m[2025-07-02 07:12:05,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10780672. Throughput: 0: 277.4. Samples: 10785632. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:12:05,965][166323] Avg episode reward: [(0, '1231.358')]
[36m[2025-07-02 07:12:10,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10780672. Throughput: 0: 274.1. Samples: 10787296. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:12:10,963][166323] Avg episode reward: [(0, '1279.520')]
[36m[2025-07-02 07:12:16,009][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10780672. Throughput: 0: 275.7. Samples: 10788960. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:12:16,009][166323] Avg episode reward: [(0, '1282.854')]
[36m[2025-07-02 07:12:21,000][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 10780672. Throughput: 0: 276.5. Samples: 10789808. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:12:21,000][166323] Avg episode reward: [(0, '1263.555')]
[36m[2025-07-02 07:12:25,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10780672. Throughput: 0: 277.0. Samples: 10791456. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:12:25,993][166323] Avg episode reward: [(0, '1261.160')]
[36m[2025-07-02 07:12:31,026][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 10780672. Throughput: 0: 275.0. Samples: 10793088. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:12:31,026][166323] Avg episode reward: [(0, '1297.426')]
[36m[2025-07-02 07:12:35,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10780672. Throughput: 0: 274.3. Samples: 10793920. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:12:35,977][166323] Avg episode reward: [(0, '1329.172')]
[36m[2025-07-02 07:12:40,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10780672. Throughput: 0: 272.6. Samples: 10795504. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:12:40,944][166323] Avg episode reward: [(0, '1325.989')]
[33m[2025-07-02 07:12:45,277][166323] KL-divergence is very high: 136.9853
[36m[2025-07-02 07:12:45,956][166323] Fps is (10 sec: 1641.7, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10797056. Throughput: 0: 272.9. Samples: 10797136. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 07:12:45,957][166323] Avg episode reward: [(0, '1318.535')]
[36m[2025-07-02 07:12:50,989][166323] Fps is (10 sec: 1631.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10797056. Throughput: 0: 272.9. Samples: 10797920. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 07:12:50,990][166323] Avg episode reward: [(0, '1282.418')]
[31m[38361677 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[38361677 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[38361678 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:12:55,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10797056. Throughput: 0: 272.8. Samples: 10799568. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 07:12:55,955][166323] Avg episode reward: [(0, '1258.932')]
[36m[2025-07-02 07:13:00,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10797056. Throughput: 0: 274.5. Samples: 10801296. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 07:13:00,957][166323] Avg episode reward: [(0, '1245.071')]
[36m[2025-07-02 07:13:05,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10797056. Throughput: 0: 273.2. Samples: 10802096. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 07:13:05,986][166323] Avg episode reward: [(0, '1152.505')]
[36m[2025-07-02 07:13:10,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10797056. Throughput: 0: 274.2. Samples: 10803792. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 07:13:10,976][166323] Avg episode reward: [(0, '1203.131')]
[36m[2025-07-02 07:13:15,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10797056. Throughput: 0: 272.8. Samples: 10805344. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 07:13:15,951][166323] Avg episode reward: [(0, '1192.195')]
[36m[2025-07-02 07:13:20,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10797056. Throughput: 0: 272.2. Samples: 10806160. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 07:13:20,950][166323] Avg episode reward: [(0, '1186.167')]
[36m[2025-07-02 07:13:25,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10797056. Throughput: 0: 273.0. Samples: 10807792. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 07:13:25,962][166323] Avg episode reward: [(0, '1201.534')]
[36m[2025-07-02 07:13:30,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10797056. Throughput: 0: 273.7. Samples: 10809456. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 07:13:30,973][166323] Avg episode reward: [(0, '1263.242')]
[36m[2025-07-02 07:13:36,004][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10797056. Throughput: 0: 275.1. Samples: 10810304. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 07:13:36,004][166323] Avg episode reward: [(0, '1277.471')]
[37m[1m[2025-07-02 07:13:36,073][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021080_10797056.pth...
[36m[2025-07-02 07:13:36,077][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000020952_10731520.pth
[36m[2025-07-02 07:13:40,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 10797056. Throughput: 0: 274.8. Samples: 10811936. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 07:13:40,965][166323] Avg episode reward: [(0, '1365.169')]
[36m[2025-07-02 07:13:45,988][166323] Fps is (10 sec: 1641.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10813440. Throughput: 0: 272.9. Samples: 10813584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:13:45,988][166323] Avg episode reward: [(0, '1339.748')]
[36m[2025-07-02 07:13:50,989][166323] Fps is (10 sec: 1634.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10813440. Throughput: 0: 273.0. Samples: 10814384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:13:50,989][166323] Avg episode reward: [(0, '1344.412')]
[31m[38423096 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[38423096 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[38423096 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:13:55,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10813440. Throughput: 0: 270.7. Samples: 10815968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:13:55,955][166323] Avg episode reward: [(0, '1322.996')]
[36m[2025-07-02 07:14:01,006][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 10813440. Throughput: 0: 271.0. Samples: 10817552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:14:01,006][166323] Avg episode reward: [(0, '1313.553')]
[36m[2025-07-02 07:14:05,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10813440. Throughput: 0: 271.6. Samples: 10818384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:14:05,964][166323] Avg episode reward: [(0, '1303.042')]
[36m[2025-07-02 07:14:10,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10813440. Throughput: 0: 273.0. Samples: 10820080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:14:10,973][166323] Avg episode reward: [(0, '1329.748')]
[36m[2025-07-02 07:14:15,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10813440. Throughput: 0: 271.4. Samples: 10821664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:14:15,957][166323] Avg episode reward: [(0, '1307.924')]
[36m[2025-07-02 07:14:20,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10813440. Throughput: 0: 270.1. Samples: 10822448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:14:20,965][166323] Avg episode reward: [(0, '1357.042')]
[36m[2025-07-02 07:14:25,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10813440. Throughput: 0: 266.6. Samples: 10823936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:14:25,972][166323] Avg episode reward: [(0, '1333.845')]
[31m[38458224 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[38458224 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[38458224 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:14:30,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10813440. Throughput: 0: 269.9. Samples: 10825728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:14:30,980][166323] Avg episode reward: [(0, '1373.820')]
[36m[2025-07-02 07:14:35,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10813440. Throughput: 0: 268.8. Samples: 10826480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:14:35,984][166323] Avg episode reward: [(0, '1361.466')]
[36m[2025-07-02 07:14:40,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 10813440. Throughput: 0: 269.5. Samples: 10828096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:14:40,950][166323] Avg episode reward: [(0, '1328.363')]
[36m[2025-07-02 07:14:46,011][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 222.1). Total num frames: 10813440. Throughput: 0: 269.5. Samples: 10829680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:14:46,012][166323] Avg episode reward: [(0, '1302.625')]
[36m[2025-07-02 07:14:50,985][166323] Fps is (10 sec: 1632.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10829824. Throughput: 0: 267.6. Samples: 10830432. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 07:14:50,985][166323] Avg episode reward: [(0, '1338.088')]
[36m[2025-07-02 07:14:55,981][166323] Fps is (10 sec: 1643.4, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10829824. Throughput: 0: 269.5. Samples: 10832208. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 07:14:55,981][166323] Avg episode reward: [(0, '1297.290')]
[36m[2025-07-02 07:15:00,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10829824. Throughput: 0: 268.8. Samples: 10833760. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 07:15:00,959][166323] Avg episode reward: [(0, '1328.702')]
[36m[2025-07-02 07:15:05,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10829824. Throughput: 0: 271.0. Samples: 10834640. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 07:15:05,955][166323] Avg episode reward: [(0, '1336.998')]
[36m[2025-07-02 07:15:10,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10829824. Throughput: 0: 276.8. Samples: 10836400. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 07:15:10,996][166323] Avg episode reward: [(0, '1362.857')]
[36m[2025-07-02 07:15:15,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10829824. Throughput: 0: 272.8. Samples: 10838000. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 07:15:15,971][166323] Avg episode reward: [(0, '1341.104')]
[36m[2025-07-02 07:15:20,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10829824. Throughput: 0: 274.8. Samples: 10838848. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 07:15:20,993][166323] Avg episode reward: [(0, '1392.142')]
[36m[2025-07-02 07:15:26,006][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10829824. Throughput: 0: 275.6. Samples: 10840512. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 07:15:26,006][166323] Avg episode reward: [(0, '1319.813')]
[36m[2025-07-02 07:15:30,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10829824. Throughput: 0: 276.9. Samples: 10842128. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 07:15:30,973][166323] Avg episode reward: [(0, '1359.592')]
[36m[2025-07-02 07:15:35,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10829824. Throughput: 0: 276.7. Samples: 10842880. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 07:15:35,981][166323] Avg episode reward: [(0, '1329.484')]
[37m[1m[2025-07-02 07:15:36,045][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021144_10829824.pth...
[36m[2025-07-02 07:15:36,049][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021016_10764288.pth
[36m[2025-07-02 07:15:40,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10829824. Throughput: 0: 275.5. Samples: 10844608. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 07:15:40,990][166323] Avg episode reward: [(0, '1283.229')]
[33m[38530342 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[38530342 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.87060546875
[33mCrash Rate: 0.1162109375
[33mTimeout Rate: 0.01318359375 (navigation_task.py:265)
[33m[38530343 ms][navigation_task] - WARNING : 
[33mSuccesses: 1783
[33mCrashes : 238
[33mTimeouts: 27 (navigation_task.py:268)
[36m[2025-07-02 07:15:45,954][166323] Fps is (10 sec: 1642.8, 60 sec: 546.7, 300 sec: 277.7). Total num frames: 10846208. Throughput: 0: 277.0. Samples: 10846224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:15:45,954][166323] Avg episode reward: [(0, '1266.808')]
[36m[2025-07-02 07:15:50,951][166323] Fps is (10 sec: 1644.7, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10846208. Throughput: 0: 274.2. Samples: 10846976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:15:50,952][166323] Avg episode reward: [(0, '1270.128')]
[36m[2025-07-02 07:15:55,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10846208. Throughput: 0: 272.8. Samples: 10848672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:15:55,988][166323] Avg episode reward: [(0, '1288.237')]
[36m[2025-07-02 07:16:00,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10846208. Throughput: 0: 276.3. Samples: 10850432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:16:00,962][166323] Avg episode reward: [(0, '1286.484')]
[36m[2025-07-02 07:16:05,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10846208. Throughput: 0: 276.0. Samples: 10851264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:16:05,971][166323] Avg episode reward: [(0, '1250.764')]
[36m[2025-07-02 07:16:10,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10846208. Throughput: 0: 275.5. Samples: 10852896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:16:10,965][166323] Avg episode reward: [(0, '1264.909')]
[31m[38563633 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[38563634 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[38563634 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:16:15,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10846208. Throughput: 0: 276.6. Samples: 10854576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:16:15,979][166323] Avg episode reward: [(0, '1276.434')]
[31m[38565490 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[38565490 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[38565490 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:16:20,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10846208. Throughput: 0: 278.2. Samples: 10855392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:16:20,957][166323] Avg episode reward: [(0, '1241.932')]
[36m[2025-07-02 07:16:26,003][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10846208. Throughput: 0: 275.8. Samples: 10857024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:16:26,003][166323] Avg episode reward: [(0, '1210.725')]
[31m[38575379 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[38575380 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[38575380 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:16:30,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10846208. Throughput: 0: 273.3. Samples: 10858528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:16:30,971][166323] Avg episode reward: [(0, '1165.442')]
[36m[2025-07-02 07:16:35,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10846208. Throughput: 0: 274.8. Samples: 10859344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:16:35,954][166323] Avg episode reward: [(0, '1180.883')]
[36m[2025-07-02 07:16:40,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 10846208. Throughput: 0: 270.6. Samples: 10860848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:16:40,984][166323] Avg episode reward: [(0, '1216.322')]
[36m[2025-07-02 07:16:45,998][166323] Fps is (10 sec: 1631.1, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 10862592. Throughput: 0: 270.4. Samples: 10862608. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:16:45,999][166323] Avg episode reward: [(0, '1277.802')]
[36m[2025-07-02 07:16:51,013][166323] Fps is (10 sec: 1633.7, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 10862592. Throughput: 0: 268.2. Samples: 10863344. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:16:51,013][166323] Avg episode reward: [(0, '1281.954')]
[36m[2025-07-02 07:16:55,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10862592. Throughput: 0: 269.7. Samples: 10865040. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:16:55,987][166323] Avg episode reward: [(0, '1307.080')]
[31m[38605504 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[38605504 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[38605505 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:17:00,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10862592. Throughput: 0: 269.8. Samples: 10866720. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:17:00,988][166323] Avg episode reward: [(0, '1323.955')]
[36m[2025-07-02 07:17:05,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10862592. Throughput: 0: 270.2. Samples: 10867552. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:17:05,960][166323] Avg episode reward: [(0, '1361.067')]
[36m[2025-07-02 07:17:11,002][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10862592. Throughput: 0: 267.4. Samples: 10869056. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:17:11,003][166323] Avg episode reward: [(0, '1350.683')]
[36m[2025-07-02 07:17:15,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10862592. Throughput: 0: 268.9. Samples: 10870624. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:17:15,946][166323] Avg episode reward: [(0, '1316.508')]
[36m[2025-07-02 07:17:20,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10862592. Throughput: 0: 269.9. Samples: 10871488. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:17:20,952][166323] Avg episode reward: [(0, '1302.156')]
[36m[2025-07-02 07:17:25,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.8). Total num frames: 10862592. Throughput: 0: 272.9. Samples: 10873120. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:17:25,957][166323] Avg episode reward: [(0, '1235.514')]
[36m[2025-07-02 07:17:30,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10862592. Throughput: 0: 267.6. Samples: 10874640. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:17:30,961][166323] Avg episode reward: [(0, '1224.183')]
[36m[2025-07-02 07:17:35,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10862592. Throughput: 0: 268.0. Samples: 10875392. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:17:35,961][166323] Avg episode reward: [(0, '1206.070')]
[37m[1m[2025-07-02 07:17:36,038][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021208_10862592.pth...
[36m[2025-07-02 07:17:36,043][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021080_10797056.pth
[31m[38646768 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[38646769 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[38646769 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:17:41,016][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 10862592. Throughput: 0: 265.4. Samples: 10876992. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:17:41,017][166323] Avg episode reward: [(0, '1150.351')]
[31m[38650391 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[38650392 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[38650392 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:17:45,959][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 222.2). Total num frames: 10862592. Throughput: 0: 266.1. Samples: 10878688. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:17:45,960][166323] Avg episode reward: [(0, '1134.359')]
[36m[2025-07-02 07:17:50,974][166323] Fps is (10 sec: 1645.4, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10878976. Throughput: 0: 266.6. Samples: 10879552. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 07:17:50,974][166323] Avg episode reward: [(0, '1157.299')]
[36m[2025-07-02 07:17:55,964][166323] Fps is (10 sec: 1637.5, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10878976. Throughput: 0: 271.2. Samples: 10881248. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 07:17:55,965][166323] Avg episode reward: [(0, '1171.260')]
[36m[2025-07-02 07:18:01,007][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10878976. Throughput: 0: 272.0. Samples: 10882880. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 07:18:01,008][166323] Avg episode reward: [(0, '1246.885')]
[31m[38672990 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[38672990 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[38672991 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:18:05,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10878976. Throughput: 0: 269.4. Samples: 10883616. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 07:18:05,966][166323] Avg episode reward: [(0, '1176.416')]
[36m[2025-07-02 07:18:10,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10878976. Throughput: 0: 268.6. Samples: 10885216. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 07:18:10,996][166323] Avg episode reward: [(0, '1199.757')]
[31m[38680432 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[38680432 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[38680432 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:18:15,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10878976. Throughput: 0: 268.3. Samples: 10886720. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 07:18:15,987][166323] Avg episode reward: [(0, '1268.839')]
[36m[2025-07-02 07:18:20,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10878976. Throughput: 0: 270.2. Samples: 10887552. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 07:18:20,956][166323] Avg episode reward: [(0, '1273.652')]
[36m[2025-07-02 07:18:25,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10878976. Throughput: 0: 272.6. Samples: 10889248. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 07:18:25,982][166323] Avg episode reward: [(0, '1286.968')]
[36m[2025-07-02 07:18:30,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10878976. Throughput: 0: 271.7. Samples: 10890912. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 07:18:30,951][166323] Avg episode reward: [(0, '1269.576')]
[36m[2025-07-02 07:18:35,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10878976. Throughput: 0: 272.0. Samples: 10891792. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 07:18:35,969][166323] Avg episode reward: [(0, '1220.371')]
[36m[2025-07-02 07:18:40,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 10878976. Throughput: 0: 270.1. Samples: 10893408. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 07:18:40,985][166323] Avg episode reward: [(0, '1230.110')]
[36m[2025-07-02 07:18:45,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.2). Total num frames: 10878976. Throughput: 0: 270.3. Samples: 10895040. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 07:18:45,991][166323] Avg episode reward: [(0, '1241.310')]
[36m[2025-07-02 07:18:50,986][166323] Fps is (10 sec: 1638.1, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10895360. Throughput: 0: 272.6. Samples: 10895888. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 07:18:50,987][166323] Avg episode reward: [(0, '1286.185')]
[36m[2025-07-02 07:18:55,955][166323] Fps is (10 sec: 1644.3, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10895360. Throughput: 0: 274.4. Samples: 10897552. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 07:18:55,956][166323] Avg episode reward: [(0, '1269.553')]
[31m[38728931 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[38728932 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[38728932 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:19:00,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10895360. Throughput: 0: 277.5. Samples: 10899200. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 07:19:00,960][166323] Avg episode reward: [(0, '1198.817')]
[31m[38732110 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[38732111 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[38732111 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:19:05,998][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10895360. Throughput: 0: 276.0. Samples: 10899984. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 07:19:05,998][166323] Avg episode reward: [(0, '1211.575')]
[36m[2025-07-02 07:19:10,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10895360. Throughput: 0: 275.0. Samples: 10901616. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 07:19:10,958][166323] Avg episode reward: [(0, '1250.094')]
[36m[2025-07-02 07:19:15,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10895360. Throughput: 0: 272.9. Samples: 10903200. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 07:19:15,972][166323] Avg episode reward: [(0, '1304.857')]
[36m[2025-07-02 07:19:20,983][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10895360. Throughput: 0: 271.9. Samples: 10904032. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 07:19:20,983][166323] Avg episode reward: [(0, '1251.651')]
[36m[2025-07-02 07:19:25,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10895360. Throughput: 0: 271.0. Samples: 10905600. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 07:19:25,968][166323] Avg episode reward: [(0, '1217.608')]
[36m[2025-07-02 07:19:30,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10895360. Throughput: 0: 271.6. Samples: 10907264. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 07:19:30,994][166323] Avg episode reward: [(0, '1250.682')]
[36m[2025-07-02 07:19:35,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10895360. Throughput: 0: 270.8. Samples: 10908064. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 07:19:35,950][166323] Avg episode reward: [(0, '1266.516')]
[37m[1m[2025-07-02 07:19:36,001][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021272_10895360.pth...
[36m[2025-07-02 07:19:36,005][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021144_10829824.pth
[36m[2025-07-02 07:19:40,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10895360. Throughput: 0: 268.0. Samples: 10909616. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 07:19:40,969][166323] Avg episode reward: [(0, '1292.830')]
[36m[2025-07-02 07:19:45,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 10895360. Throughput: 0: 269.1. Samples: 10911312. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 07:19:45,969][166323] Avg episode reward: [(0, '1273.415')]
[36m[2025-07-02 07:19:50,997][166323] Fps is (10 sec: 1633.9, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10911744. Throughput: 0: 267.7. Samples: 10912032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:19:50,997][166323] Avg episode reward: [(0, '1289.773')]
[36m[2025-07-02 07:19:55,955][166323] Fps is (10 sec: 1640.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10911744. Throughput: 0: 267.7. Samples: 10913664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:19:55,955][166323] Avg episode reward: [(0, '1327.067')]
[36m[2025-07-02 07:20:00,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10911744. Throughput: 0: 271.3. Samples: 10915408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:20:00,976][166323] Avg episode reward: [(0, '1348.792')]
[36m[2025-07-02 07:20:05,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10911744. Throughput: 0: 271.0. Samples: 10916224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:20:05,975][166323] Avg episode reward: [(0, '1378.390')]
[36m[2025-07-02 07:20:10,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10911744. Throughput: 0: 268.0. Samples: 10917664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:20:10,977][166323] Avg episode reward: [(0, '1427.590')]
[36m[2025-07-02 07:20:15,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10911744. Throughput: 0: 268.9. Samples: 10919360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:20:15,979][166323] Avg episode reward: [(0, '1397.677')]
[31m[38806936 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[38806936 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[38806936 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:20:20,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10911744. Throughput: 0: 268.6. Samples: 10920160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:20:20,987][166323] Avg episode reward: [(0, '1380.723')]
[36m[2025-07-02 07:20:25,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10911744. Throughput: 0: 273.8. Samples: 10921936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:20:25,973][166323] Avg episode reward: [(0, '1334.666')]
[36m[2025-07-02 07:20:30,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10911744. Throughput: 0: 272.4. Samples: 10923568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:20:30,970][166323] Avg episode reward: [(0, '1373.953')]
[36m[2025-07-02 07:20:35,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10911744. Throughput: 0: 275.4. Samples: 10924416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:20:35,961][166323] Avg episode reward: [(0, '1362.891')]
[31m[38827696 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[38827697 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[38827697 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:20:40,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.1). Total num frames: 10911744. Throughput: 0: 278.0. Samples: 10926176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:20:40,965][166323] Avg episode reward: [(0, '1189.958')]
[36m[2025-07-02 07:20:45,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 10911744. Throughput: 0: 275.9. Samples: 10927824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:20:45,974][166323] Avg episode reward: [(0, '1202.321')]
[36m[2025-07-02 07:20:50,952][166323] Fps is (10 sec: 1640.4, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10928128. Throughput: 0: 273.9. Samples: 10928544. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:20:50,953][166323] Avg episode reward: [(0, '1210.698')]
[36m[2025-07-02 07:20:55,989][166323] Fps is (10 sec: 1635.9, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10928128. Throughput: 0: 275.5. Samples: 10930064. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:20:55,989][166323] Avg episode reward: [(0, '1232.478')]
[36m[2025-07-02 07:21:00,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10928128. Throughput: 0: 276.1. Samples: 10931776. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:21:00,945][166323] Avg episode reward: [(0, '1145.164')]
[36m[2025-07-02 07:21:05,995][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10928128. Throughput: 0: 276.2. Samples: 10932592. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:21:05,996][166323] Avg episode reward: [(0, '1127.170')]
[36m[2025-07-02 07:21:10,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10928128. Throughput: 0: 273.3. Samples: 10934240. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:21:10,986][166323] Avg episode reward: [(0, '1169.587')]
[36m[2025-07-02 07:21:15,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10928128. Throughput: 0: 273.3. Samples: 10935872. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:21:15,988][166323] Avg episode reward: [(0, '1267.996')]
[36m[2025-07-02 07:21:20,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10928128. Throughput: 0: 273.4. Samples: 10936720. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:21:20,963][166323] Avg episode reward: [(0, '1273.010')]
[36m[2025-07-02 07:21:25,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10928128. Throughput: 0: 269.4. Samples: 10938304. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:21:25,981][166323] Avg episode reward: [(0, '1289.791')]
[31m[38875676 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[38875676 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[38875677 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:21:30,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10928128. Throughput: 0: 267.0. Samples: 10939840. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:21:30,976][166323] Avg episode reward: [(0, '1293.725')]
[36m[2025-07-02 07:21:35,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10928128. Throughput: 0: 269.8. Samples: 10940688. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:21:35,967][166323] Avg episode reward: [(0, '1312.414')]
[37m[1m[2025-07-02 07:21:36,018][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021336_10928128.pth...
[36m[2025-07-02 07:21:36,022][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021208_10862592.pth
[36m[2025-07-02 07:21:40,998][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.2). Total num frames: 10928128. Throughput: 0: 271.9. Samples: 10942304. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:21:40,998][166323] Avg episode reward: [(0, '1311.421')]
[36m[2025-07-02 07:21:45,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 10928128. Throughput: 0: 270.1. Samples: 10943936. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:21:45,972][166323] Avg episode reward: [(0, '1321.136')]
[36m[2025-07-02 07:21:50,963][166323] Fps is (10 sec: 1644.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10944512. Throughput: 0: 269.7. Samples: 10944720. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 07:21:50,963][166323] Avg episode reward: [(0, '1315.607')]
[36m[2025-07-02 07:21:55,949][166323] Fps is (10 sec: 1642.1, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10944512. Throughput: 0: 271.5. Samples: 10946448. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 07:21:55,949][166323] Avg episode reward: [(0, '1279.385')]
[36m[2025-07-02 07:22:00,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10944512. Throughput: 0: 273.6. Samples: 10948176. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 07:22:00,957][166323] Avg episode reward: [(0, '1233.960')]
[36m[2025-07-02 07:22:05,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10944512. Throughput: 0: 276.7. Samples: 10949168. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 07:22:05,949][166323] Avg episode reward: [(0, '1246.217')]
[36m[2025-07-02 07:22:10,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10944512. Throughput: 0: 279.5. Samples: 10950880. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 07:22:10,977][166323] Avg episode reward: [(0, '1235.266')]
[31m[38920353 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[38920354 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[38920354 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:22:15,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10944512. Throughput: 0: 281.6. Samples: 10952512. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 07:22:15,980][166323] Avg episode reward: [(0, '1172.673')]
[36m[2025-07-02 07:22:20,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10944512. Throughput: 0: 280.7. Samples: 10953312. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 07:22:20,947][166323] Avg episode reward: [(0, '1180.651')]
[36m[2025-07-02 07:22:25,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10944512. Throughput: 0: 280.4. Samples: 10954912. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 07:22:25,962][166323] Avg episode reward: [(0, '1229.314')]
[36m[2025-07-02 07:22:30,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10944512. Throughput: 0: 279.6. Samples: 10956512. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 07:22:30,947][166323] Avg episode reward: [(0, '1230.943')]
[36m[2025-07-02 07:22:35,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10944512. Throughput: 0: 281.2. Samples: 10957376. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 07:22:35,976][166323] Avg episode reward: [(0, '1190.227')]
[36m[2025-07-02 07:22:40,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10944512. Throughput: 0: 275.6. Samples: 10958848. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 07:22:40,946][166323] Avg episode reward: [(0, '1196.747')]
[36m[2025-07-02 07:22:45,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 10944512. Throughput: 0: 271.1. Samples: 10960384. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-02 07:22:45,988][166323] Avg episode reward: [(0, '1175.657')]
[36m[2025-07-02 07:22:50,990][166323] Fps is (10 sec: 1631.3, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10960896. Throughput: 0: 266.1. Samples: 10961152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:22:50,990][166323] Avg episode reward: [(0, '1221.422')]
[36m[2025-07-02 07:22:55,945][166323] Fps is (10 sec: 1645.4, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 10960896. Throughput: 0: 265.8. Samples: 10962832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:22:55,945][166323] Avg episode reward: [(0, '1163.927')]
[36m[2025-07-02 07:23:01,004][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10960896. Throughput: 0: 262.3. Samples: 10964320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:23:01,004][166323] Avg episode reward: [(0, '1163.824')]
[36m[2025-07-02 07:23:05,977][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10960896. Throughput: 0: 262.6. Samples: 10965136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:23:05,977][166323] Avg episode reward: [(0, '1140.951')]
[36m[2025-07-02 07:23:11,003][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10960896. Throughput: 0: 266.1. Samples: 10966896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:23:11,004][166323] Avg episode reward: [(0, '1163.802')]
[36m[2025-07-02 07:23:16,001][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10960896. Throughput: 0: 266.3. Samples: 10968512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:23:16,002][166323] Avg episode reward: [(0, '1215.245')]
[36m[2025-07-02 07:23:20,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10960896. Throughput: 0: 265.2. Samples: 10969312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:23:20,981][166323] Avg episode reward: [(0, '1209.201')]
[36m[2025-07-02 07:23:25,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10960896. Throughput: 0: 268.8. Samples: 10970944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:23:25,948][166323] Avg episode reward: [(0, '1262.980')]
[36m[2025-07-02 07:23:30,973][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10960896. Throughput: 0: 272.1. Samples: 10972624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:23:30,973][166323] Avg episode reward: [(0, '1222.076')]
[36m[2025-07-02 07:23:35,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10960896. Throughput: 0: 272.4. Samples: 10973408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:23:35,986][166323] Avg episode reward: [(0, '1283.418')]
[37m[1m[2025-07-02 07:23:36,073][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021400_10960896.pth...
[36m[2025-07-02 07:23:36,077][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021272_10895360.pth
[36m[2025-07-02 07:23:40,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10960896. Throughput: 0: 271.7. Samples: 10975072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:23:40,994][166323] Avg episode reward: [(0, '1315.026')]
[36m[2025-07-02 07:23:45,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 10960896. Throughput: 0: 274.1. Samples: 10976640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:23:45,956][166323] Avg episode reward: [(0, '1301.638')]
[36m[2025-07-02 07:23:50,993][166323] Fps is (10 sec: 1638.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10977280. Throughput: 0: 273.7. Samples: 10977456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:23:50,993][166323] Avg episode reward: [(0, '1273.403')]
[31m[39020683 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[39020683 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[39020683 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:23:55,991][166323] Fps is (10 sec: 1632.6, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10977280. Throughput: 0: 273.9. Samples: 10979216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:23:55,991][166323] Avg episode reward: [(0, '1301.013')]
[31m[39027221 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[39027221 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[39027222 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[39027429 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[39027429 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[39027430 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:24:00,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10977280. Throughput: 0: 273.6. Samples: 10980816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:24:00,978][166323] Avg episode reward: [(0, '1278.723')]
[36m[2025-07-02 07:24:05,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10977280. Throughput: 0: 273.2. Samples: 10981600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:24:05,966][166323] Avg episode reward: [(0, '1320.312')]
[36m[2025-07-02 07:24:10,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10977280. Throughput: 0: 272.2. Samples: 10983200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:24:10,973][166323] Avg episode reward: [(0, '1310.078')]
[36m[2025-07-02 07:24:15,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 10977280. Throughput: 0: 269.3. Samples: 10984736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:24:15,944][166323] Avg episode reward: [(0, '1271.596')]
[36m[2025-07-02 07:24:20,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10977280. Throughput: 0: 268.6. Samples: 10985488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:24:20,952][166323] Avg episode reward: [(0, '1304.292')]
[36m[2025-07-02 07:24:25,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10977280. Throughput: 0: 269.7. Samples: 10987200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:24:25,964][166323] Avg episode reward: [(0, '1343.295')]
[36m[2025-07-02 07:24:30,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10977280. Throughput: 0: 269.2. Samples: 10988752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:24:30,946][166323] Avg episode reward: [(0, '1370.387')]
[36m[2025-07-02 07:24:35,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10977280. Throughput: 0: 267.1. Samples: 10989472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:24:35,987][166323] Avg episode reward: [(0, '1343.723')]
[36m[2025-07-02 07:24:40,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10977280. Throughput: 0: 262.9. Samples: 10991040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:24:40,962][166323] Avg episode reward: [(0, '1343.320')]
[36m[2025-07-02 07:24:45,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 10977280. Throughput: 0: 267.6. Samples: 10992848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:24:45,945][166323] Avg episode reward: [(0, '1265.102')]
[36m[2025-07-02 07:24:50,974][166323] Fps is (10 sec: 1636.3, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10993664. Throughput: 0: 269.5. Samples: 10993728. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 07:24:50,975][166323] Avg episode reward: [(0, '1272.585')]
[36m[2025-07-02 07:24:55,967][166323] Fps is (10 sec: 1634.8, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10993664. Throughput: 0: 270.3. Samples: 10995360. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 07:24:55,967][166323] Avg episode reward: [(0, '1262.316')]
[31m[39087699 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[39087699 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[39087700 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:25:00,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10993664. Throughput: 0: 271.0. Samples: 10996944. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 07:25:00,990][166323] Avg episode reward: [(0, '1236.123')]
[36m[2025-07-02 07:25:06,007][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10993664. Throughput: 0: 273.4. Samples: 10997808. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 07:25:06,007][166323] Avg episode reward: [(0, '1189.618')]
[36m[2025-07-02 07:25:10,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 10993664. Throughput: 0: 270.7. Samples: 10999376. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 07:25:10,943][166323] Avg episode reward: [(0, '1143.336')]
[36m[2025-07-02 07:25:15,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10993664. Throughput: 0: 270.9. Samples: 11000944. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 07:25:15,951][166323] Avg episode reward: [(0, '1213.552')]
[36m[2025-07-02 07:25:20,998][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10993664. Throughput: 0: 274.4. Samples: 11001824. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 07:25:20,999][166323] Avg episode reward: [(0, '1179.570')]
[36m[2025-07-02 07:25:25,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 10993664. Throughput: 0: 277.3. Samples: 11003520. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 07:25:25,972][166323] Avg episode reward: [(0, '1245.771')]
[36m[2025-07-02 07:25:30,980][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10993664. Throughput: 0: 273.9. Samples: 11005184. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 07:25:30,980][166323] Avg episode reward: [(0, '1286.749')]
[36m[2025-07-02 07:25:35,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 10993664. Throughput: 0: 271.9. Samples: 11005968. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 07:25:35,985][166323] Avg episode reward: [(0, '1271.349')]
[37m[1m[2025-07-02 07:25:36,035][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021464_10993664.pth...
[36m[2025-07-02 07:25:36,039][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021336_10928128.pth
[36m[2025-07-02 07:25:40,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 10993664. Throughput: 0: 274.3. Samples: 11007712. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 07:25:40,996][166323] Avg episode reward: [(0, '1264.613')]
[36m[2025-07-02 07:25:45,973][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 10993664. Throughput: 0: 277.1. Samples: 11009408. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 07:25:45,974][166323] Avg episode reward: [(0, '1289.149')]
[36m[2025-07-02 07:25:50,982][166323] Fps is (10 sec: 1640.7, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11010048. Throughput: 0: 273.6. Samples: 11010112. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 07:25:50,982][166323] Avg episode reward: [(0, '1269.209')]
[36m[2025-07-02 07:25:55,961][166323] Fps is (10 sec: 1640.3, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11010048. Throughput: 0: 276.9. Samples: 11011840. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 07:25:55,962][166323] Avg episode reward: [(0, '1286.697')]
[31m[39148563 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[39148563 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[39148563 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:26:00,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11010048. Throughput: 0: 278.9. Samples: 11013504. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 07:26:00,983][166323] Avg episode reward: [(0, '1270.622')]
[36m[2025-07-02 07:26:05,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11010048. Throughput: 0: 279.5. Samples: 11014400. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 07:26:05,986][166323] Avg episode reward: [(0, '1232.039')]
[31m[39155169 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[39155169 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[39155169 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:26:10,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11010048. Throughput: 0: 278.7. Samples: 11016064. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 07:26:10,985][166323] Avg episode reward: [(0, '1254.207')]
[36m[2025-07-02 07:26:15,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 11010048. Throughput: 0: 278.3. Samples: 11017712. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 07:26:15,999][166323] Avg episode reward: [(0, '1274.446')]
[36m[2025-07-02 07:26:20,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11010048. Throughput: 0: 279.7. Samples: 11018544. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 07:26:20,947][166323] Avg episode reward: [(0, '1290.968')]
[36m[2025-07-02 07:26:25,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11010048. Throughput: 0: 278.4. Samples: 11020240. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 07:26:25,990][166323] Avg episode reward: [(0, '1274.839')]
[36m[2025-07-02 07:26:30,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11010048. Throughput: 0: 276.4. Samples: 11021840. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 07:26:30,953][166323] Avg episode reward: [(0, '1236.681')]
[36m[2025-07-02 07:26:35,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11010048. Throughput: 0: 280.6. Samples: 11022736. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 07:26:35,964][166323] Avg episode reward: [(0, '1235.135')]
[36m[2025-07-02 07:26:40,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11010048. Throughput: 0: 277.2. Samples: 11024320. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 07:26:40,984][166323] Avg episode reward: [(0, '1236.046')]
[36m[2025-07-02 07:26:45,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.1). Total num frames: 11010048. Throughput: 0: 278.8. Samples: 11026048. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 07:26:45,972][166323] Avg episode reward: [(0, '1137.557')]
[36m[2025-07-02 07:26:50,954][166323] Fps is (10 sec: 1643.3, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11026432. Throughput: 0: 275.8. Samples: 11026800. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:26:50,954][166323] Avg episode reward: [(0, '1162.720')]
[36m[2025-07-02 07:26:55,949][166323] Fps is (10 sec: 1642.1, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11026432. Throughput: 0: 275.8. Samples: 11028464. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:26:55,949][166323] Avg episode reward: [(0, '1154.839')]
[36m[2025-07-02 07:27:00,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11026432. Throughput: 0: 275.1. Samples: 11030080. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:27:00,963][166323] Avg episode reward: [(0, '1205.443')]
[36m[2025-07-02 07:27:06,025][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 11026432. Throughput: 0: 272.2. Samples: 11030816. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:27:06,026][166323] Avg episode reward: [(0, '1234.424')]
[36m[2025-07-02 07:27:10,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11026432. Throughput: 0: 273.5. Samples: 11032544. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:27:10,977][166323] Avg episode reward: [(0, '1280.714')]
[36m[2025-07-02 07:27:15,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.6). Total num frames: 11026432. Throughput: 0: 274.6. Samples: 11034208. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:27:15,999][166323] Avg episode reward: [(0, '1264.593')]
[36m[2025-07-02 07:27:20,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11026432. Throughput: 0: 271.6. Samples: 11034960. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:27:20,969][166323] Avg episode reward: [(0, '1319.718')]
[33m[39229993 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[39229993 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.86328125
[33mCrash Rate: 0.12158203125
[33mTimeout Rate: 0.01513671875 (navigation_task.py:265)
[33m[39229993 ms][navigation_task] - WARNING : 
[33mSuccesses: 1768
[33mCrashes : 249
[33mTimeouts: 31 (navigation_task.py:268)
[36m[2025-07-02 07:27:25,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11026432. Throughput: 0: 273.9. Samples: 11036640. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:27:25,958][166323] Avg episode reward: [(0, '1299.411')]
[36m[2025-07-02 07:27:30,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11026432. Throughput: 0: 273.8. Samples: 11038368. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:27:30,973][166323] Avg episode reward: [(0, '1283.739')]
[36m[2025-07-02 07:27:35,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11026432. Throughput: 0: 276.3. Samples: 11039232. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:27:35,947][166323] Avg episode reward: [(0, '1286.266')]
[37m[1m[2025-07-02 07:27:36,039][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021528_11026432.pth...
[36m[2025-07-02 07:27:36,043][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021400_10960896.pth
[31m[39247827 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[39247827 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[39247827 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:27:40,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11026432. Throughput: 0: 273.3. Samples: 11040768. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:27:40,974][166323] Avg episode reward: [(0, '1274.966')]
[36m[2025-07-02 07:27:45,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 11026432. Throughput: 0: 275.6. Samples: 11042480. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 07:27:45,953][166323] Avg episode reward: [(0, '1262.723')]
[36m[2025-07-02 07:27:50,964][166323] Fps is (10 sec: 1640.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11042816. Throughput: 0: 276.6. Samples: 11043248. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 07:27:50,965][166323] Avg episode reward: [(0, '1288.204')]
[36m[2025-07-02 07:27:55,986][166323] Fps is (10 sec: 1633.1, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11042816. Throughput: 0: 274.1. Samples: 11044880. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 07:27:55,986][166323] Avg episode reward: [(0, '1231.216')]
[36m[2025-07-02 07:28:00,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11042816. Throughput: 0: 274.7. Samples: 11046560. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 07:28:00,966][166323] Avg episode reward: [(0, '1242.681')]
[36m[2025-07-02 07:28:05,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 11042816. Throughput: 0: 275.6. Samples: 11047360. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 07:28:05,954][166323] Avg episode reward: [(0, '1237.508')]
[36m[2025-07-02 07:28:10,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11042816. Throughput: 0: 278.5. Samples: 11049168. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 07:28:10,946][166323] Avg episode reward: [(0, '1215.161')]
[36m[2025-07-02 07:28:15,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11042816. Throughput: 0: 277.8. Samples: 11050864. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 07:28:15,949][166323] Avg episode reward: [(0, '1252.562')]
[36m[2025-07-02 07:28:20,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11042816. Throughput: 0: 277.9. Samples: 11051744. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 07:28:20,975][166323] Avg episode reward: [(0, '1250.029')]
[31m[39291190 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[39291190 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[39291191 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:28:25,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11042816. Throughput: 0: 282.7. Samples: 11053488. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 07:28:25,971][166323] Avg episode reward: [(0, '1293.310')]
[36m[2025-07-02 07:28:30,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11042816. Throughput: 0: 280.9. Samples: 11055120. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 07:28:30,953][166323] Avg episode reward: [(0, '1295.008')]
[36m[2025-07-02 07:28:35,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11042816. Throughput: 0: 283.1. Samples: 11055984. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 07:28:35,950][166323] Avg episode reward: [(0, '1334.675')]
[36m[2025-07-02 07:28:40,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11042816. Throughput: 0: 283.0. Samples: 11057616. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 07:28:40,990][166323] Avg episode reward: [(0, '1365.759')]
[36m[2025-07-02 07:28:45,990][166323] Fps is (10 sec: 1631.8, 60 sec: 545.8, 300 sec: 277.7). Total num frames: 11059200. Throughput: 0: 281.4. Samples: 11059232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:28:45,990][166323] Avg episode reward: [(0, '1343.815')]
[36m[2025-07-02 07:28:50,951][166323] Fps is (10 sec: 1644.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11059200. Throughput: 0: 282.7. Samples: 11060080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:28:50,951][166323] Avg episode reward: [(0, '1326.960')]
[36m[2025-07-02 07:28:55,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11059200. Throughput: 0: 276.3. Samples: 11061616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:28:55,992][166323] Avg episode reward: [(0, '1297.905')]
[31m[39328982 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[39328982 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[39328983 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:29:00,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11059200. Throughput: 0: 276.3. Samples: 11063296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:29:00,946][166323] Avg episode reward: [(0, '1307.474')]
[36m[2025-07-02 07:29:05,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11059200. Throughput: 0: 273.5. Samples: 11064048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:29:05,958][166323] Avg episode reward: [(0, '1326.381')]
[36m[2025-07-02 07:29:10,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11059200. Throughput: 0: 271.3. Samples: 11065696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:29:10,962][166323] Avg episode reward: [(0, '1261.613')]
[36m[2025-07-02 07:29:15,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11059200. Throughput: 0: 270.2. Samples: 11067280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:29:15,964][166323] Avg episode reward: [(0, '1240.281')]
[36m[2025-07-02 07:29:20,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11059200. Throughput: 0: 270.2. Samples: 11068144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:29:20,947][166323] Avg episode reward: [(0, '1238.064')]
[36m[2025-07-02 07:29:25,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11059200. Throughput: 0: 269.2. Samples: 11069728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:29:25,977][166323] Avg episode reward: [(0, '1226.844')]
[36m[2025-07-02 07:29:30,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11059200. Throughput: 0: 271.9. Samples: 11071456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:29:30,946][166323] Avg episode reward: [(0, '1246.143')]
[36m[2025-07-02 07:29:35,978][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11059200. Throughput: 0: 271.1. Samples: 11072288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:29:35,979][166323] Avg episode reward: [(0, '1242.158')]
[37m[1m[2025-07-02 07:29:36,067][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021592_11059200.pth...
[36m[2025-07-02 07:29:36,073][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021464_10993664.pth
[36m[2025-07-02 07:29:40,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11059200. Throughput: 0: 271.8. Samples: 11073840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:29:40,960][166323] Avg episode reward: [(0, '1216.424')]
[31m[39372279 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[39372279 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[39372279 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:29:45,968][166323] Fps is (10 sec: 1640.1, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11075584. Throughput: 0: 272.9. Samples: 11075584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:29:45,968][166323] Avg episode reward: [(0, '1215.883')]
[36m[2025-07-02 07:29:50,945][166323] Fps is (10 sec: 1640.9, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11075584. Throughput: 0: 274.6. Samples: 11076400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:29:50,945][166323] Avg episode reward: [(0, '1194.322')]
[36m[2025-07-02 07:29:55,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11075584. Throughput: 0: 275.1. Samples: 11078080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:29:55,979][166323] Avg episode reward: [(0, '1177.180')]
[36m[2025-07-02 07:30:00,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11075584. Throughput: 0: 274.6. Samples: 11079632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:30:00,949][166323] Avg episode reward: [(0, '1216.799')]
[36m[2025-07-02 07:30:05,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11075584. Throughput: 0: 274.7. Samples: 11080512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:30:05,973][166323] Avg episode reward: [(0, '1204.708')]
[36m[2025-07-02 07:30:10,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11075584. Throughput: 0: 275.7. Samples: 11082128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:30:10,947][166323] Avg episode reward: [(0, '1201.400')]
[36m[2025-07-02 07:30:15,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11075584. Throughput: 0: 276.9. Samples: 11083920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:30:15,960][166323] Avg episode reward: [(0, '1189.923')]
[36m[2025-07-02 07:30:20,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11075584. Throughput: 0: 279.2. Samples: 11084848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:30:20,963][166323] Avg episode reward: [(0, '1291.330')]
[36m[2025-07-02 07:30:26,010][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11075584. Throughput: 0: 278.8. Samples: 11086400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:30:26,011][166323] Avg episode reward: [(0, '1309.530')]
[36m[2025-07-02 07:30:30,974][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11075584. Throughput: 0: 278.7. Samples: 11088128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:30:30,974][166323] Avg episode reward: [(0, '1298.590')]
[36m[2025-07-02 07:30:35,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11075584. Throughput: 0: 279.7. Samples: 11088992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:30:35,970][166323] Avg episode reward: [(0, '1305.434')]
[36m[2025-07-02 07:30:40,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11075584. Throughput: 0: 281.2. Samples: 11090736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:30:40,984][166323] Avg episode reward: [(0, '1326.916')]
[36m[2025-07-02 07:30:45,988][166323] Fps is (10 sec: 1635.4, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11091968. Throughput: 0: 283.8. Samples: 11092416. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:30:45,989][166323] Avg episode reward: [(0, '1257.214')]
[36m[2025-07-02 07:30:50,950][166323] Fps is (10 sec: 1643.9, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11091968. Throughput: 0: 282.1. Samples: 11093200. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:30:50,950][166323] Avg episode reward: [(0, '1254.520')]
[36m[2025-07-02 07:30:55,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11091968. Throughput: 0: 283.3. Samples: 11094880. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:30:55,953][166323] Avg episode reward: [(0, '1213.630')]
[36m[2025-07-02 07:31:00,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11091968. Throughput: 0: 279.6. Samples: 11096512. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:31:00,995][166323] Avg episode reward: [(0, '1205.249')]
[36m[2025-07-02 07:31:05,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11091968. Throughput: 0: 275.7. Samples: 11097264. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:31:05,999][166323] Avg episode reward: [(0, '1274.984')]
[36m[2025-07-02 07:31:10,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11091968. Throughput: 0: 275.9. Samples: 11098800. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:31:10,953][166323] Avg episode reward: [(0, '1256.297')]
[36m[2025-07-02 07:31:15,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11091968. Throughput: 0: 274.4. Samples: 11100480. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:31:15,989][166323] Avg episode reward: [(0, '1231.781')]
[36m[2025-07-02 07:31:20,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11091968. Throughput: 0: 273.7. Samples: 11101312. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:31:20,990][166323] Avg episode reward: [(0, '1276.899')]
[31m[39474350 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[39474351 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[39474351 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:31:25,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11091968. Throughput: 0: 268.1. Samples: 11102800. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:31:25,988][166323] Avg episode reward: [(0, '1213.642')]
[36m[2025-07-02 07:31:30,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11091968. Throughput: 0: 267.7. Samples: 11104464. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:31:30,988][166323] Avg episode reward: [(0, '1276.362')]
[36m[2025-07-02 07:31:35,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11091968. Throughput: 0: 268.2. Samples: 11105280. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:31:35,993][166323] Avg episode reward: [(0, '1284.939')]
[37m[1m[2025-07-02 07:31:36,050][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021656_11091968.pth...
[36m[2025-07-02 07:31:36,054][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021528_11026432.pth
[36m[2025-07-02 07:31:40,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11091968. Throughput: 0: 267.3. Samples: 11106912. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:31:40,962][166323] Avg episode reward: [(0, '1253.286')]
[36m[2025-07-02 07:31:45,950][166323] Fps is (10 sec: 1645.4, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11108352. Throughput: 0: 266.9. Samples: 11108512. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:31:45,950][166323] Avg episode reward: [(0, '1284.568')]
[31m[39494612 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[39494613 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[39494613 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:31:50,988][166323] Fps is (10 sec: 1634.1, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11108352. Throughput: 0: 266.4. Samples: 11109248. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:31:50,988][166323] Avg episode reward: [(0, '1281.972')]
[36m[2025-07-02 07:31:55,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11108352. Throughput: 0: 269.8. Samples: 11110944. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:31:55,964][166323] Avg episode reward: [(0, '1313.576')]
[36m[2025-07-02 07:32:00,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 11108352. Throughput: 0: 267.2. Samples: 11112496. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:32:00,965][166323] Avg episode reward: [(0, '1372.406')]
[31m[39511493 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[39511494 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[39511494 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:32:06,000][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11108352. Throughput: 0: 265.2. Samples: 11113248. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:32:06,001][166323] Avg episode reward: [(0, '1338.593')]
[36m[2025-07-02 07:32:10,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11108352. Throughput: 0: 267.1. Samples: 11114816. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:32:10,967][166323] Avg episode reward: [(0, '1335.198')]
[36m[2025-07-02 07:32:15,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11108352. Throughput: 0: 269.0. Samples: 11116560. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:32:15,955][166323] Avg episode reward: [(0, '1336.748')]
[36m[2025-07-02 07:32:20,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11108352. Throughput: 0: 268.9. Samples: 11117376. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:32:20,970][166323] Avg episode reward: [(0, '1327.849')]
[36m[2025-07-02 07:32:25,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11108352. Throughput: 0: 271.4. Samples: 11119120. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:32:25,944][166323] Avg episode reward: [(0, '1375.024')]
[36m[2025-07-02 07:32:30,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11108352. Throughput: 0: 274.4. Samples: 11120864. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:32:30,964][166323] Avg episode reward: [(0, '1343.058')]
[36m[2025-07-02 07:32:35,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11108352. Throughput: 0: 276.3. Samples: 11121680. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:32:35,990][166323] Avg episode reward: [(0, '1374.134')]
[36m[2025-07-02 07:32:41,009][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 11108352. Throughput: 0: 274.2. Samples: 11123296. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 07:32:41,009][166323] Avg episode reward: [(0, '1399.589')]
[36m[2025-07-02 07:32:45,944][166323] Fps is (10 sec: 1645.9, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11124736. Throughput: 0: 274.6. Samples: 11124848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:32:45,944][166323] Avg episode reward: [(0, '1369.461')]
[36m[2025-07-02 07:32:50,949][166323] Fps is (10 sec: 1648.2, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11124736. Throughput: 0: 276.2. Samples: 11125664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:32:50,949][166323] Avg episode reward: [(0, '1348.089')]
[36m[2025-07-02 07:32:55,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11124736. Throughput: 0: 281.3. Samples: 11127472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:32:55,952][166323] Avg episode reward: [(0, '1337.359')]
[36m[2025-07-02 07:33:00,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11124736. Throughput: 0: 279.9. Samples: 11129152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:33:00,947][166323] Avg episode reward: [(0, '1335.280')]
[36m[2025-07-02 07:33:05,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11124736. Throughput: 0: 278.3. Samples: 11129904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:33:05,981][166323] Avg episode reward: [(0, '1363.524')]
[36m[2025-07-02 07:33:10,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11124736. Throughput: 0: 272.9. Samples: 11131408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:33:10,977][166323] Avg episode reward: [(0, '1331.726')]
[36m[2025-07-02 07:33:15,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11124736. Throughput: 0: 269.6. Samples: 11132992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:33:15,954][166323] Avg episode reward: [(0, '1267.920')]
[36m[2025-07-02 07:33:20,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11124736. Throughput: 0: 269.8. Samples: 11133824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:33:20,993][166323] Avg episode reward: [(0, '1317.383')]
[36m[2025-07-02 07:33:25,983][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11124736. Throughput: 0: 270.0. Samples: 11135440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:33:25,984][166323] Avg episode reward: [(0, '1313.080')]
[36m[2025-07-02 07:33:30,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11124736. Throughput: 0: 274.0. Samples: 11137184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:33:30,971][166323] Avg episode reward: [(0, '1344.351')]
[36m[2025-07-02 07:33:35,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11124736. Throughput: 0: 273.4. Samples: 11137968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:33:35,960][166323] Avg episode reward: [(0, '1365.084')]
[37m[1m[2025-07-02 07:33:36,012][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021720_11124736.pth...
[36m[2025-07-02 07:33:36,016][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021592_11059200.pth
[36m[2025-07-02 07:33:40,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 11124736. Throughput: 0: 272.5. Samples: 11139744. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:33:40,980][166323] Avg episode reward: [(0, '1317.383')]
[36m[2025-07-02 07:33:45,975][166323] Fps is (10 sec: 1635.9, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11141120. Throughput: 0: 272.9. Samples: 11141440. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 07:33:45,975][166323] Avg episode reward: [(0, '1307.696')]
[36m[2025-07-02 07:33:50,972][166323] Fps is (10 sec: 1639.7, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11141120. Throughput: 0: 274.9. Samples: 11142272. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 07:33:50,972][166323] Avg episode reward: [(0, '1286.726')]
[36m[2025-07-02 07:33:55,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11141120. Throughput: 0: 278.9. Samples: 11143952. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 07:33:55,954][166323] Avg episode reward: [(0, '1272.448')]
[36m[2025-07-02 07:34:00,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11141120. Throughput: 0: 279.3. Samples: 11145568. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 07:34:00,986][166323] Avg episode reward: [(0, '1276.285')]
[36m[2025-07-02 07:34:05,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11141120. Throughput: 0: 281.3. Samples: 11146480. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 07:34:05,977][166323] Avg episode reward: [(0, '1280.083')]
[36m[2025-07-02 07:34:11,004][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11141120. Throughput: 0: 281.8. Samples: 11148128. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 07:34:11,004][166323] Avg episode reward: [(0, '1235.323')]
[31m[39643489 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[39643490 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[39643490 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:34:16,005][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 11141120. Throughput: 0: 281.7. Samples: 11149872. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 07:34:16,005][166323] Avg episode reward: [(0, '1224.117')]
[36m[2025-07-02 07:34:20,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11141120. Throughput: 0: 283.6. Samples: 11150736. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 07:34:20,986][166323] Avg episode reward: [(0, '1285.648')]
[36m[2025-07-02 07:34:25,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11141120. Throughput: 0: 283.6. Samples: 11152496. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 07:34:25,944][166323] Avg episode reward: [(0, '1300.406')]
[36m[2025-07-02 07:34:30,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11141120. Throughput: 0: 285.5. Samples: 11154288. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 07:34:30,980][166323] Avg episode reward: [(0, '1250.590')]
[36m[2025-07-02 07:34:35,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11141120. Throughput: 0: 285.2. Samples: 11155104. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 07:34:35,965][166323] Avg episode reward: [(0, '1285.097')]
[36m[2025-07-02 07:34:40,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 11141120. Throughput: 0: 284.1. Samples: 11156736. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 07:34:40,953][166323] Avg episode reward: [(0, '1264.360')]
[36m[2025-07-02 07:34:46,000][166323] Fps is (10 sec: 1632.7, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 11157504. Throughput: 0: 284.0. Samples: 11158352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:34:46,000][166323] Avg episode reward: [(0, '1266.215')]
[36m[2025-07-02 07:34:50,983][166323] Fps is (10 sec: 1633.4, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11157504. Throughput: 0: 282.3. Samples: 11159184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:34:50,983][166323] Avg episode reward: [(0, '1245.431')]
[36m[2025-07-02 07:34:55,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11157504. Throughput: 0: 281.7. Samples: 11160800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:34:55,985][166323] Avg episode reward: [(0, '1204.376')]
[36m[2025-07-02 07:35:00,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11157504. Throughput: 0: 277.9. Samples: 11162368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:35:00,972][166323] Avg episode reward: [(0, '1154.806')]
[36m[2025-07-02 07:35:05,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11157504. Throughput: 0: 278.4. Samples: 11163264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:35:05,989][166323] Avg episode reward: [(0, '1189.510')]
[36m[2025-07-02 07:35:10,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11157504. Throughput: 0: 278.8. Samples: 11165056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:35:10,994][166323] Avg episode reward: [(0, '1137.893')]
[36m[2025-07-02 07:35:15,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11157504. Throughput: 0: 276.2. Samples: 11166720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:35:15,991][166323] Avg episode reward: [(0, '1154.036')]
[36m[2025-07-02 07:35:20,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11157504. Throughput: 0: 276.1. Samples: 11167536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:35:20,992][166323] Avg episode reward: [(0, '1244.687')]
[36m[2025-07-02 07:35:25,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11157504. Throughput: 0: 274.9. Samples: 11169104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:35:25,949][166323] Avg episode reward: [(0, '1250.682')]
[36m[2025-07-02 07:35:30,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11157504. Throughput: 0: 279.7. Samples: 11170928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:35:30,957][166323] Avg episode reward: [(0, '1262.070')]
[36m[2025-07-02 07:35:35,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11157504. Throughput: 0: 279.6. Samples: 11171760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:35:35,958][166323] Avg episode reward: [(0, '1354.536')]
[37m[1m[2025-07-02 07:35:36,025][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021784_11157504.pth...
[36m[2025-07-02 07:35:36,029][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021656_11091968.pth
[36m[2025-07-02 07:35:41,016][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 222.1). Total num frames: 11157504. Throughput: 0: 277.9. Samples: 11173312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:35:41,016][166323] Avg episode reward: [(0, '1343.678')]
[36m[2025-07-02 07:35:45,958][166323] Fps is (10 sec: 1638.3, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11173888. Throughput: 0: 279.6. Samples: 11174944. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 07:35:45,958][166323] Avg episode reward: [(0, '1370.782')]
[36m[2025-07-02 07:35:50,990][166323] Fps is (10 sec: 1642.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11173888. Throughput: 0: 278.0. Samples: 11175776. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 07:35:50,991][166323] Avg episode reward: [(0, '1297.543')]
[31m[39740242 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[39740242 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[39740242 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:35:56,007][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11173888. Throughput: 0: 272.3. Samples: 11177312. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 07:35:56,007][166323] Avg episode reward: [(0, '1261.965')]
[36m[2025-07-02 07:36:00,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11173888. Throughput: 0: 271.7. Samples: 11178944. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 07:36:00,978][166323] Avg episode reward: [(0, '1314.178')]
[36m[2025-07-02 07:36:05,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11173888. Throughput: 0: 271.6. Samples: 11179760. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 07:36:05,994][166323] Avg episode reward: [(0, '1317.428')]
[36m[2025-07-02 07:36:10,995][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11173888. Throughput: 0: 274.6. Samples: 11181472. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 07:36:10,995][166323] Avg episode reward: [(0, '1302.778')]
[36m[2025-07-02 07:36:16,000][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11173888. Throughput: 0: 276.4. Samples: 11183376. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 07:36:16,000][166323] Avg episode reward: [(0, '1272.915')]
[36m[2025-07-02 07:36:20,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11173888. Throughput: 0: 276.3. Samples: 11184192. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 07:36:20,960][166323] Avg episode reward: [(0, '1310.564')]
[36m[2025-07-02 07:36:25,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11173888. Throughput: 0: 277.7. Samples: 11185792. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 07:36:25,961][166323] Avg episode reward: [(0, '1290.278')]
[36m[2025-07-02 07:36:30,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11173888. Throughput: 0: 276.3. Samples: 11187376. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 07:36:30,950][166323] Avg episode reward: [(0, '1314.420')]
[36m[2025-07-02 07:36:35,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11173888. Throughput: 0: 276.1. Samples: 11188192. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 07:36:35,963][166323] Avg episode reward: [(0, '1322.774')]
[36m[2025-07-02 07:36:40,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 222.2). Total num frames: 11173888. Throughput: 0: 277.3. Samples: 11189776. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 07:36:40,948][166323] Avg episode reward: [(0, '1255.754')]
[36m[2025-07-02 07:36:45,987][166323] Fps is (10 sec: 1634.5, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11190272. Throughput: 0: 274.1. Samples: 11191280. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 07:36:45,987][166323] Avg episode reward: [(0, '1225.765')]
[36m[2025-07-02 07:36:50,944][166323] Fps is (10 sec: 1639.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11190272. Throughput: 0: 275.5. Samples: 11192144. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 07:36:50,945][166323] Avg episode reward: [(0, '1269.664')]
[36m[2025-07-02 07:36:55,995][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11190272. Throughput: 0: 272.0. Samples: 11193712. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 07:36:55,996][166323] Avg episode reward: [(0, '1242.090')]
[36m[2025-07-02 07:37:00,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11190272. Throughput: 0: 265.3. Samples: 11195312. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 07:37:00,983][166323] Avg episode reward: [(0, '1276.587')]
[36m[2025-07-02 07:37:05,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11190272. Throughput: 0: 265.1. Samples: 11196128. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 07:37:05,982][166323] Avg episode reward: [(0, '1206.080')]
[31m[39818540 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[39818540 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[39818540 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:37:11,003][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 11190272. Throughput: 0: 265.4. Samples: 11197744. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 07:37:11,003][166323] Avg episode reward: [(0, '1225.088')]
[36m[2025-07-02 07:37:15,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11190272. Throughput: 0: 267.6. Samples: 11199424. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 07:37:15,969][166323] Avg episode reward: [(0, '1274.641')]
[36m[2025-07-02 07:37:20,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11190272. Throughput: 0: 268.6. Samples: 11200288. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 07:37:20,989][166323] Avg episode reward: [(0, '1296.107')]
[36m[2025-07-02 07:37:25,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11190272. Throughput: 0: 269.3. Samples: 11201904. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 07:37:25,984][166323] Avg episode reward: [(0, '1269.954')]
[36m[2025-07-02 07:37:31,010][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 11190272. Throughput: 0: 273.6. Samples: 11203600. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 07:37:31,010][166323] Avg episode reward: [(0, '1306.867')]
[36m[2025-07-02 07:37:35,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11190272. Throughput: 0: 271.8. Samples: 11204384. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 07:37:35,981][166323] Avg episode reward: [(0, '1327.478')]
[37m[1m[2025-07-02 07:37:36,079][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021848_11190272.pth...
[36m[2025-07-02 07:37:36,090][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021720_11124736.pth
[36m[2025-07-02 07:37:41,005][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 222.1). Total num frames: 11190272. Throughput: 0: 271.9. Samples: 11205952. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 07:37:41,006][166323] Avg episode reward: [(0, '1326.184')]
[36m[2025-07-02 07:37:45,973][166323] Fps is (10 sec: 1639.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11206656. Throughput: 0: 271.7. Samples: 11207536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:37:45,973][166323] Avg episode reward: [(0, '1316.639')]
[36m[2025-07-02 07:37:50,986][166323] Fps is (10 sec: 1641.6, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11206656. Throughput: 0: 273.0. Samples: 11208416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:37:50,986][166323] Avg episode reward: [(0, '1251.916')]
[36m[2025-07-02 07:37:55,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11206656. Throughput: 0: 276.5. Samples: 11210176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:37:55,968][166323] Avg episode reward: [(0, '1248.337')]
[36m[2025-07-02 07:38:00,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11206656. Throughput: 0: 276.3. Samples: 11211856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:38:00,963][166323] Avg episode reward: [(0, '1246.852')]
[36m[2025-07-02 07:38:05,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11206656. Throughput: 0: 276.0. Samples: 11212704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:38:05,978][166323] Avg episode reward: [(0, '1214.142')]
[36m[2025-07-02 07:38:10,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11206656. Throughput: 0: 277.5. Samples: 11214384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:38:10,963][166323] Avg episode reward: [(0, '1204.855')]
[36m[2025-07-02 07:38:16,000][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11206656. Throughput: 0: 276.0. Samples: 11216016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:38:16,001][166323] Avg episode reward: [(0, '1207.202')]
[36m[2025-07-02 07:38:20,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11206656. Throughput: 0: 276.6. Samples: 11216832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:38:20,983][166323] Avg episode reward: [(0, '1204.206')]
[36m[2025-07-02 07:38:26,016][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11206656. Throughput: 0: 281.9. Samples: 11218640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:38:26,016][166323] Avg episode reward: [(0, '1266.766')]
[36m[2025-07-02 07:38:30,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11206656. Throughput: 0: 282.5. Samples: 11220240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:38:30,948][166323] Avg episode reward: [(0, '1287.127')]
[36m[2025-07-02 07:38:35,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11206656. Throughput: 0: 280.6. Samples: 11221040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:38:35,970][166323] Avg episode reward: [(0, '1250.161')]
[31m[39908441 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[39908442 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[39908442 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:38:40,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 222.2). Total num frames: 11206656. Throughput: 0: 276.0. Samples: 11222592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:38:40,954][166323] Avg episode reward: [(0, '1219.455')]
[36m[2025-07-02 07:38:45,951][166323] Fps is (10 sec: 1641.4, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11223040. Throughput: 0: 273.5. Samples: 11224160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:38:45,952][166323] Avg episode reward: [(0, '1214.263')]
[36m[2025-07-02 07:38:50,962][166323] Fps is (10 sec: 1636.9, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11223040. Throughput: 0: 271.4. Samples: 11224912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:38:50,963][166323] Avg episode reward: [(0, '1169.958')]
[33m[39922456 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[39922456 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.86572265625
[33mCrash Rate: 0.123046875
[33mTimeout Rate: 0.01123046875 (navigation_task.py:265)
[33m[39922456 ms][navigation_task] - WARNING : 
[33mSuccesses: 1773
[33mCrashes : 252
[33mTimeouts: 23 (navigation_task.py:268)
[36m[2025-07-02 07:38:55,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11223040. Throughput: 0: 269.6. Samples: 11226512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:38:55,947][166323] Avg episode reward: [(0, '1171.879')]
[36m[2025-07-02 07:39:00,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11223040. Throughput: 0: 270.0. Samples: 11228160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:39:00,983][166323] Avg episode reward: [(0, '1178.404')]
[36m[2025-07-02 07:39:06,012][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11223040. Throughput: 0: 269.0. Samples: 11228944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:39:06,012][166323] Avg episode reward: [(0, '1148.841')]
[36m[2025-07-02 07:39:10,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11223040. Throughput: 0: 264.1. Samples: 11230512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:39:10,976][166323] Avg episode reward: [(0, '1159.042')]
[36m[2025-07-02 07:39:15,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11223040. Throughput: 0: 261.9. Samples: 11232032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:39:15,976][166323] Avg episode reward: [(0, '1182.448')]
[36m[2025-07-02 07:39:20,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11223040. Throughput: 0: 263.5. Samples: 11232896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:39:20,972][166323] Avg episode reward: [(0, '1224.092')]
[36m[2025-07-02 07:39:25,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11223040. Throughput: 0: 265.9. Samples: 11234560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:39:25,962][166323] Avg episode reward: [(0, '1246.911')]
[36m[2025-07-02 07:39:30,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11223040. Throughput: 0: 268.4. Samples: 11236240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:39:30,964][166323] Avg episode reward: [(0, '1285.025')]
[36m[2025-07-02 07:39:35,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11223040. Throughput: 0: 270.2. Samples: 11237072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:39:35,962][166323] Avg episode reward: [(0, '1248.564')]
[37m[1m[2025-07-02 07:39:36,033][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021912_11223040.pth...
[36m[2025-07-02 07:39:36,037][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021784_11157504.pth
[36m[2025-07-02 07:39:40,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 11223040. Throughput: 0: 273.2. Samples: 11238816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:39:40,975][166323] Avg episode reward: [(0, '1247.242')]
[36m[2025-07-02 07:39:45,981][166323] Fps is (10 sec: 1635.3, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11239424. Throughput: 0: 274.1. Samples: 11240496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:39:45,981][166323] Avg episode reward: [(0, '1312.099')]
[36m[2025-07-02 07:39:50,979][166323] Fps is (10 sec: 1637.8, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11239424. Throughput: 0: 274.7. Samples: 11241296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:39:50,979][166323] Avg episode reward: [(0, '1329.365')]
[36m[2025-07-02 07:39:55,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11239424. Throughput: 0: 274.3. Samples: 11242848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:39:55,955][166323] Avg episode reward: [(0, '1279.155')]
[36m[2025-07-02 07:40:00,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11239424. Throughput: 0: 278.8. Samples: 11244576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:40:00,976][166323] Avg episode reward: [(0, '1286.420')]
[36m[2025-07-02 07:40:05,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11239424. Throughput: 0: 278.8. Samples: 11245440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:40:05,971][166323] Avg episode reward: [(0, '1310.743')]
[36m[2025-07-02 07:40:10,995][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11239424. Throughput: 0: 280.7. Samples: 11247200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:40:10,996][166323] Avg episode reward: [(0, '1319.116')]
[36m[2025-07-02 07:40:15,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11239424. Throughput: 0: 281.8. Samples: 11248928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:40:15,994][166323] Avg episode reward: [(0, '1332.732')]
[31m[40005982 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[40005982 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[40005983 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:40:21,013][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 11239424. Throughput: 0: 280.6. Samples: 11249712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:40:21,013][166323] Avg episode reward: [(0, '1259.719')]
[31m[40012413 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[40012414 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[40012414 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:40:25,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11239424. Throughput: 0: 276.9. Samples: 11251280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:40:25,993][166323] Avg episode reward: [(0, '1270.290')]
[36m[2025-07-02 07:40:30,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11239424. Throughput: 0: 275.2. Samples: 11252880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:40:30,987][166323] Avg episode reward: [(0, '1308.985')]
[36m[2025-07-02 07:40:35,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 11239424. Throughput: 0: 274.7. Samples: 11253648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:40:35,948][166323] Avg episode reward: [(0, '1292.897')]
[36m[2025-07-02 07:40:40,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 11239424. Throughput: 0: 275.4. Samples: 11255248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:40:40,988][166323] Avg episode reward: [(0, '1252.746')]
[36m[2025-07-02 07:40:45,990][166323] Fps is (10 sec: 1631.5, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11255808. Throughput: 0: 271.2. Samples: 11256784. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 07:40:45,990][166323] Avg episode reward: [(0, '1260.548')]
[36m[2025-07-02 07:40:50,997][166323] Fps is (10 sec: 1636.9, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11255808. Throughput: 0: 270.8. Samples: 11257632. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 07:40:50,997][166323] Avg episode reward: [(0, '1307.676')]
[36m[2025-07-02 07:40:55,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11255808. Throughput: 0: 267.8. Samples: 11259248. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 07:40:55,979][166323] Avg episode reward: [(0, '1311.105')]
[36m[2025-07-02 07:41:00,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11255808. Throughput: 0: 264.4. Samples: 11260816. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 07:41:00,954][166323] Avg episode reward: [(0, '1303.228')]
[36m[2025-07-02 07:41:05,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11255808. Throughput: 0: 263.4. Samples: 11261552. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 07:41:05,958][166323] Avg episode reward: [(0, '1255.993')]
[36m[2025-07-02 07:41:10,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11255808. Throughput: 0: 260.6. Samples: 11263008. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 07:41:10,990][166323] Avg episode reward: [(0, '1239.668')]
[36m[2025-07-02 07:41:15,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11255808. Throughput: 0: 261.1. Samples: 11264624. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 07:41:15,961][166323] Avg episode reward: [(0, '1243.952')]
[36m[2025-07-02 07:41:20,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11255808. Throughput: 0: 261.8. Samples: 11265440. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 07:41:20,990][166323] Avg episode reward: [(0, '1187.706')]
[36m[2025-07-02 07:41:25,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11255808. Throughput: 0: 266.8. Samples: 11267248. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 07:41:25,965][166323] Avg episode reward: [(0, '1162.848')]
[36m[2025-07-02 07:41:30,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11255808. Throughput: 0: 268.2. Samples: 11268848. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 07:41:30,976][166323] Avg episode reward: [(0, '1151.631')]
[36m[2025-07-02 07:41:35,974][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11255808. Throughput: 0: 266.1. Samples: 11269600. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 07:41:35,974][166323] Avg episode reward: [(0, '1171.713')]
[37m[1m[2025-07-02 07:41:36,029][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021976_11255808.pth...
[36m[2025-07-02 07:41:36,033][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021848_11190272.pth
[36m[2025-07-02 07:41:40,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 222.2). Total num frames: 11255808. Throughput: 0: 264.0. Samples: 11271120. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 07:41:40,946][166323] Avg episode reward: [(0, '1181.553')]
[36m[2025-07-02 07:41:45,975][166323] Fps is (10 sec: 1638.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11272192. Throughput: 0: 265.1. Samples: 11272752. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 07:41:45,976][166323] Avg episode reward: [(0, '1190.737')]
[36m[2025-07-02 07:41:50,971][166323] Fps is (10 sec: 1634.3, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11272192. Throughput: 0: 267.3. Samples: 11273584. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 07:41:50,971][166323] Avg episode reward: [(0, '1191.568')]
[36m[2025-07-02 07:41:56,009][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11272192. Throughput: 0: 270.8. Samples: 11275200. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 07:41:56,009][166323] Avg episode reward: [(0, '1219.459')]
[36m[2025-07-02 07:42:00,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11272192. Throughput: 0: 272.9. Samples: 11276912. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 07:42:00,988][166323] Avg episode reward: [(0, '1261.700')]
[36m[2025-07-02 07:42:05,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 11272192. Throughput: 0: 273.3. Samples: 11277728. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 07:42:05,944][166323] Avg episode reward: [(0, '1309.522')]
[36m[2025-07-02 07:42:10,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11272192. Throughput: 0: 270.3. Samples: 11279408. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 07:42:10,954][166323] Avg episode reward: [(0, '1357.000')]
[36m[2025-07-02 07:42:15,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11272192. Throughput: 0: 271.8. Samples: 11281072. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 07:42:15,954][166323] Avg episode reward: [(0, '1302.002')]
[36m[2025-07-02 07:42:20,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11272192. Throughput: 0: 270.8. Samples: 11281792. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 07:42:20,990][166323] Avg episode reward: [(0, '1334.225')]
[36m[2025-07-02 07:42:25,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11272192. Throughput: 0: 277.5. Samples: 11283616. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 07:42:25,982][166323] Avg episode reward: [(0, '1342.281')]
[36m[2025-07-02 07:42:30,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11272192. Throughput: 0: 277.7. Samples: 11285248. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 07:42:30,972][166323] Avg episode reward: [(0, '1351.039')]
[36m[2025-07-02 07:42:35,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11272192. Throughput: 0: 277.9. Samples: 11286096. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 07:42:35,999][166323] Avg episode reward: [(0, '1289.261')]
[36m[2025-07-02 07:42:40,978][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.2). Total num frames: 11272192. Throughput: 0: 278.6. Samples: 11287728. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 07:42:40,979][166323] Avg episode reward: [(0, '1260.167')]
[36m[2025-07-02 07:42:45,988][166323] Fps is (10 sec: 1640.1, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11288576. Throughput: 0: 277.0. Samples: 11289376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:42:45,989][166323] Avg episode reward: [(0, '1259.514')]
[36m[2025-07-02 07:42:51,002][166323] Fps is (10 sec: 1634.5, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11288576. Throughput: 0: 277.7. Samples: 11290240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:42:51,003][166323] Avg episode reward: [(0, '1289.529')]
[36m[2025-07-02 07:42:55,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11288576. Throughput: 0: 275.9. Samples: 11291824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:42:55,952][166323] Avg episode reward: [(0, '1305.397')]
[36m[2025-07-02 07:43:00,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11288576. Throughput: 0: 272.9. Samples: 11293360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:43:00,985][166323] Avg episode reward: [(0, '1300.223')]
[36m[2025-07-02 07:43:05,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11288576. Throughput: 0: 273.9. Samples: 11294112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:43:05,969][166323] Avg episode reward: [(0, '1319.461')]
[36m[2025-07-02 07:43:10,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11288576. Throughput: 0: 268.3. Samples: 11295680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:43:10,950][166323] Avg episode reward: [(0, '1351.544')]
[36m[2025-07-02 07:43:15,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11288576. Throughput: 0: 269.3. Samples: 11297360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:43:15,946][166323] Avg episode reward: [(0, '1364.424')]
[36m[2025-07-02 07:43:20,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11288576. Throughput: 0: 269.6. Samples: 11298224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:43:20,986][166323] Avg episode reward: [(0, '1410.902')]
[36m[2025-07-02 07:43:25,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11288576. Throughput: 0: 269.8. Samples: 11299872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:43:25,988][166323] Avg episode reward: [(0, '1407.764')]
[36m[2025-07-02 07:43:30,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11288576. Throughput: 0: 270.1. Samples: 11301520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:43:30,952][166323] Avg episode reward: [(0, '1353.091')]
[36m[2025-07-02 07:43:35,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11288576. Throughput: 0: 267.6. Samples: 11302272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:43:35,957][166323] Avg episode reward: [(0, '1371.256')]
[37m[1m[2025-07-02 07:43:36,015][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000022040_11288576.pth...
[36m[2025-07-02 07:43:36,019][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021912_11223040.pth
[36m[2025-07-02 07:43:40,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 11288576. Throughput: 0: 267.8. Samples: 11303872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:43:40,946][166323] Avg episode reward: [(0, '1326.190')]
[36m[2025-07-02 07:43:45,973][166323] Fps is (10 sec: 1635.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11304960. Throughput: 0: 271.7. Samples: 11305584. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 07:43:45,973][166323] Avg episode reward: [(0, '1259.501')]
[36m[2025-07-02 07:43:50,948][166323] Fps is (10 sec: 1638.1, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11304960. Throughput: 0: 274.6. Samples: 11306464. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 07:43:50,948][166323] Avg episode reward: [(0, '1234.822')]
[36m[2025-07-02 07:43:55,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11304960. Throughput: 0: 279.8. Samples: 11308272. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 07:43:55,950][166323] Avg episode reward: [(0, '1234.282')]
[36m[2025-07-02 07:44:00,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11304960. Throughput: 0: 277.1. Samples: 11309840. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 07:44:00,988][166323] Avg episode reward: [(0, '1178.552')]
[36m[2025-07-02 07:44:05,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11304960. Throughput: 0: 276.1. Samples: 11310640. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 07:44:05,952][166323] Avg episode reward: [(0, '1152.313')]
[36m[2025-07-02 07:44:10,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11304960. Throughput: 0: 277.3. Samples: 11312352. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 07:44:10,985][166323] Avg episode reward: [(0, '1140.761')]
[36m[2025-07-02 07:44:15,979][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11304960. Throughput: 0: 278.2. Samples: 11314048. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 07:44:15,979][166323] Avg episode reward: [(0, '1096.945')]
[36m[2025-07-02 07:44:20,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11304960. Throughput: 0: 281.5. Samples: 11314944. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 07:44:20,980][166323] Avg episode reward: [(0, '1191.111')]
[36m[2025-07-02 07:44:25,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11304960. Throughput: 0: 282.2. Samples: 11316576. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 07:44:25,967][166323] Avg episode reward: [(0, '1156.322')]
[31m[40258021 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[40258022 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[40258022 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:44:30,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11304960. Throughput: 0: 278.7. Samples: 11318128. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 07:44:30,976][166323] Avg episode reward: [(0, '1086.680')]
[36m[2025-07-02 07:44:35,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11304960. Throughput: 0: 276.6. Samples: 11318912. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 07:44:35,953][166323] Avg episode reward: [(0, '1135.004')]
[36m[2025-07-02 07:44:40,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 11304960. Throughput: 0: 276.4. Samples: 11320720. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 07:44:40,989][166323] Avg episode reward: [(0, '1164.380')]
[36m[2025-07-02 07:44:45,985][166323] Fps is (10 sec: 1633.1, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11321344. Throughput: 0: 281.3. Samples: 11322496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:44:45,986][166323] Avg episode reward: [(0, '1196.910')]
[36m[2025-07-02 07:44:50,953][166323] Fps is (10 sec: 1644.3, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11321344. Throughput: 0: 281.6. Samples: 11323312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:44:50,954][166323] Avg episode reward: [(0, '1185.767')]
[36m[2025-07-02 07:44:55,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11321344. Throughput: 0: 282.6. Samples: 11325072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:44:55,995][166323] Avg episode reward: [(0, '1171.082')]
[36m[2025-07-02 07:45:00,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11321344. Throughput: 0: 280.5. Samples: 11326672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:45:00,979][166323] Avg episode reward: [(0, '1187.075')]
[36m[2025-07-02 07:45:05,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11321344. Throughput: 0: 278.2. Samples: 11327456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:45:05,954][166323] Avg episode reward: [(0, '1256.590')]
[31m[40294936 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[40294937 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[40294937 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:45:10,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11321344. Throughput: 0: 277.0. Samples: 11329040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:45:10,958][166323] Avg episode reward: [(0, '1287.240')]
[36m[2025-07-02 07:45:15,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11321344. Throughput: 0: 277.7. Samples: 11330624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:45:15,976][166323] Avg episode reward: [(0, '1228.666')]
[36m[2025-07-02 07:45:20,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11321344. Throughput: 0: 278.4. Samples: 11331440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:45:20,945][166323] Avg episode reward: [(0, '1196.967')]
[36m[2025-07-02 07:45:25,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11321344. Throughput: 0: 273.5. Samples: 11333024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:45:25,983][166323] Avg episode reward: [(0, '1236.977')]
[36m[2025-07-02 07:45:30,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11321344. Throughput: 0: 270.9. Samples: 11334688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:45:30,986][166323] Avg episode reward: [(0, '1269.248')]
[36m[2025-07-02 07:45:35,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11321344. Throughput: 0: 270.2. Samples: 11335472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:45:35,962][166323] Avg episode reward: [(0, '1254.406')]
[37m[1m[2025-07-02 07:45:36,029][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000022104_11321344.pth...
[36m[2025-07-02 07:45:36,033][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000021976_11255808.pth
[36m[2025-07-02 07:45:40,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 11321344. Throughput: 0: 268.0. Samples: 11337120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:45:40,955][166323] Avg episode reward: [(0, '1234.291')]
[36m[2025-07-02 07:45:45,965][166323] Fps is (10 sec: 1637.9, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11337728. Throughput: 0: 268.2. Samples: 11338736. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 07:45:45,965][166323] Avg episode reward: [(0, '1246.130')]
[36m[2025-07-02 07:45:50,953][166323] Fps is (10 sec: 1638.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11337728. Throughput: 0: 271.3. Samples: 11339664. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 07:45:50,953][166323] Avg episode reward: [(0, '1252.805')]
[36m[2025-07-02 07:45:55,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11337728. Throughput: 0: 275.0. Samples: 11341424. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 07:45:55,990][166323] Avg episode reward: [(0, '1275.789')]
[36m[2025-07-02 07:46:00,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11337728. Throughput: 0: 277.4. Samples: 11343104. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 07:46:00,964][166323] Avg episode reward: [(0, '1272.993')]
[36m[2025-07-02 07:46:05,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11337728. Throughput: 0: 277.7. Samples: 11343936. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 07:46:05,949][166323] Avg episode reward: [(0, '1292.142')]
[36m[2025-07-02 07:46:10,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11337728. Throughput: 0: 279.3. Samples: 11345584. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 07:46:10,960][166323] Avg episode reward: [(0, '1317.394')]
[36m[2025-07-02 07:46:15,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11337728. Throughput: 0: 278.5. Samples: 11347216. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 07:46:15,966][166323] Avg episode reward: [(0, '1278.875')]
[36m[2025-07-02 07:46:20,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11337728. Throughput: 0: 279.3. Samples: 11348048. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 07:46:20,990][166323] Avg episode reward: [(0, '1267.559')]
[36m[2025-07-02 07:46:25,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11337728. Throughput: 0: 278.4. Samples: 11349648. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 07:46:25,957][166323] Avg episode reward: [(0, '1281.225')]
[31m[40374588 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[40374588 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[40374588 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:46:30,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11337728. Throughput: 0: 276.8. Samples: 11351200. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 07:46:30,990][166323] Avg episode reward: [(0, '1239.297')]
[36m[2025-07-02 07:46:35,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11337728. Throughput: 0: 273.2. Samples: 11351968. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 07:46:35,983][166323] Avg episode reward: [(0, '1215.206')]
[36m[2025-07-02 07:46:40,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 11337728. Throughput: 0: 270.4. Samples: 11353584. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 07:46:40,962][166323] Avg episode reward: [(0, '1211.910')]
[36m[2025-07-02 07:46:45,979][166323] Fps is (10 sec: 1639.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11354112. Throughput: 0: 267.6. Samples: 11355152. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:46:45,983][166323] Avg episode reward: [(0, '1196.037')]
[36m[2025-07-02 07:46:50,972][166323] Fps is (10 sec: 1636.7, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11354112. Throughput: 0: 265.8. Samples: 11355904. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:46:50,972][166323] Avg episode reward: [(0, '1230.859')]
[36m[2025-07-02 07:46:55,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11354112. Throughput: 0: 267.0. Samples: 11357600. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:46:55,964][166323] Avg episode reward: [(0, '1214.560')]
[36m[2025-07-02 07:47:00,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11354112. Throughput: 0: 265.0. Samples: 11359136. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:47:00,950][166323] Avg episode reward: [(0, '1226.727')]
[36m[2025-07-02 07:47:06,010][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 11354112. Throughput: 0: 264.4. Samples: 11359952. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:47:06,011][166323] Avg episode reward: [(0, '1224.742')]
[36m[2025-07-02 07:47:10,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11354112. Throughput: 0: 266.3. Samples: 11361632. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:47:10,953][166323] Avg episode reward: [(0, '1259.820')]
[36m[2025-07-02 07:47:15,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11354112. Throughput: 0: 267.8. Samples: 11363248. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:47:15,981][166323] Avg episode reward: [(0, '1248.213')]
[36m[2025-07-02 07:47:20,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11354112. Throughput: 0: 266.7. Samples: 11363968. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:47:20,978][166323] Avg episode reward: [(0, '1292.746')]
[36m[2025-07-02 07:47:25,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11354112. Throughput: 0: 267.8. Samples: 11365632. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:47:25,954][166323] Avg episode reward: [(0, '1270.967')]
[36m[2025-07-02 07:47:30,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11354112. Throughput: 0: 265.7. Samples: 11367104. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:47:30,966][166323] Avg episode reward: [(0, '1296.361')]
[36m[2025-07-02 07:47:35,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11354112. Throughput: 0: 268.0. Samples: 11367968. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:47:35,981][166323] Avg episode reward: [(0, '1312.777')]
[37m[1m[2025-07-02 07:47:36,037][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000022168_11354112.pth...
[36m[2025-07-02 07:47:36,044][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000022040_11288576.pth
[36m[2025-07-02 07:47:40,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 11354112. Throughput: 0: 266.6. Samples: 11369600. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 07:47:40,977][166323] Avg episode reward: [(0, '1212.798')]
[36m[2025-07-02 07:47:45,972][166323] Fps is (10 sec: 1639.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11370496. Throughput: 0: 267.6. Samples: 11371184. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 07:47:45,972][166323] Avg episode reward: [(0, '1161.065')]
[36m[2025-07-02 07:47:50,952][166323] Fps is (10 sec: 1642.4, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11370496. Throughput: 0: 268.1. Samples: 11372000. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 07:47:50,952][166323] Avg episode reward: [(0, '1201.197')]
[36m[2025-07-02 07:47:55,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11370496. Throughput: 0: 264.9. Samples: 11373552. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 07:47:55,957][166323] Avg episode reward: [(0, '1141.339')]
[31m[40466165 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[40466166 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[40466166 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:48:00,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11370496. Throughput: 0: 268.2. Samples: 11375312. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 07:48:00,970][166323] Avg episode reward: [(0, '1138.208')]
[36m[2025-07-02 07:48:05,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11370496. Throughput: 0: 271.4. Samples: 11376176. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 07:48:05,966][166323] Avg episode reward: [(0, '1160.759')]
[36m[2025-07-02 07:48:10,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11370496. Throughput: 0: 266.7. Samples: 11377632. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 07:48:10,948][166323] Avg episode reward: [(0, '1126.525')]
[36m[2025-07-02 07:48:15,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11370496. Throughput: 0: 270.2. Samples: 11379264. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 07:48:15,973][166323] Avg episode reward: [(0, '1201.249')]
[36m[2025-07-02 07:48:20,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11370496. Throughput: 0: 270.3. Samples: 11380128. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 07:48:20,961][166323] Avg episode reward: [(0, '1234.454')]
[36m[2025-07-02 07:48:25,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11370496. Throughput: 0: 269.8. Samples: 11381744. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 07:48:25,991][166323] Avg episode reward: [(0, '1194.941')]
[36m[2025-07-02 07:48:30,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11370496. Throughput: 0: 271.3. Samples: 11383392. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 07:48:30,967][166323] Avg episode reward: [(0, '1184.900')]
[36m[2025-07-02 07:48:35,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11370496. Throughput: 0: 271.0. Samples: 11384208. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 07:48:35,994][166323] Avg episode reward: [(0, '1239.120')]
[36m[2025-07-02 07:48:40,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 11370496. Throughput: 0: 275.0. Samples: 11385936. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 07:48:40,987][166323] Avg episode reward: [(0, '1226.885')]
[36m[2025-07-02 07:48:46,021][166323] Fps is (10 sec: 1633.9, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 11386880. Throughput: 0: 273.5. Samples: 11387632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:48:46,021][166323] Avg episode reward: [(0, '1242.506')]
[36m[2025-07-02 07:48:50,966][166323] Fps is (10 sec: 1641.7, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11386880. Throughput: 0: 272.4. Samples: 11388432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:48:50,966][166323] Avg episode reward: [(0, '1295.138')]
[36m[2025-07-02 07:48:55,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11386880. Throughput: 0: 277.6. Samples: 11390128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:48:55,970][166323] Avg episode reward: [(0, '1251.617')]
[36m[2025-07-02 07:49:00,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11386880. Throughput: 0: 277.7. Samples: 11391760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:49:00,977][166323] Avg episode reward: [(0, '1261.060')]
[36m[2025-07-02 07:49:05,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11386880. Throughput: 0: 274.9. Samples: 11392496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:49:05,946][166323] Avg episode reward: [(0, '1283.810')]
[36m[2025-07-02 07:49:10,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11386880. Throughput: 0: 277.5. Samples: 11394224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:49:10,965][166323] Avg episode reward: [(0, '1288.287')]
[36m[2025-07-02 07:49:15,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11386880. Throughput: 0: 280.6. Samples: 11396016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:49:15,951][166323] Avg episode reward: [(0, '1264.918')]
[36m[2025-07-02 07:49:20,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11386880. Throughput: 0: 280.7. Samples: 11396832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:49:20,969][166323] Avg episode reward: [(0, '1271.406')]
[36m[2025-07-02 07:49:25,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11386880. Throughput: 0: 280.0. Samples: 11398528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:49:25,957][166323] Avg episode reward: [(0, '1210.101')]
[36m[2025-07-02 07:49:30,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11386880. Throughput: 0: 282.1. Samples: 11400304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:49:30,947][166323] Avg episode reward: [(0, '1258.919')]
[36m[2025-07-02 07:49:35,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11386880. Throughput: 0: 282.7. Samples: 11401152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:49:35,954][166323] Avg episode reward: [(0, '1257.665')]
[37m[1m[2025-07-02 07:49:36,005][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000022232_11386880.pth...
[36m[2025-07-02 07:49:36,009][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000022104_11321344.pth
[36m[2025-07-02 07:49:41,143][166323] Fps is (10 sec: 0.0, 60 sec: 272.4, 300 sec: 222.0). Total num frames: 11386880. Throughput: 0: 282.3. Samples: 11402880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:49:41,143][166323] Avg episode reward: [(0, '1178.143')]
[36m[2025-07-02 07:49:45,953][166323] Fps is (10 sec: 1638.5, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 11403264. Throughput: 0: 283.9. Samples: 11404528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:49:45,953][166323] Avg episode reward: [(0, '1146.871')]
[31m[40577793 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[40577793 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[40577793 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:49:50,949][166323] Fps is (10 sec: 1670.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11403264. Throughput: 0: 285.8. Samples: 11405360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:49:50,949][166323] Avg episode reward: [(0, '1132.394')]
[36m[2025-07-02 07:49:55,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11403264. Throughput: 0: 281.6. Samples: 11406896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:49:55,959][166323] Avg episode reward: [(0, '1158.163')]
[36m[2025-07-02 07:50:00,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11403264. Throughput: 0: 274.2. Samples: 11408368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:50:00,996][166323] Avg episode reward: [(0, '1195.099')]
[36m[2025-07-02 07:50:05,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11403264. Throughput: 0: 273.1. Samples: 11409120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:50:05,958][166323] Avg episode reward: [(0, '1209.951')]
[36m[2025-07-02 07:50:10,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11403264. Throughput: 0: 273.2. Samples: 11410832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:50:10,986][166323] Avg episode reward: [(0, '1218.343')]
[36m[2025-07-02 07:50:15,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11403264. Throughput: 0: 272.0. Samples: 11412544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:50:15,945][166323] Avg episode reward: [(0, '1240.669')]
[36m[2025-07-02 07:50:20,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11403264. Throughput: 0: 271.2. Samples: 11413360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:50:20,967][166323] Avg episode reward: [(0, '1295.505')]
[36m[2025-07-02 07:50:25,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11403264. Throughput: 0: 272.4. Samples: 11415088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:50:25,960][166323] Avg episode reward: [(0, '1316.754')]
[36m[2025-07-02 07:50:30,977][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11403264. Throughput: 0: 269.4. Samples: 11416656. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:50:30,977][166323] Avg episode reward: [(0, '1310.670')]
[31m[40622286 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[40622286 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[40622287 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[33m[40623761 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[40623761 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.8672522902488708
[33mCrash Rate: 0.11810639500617981
[33mTimeout Rate: 0.014641288667917252 (navigation_task.py:265)
[33m[40623761 ms][navigation_task] - WARNING : 
[33mSuccesses: 1777
[33mCrashes : 242
[33mTimeouts: 30 (navigation_task.py:268)
[36m[2025-07-02 07:50:35,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11403264. Throughput: 0: 269.1. Samples: 11417472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:50:35,952][166323] Avg episode reward: [(0, '1290.665')]
[36m[2025-07-02 07:50:40,948][166323] Fps is (10 sec: 0.0, 60 sec: 274.0, 300 sec: 222.2). Total num frames: 11403264. Throughput: 0: 272.1. Samples: 11419136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:50:40,948][166323] Avg episode reward: [(0, '1251.793')]
[36m[2025-07-02 07:50:45,975][166323] Fps is (10 sec: 1634.7, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11419648. Throughput: 0: 276.0. Samples: 11420784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:50:45,975][166323] Avg episode reward: [(0, '1228.963')]
[36m[2025-07-02 07:50:50,986][166323] Fps is (10 sec: 1632.3, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11419648. Throughput: 0: 278.2. Samples: 11421648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:50:50,986][166323] Avg episode reward: [(0, '1281.442')]
[31m[40639924 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[40639925 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[40639925 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[40643678 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[40643679 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[40643679 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:50:55,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11419648. Throughput: 0: 279.2. Samples: 11423392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:50:55,975][166323] Avg episode reward: [(0, '1235.968')]
[36m[2025-07-02 07:51:00,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11419648. Throughput: 0: 278.0. Samples: 11425056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:51:00,949][166323] Avg episode reward: [(0, '1241.570')]
[36m[2025-07-02 07:51:05,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11419648. Throughput: 0: 277.8. Samples: 11425856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:51:05,952][166323] Avg episode reward: [(0, '1292.763')]
[36m[2025-07-02 07:51:10,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11419648. Throughput: 0: 276.0. Samples: 11427504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:51:10,947][166323] Avg episode reward: [(0, '1333.454')]
[36m[2025-07-02 07:51:15,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11419648. Throughput: 0: 277.1. Samples: 11429120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:51:15,965][166323] Avg episode reward: [(0, '1326.108')]
[36m[2025-07-02 07:51:20,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11419648. Throughput: 0: 276.5. Samples: 11429920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:51:20,977][166323] Avg episode reward: [(0, '1335.412')]
[36m[2025-07-02 07:51:25,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11419648. Throughput: 0: 277.5. Samples: 11431632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:51:25,983][166323] Avg episode reward: [(0, '1359.959')]
[31m[40677290 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[40677291 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[40677291 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:51:30,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11419648. Throughput: 0: 277.1. Samples: 11433248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:51:30,962][166323] Avg episode reward: [(0, '1372.449')]
[36m[2025-07-02 07:51:35,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11419648. Throughput: 0: 276.0. Samples: 11434064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:51:35,970][166323] Avg episode reward: [(0, '1400.170')]
[37m[1m[2025-07-02 07:51:36,042][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000022296_11419648.pth...
[36m[2025-07-02 07:51:36,048][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000022168_11354112.pth
[36m[2025-07-02 07:51:40,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 11419648. Throughput: 0: 273.1. Samples: 11435680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:51:40,965][166323] Avg episode reward: [(0, '1370.909')]
[36m[2025-07-02 07:51:45,997][166323] Fps is (10 sec: 1634.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11436032. Throughput: 0: 271.7. Samples: 11437296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:51:45,997][166323] Avg episode reward: [(0, '1371.205')]
[36m[2025-07-02 07:51:51,003][166323] Fps is (10 sec: 1632.3, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11436032. Throughput: 0: 272.0. Samples: 11438112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:51:51,003][166323] Avg episode reward: [(0, '1406.240')]
[36m[2025-07-02 07:51:55,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11436032. Throughput: 0: 272.6. Samples: 11439776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:51:55,962][166323] Avg episode reward: [(0, '1403.621')]
[36m[2025-07-02 07:52:00,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11436032. Throughput: 0: 274.9. Samples: 11441488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:52:00,957][166323] Avg episode reward: [(0, '1370.484')]
[36m[2025-07-02 07:52:05,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11436032. Throughput: 0: 276.7. Samples: 11442368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:52:05,967][166323] Avg episode reward: [(0, '1381.866')]
[36m[2025-07-02 07:52:10,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11436032. Throughput: 0: 276.0. Samples: 11444048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:52:10,965][166323] Avg episode reward: [(0, '1386.084')]
[36m[2025-07-02 07:52:15,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11436032. Throughput: 0: 276.9. Samples: 11445712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:52:15,975][166323] Avg episode reward: [(0, '1391.606')]
[36m[2025-07-02 07:52:21,018][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 11436032. Throughput: 0: 275.3. Samples: 11446464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:52:21,018][166323] Avg episode reward: [(0, '1343.428')]
[36m[2025-07-02 07:52:25,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11436032. Throughput: 0: 274.8. Samples: 11448048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:52:25,968][166323] Avg episode reward: [(0, '1339.004')]
[36m[2025-07-02 07:52:30,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11436032. Throughput: 0: 276.1. Samples: 11449712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:52:30,966][166323] Avg episode reward: [(0, '1317.017')]
[36m[2025-07-02 07:52:35,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11436032. Throughput: 0: 275.0. Samples: 11450480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:52:35,980][166323] Avg episode reward: [(0, '1302.080')]
[36m[2025-07-02 07:52:40,950][166323] Fps is (10 sec: 1641.0, 60 sec: 546.3, 300 sec: 277.7). Total num frames: 11452416. Throughput: 0: 274.6. Samples: 11452128. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 07:52:40,950][166323] Avg episode reward: [(0, '1297.775')]
[31m[40750767 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[40750768 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[40750768 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:52:45,990][166323] Fps is (10 sec: 1636.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11452416. Throughput: 0: 274.3. Samples: 11453840. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 07:52:45,990][166323] Avg episode reward: [(0, '1229.550')]
[31m[40755480 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[40755480 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[40755481 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:52:50,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11452416. Throughput: 0: 272.4. Samples: 11454624. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 07:52:50,954][166323] Avg episode reward: [(0, '1156.519')]
[36m[2025-07-02 07:52:55,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11452416. Throughput: 0: 273.3. Samples: 11456352. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 07:52:55,986][166323] Avg episode reward: [(0, '1186.534')]
[36m[2025-07-02 07:53:00,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11452416. Throughput: 0: 274.4. Samples: 11458064. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 07:53:00,982][166323] Avg episode reward: [(0, '1143.045')]
[36m[2025-07-02 07:53:05,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11452416. Throughput: 0: 277.4. Samples: 11458928. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 07:53:05,943][166323] Avg episode reward: [(0, '1189.427')]
[36m[2025-07-02 07:53:10,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11452416. Throughput: 0: 280.2. Samples: 11460656. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 07:53:10,968][166323] Avg episode reward: [(0, '1188.305')]
[36m[2025-07-02 07:53:16,008][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11452416. Throughput: 0: 279.6. Samples: 11462304. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 07:53:16,008][166323] Avg episode reward: [(0, '1197.080')]
[36m[2025-07-02 07:53:20,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11452416. Throughput: 0: 280.6. Samples: 11463104. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 07:53:20,965][166323] Avg episode reward: [(0, '1253.573')]
[36m[2025-07-02 07:53:25,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11452416. Throughput: 0: 279.3. Samples: 11464704. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 07:53:25,973][166323] Avg episode reward: [(0, '1328.902')]
[36m[2025-07-02 07:53:30,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11452416. Throughput: 0: 277.6. Samples: 11466320. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 07:53:30,947][166323] Avg episode reward: [(0, '1284.020')]
[36m[2025-07-02 07:53:35,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11452416. Throughput: 0: 277.6. Samples: 11467120. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 07:53:35,973][166323] Avg episode reward: [(0, '1304.240')]
[37m[1m[2025-07-02 07:53:36,029][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000022360_11452416.pth...
[36m[2025-07-02 07:53:36,036][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000022232_11386880.pth
[36m[2025-07-02 07:53:40,981][166323] Fps is (10 sec: 1632.9, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11468800. Throughput: 0: 277.4. Samples: 11468832. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 07:53:40,981][166323] Avg episode reward: [(0, '1262.823')]
[36m[2025-07-02 07:53:45,971][166323] Fps is (10 sec: 1638.7, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11468800. Throughput: 0: 274.9. Samples: 11470432. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 07:53:45,972][166323] Avg episode reward: [(0, '1291.569')]
[36m[2025-07-02 07:53:50,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11468800. Throughput: 0: 273.0. Samples: 11471216. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 07:53:50,954][166323] Avg episode reward: [(0, '1281.288')]
[36m[2025-07-02 07:53:55,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11468800. Throughput: 0: 271.2. Samples: 11472864. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 07:53:55,989][166323] Avg episode reward: [(0, '1279.249')]
[36m[2025-07-02 07:54:00,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11468800. Throughput: 0: 271.8. Samples: 11474528. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 07:54:00,978][166323] Avg episode reward: [(0, '1298.117')]
[36m[2025-07-02 07:54:05,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 11468800. Throughput: 0: 273.6. Samples: 11475424. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 07:54:05,997][166323] Avg episode reward: [(0, '1332.938')]
[36m[2025-07-02 07:54:10,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11468800. Throughput: 0: 272.1. Samples: 11476944. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 07:54:10,962][166323] Avg episode reward: [(0, '1321.148')]
[36m[2025-07-02 07:54:15,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11468800. Throughput: 0: 272.8. Samples: 11478608. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 07:54:15,992][166323] Avg episode reward: [(0, '1335.825')]
[36m[2025-07-02 07:54:20,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11468800. Throughput: 0: 274.1. Samples: 11479456. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 07:54:20,976][166323] Avg episode reward: [(0, '1283.109')]
[36m[2025-07-02 07:54:25,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 11468800. Throughput: 0: 270.8. Samples: 11481024. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 07:54:25,999][166323] Avg episode reward: [(0, '1227.217')]
[36m[2025-07-02 07:54:31,002][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 11468800. Throughput: 0: 272.5. Samples: 11482704. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 07:54:31,003][166323] Avg episode reward: [(0, '1192.572')]
[36m[2025-07-02 07:54:35,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.8). Total num frames: 11468800. Throughput: 0: 272.2. Samples: 11483472. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 07:54:35,983][166323] Avg episode reward: [(0, '1191.193')]
[36m[2025-07-02 07:54:40,992][166323] Fps is (10 sec: 1640.1, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11485184. Throughput: 0: 271.3. Samples: 11485072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:54:40,992][166323] Avg episode reward: [(0, '1168.484')]
[36m[2025-07-02 07:54:45,959][166323] Fps is (10 sec: 1642.3, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11485184. Throughput: 0: 267.5. Samples: 11486560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:54:45,959][166323] Avg episode reward: [(0, '1228.703')]
[36m[2025-07-02 07:54:50,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11485184. Throughput: 0: 264.2. Samples: 11487312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:54:50,990][166323] Avg episode reward: [(0, '1198.352')]
[36m[2025-07-02 07:54:55,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11485184. Throughput: 0: 263.7. Samples: 11488816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:54:55,986][166323] Avg episode reward: [(0, '1215.149')]
[31m[40888449 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[40888449 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[40888449 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:55:00,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11485184. Throughput: 0: 262.8. Samples: 11490432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:55:00,989][166323] Avg episode reward: [(0, '1195.418')]
[36m[2025-07-02 07:55:05,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11485184. Throughput: 0: 259.6. Samples: 11491136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:55:05,977][166323] Avg episode reward: [(0, '1245.701')]
[36m[2025-07-02 07:55:10,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11485184. Throughput: 0: 260.6. Samples: 11492736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:55:10,945][166323] Avg episode reward: [(0, '1196.056')]
[36m[2025-07-02 07:55:15,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11485184. Throughput: 0: 261.4. Samples: 11494464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:55:15,991][166323] Avg episode reward: [(0, '1165.888')]
[36m[2025-07-02 07:55:20,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11485184. Throughput: 0: 261.9. Samples: 11495248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:55:20,951][166323] Avg episode reward: [(0, '1118.960')]
[31m[40914141 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[40914142 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[40914142 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:55:25,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11485184. Throughput: 0: 262.9. Samples: 11496896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:55:25,972][166323] Avg episode reward: [(0, '1054.983')]
[36m[2025-07-02 07:55:30,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11485184. Throughput: 0: 268.2. Samples: 11498624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:55:30,948][166323] Avg episode reward: [(0, '1087.504')]
[36m[2025-07-02 07:55:35,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11485184. Throughput: 0: 270.3. Samples: 11499472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:55:35,971][166323] Avg episode reward: [(0, '1082.830')]
[37m[1m[2025-07-02 07:55:36,035][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000022424_11485184.pth...
[36m[2025-07-02 07:55:36,043][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000022296_11419648.pth
[36m[2025-07-02 07:55:40,984][166323] Fps is (10 sec: 1632.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11501568. Throughput: 0: 275.6. Samples: 11501216. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 07:55:40,985][166323] Avg episode reward: [(0, '1085.589')]
[31m[40934201 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[40934201 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[40934201 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:55:45,998][166323] Fps is (10 sec: 1634.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11501568. Throughput: 0: 275.1. Samples: 11502816. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 07:55:45,999][166323] Avg episode reward: [(0, '1070.443')]
[36m[2025-07-02 07:55:50,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11501568. Throughput: 0: 277.6. Samples: 11503632. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 07:55:50,985][166323] Avg episode reward: [(0, '1090.250')]
[36m[2025-07-02 07:55:55,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11501568. Throughput: 0: 279.8. Samples: 11505328. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 07:55:55,947][166323] Avg episode reward: [(0, '1163.417')]
[36m[2025-07-02 07:56:00,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11501568. Throughput: 0: 278.5. Samples: 11506992. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 07:56:00,972][166323] Avg episode reward: [(0, '1206.609')]
[36m[2025-07-02 07:56:05,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11501568. Throughput: 0: 278.4. Samples: 11507776. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 07:56:05,955][166323] Avg episode reward: [(0, '1240.861')]
[36m[2025-07-02 07:56:10,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11501568. Throughput: 0: 277.6. Samples: 11509392. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 07:56:10,986][166323] Avg episode reward: [(0, '1257.557')]
[36m[2025-07-02 07:56:15,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11501568. Throughput: 0: 277.6. Samples: 11511120. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 07:56:15,956][166323] Avg episode reward: [(0, '1225.572')]
[31m[40965753 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[40965754 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[40965754 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:56:20,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11501568. Throughput: 0: 276.1. Samples: 11511888. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 07:56:20,948][166323] Avg episode reward: [(0, '1236.876')]
[36m[2025-07-02 07:56:25,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11501568. Throughput: 0: 273.4. Samples: 11513520. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 07:56:25,994][166323] Avg episode reward: [(0, '1304.389')]
[36m[2025-07-02 07:56:30,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11501568. Throughput: 0: 274.4. Samples: 11515152. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 07:56:30,951][166323] Avg episode reward: [(0, '1233.873')]
[36m[2025-07-02 07:56:35,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11501568. Throughput: 0: 274.0. Samples: 11515952. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 07:56:35,955][166323] Avg episode reward: [(0, '1262.880')]
[36m[2025-07-02 07:56:40,972][166323] Fps is (10 sec: 1634.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11517952. Throughput: 0: 275.0. Samples: 11517712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:56:40,973][166323] Avg episode reward: [(0, '1243.098')]
[36m[2025-07-02 07:56:45,981][166323] Fps is (10 sec: 1634.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11517952. Throughput: 0: 271.9. Samples: 11519232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:56:45,981][166323] Avg episode reward: [(0, '1248.758')]
[36m[2025-07-02 07:56:50,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11517952. Throughput: 0: 273.2. Samples: 11520080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:56:50,989][166323] Avg episode reward: [(0, '1264.736')]
[36m[2025-07-02 07:56:55,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11517952. Throughput: 0: 275.5. Samples: 11521792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:56:55,992][166323] Avg episode reward: [(0, '1285.028')]
[36m[2025-07-02 07:57:00,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11517952. Throughput: 0: 272.0. Samples: 11523360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:57:00,964][166323] Avg episode reward: [(0, '1225.566')]
[36m[2025-07-02 07:57:05,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11517952. Throughput: 0: 273.2. Samples: 11524192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:57:05,984][166323] Avg episode reward: [(0, '1198.756')]
[36m[2025-07-02 07:57:10,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11517952. Throughput: 0: 273.5. Samples: 11525824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:57:10,982][166323] Avg episode reward: [(0, '1129.832')]
[36m[2025-07-02 07:57:15,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11517952. Throughput: 0: 272.3. Samples: 11527408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:57:15,960][166323] Avg episode reward: [(0, '1181.680')]
[36m[2025-07-02 07:57:20,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11517952. Throughput: 0: 273.4. Samples: 11528256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:57:20,954][166323] Avg episode reward: [(0, '1176.611')]
[36m[2025-07-02 07:57:25,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11517952. Throughput: 0: 271.4. Samples: 11529920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:57:25,953][166323] Avg episode reward: [(0, '1153.266')]
[36m[2025-07-02 07:57:31,001][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 11517952. Throughput: 0: 276.1. Samples: 11531664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:57:31,001][166323] Avg episode reward: [(0, '1217.299')]
[36m[2025-07-02 07:57:35,998][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 11517952. Throughput: 0: 276.9. Samples: 11532544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:57:35,999][166323] Avg episode reward: [(0, '1256.040')]
[37m[1m[2025-07-02 07:57:36,071][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000022488_11517952.pth...
[36m[2025-07-02 07:57:36,078][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000022360_11452416.pth
[36m[2025-07-02 07:57:40,944][166323] Fps is (10 sec: 1647.8, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11534336. Throughput: 0: 275.1. Samples: 11534160. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 07:57:40,944][166323] Avg episode reward: [(0, '1313.604')]
[36m[2025-07-02 07:57:45,951][166323] Fps is (10 sec: 1646.2, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11534336. Throughput: 0: 276.3. Samples: 11535792. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 07:57:45,952][166323] Avg episode reward: [(0, '1300.130')]
[36m[2025-07-02 07:57:50,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11534336. Throughput: 0: 275.0. Samples: 11536560. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 07:57:50,961][166323] Avg episode reward: [(0, '1279.586')]
[36m[2025-07-02 07:57:55,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11534336. Throughput: 0: 273.8. Samples: 11538144. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 07:57:55,974][166323] Avg episode reward: [(0, '1255.750')]
[36m[2025-07-02 07:58:00,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11534336. Throughput: 0: 276.8. Samples: 11539872. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 07:58:00,982][166323] Avg episode reward: [(0, '1246.905')]
[36m[2025-07-02 07:58:05,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11534336. Throughput: 0: 276.9. Samples: 11540720. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 07:58:05,975][166323] Avg episode reward: [(0, '1192.563')]
[36m[2025-07-02 07:58:11,025][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11534336. Throughput: 0: 279.7. Samples: 11542528. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 07:58:11,025][166323] Avg episode reward: [(0, '1195.611')]
[36m[2025-07-02 07:58:15,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11534336. Throughput: 0: 278.2. Samples: 11544176. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 07:58:15,980][166323] Avg episode reward: [(0, '1213.168')]
[36m[2025-07-02 07:58:20,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11534336. Throughput: 0: 275.2. Samples: 11544912. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 07:58:20,946][166323] Avg episode reward: [(0, '1264.811')]
[36m[2025-07-02 07:58:25,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11534336. Throughput: 0: 278.3. Samples: 11546688. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 07:58:25,971][166323] Avg episode reward: [(0, '1212.465')]
[36m[2025-07-02 07:58:30,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11534336. Throughput: 0: 278.3. Samples: 11548320. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 07:58:30,966][166323] Avg episode reward: [(0, '1261.528')]
[36m[2025-07-02 07:58:36,000][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.1). Total num frames: 11534336. Throughput: 0: 281.7. Samples: 11549248. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 07:58:36,000][166323] Avg episode reward: [(0, '1247.827')]
[36m[2025-07-02 07:58:40,962][166323] Fps is (10 sec: 1638.9, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11550720. Throughput: 0: 283.8. Samples: 11550912. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 07:58:40,963][166323] Avg episode reward: [(0, '1270.085')]
[36m[2025-07-02 07:58:45,986][166323] Fps is (10 sec: 1640.8, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11550720. Throughput: 0: 282.6. Samples: 11552592. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 07:58:45,986][166323] Avg episode reward: [(0, '1246.868')]
[36m[2025-07-02 07:58:51,013][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 11550720. Throughput: 0: 282.1. Samples: 11553424. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 07:58:51,013][166323] Avg episode reward: [(0, '1225.632')]
[36m[2025-07-02 07:58:55,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11550720. Throughput: 0: 281.6. Samples: 11555184. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 07:58:55,961][166323] Avg episode reward: [(0, '1316.268')]
[31m[41129301 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[41129302 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[41129302 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:59:01,013][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11550720. Throughput: 0: 283.5. Samples: 11556944. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 07:59:01,014][166323] Avg episode reward: [(0, '1296.758')]
[36m[2025-07-02 07:59:05,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11550720. Throughput: 0: 283.2. Samples: 11557664. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 07:59:05,971][166323] Avg episode reward: [(0, '1239.344')]
[36m[2025-07-02 07:59:10,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 11550720. Throughput: 0: 279.6. Samples: 11559264. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 07:59:10,946][166323] Avg episode reward: [(0, '1277.341')]
[36m[2025-07-02 07:59:15,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11550720. Throughput: 0: 281.2. Samples: 11560976. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 07:59:15,978][166323] Avg episode reward: [(0, '1195.387')]
[36m[2025-07-02 07:59:20,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 11550720. Throughput: 0: 278.1. Samples: 11561760. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 07:59:20,994][166323] Avg episode reward: [(0, '1229.307')]
[36m[2025-07-02 07:59:25,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11550720. Throughput: 0: 275.5. Samples: 11563312. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 07:59:25,967][166323] Avg episode reward: [(0, '1135.022')]
[36m[2025-07-02 07:59:30,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11550720. Throughput: 0: 275.7. Samples: 11564992. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 07:59:30,964][166323] Avg episode reward: [(0, '1131.279')]
[36m[2025-07-02 07:59:35,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 11550720. Throughput: 0: 275.0. Samples: 11565792. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 07:59:35,990][166323] Avg episode reward: [(0, '1173.064')]
[37m[1m[2025-07-02 07:59:36,049][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000022552_11550720.pth...
[36m[2025-07-02 07:59:36,053][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000022424_11485184.pth
[36m[2025-07-02 07:59:40,951][166323] Fps is (10 sec: 1640.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11567104. Throughput: 0: 271.3. Samples: 11567392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:59:40,952][166323] Avg episode reward: [(0, '1214.626')]
[31m[41170580 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[41170580 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[41170580 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 07:59:45,967][166323] Fps is (10 sec: 1642.2, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11567104. Throughput: 0: 267.7. Samples: 11568976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:59:45,967][166323] Avg episode reward: [(0, '1147.311')]
[36m[2025-07-02 07:59:50,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11567104. Throughput: 0: 269.4. Samples: 11569792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:59:50,989][166323] Avg episode reward: [(0, '1183.656')]
[36m[2025-07-02 07:59:55,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11567104. Throughput: 0: 270.0. Samples: 11571424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 07:59:55,976][166323] Avg episode reward: [(0, '1180.896')]
[36m[2025-07-02 08:00:00,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11567104. Throughput: 0: 268.1. Samples: 11573040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:00:00,975][166323] Avg episode reward: [(0, '1301.359')]
[36m[2025-07-02 08:00:05,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11567104. Throughput: 0: 270.8. Samples: 11573936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:00:05,966][166323] Avg episode reward: [(0, '1255.301')]
[36m[2025-07-02 08:00:11,026][166323] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 277.7). Total num frames: 11567104. Throughput: 0: 270.2. Samples: 11575488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:00:11,027][166323] Avg episode reward: [(0, '1280.951')]
[36m[2025-07-02 08:00:15,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11567104. Throughput: 0: 268.7. Samples: 11577088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:00:15,974][166323] Avg episode reward: [(0, '1280.610')]
[36m[2025-07-02 08:00:20,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11567104. Throughput: 0: 270.4. Samples: 11577952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:00:20,960][166323] Avg episode reward: [(0, '1309.741')]
[36m[2025-07-02 08:00:25,997][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 11567104. Throughput: 0: 271.0. Samples: 11579600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:00:25,998][166323] Avg episode reward: [(0, '1339.194')]
[36m[2025-07-02 08:00:30,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11567104. Throughput: 0: 269.6. Samples: 11581104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:00:30,953][166323] Avg episode reward: [(0, '1342.217')]
[36m[2025-07-02 08:00:35,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 11567104. Throughput: 0: 268.6. Samples: 11581872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:00:35,963][166323] Avg episode reward: [(0, '1320.936')]
[36m[2025-07-02 08:00:40,985][166323] Fps is (10 sec: 1633.1, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11583488. Throughput: 0: 268.7. Samples: 11583520. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 08:00:40,985][166323] Avg episode reward: [(0, '1365.795')]
[36m[2025-07-02 08:00:45,980][166323] Fps is (10 sec: 1635.5, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11583488. Throughput: 0: 265.6. Samples: 11584992. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 08:00:45,981][166323] Avg episode reward: [(0, '1320.803')]
[36m[2025-07-02 08:00:50,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11583488. Throughput: 0: 263.5. Samples: 11585792. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 08:00:50,957][166323] Avg episode reward: [(0, '1319.878')]
[36m[2025-07-02 08:00:55,995][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11583488. Throughput: 0: 268.3. Samples: 11587552. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 08:00:55,995][166323] Avg episode reward: [(0, '1337.139')]
[36m[2025-07-02 08:01:00,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11583488. Throughput: 0: 268.4. Samples: 11589168. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 08:01:00,983][166323] Avg episode reward: [(0, '1321.475')]
[36m[2025-07-02 08:01:05,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11583488. Throughput: 0: 269.2. Samples: 11590064. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 08:01:05,949][166323] Avg episode reward: [(0, '1275.143')]
[36m[2025-07-02 08:01:10,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 11583488. Throughput: 0: 270.5. Samples: 11591760. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 08:01:10,952][166323] Avg episode reward: [(0, '1267.409')]
[36m[2025-07-02 08:01:15,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11583488. Throughput: 0: 271.4. Samples: 11593328. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 08:01:15,989][166323] Avg episode reward: [(0, '1306.186')]
[31m[41265039 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[41265039 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[41265040 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:01:20,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11583488. Throughput: 0: 272.8. Samples: 11594144. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 08:01:20,948][166323] Avg episode reward: [(0, '1264.890')]
[36m[2025-07-02 08:01:25,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.6). Total num frames: 11583488. Throughput: 0: 274.4. Samples: 11595872. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 08:01:25,998][166323] Avg episode reward: [(0, '1203.735')]
[36m[2025-07-02 08:01:30,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11583488. Throughput: 0: 277.8. Samples: 11597488. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 08:01:30,968][166323] Avg episode reward: [(0, '1232.030')]
[31m[41283459 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[41283459 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[41283460 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:01:36,007][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 11583488. Throughput: 0: 277.7. Samples: 11598304. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 08:01:36,008][166323] Avg episode reward: [(0, '1173.943')]
[37m[1m[2025-07-02 08:01:36,062][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000022616_11583488.pth...
[36m[2025-07-02 08:01:36,066][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000022488_11517952.pth
[36m[2025-07-02 08:01:40,958][166323] Fps is (10 sec: 1639.9, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11599872. Throughput: 0: 274.7. Samples: 11599904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:01:40,959][166323] Avg episode reward: [(0, '1183.655')]
[36m[2025-07-02 08:01:45,960][166323] Fps is (10 sec: 1646.2, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11599872. Throughput: 0: 275.0. Samples: 11601536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:01:45,960][166323] Avg episode reward: [(0, '1173.985')]
[36m[2025-07-02 08:01:51,003][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11599872. Throughput: 0: 272.7. Samples: 11602352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:01:51,004][166323] Avg episode reward: [(0, '1168.361')]
[36m[2025-07-02 08:01:55,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11599872. Throughput: 0: 270.4. Samples: 11603936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:01:55,976][166323] Avg episode reward: [(0, '1138.928')]
[36m[2025-07-02 08:02:00,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11599872. Throughput: 0: 274.9. Samples: 11605696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:02:00,976][166323] Avg episode reward: [(0, '1168.011')]
[36m[2025-07-02 08:02:06,000][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 11599872. Throughput: 0: 276.3. Samples: 11606592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:02:06,000][166323] Avg episode reward: [(0, '1226.356')]
[33m[41319503 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[41319503 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.86083984375
[33mCrash Rate: 0.12548828125
[33mTimeout Rate: 0.013671875 (navigation_task.py:265)
[33m[41319503 ms][navigation_task] - WARNING : 
[33mSuccesses: 1763
[33mCrashes : 257
[33mTimeouts: 28 (navigation_task.py:268)
[36m[2025-07-02 08:02:10,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11599872. Throughput: 0: 274.5. Samples: 11608224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:02:10,994][166323] Avg episode reward: [(0, '1302.437')]
[36m[2025-07-02 08:02:15,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11599872. Throughput: 0: 273.5. Samples: 11609792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:02:15,954][166323] Avg episode reward: [(0, '1260.756')]
[36m[2025-07-02 08:02:21,010][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 11599872. Throughput: 0: 272.3. Samples: 11610560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:02:21,010][166323] Avg episode reward: [(0, '1248.962')]
[36m[2025-07-02 08:02:25,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11599872. Throughput: 0: 273.6. Samples: 11612224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:02:25,985][166323] Avg episode reward: [(0, '1222.812')]
[36m[2025-07-02 08:02:30,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11599872. Throughput: 0: 278.7. Samples: 11614080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:02:30,972][166323] Avg episode reward: [(0, '1269.258')]
[36m[2025-07-02 08:02:35,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 222.2). Total num frames: 11599872. Throughput: 0: 280.9. Samples: 11614976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:02:35,951][166323] Avg episode reward: [(0, '1276.231')]
[36m[2025-07-02 08:02:40,992][166323] Fps is (10 sec: 1635.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11616256. Throughput: 0: 281.1. Samples: 11616592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:02:40,993][166323] Avg episode reward: [(0, '1271.324')]
[36m[2025-07-02 08:02:45,957][166323] Fps is (10 sec: 1637.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11616256. Throughput: 0: 280.7. Samples: 11618320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:02:45,957][166323] Avg episode reward: [(0, '1229.940')]
[36m[2025-07-02 08:02:50,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11616256. Throughput: 0: 278.6. Samples: 11619120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:02:50,973][166323] Avg episode reward: [(0, '1247.584')]
[36m[2025-07-02 08:02:55,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11616256. Throughput: 0: 277.2. Samples: 11620688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:02:55,955][166323] Avg episode reward: [(0, '1260.158')]
[36m[2025-07-02 08:03:00,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11616256. Throughput: 0: 277.4. Samples: 11622288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:03:00,998][166323] Avg episode reward: [(0, '1289.878')]
[36m[2025-07-02 08:03:05,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.8). Total num frames: 11616256. Throughput: 0: 282.0. Samples: 11623232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:03:05,952][166323] Avg episode reward: [(0, '1224.209')]
[36m[2025-07-02 08:03:10,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11616256. Throughput: 0: 282.1. Samples: 11624912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:03:10,967][166323] Avg episode reward: [(0, '1243.833')]
[36m[2025-07-02 08:03:15,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11616256. Throughput: 0: 277.4. Samples: 11626560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:03:15,957][166323] Avg episode reward: [(0, '1239.251')]
[36m[2025-07-02 08:03:20,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11616256. Throughput: 0: 277.2. Samples: 11627456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:03:20,966][166323] Avg episode reward: [(0, '1271.843')]
[36m[2025-07-02 08:03:25,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11616256. Throughput: 0: 279.2. Samples: 11629152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:03:25,979][166323] Avg episode reward: [(0, '1306.401')]
[36m[2025-07-02 08:03:30,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11616256. Throughput: 0: 278.4. Samples: 11630848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:03:30,961][166323] Avg episode reward: [(0, '1305.129')]
[36m[2025-07-02 08:03:35,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 11616256. Throughput: 0: 278.3. Samples: 11631648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:03:35,986][166323] Avg episode reward: [(0, '1308.428')]
[37m[1m[2025-07-02 08:03:36,063][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000022680_11616256.pth...
[36m[2025-07-02 08:03:36,071][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000022552_11550720.pth
[36m[2025-07-02 08:03:40,967][166323] Fps is (10 sec: 1637.5, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11632640. Throughput: 0: 276.2. Samples: 11633120. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 08:03:40,967][166323] Avg episode reward: [(0, '1365.997')]
[36m[2025-07-02 08:03:45,952][166323] Fps is (10 sec: 1643.9, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 11632640. Throughput: 0: 276.2. Samples: 11634704. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 08:03:45,952][166323] Avg episode reward: [(0, '1281.531')]
[36m[2025-07-02 08:03:50,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11632640. Throughput: 0: 272.6. Samples: 11635504. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 08:03:50,974][166323] Avg episode reward: [(0, '1232.586')]
[36m[2025-07-02 08:03:55,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 11632640. Throughput: 0: 273.2. Samples: 11637200. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 08:03:55,948][166323] Avg episode reward: [(0, '1221.964')]
[36m[2025-07-02 08:04:01,011][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11632640. Throughput: 0: 272.0. Samples: 11638816. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 08:04:01,012][166323] Avg episode reward: [(0, '1231.012')]
[36m[2025-07-02 08:04:05,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11632640. Throughput: 0: 269.0. Samples: 11639568. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 08:04:05,984][166323] Avg episode reward: [(0, '1166.317')]
[36m[2025-07-02 08:04:11,018][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 11632640. Throughput: 0: 270.0. Samples: 11641312. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 08:04:11,018][166323] Avg episode reward: [(0, '1215.676')]
[36m[2025-07-02 08:04:15,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11632640. Throughput: 0: 271.3. Samples: 11643056. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 08:04:15,952][166323] Avg episode reward: [(0, '1211.060')]
[36m[2025-07-02 08:04:20,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11632640. Throughput: 0: 271.2. Samples: 11643840. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 08:04:20,947][166323] Avg episode reward: [(0, '1221.921')]
[36m[2025-07-02 08:04:25,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11632640. Throughput: 0: 273.4. Samples: 11645424. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 08:04:25,971][166323] Avg episode reward: [(0, '1282.767')]
[36m[2025-07-02 08:04:30,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11632640. Throughput: 0: 276.1. Samples: 11647136. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 08:04:30,973][166323] Avg episode reward: [(0, '1270.974')]
[36m[2025-07-02 08:04:35,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 11632640. Throughput: 0: 278.1. Samples: 11648016. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 08:04:35,959][166323] Avg episode reward: [(0, '1321.911')]
[36m[2025-07-02 08:04:40,960][166323] Fps is (10 sec: 1640.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11649024. Throughput: 0: 275.8. Samples: 11649616. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 08:04:40,961][166323] Avg episode reward: [(0, '1326.928')]
[36m[2025-07-02 08:04:45,946][166323] Fps is (10 sec: 1640.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11649024. Throughput: 0: 276.0. Samples: 11651216. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 08:04:45,946][166323] Avg episode reward: [(0, '1306.803')]
[36m[2025-07-02 08:04:50,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11649024. Throughput: 0: 276.5. Samples: 11652000. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 08:04:50,944][166323] Avg episode reward: [(0, '1304.203')]
[36m[2025-07-02 08:04:55,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11649024. Throughput: 0: 275.7. Samples: 11653712. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 08:04:55,992][166323] Avg episode reward: [(0, '1330.349')]
[36m[2025-07-02 08:05:01,005][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11649024. Throughput: 0: 272.0. Samples: 11655312. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 08:05:01,005][166323] Avg episode reward: [(0, '1258.582')]
[36m[2025-07-02 08:05:06,005][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11649024. Throughput: 0: 274.1. Samples: 11656192. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 08:05:06,005][166323] Avg episode reward: [(0, '1314.957')]
[36m[2025-07-02 08:05:10,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 11649024. Throughput: 0: 274.6. Samples: 11657776. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 08:05:10,945][166323] Avg episode reward: [(0, '1281.497')]
[36m[2025-07-02 08:05:15,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11649024. Throughput: 0: 274.6. Samples: 11659488. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 08:05:15,961][166323] Avg episode reward: [(0, '1292.975')]
[36m[2025-07-02 08:05:20,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11649024. Throughput: 0: 272.1. Samples: 11660256. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 08:05:20,945][166323] Avg episode reward: [(0, '1196.743')]
[36m[2025-07-02 08:05:25,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11649024. Throughput: 0: 274.1. Samples: 11661952. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 08:05:25,970][166323] Avg episode reward: [(0, '1254.079')]
[36m[2025-07-02 08:05:30,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11649024. Throughput: 0: 274.9. Samples: 11663584. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 08:05:30,944][166323] Avg episode reward: [(0, '1239.791')]
[36m[2025-07-02 08:05:35,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.2). Total num frames: 11649024. Throughput: 0: 274.9. Samples: 11664384. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 08:05:35,991][166323] Avg episode reward: [(0, '1238.610')]
[37m[1m[2025-07-02 08:05:36,043][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000022744_11649024.pth...
[36m[2025-07-02 08:05:36,056][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000022616_11583488.pth
[31m[41527760 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[41527760 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[41527760 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:05:40,947][166323] Fps is (10 sec: 1637.9, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11665408. Throughput: 0: 274.0. Samples: 11666032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:05:40,948][166323] Avg episode reward: [(0, '1212.565')]
[36m[2025-07-02 08:05:45,976][166323] Fps is (10 sec: 1640.8, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11665408. Throughput: 0: 276.8. Samples: 11667760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:05:45,976][166323] Avg episode reward: [(0, '1190.419')]
[36m[2025-07-02 08:05:50,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11665408. Throughput: 0: 275.1. Samples: 11668560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:05:50,968][166323] Avg episode reward: [(0, '1181.116')]
[36m[2025-07-02 08:05:55,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11665408. Throughput: 0: 273.9. Samples: 11670112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:05:55,976][166323] Avg episode reward: [(0, '1209.246')]
[36m[2025-07-02 08:06:00,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11665408. Throughput: 0: 275.9. Samples: 11671904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:06:00,970][166323] Avg episode reward: [(0, '1218.711')]
[36m[2025-07-02 08:06:06,008][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.6). Total num frames: 11665408. Throughput: 0: 276.9. Samples: 11672736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:06:06,009][166323] Avg episode reward: [(0, '1262.036')]
[36m[2025-07-02 08:06:10,979][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11665408. Throughput: 0: 275.1. Samples: 11674336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:06:10,979][166323] Avg episode reward: [(0, '1271.747')]
[36m[2025-07-02 08:06:15,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11665408. Throughput: 0: 278.7. Samples: 11676128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:06:15,961][166323] Avg episode reward: [(0, '1304.910')]
[36m[2025-07-02 08:06:20,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11665408. Throughput: 0: 278.5. Samples: 11676912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:06:20,970][166323] Avg episode reward: [(0, '1298.548')]
[36m[2025-07-02 08:06:25,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11665408. Throughput: 0: 280.1. Samples: 11678640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:06:25,962][166323] Avg episode reward: [(0, '1294.240')]
[36m[2025-07-02 08:06:31,000][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 11665408. Throughput: 0: 279.3. Samples: 11680336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:06:31,001][166323] Avg episode reward: [(0, '1263.098')]
[36m[2025-07-02 08:06:35,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 11665408. Throughput: 0: 280.2. Samples: 11681168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:06:35,962][166323] Avg episode reward: [(0, '1264.253')]
[36m[2025-07-02 08:06:40,982][166323] Fps is (10 sec: 1641.3, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11681792. Throughput: 0: 282.3. Samples: 11682816. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 08:06:40,983][166323] Avg episode reward: [(0, '1271.948')]
[36m[2025-07-02 08:06:45,984][166323] Fps is (10 sec: 1634.8, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11681792. Throughput: 0: 280.4. Samples: 11684528. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 08:06:45,984][166323] Avg episode reward: [(0, '1265.577')]
[36m[2025-07-02 08:06:50,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11681792. Throughput: 0: 281.0. Samples: 11685376. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 08:06:50,984][166323] Avg episode reward: [(0, '1259.478')]
[36m[2025-07-02 08:06:55,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11681792. Throughput: 0: 283.3. Samples: 11687088. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 08:06:55,987][166323] Avg episode reward: [(0, '1300.929')]
[36m[2025-07-02 08:07:00,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11681792. Throughput: 0: 277.9. Samples: 11688640. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 08:07:00,985][166323] Avg episode reward: [(0, '1269.508')]
[36m[2025-07-02 08:07:05,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11681792. Throughput: 0: 280.7. Samples: 11689552. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 08:07:05,992][166323] Avg episode reward: [(0, '1304.848')]
[36m[2025-07-02 08:07:10,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11681792. Throughput: 0: 277.0. Samples: 11691104. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 08:07:10,956][166323] Avg episode reward: [(0, '1288.131')]
[36m[2025-07-02 08:07:15,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 11681792. Throughput: 0: 272.7. Samples: 11692592. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 08:07:15,949][166323] Avg episode reward: [(0, '1266.833')]
[36m[2025-07-02 08:07:20,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11681792. Throughput: 0: 272.6. Samples: 11693440. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 08:07:20,985][166323] Avg episode reward: [(0, '1280.517')]
[36m[2025-07-02 08:07:25,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11681792. Throughput: 0: 274.5. Samples: 11695168. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 08:07:25,988][166323] Avg episode reward: [(0, '1195.027')]
[36m[2025-07-02 08:07:30,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11681792. Throughput: 0: 272.7. Samples: 11696800. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 08:07:30,990][166323] Avg episode reward: [(0, '1182.914')]
[36m[2025-07-02 08:07:35,969][166323] Fps is (10 sec: 1641.5, 60 sec: 546.1, 300 sec: 277.7). Total num frames: 11698176. Throughput: 0: 272.8. Samples: 11697648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:07:35,969][166323] Avg episode reward: [(0, '1216.228')]
[37m[1m[2025-07-02 08:07:36,038][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000022840_11698176.pth...
[36m[2025-07-02 08:07:36,042][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000022680_11616256.pth
[36m[2025-07-02 08:07:40,976][166323] Fps is (10 sec: 1640.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11698176. Throughput: 0: 270.6. Samples: 11699264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:07:40,976][166323] Avg episode reward: [(0, '1225.417')]
[31m[41651345 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[41651346 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[41651346 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:07:45,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11698176. Throughput: 0: 274.7. Samples: 11700992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:07:45,956][166323] Avg episode reward: [(0, '1129.882')]
[36m[2025-07-02 08:07:50,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11698176. Throughput: 0: 272.2. Samples: 11701792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:07:50,952][166323] Avg episode reward: [(0, '1144.491')]
[36m[2025-07-02 08:07:55,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11698176. Throughput: 0: 275.3. Samples: 11703504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:07:55,991][166323] Avg episode reward: [(0, '1145.603')]
[36m[2025-07-02 08:08:00,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11698176. Throughput: 0: 276.5. Samples: 11705040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:08:00,964][166323] Avg episode reward: [(0, '1176.469')]
[36m[2025-07-02 08:08:05,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11698176. Throughput: 0: 276.6. Samples: 11705888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:08:05,986][166323] Avg episode reward: [(0, '1131.335')]
[36m[2025-07-02 08:08:10,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11698176. Throughput: 0: 273.9. Samples: 11707488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:08:10,961][166323] Avg episode reward: [(0, '1095.465')]
[31m[41680160 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[41680160 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[41680161 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:08:15,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11698176. Throughput: 0: 274.4. Samples: 11709136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:08:15,952][166323] Avg episode reward: [(0, '1196.009')]
[36m[2025-07-02 08:08:20,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11698176. Throughput: 0: 272.8. Samples: 11709920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:08:20,951][166323] Avg episode reward: [(0, '1119.354')]
[36m[2025-07-02 08:08:25,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11698176. Throughput: 0: 274.3. Samples: 11711600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:08:25,953][166323] Avg episode reward: [(0, '1163.458')]
[36m[2025-07-02 08:08:30,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11698176. Throughput: 0: 274.5. Samples: 11713344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:08:30,958][166323] Avg episode reward: [(0, '1147.315')]
[36m[2025-07-02 08:08:35,971][166323] Fps is (10 sec: 1635.3, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11714560. Throughput: 0: 272.9. Samples: 11714080. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 08:08:35,971][166323] Avg episode reward: [(0, '1205.092')]
[36m[2025-07-02 08:08:41,002][166323] Fps is (10 sec: 1631.2, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 11714560. Throughput: 0: 272.3. Samples: 11715760. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 08:08:41,003][166323] Avg episode reward: [(0, '1177.773')]
[36m[2025-07-02 08:08:45,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11714560. Throughput: 0: 272.1. Samples: 11717280. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 08:08:45,945][166323] Avg episode reward: [(0, '1198.157')]
[36m[2025-07-02 08:08:50,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11714560. Throughput: 0: 273.8. Samples: 11718208. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 08:08:50,976][166323] Avg episode reward: [(0, '1246.228')]
[36m[2025-07-02 08:08:55,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11714560. Throughput: 0: 274.5. Samples: 11719840. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 08:08:55,954][166323] Avg episode reward: [(0, '1240.593')]
[31m[41726234 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[41726235 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[41726235 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:09:00,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11714560. Throughput: 0: 276.5. Samples: 11721584. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 08:09:00,966][166323] Avg episode reward: [(0, '1236.505')]
[36m[2025-07-02 08:09:05,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11714560. Throughput: 0: 277.6. Samples: 11722416. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 08:09:05,972][166323] Avg episode reward: [(0, '1258.186')]
[36m[2025-07-02 08:09:10,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11714560. Throughput: 0: 275.7. Samples: 11724016. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 08:09:10,981][166323] Avg episode reward: [(0, '1260.332')]
[36m[2025-07-02 08:09:15,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11714560. Throughput: 0: 274.5. Samples: 11725696. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 08:09:15,959][166323] Avg episode reward: [(0, '1309.942')]
[36m[2025-07-02 08:09:20,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11714560. Throughput: 0: 276.8. Samples: 11726528. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 08:09:20,946][166323] Avg episode reward: [(0, '1317.651')]
[31m[41750119 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[41750120 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[41750120 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:09:25,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11714560. Throughput: 0: 278.0. Samples: 11728256. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 08:09:25,949][166323] Avg episode reward: [(0, '1271.983')]
[36m[2025-07-02 08:09:31,006][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 11714560. Throughput: 0: 281.2. Samples: 11729952. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 08:09:31,007][166323] Avg episode reward: [(0, '1242.227')]
[36m[2025-07-02 08:09:35,997][166323] Fps is (10 sec: 1630.5, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11730944. Throughput: 0: 279.0. Samples: 11730768. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 08:09:35,998][166323] Avg episode reward: [(0, '1288.902')]
[37m[1m[2025-07-02 08:09:36,004][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000022904_11730944.pth...
[36m[2025-07-02 08:09:36,011][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000022744_11649024.pth
[31m[41769277 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[41769278 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[41769278 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:09:40,981][166323] Fps is (10 sec: 1642.5, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11730944. Throughput: 0: 277.5. Samples: 11732336. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 08:09:40,981][166323] Avg episode reward: [(0, '1212.902')]
[36m[2025-07-02 08:09:45,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 11730944. Throughput: 0: 274.3. Samples: 11733936. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 08:09:45,992][166323] Avg episode reward: [(0, '1179.448')]
[36m[2025-07-02 08:09:50,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11730944. Throughput: 0: 274.5. Samples: 11734768. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 08:09:50,962][166323] Avg episode reward: [(0, '1216.068')]
[36m[2025-07-02 08:09:55,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 11730944. Throughput: 0: 276.1. Samples: 11736432. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 08:09:55,947][166323] Avg episode reward: [(0, '1180.822')]
[36m[2025-07-02 08:10:00,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11730944. Throughput: 0: 273.5. Samples: 11738000. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 08:10:00,952][166323] Avg episode reward: [(0, '1203.292')]
[36m[2025-07-02 08:10:05,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11730944. Throughput: 0: 274.2. Samples: 11738880. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 08:10:05,991][166323] Avg episode reward: [(0, '1214.681')]
[31m[41797878 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[41797879 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[41797879 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:10:11,003][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11730944. Throughput: 0: 273.1. Samples: 11740560. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 08:10:11,003][166323] Avg episode reward: [(0, '1164.924')]
[36m[2025-07-02 08:10:15,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11730944. Throughput: 0: 271.2. Samples: 11742144. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 08:10:15,966][166323] Avg episode reward: [(0, '1279.131')]
[36m[2025-07-02 08:10:20,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11730944. Throughput: 0: 272.2. Samples: 11743008. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 08:10:20,958][166323] Avg episode reward: [(0, '1245.298')]
[36m[2025-07-02 08:10:26,010][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 11730944. Throughput: 0: 273.2. Samples: 11744640. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 08:10:26,010][166323] Avg episode reward: [(0, '1250.833')]
[36m[2025-07-02 08:10:30,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11730944. Throughput: 0: 272.2. Samples: 11746176. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 08:10:30,951][166323] Avg episode reward: [(0, '1302.754')]
[36m[2025-07-02 08:10:35,992][166323] Fps is (10 sec: 1641.3, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11747328. Throughput: 0: 268.6. Samples: 11746864. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 08:10:35,993][166323] Avg episode reward: [(0, '1323.863')]
[36m[2025-07-02 08:10:40,952][166323] Fps is (10 sec: 1638.1, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11747328. Throughput: 0: 263.1. Samples: 11748272. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 08:10:40,953][166323] Avg episode reward: [(0, '1273.321')]
[31m[41830757 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[41830758 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[41830758 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:10:45,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11747328. Throughput: 0: 265.6. Samples: 11749952. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 08:10:45,954][166323] Avg episode reward: [(0, '1287.597')]
[36m[2025-07-02 08:10:50,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11747328. Throughput: 0: 263.7. Samples: 11750736. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 08:10:50,951][166323] Avg episode reward: [(0, '1225.645')]
[36m[2025-07-02 08:10:56,011][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 11747328. Throughput: 0: 263.8. Samples: 11752432. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 08:10:56,011][166323] Avg episode reward: [(0, '1249.558')]
[36m[2025-07-02 08:11:00,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 11747328. Throughput: 0: 262.9. Samples: 11753968. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 08:11:00,947][166323] Avg episode reward: [(0, '1292.476')]
[36m[2025-07-02 08:11:05,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11747328. Throughput: 0: 259.8. Samples: 11754704. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 08:11:05,970][166323] Avg episode reward: [(0, '1297.736')]
[36m[2025-07-02 08:11:10,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11747328. Throughput: 0: 260.3. Samples: 11756336. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 08:11:10,951][166323] Avg episode reward: [(0, '1281.824')]
[36m[2025-07-02 08:11:15,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11747328. Throughput: 0: 259.7. Samples: 11757872. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 08:11:15,985][166323] Avg episode reward: [(0, '1315.821')]
[31m[41864858 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[41864859 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[41864859 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:11:20,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11747328. Throughput: 0: 264.5. Samples: 11758768. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 08:11:20,993][166323] Avg episode reward: [(0, '1248.223')]
[36m[2025-07-02 08:11:25,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11747328. Throughput: 0: 267.8. Samples: 11760320. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 08:11:25,949][166323] Avg episode reward: [(0, '1280.275')]
[36m[2025-07-02 08:11:30,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11747328. Throughput: 0: 265.4. Samples: 11761904. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 08:11:30,993][166323] Avg episode reward: [(0, '1260.681')]
[31m[41881006 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[41881007 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[41881007 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:11:35,969][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 222.2). Total num frames: 11747328. Throughput: 0: 266.2. Samples: 11762720. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 08:11:35,969][166323] Avg episode reward: [(0, '1234.598')]
[37m[1m[2025-07-02 08:11:36,021][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000022936_11747328.pth...
[36m[2025-07-02 08:11:36,025][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000022840_11698176.pth
[36m[2025-07-02 08:11:40,956][166323] Fps is (10 sec: 1644.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11763712. Throughput: 0: 265.6. Samples: 11764368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:11:40,956][166323] Avg episode reward: [(0, '1212.689')]
[36m[2025-07-02 08:11:45,949][166323] Fps is (10 sec: 1641.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11763712. Throughput: 0: 269.1. Samples: 11766080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:11:45,949][166323] Avg episode reward: [(0, '1223.086')]
[36m[2025-07-02 08:11:50,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11763712. Throughput: 0: 271.3. Samples: 11766912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:11:50,967][166323] Avg episode reward: [(0, '1215.781')]
[36m[2025-07-02 08:11:55,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11763712. Throughput: 0: 271.4. Samples: 11768560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:11:55,984][166323] Avg episode reward: [(0, '1260.952')]
[36m[2025-07-02 08:12:00,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11763712. Throughput: 0: 276.8. Samples: 11770320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:12:00,963][166323] Avg episode reward: [(0, '1186.833')]
[36m[2025-07-02 08:12:06,013][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 11763712. Throughput: 0: 275.1. Samples: 11771152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:12:06,014][166323] Avg episode reward: [(0, '1238.411')]
[36m[2025-07-02 08:12:11,009][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 11763712. Throughput: 0: 275.2. Samples: 11772720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:12:11,010][166323] Avg episode reward: [(0, '1221.138')]
[31m[41923269 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[41923270 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[41923270 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:12:15,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11763712. Throughput: 0: 275.0. Samples: 11774272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:12:15,961][166323] Avg episode reward: [(0, '1225.514')]
[36m[2025-07-02 08:12:20,997][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11763712. Throughput: 0: 273.6. Samples: 11775040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:12:20,998][166323] Avg episode reward: [(0, '1253.715')]
[36m[2025-07-02 08:12:25,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11763712. Throughput: 0: 274.2. Samples: 11776704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:12:25,950][166323] Avg episode reward: [(0, '1208.865')]
[36m[2025-07-02 08:12:30,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 11763712. Throughput: 0: 273.5. Samples: 11778400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:12:30,999][166323] Avg episode reward: [(0, '1234.243')]
[36m[2025-07-02 08:12:35,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 11763712. Throughput: 0: 273.7. Samples: 11779232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:12:35,977][166323] Avg episode reward: [(0, '1269.011')]
[36m[2025-07-02 08:12:40,983][166323] Fps is (10 sec: 1641.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11780096. Throughput: 0: 272.4. Samples: 11780816. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 08:12:40,983][166323] Avg episode reward: [(0, '1293.081')]
[31m[41950035 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[41950036 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[41950036 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:12:45,959][166323] Fps is (10 sec: 1641.2, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11780096. Throughput: 0: 267.8. Samples: 11782368. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 08:12:45,959][166323] Avg episode reward: [(0, '1233.509')]
[36m[2025-07-02 08:12:51,011][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11780096. Throughput: 0: 268.5. Samples: 11783232. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 08:12:51,011][166323] Avg episode reward: [(0, '1258.699')]
[36m[2025-07-02 08:12:55,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11780096. Throughput: 0: 268.2. Samples: 11784784. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 08:12:55,991][166323] Avg episode reward: [(0, '1222.864')]
[36m[2025-07-02 08:13:00,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11780096. Throughput: 0: 272.9. Samples: 11786560. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 08:13:00,983][166323] Avg episode reward: [(0, '1251.792')]
[36m[2025-07-02 08:13:05,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11780096. Throughput: 0: 274.2. Samples: 11787376. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 08:13:05,983][166323] Avg episode reward: [(0, '1144.821')]
[31m[41979404 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[41979405 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[41979405 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:13:10,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11780096. Throughput: 0: 274.5. Samples: 11789056. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 08:13:10,953][166323] Avg episode reward: [(0, '1148.811')]
[36m[2025-07-02 08:13:15,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11780096. Throughput: 0: 271.0. Samples: 11790592. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 08:13:15,991][166323] Avg episode reward: [(0, '1168.614')]
[36m[2025-07-02 08:13:20,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11780096. Throughput: 0: 271.8. Samples: 11791456. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 08:13:20,956][166323] Avg episode reward: [(0, '1180.014')]
[36m[2025-07-02 08:13:25,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11780096. Throughput: 0: 274.3. Samples: 11793152. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 08:13:25,959][166323] Avg episode reward: [(0, '1167.050')]
[36m[2025-07-02 08:13:30,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.1). Total num frames: 11780096. Throughput: 0: 279.3. Samples: 11794944. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 08:13:30,991][166323] Avg episode reward: [(0, '1143.197')]
[36m[2025-07-02 08:13:36,132][166323] Fps is (10 sec: 1610.5, 60 sec: 544.7, 300 sec: 277.6). Total num frames: 11796480. Throughput: 0: 279.1. Samples: 11795824. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 08:13:36,133][166323] Avg episode reward: [(0, '1216.549')]
[37m[1m[2025-07-02 08:13:36,202][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023032_11796480.pth...
[36m[2025-07-02 08:13:36,206][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000022904_11730944.pth
[36m[2025-07-02 08:13:40,962][166323] Fps is (10 sec: 1643.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11796480. Throughput: 0: 281.4. Samples: 11797440. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 08:13:40,963][166323] Avg episode reward: [(0, '1265.415')]
[33m[42011801 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[42011801 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.85205078125
[33mCrash Rate: 0.1279296875
[33mTimeout Rate: 0.02001953125 (navigation_task.py:265)
[33m[42011802 ms][navigation_task] - WARNING : 
[33mSuccesses: 1745
[33mCrashes : 262
[33mTimeouts: 41 (navigation_task.py:268)
[36m[2025-07-02 08:13:45,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11796480. Throughput: 0: 278.8. Samples: 11799104. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 08:13:45,969][166323] Avg episode reward: [(0, '1243.581')]
[36m[2025-07-02 08:13:50,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11796480. Throughput: 0: 278.8. Samples: 11799920. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 08:13:50,984][166323] Avg episode reward: [(0, '1308.878')]
[36m[2025-07-02 08:13:55,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11796480. Throughput: 0: 279.5. Samples: 11801632. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 08:13:55,951][166323] Avg episode reward: [(0, '1325.404')]
[36m[2025-07-02 08:14:01,001][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11796480. Throughput: 0: 280.1. Samples: 11803200. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 08:14:01,001][166323] Avg episode reward: [(0, '1331.557')]
[36m[2025-07-02 08:14:06,021][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11796480. Throughput: 0: 278.4. Samples: 11804000. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 08:14:06,021][166323] Avg episode reward: [(0, '1399.725')]
[36m[2025-07-02 08:14:10,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11796480. Throughput: 0: 275.0. Samples: 11805536. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 08:14:10,992][166323] Avg episode reward: [(0, '1324.558')]
[36m[2025-07-02 08:14:15,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11796480. Throughput: 0: 270.8. Samples: 11807120. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 08:14:15,950][166323] Avg episode reward: [(0, '1281.981')]
[36m[2025-07-02 08:14:20,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11796480. Throughput: 0: 272.0. Samples: 11808016. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 08:14:20,951][166323] Avg episode reward: [(0, '1289.350')]
[36m[2025-07-02 08:14:25,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11796480. Throughput: 0: 271.9. Samples: 11809680. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 08:14:25,972][166323] Avg episode reward: [(0, '1264.948')]
[36m[2025-07-02 08:14:30,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 11796480. Throughput: 0: 273.5. Samples: 11811408. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 08:14:30,959][166323] Avg episode reward: [(0, '1221.952')]
[36m[2025-07-02 08:14:35,967][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 222.2). Total num frames: 11796480. Throughput: 0: 272.5. Samples: 11812176. Policy #0 lag: (min: 29.0, avg: 29.0, max: 29.0)
[36m[2025-07-02 08:14:35,967][166323] Avg episode reward: [(0, '1181.267')]
[36m[2025-07-02 08:14:40,954][166323] Fps is (10 sec: 1639.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11812864. Throughput: 0: 273.4. Samples: 11813936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:14:40,954][166323] Avg episode reward: [(0, '1191.972')]
[36m[2025-07-02 08:14:45,944][166323] Fps is (10 sec: 1642.2, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11812864. Throughput: 0: 274.1. Samples: 11815520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:14:45,944][166323] Avg episode reward: [(0, '1160.039')]
[36m[2025-07-02 08:14:50,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11812864. Throughput: 0: 274.2. Samples: 11816320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:14:50,952][166323] Avg episode reward: [(0, '1187.021')]
[36m[2025-07-02 08:14:56,010][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 11812864. Throughput: 0: 273.3. Samples: 11817840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:14:56,011][166323] Avg episode reward: [(0, '1155.503')]
[36m[2025-07-02 08:15:00,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11812864. Throughput: 0: 273.4. Samples: 11819424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:15:00,951][166323] Avg episode reward: [(0, '1148.009')]
[31m[42092613 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[42092613 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[42092613 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:15:05,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11812864. Throughput: 0: 272.7. Samples: 11820288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:15:05,959][166323] Avg episode reward: [(0, '1111.689')]
[36m[2025-07-02 08:15:10,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11812864. Throughput: 0: 273.9. Samples: 11822000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:15:10,950][166323] Avg episode reward: [(0, '1153.744')]
[36m[2025-07-02 08:15:15,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11812864. Throughput: 0: 275.3. Samples: 11823792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:15:15,946][166323] Avg episode reward: [(0, '1139.019')]
[36m[2025-07-02 08:15:20,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 11812864. Throughput: 0: 277.8. Samples: 11824672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:15:20,948][166323] Avg episode reward: [(0, '1166.258')]
[31m[42111308 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[42111309 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[42111309 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:15:25,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11812864. Throughput: 0: 275.3. Samples: 11826336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:15:25,992][166323] Avg episode reward: [(0, '1126.434')]
[36m[2025-07-02 08:15:30,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.2). Total num frames: 11812864. Throughput: 0: 274.9. Samples: 11827904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:15:30,992][166323] Avg episode reward: [(0, '1188.000')]
[31m[42124060 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[42124060 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[42124060 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:15:35,992][166323] Fps is (10 sec: 1638.4, 60 sec: 545.9, 300 sec: 277.7). Total num frames: 11829248. Throughput: 0: 273.2. Samples: 11828624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:15:35,992][166323] Avg episode reward: [(0, '1232.130')]
[37m[1m[2025-07-02 08:15:36,043][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023096_11829248.pth...
[36m[2025-07-02 08:15:36,048][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000022936_11747328.pth
[36m[2025-07-02 08:15:40,971][166323] Fps is (10 sec: 1641.8, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11829248. Throughput: 0: 280.8. Samples: 11830464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:15:40,971][166323] Avg episode reward: [(0, '1301.410')]
[36m[2025-07-02 08:15:45,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11829248. Throughput: 0: 281.4. Samples: 11832096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:15:45,984][166323] Avg episode reward: [(0, '1241.719')]
[36m[2025-07-02 08:15:50,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11829248. Throughput: 0: 280.8. Samples: 11832928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:15:50,975][166323] Avg episode reward: [(0, '1243.088')]
[36m[2025-07-02 08:15:55,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11829248. Throughput: 0: 276.9. Samples: 11834464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:15:55,956][166323] Avg episode reward: [(0, '1211.530')]
[36m[2025-07-02 08:16:00,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11829248. Throughput: 0: 272.6. Samples: 11836064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:16:00,965][166323] Avg episode reward: [(0, '1292.150')]
[36m[2025-07-02 08:16:05,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11829248. Throughput: 0: 270.7. Samples: 11836864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:16:05,981][166323] Avg episode reward: [(0, '1265.682')]
[36m[2025-07-02 08:16:10,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11829248. Throughput: 0: 271.0. Samples: 11838528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:16:10,975][166323] Avg episode reward: [(0, '1287.278')]
[36m[2025-07-02 08:16:15,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11829248. Throughput: 0: 270.1. Samples: 11840048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:16:15,951][166323] Avg episode reward: [(0, '1240.784')]
[36m[2025-07-02 08:16:20,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11829248. Throughput: 0: 269.6. Samples: 11840752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:16:20,984][166323] Avg episode reward: [(0, '1251.335')]
[36m[2025-07-02 08:16:25,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11829248. Throughput: 0: 267.1. Samples: 11842480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:16:25,961][166323] Avg episode reward: [(0, '1285.455')]
[36m[2025-07-02 08:16:30,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11829248. Throughput: 0: 268.7. Samples: 11844176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:16:30,948][166323] Avg episode reward: [(0, '1237.059')]
[36m[2025-07-02 08:16:35,987][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 222.1). Total num frames: 11829248. Throughput: 0: 266.9. Samples: 11844944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:16:35,988][166323] Avg episode reward: [(0, '1211.564')]
[36m[2025-07-02 08:16:40,959][166323] Fps is (10 sec: 1636.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11845632. Throughput: 0: 269.5. Samples: 11846592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:16:40,959][166323] Avg episode reward: [(0, '1231.442')]
[36m[2025-07-02 08:16:45,992][166323] Fps is (10 sec: 1637.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11845632. Throughput: 0: 271.8. Samples: 11848304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:16:45,992][166323] Avg episode reward: [(0, '1183.818')]
[36m[2025-07-02 08:16:50,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11845632. Throughput: 0: 273.9. Samples: 11849184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:16:50,963][166323] Avg episode reward: [(0, '1270.728')]
[36m[2025-07-02 08:16:55,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11845632. Throughput: 0: 272.8. Samples: 11850800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:16:55,966][166323] Avg episode reward: [(0, '1288.434')]
[36m[2025-07-02 08:17:00,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11845632. Throughput: 0: 279.9. Samples: 11852656. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:17:00,990][166323] Avg episode reward: [(0, '1299.895')]
[36m[2025-07-02 08:17:05,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 11845632. Throughput: 0: 281.1. Samples: 11853392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:17:05,947][166323] Avg episode reward: [(0, '1278.304')]
[36m[2025-07-02 08:17:11,022][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 11845632. Throughput: 0: 276.2. Samples: 11854928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:17:11,022][166323] Avg episode reward: [(0, '1267.537')]
[36m[2025-07-02 08:17:15,976][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11845632. Throughput: 0: 275.0. Samples: 11856560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:17:15,977][166323] Avg episode reward: [(0, '1259.713')]
[31m[42226190 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[42226191 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[42226191 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:17:20,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11845632. Throughput: 0: 276.2. Samples: 11857360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:17:20,947][166323] Avg episode reward: [(0, '1207.222')]
[36m[2025-07-02 08:17:25,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11845632. Throughput: 0: 275.5. Samples: 11858992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:17:25,966][166323] Avg episode reward: [(0, '1204.810')]
[36m[2025-07-02 08:17:30,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11845632. Throughput: 0: 273.8. Samples: 11860624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:17:30,996][166323] Avg episode reward: [(0, '1221.456')]
[36m[2025-07-02 08:17:36,044][166323] Fps is (10 sec: 1625.8, 60 sec: 545.6, 300 sec: 277.6). Total num frames: 11862016. Throughput: 0: 270.4. Samples: 11861376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:17:36,044][166323] Avg episode reward: [(0, '1226.026')]
[37m[1m[2025-07-02 08:17:36,118][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023160_11862016.pth...
[36m[2025-07-02 08:17:36,125][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023032_11796480.pth
[36m[2025-07-02 08:17:40,988][166323] Fps is (10 sec: 1639.6, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11862016. Throughput: 0: 270.8. Samples: 11862992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:17:40,989][166323] Avg episode reward: [(0, '1227.649')]
[36m[2025-07-02 08:17:45,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11862016. Throughput: 0: 265.6. Samples: 11864608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:17:45,989][166323] Avg episode reward: [(0, '1212.058')]
[36m[2025-07-02 08:17:50,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11862016. Throughput: 0: 267.8. Samples: 11865456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:17:50,990][166323] Avg episode reward: [(0, '1246.412')]
[36m[2025-07-02 08:17:56,009][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11862016. Throughput: 0: 269.6. Samples: 11867056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:17:56,009][166323] Avg episode reward: [(0, '1271.019')]
[36m[2025-07-02 08:18:00,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11862016. Throughput: 0: 272.1. Samples: 11868800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:18:00,962][166323] Avg episode reward: [(0, '1240.663')]
[36m[2025-07-02 08:18:05,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11862016. Throughput: 0: 273.4. Samples: 11869664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:18:05,955][166323] Avg episode reward: [(0, '1177.139')]
[36m[2025-07-02 08:18:10,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11862016. Throughput: 0: 273.8. Samples: 11871312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:18:10,970][166323] Avg episode reward: [(0, '1157.233')]
[31m[42281084 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[42281084 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[42281084 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:18:15,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11862016. Throughput: 0: 278.6. Samples: 11873152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:18:15,964][166323] Avg episode reward: [(0, '1166.762')]
[36m[2025-07-02 08:18:20,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11862016. Throughput: 0: 279.6. Samples: 11873936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:18:20,966][166323] Avg episode reward: [(0, '1168.486')]
[36m[2025-07-02 08:18:25,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11862016. Throughput: 0: 280.3. Samples: 11875600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:18:25,968][166323] Avg episode reward: [(0, '1181.601')]
[36m[2025-07-02 08:18:30,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.3). Total num frames: 11862016. Throughput: 0: 281.1. Samples: 11877248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:18:30,961][166323] Avg episode reward: [(0, '1123.877')]
[36m[2025-07-02 08:18:35,977][166323] Fps is (10 sec: 1637.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 11878400. Throughput: 0: 282.4. Samples: 11878160. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 08:18:35,977][166323] Avg episode reward: [(0, '1182.260')]
[36m[2025-07-02 08:18:40,982][166323] Fps is (10 sec: 1634.9, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11878400. Throughput: 0: 284.3. Samples: 11879840. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 08:18:40,982][166323] Avg episode reward: [(0, '1243.237')]
[36m[2025-07-02 08:18:45,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11878400. Throughput: 0: 284.3. Samples: 11881600. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 08:18:45,984][166323] Avg episode reward: [(0, '1242.845')]
[36m[2025-07-02 08:18:50,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11878400. Throughput: 0: 282.9. Samples: 11882400. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 08:18:50,972][166323] Avg episode reward: [(0, '1222.651')]
[36m[2025-07-02 08:18:55,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11878400. Throughput: 0: 284.0. Samples: 11884096. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 08:18:55,979][166323] Avg episode reward: [(0, '1233.061')]
[36m[2025-07-02 08:19:00,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 11878400. Throughput: 0: 282.7. Samples: 11885872. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 08:19:00,952][166323] Avg episode reward: [(0, '1279.061')]
[36m[2025-07-02 08:19:05,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11878400. Throughput: 0: 284.8. Samples: 11886752. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 08:19:05,967][166323] Avg episode reward: [(0, '1287.821')]
[36m[2025-07-02 08:19:10,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11878400. Throughput: 0: 287.8. Samples: 11888560. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 08:19:10,993][166323] Avg episode reward: [(0, '1284.414')]
[36m[2025-07-02 08:19:15,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11878400. Throughput: 0: 287.2. Samples: 11890176. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 08:19:15,974][166323] Avg episode reward: [(0, '1282.527')]
[36m[2025-07-02 08:19:20,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11878400. Throughput: 0: 287.9. Samples: 11891120. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 08:19:20,994][166323] Avg episode reward: [(0, '1321.999')]
[36m[2025-07-02 08:19:25,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11878400. Throughput: 0: 287.0. Samples: 11892752. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 08:19:25,969][166323] Avg episode reward: [(0, '1333.564')]
[36m[2025-07-02 08:19:30,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11878400. Throughput: 0: 285.9. Samples: 11894464. Policy #0 lag: (min: 27.0, avg: 27.0, max: 27.0)
[36m[2025-07-02 08:19:30,971][166323] Avg episode reward: [(0, '1362.062')]
[36m[2025-07-02 08:19:35,953][166323] Fps is (10 sec: 1641.1, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11894784. Throughput: 0: 286.7. Samples: 11895296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:19:35,953][166323] Avg episode reward: [(0, '1328.159')]
[37m[1m[2025-07-02 08:19:36,041][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023224_11894784.pth...
[36m[2025-07-02 08:19:36,049][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023096_11829248.pth
[36m[2025-07-02 08:19:40,953][166323] Fps is (10 sec: 1641.4, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11894784. Throughput: 0: 284.3. Samples: 11896880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:19:40,953][166323] Avg episode reward: [(0, '1321.758')]
[36m[2025-07-02 08:19:45,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11894784. Throughput: 0: 281.0. Samples: 11898528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:19:45,986][166323] Avg episode reward: [(0, '1273.553')]
[36m[2025-07-02 08:19:50,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11894784. Throughput: 0: 279.9. Samples: 11899344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:19:50,958][166323] Avg episode reward: [(0, '1320.808')]
[36m[2025-07-02 08:19:55,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11894784. Throughput: 0: 277.7. Samples: 11901056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:19:55,999][166323] Avg episode reward: [(0, '1325.662')]
[36m[2025-07-02 08:20:00,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11894784. Throughput: 0: 280.3. Samples: 11902784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:20:00,951][166323] Avg episode reward: [(0, '1279.304')]
[36m[2025-07-02 08:20:05,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11894784. Throughput: 0: 277.3. Samples: 11903584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:20:05,946][166323] Avg episode reward: [(0, '1256.602')]
[36m[2025-07-02 08:20:10,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11894784. Throughput: 0: 280.0. Samples: 11905344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:20:10,945][166323] Avg episode reward: [(0, '1302.560')]
[36m[2025-07-02 08:20:15,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11894784. Throughput: 0: 280.6. Samples: 11907088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:20:15,958][166323] Avg episode reward: [(0, '1270.073')]
[36m[2025-07-02 08:20:20,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11894784. Throughput: 0: 281.7. Samples: 11907984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:20:20,992][166323] Avg episode reward: [(0, '1293.600')]
[36m[2025-07-02 08:20:25,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11894784. Throughput: 0: 283.0. Samples: 11909616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:20:25,958][166323] Avg episode reward: [(0, '1258.149')]
[36m[2025-07-02 08:20:30,972][166323] Fps is (10 sec: 1641.8, 60 sec: 546.1, 300 sec: 277.7). Total num frames: 11911168. Throughput: 0: 286.0. Samples: 11911392. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 08:20:30,972][166323] Avg episode reward: [(0, '1288.176')]
[36m[2025-07-02 08:20:35,981][166323] Fps is (10 sec: 1634.5, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11911168. Throughput: 0: 287.1. Samples: 11912272. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 08:20:35,982][166323] Avg episode reward: [(0, '1346.238')]
[36m[2025-07-02 08:20:40,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11911168. Throughput: 0: 290.1. Samples: 11914096. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 08:20:40,952][166323] Avg episode reward: [(0, '1308.658')]
[36m[2025-07-02 08:20:45,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11911168. Throughput: 0: 288.4. Samples: 11915760. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 08:20:45,952][166323] Avg episode reward: [(0, '1364.732')]
[36m[2025-07-02 08:20:50,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11911168. Throughput: 0: 289.2. Samples: 11916608. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 08:20:50,976][166323] Avg episode reward: [(0, '1402.259')]
[36m[2025-07-02 08:20:55,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11911168. Throughput: 0: 285.8. Samples: 11918208. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 08:20:55,951][166323] Avg episode reward: [(0, '1338.871')]
[36m[2025-07-02 08:21:00,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11911168. Throughput: 0: 284.6. Samples: 11919904. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 08:21:00,994][166323] Avg episode reward: [(0, '1374.924')]
[36m[2025-07-02 08:21:05,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11911168. Throughput: 0: 285.7. Samples: 11920832. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 08:21:05,964][166323] Avg episode reward: [(0, '1360.341')]
[36m[2025-07-02 08:21:10,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11911168. Throughput: 0: 286.0. Samples: 11922496. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 08:21:10,990][166323] Avg episode reward: [(0, '1350.175')]
[36m[2025-07-02 08:21:15,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11911168. Throughput: 0: 283.6. Samples: 11924160. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 08:21:15,990][166323] Avg episode reward: [(0, '1354.563')]
[36m[2025-07-02 08:21:20,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11911168. Throughput: 0: 283.7. Samples: 11925040. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 08:21:20,987][166323] Avg episode reward: [(0, '1270.217')]
[36m[2025-07-02 08:21:25,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11911168. Throughput: 0: 276.9. Samples: 11926560. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 08:21:25,964][166323] Avg episode reward: [(0, '1300.286')]
[36m[2025-07-02 08:21:31,001][166323] Fps is (10 sec: 1636.1, 60 sec: 272.9, 300 sec: 333.2). Total num frames: 11927552. Throughput: 0: 275.6. Samples: 11928176. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 08:21:31,001][166323] Avg episode reward: [(0, '1327.557')]
[36m[2025-07-02 08:21:35,969][166323] Fps is (10 sec: 1637.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11927552. Throughput: 0: 275.2. Samples: 11928992. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 08:21:35,969][166323] Avg episode reward: [(0, '1310.258')]
[37m[1m[2025-07-02 08:21:36,030][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023288_11927552.pth...
[36m[2025-07-02 08:21:36,036][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023160_11862016.pth
[36m[2025-07-02 08:21:40,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11927552. Throughput: 0: 278.3. Samples: 11930736. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 08:21:40,967][166323] Avg episode reward: [(0, '1342.953')]
[36m[2025-07-02 08:21:45,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11927552. Throughput: 0: 278.7. Samples: 11932432. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 08:21:45,947][166323] Avg episode reward: [(0, '1318.360')]
[36m[2025-07-02 08:21:50,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11927552. Throughput: 0: 275.5. Samples: 11933232. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 08:21:50,972][166323] Avg episode reward: [(0, '1321.501')]
[36m[2025-07-02 08:21:55,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11927552. Throughput: 0: 278.2. Samples: 11935008. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 08:21:55,967][166323] Avg episode reward: [(0, '1324.583')]
[36m[2025-07-02 08:22:00,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 11927552. Throughput: 0: 275.1. Samples: 11936528. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 08:22:00,947][166323] Avg episode reward: [(0, '1321.458')]
[36m[2025-07-02 08:22:05,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 11927552. Throughput: 0: 275.4. Samples: 11937424. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 08:22:05,963][166323] Avg episode reward: [(0, '1274.717')]
[36m[2025-07-02 08:22:10,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11927552. Throughput: 0: 280.4. Samples: 11939184. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 08:22:10,993][166323] Avg episode reward: [(0, '1299.549')]
[36m[2025-07-02 08:22:15,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11927552. Throughput: 0: 283.5. Samples: 11940928. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 08:22:15,984][166323] Avg episode reward: [(0, '1274.594')]
[36m[2025-07-02 08:22:20,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11927552. Throughput: 0: 284.3. Samples: 11941792. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 08:22:20,998][166323] Avg episode reward: [(0, '1277.984')]
[36m[2025-07-02 08:22:25,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11927552. Throughput: 0: 284.1. Samples: 11943520. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 08:22:25,970][166323] Avg episode reward: [(0, '1206.701')]
[36m[2025-07-02 08:22:30,950][166323] Fps is (10 sec: 1646.3, 60 sec: 273.3, 300 sec: 277.8). Total num frames: 11943936. Throughput: 0: 284.1. Samples: 11945216. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 08:22:30,950][166323] Avg episode reward: [(0, '1239.252')]
[36m[2025-07-02 08:22:35,997][166323] Fps is (10 sec: 1634.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11943936. Throughput: 0: 285.4. Samples: 11946080. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 08:22:35,997][166323] Avg episode reward: [(0, '1248.997')]
[36m[2025-07-02 08:22:40,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11943936. Throughput: 0: 281.3. Samples: 11947664. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 08:22:40,951][166323] Avg episode reward: [(0, '1235.502')]
[36m[2025-07-02 08:22:45,973][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11943936. Throughput: 0: 281.8. Samples: 11949216. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 08:22:45,973][166323] Avg episode reward: [(0, '1247.103')]
[36m[2025-07-02 08:22:50,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11943936. Throughput: 0: 280.8. Samples: 11950064. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 08:22:50,980][166323] Avg episode reward: [(0, '1259.121')]
[36m[2025-07-02 08:22:55,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11943936. Throughput: 0: 280.5. Samples: 11951792. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 08:22:55,944][166323] Avg episode reward: [(0, '1291.691')]
[36m[2025-07-02 08:23:00,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11943936. Throughput: 0: 281.4. Samples: 11953584. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 08:23:00,963][166323] Avg episode reward: [(0, '1327.497')]
[36m[2025-07-02 08:23:05,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11943936. Throughput: 0: 281.5. Samples: 11954448. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 08:23:05,960][166323] Avg episode reward: [(0, '1335.646')]
[31m[42579144 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[42579144 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[42579144 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[42579424 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[42579424 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[42579425 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:23:10,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11943936. Throughput: 0: 276.3. Samples: 11955952. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 08:23:10,967][166323] Avg episode reward: [(0, '1308.465')]
[36m[2025-07-02 08:23:15,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11943936. Throughput: 0: 273.0. Samples: 11957504. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 08:23:15,968][166323] Avg episode reward: [(0, '1318.805')]
[36m[2025-07-02 08:23:20,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11943936. Throughput: 0: 274.3. Samples: 11958416. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 08:23:20,977][166323] Avg episode reward: [(0, '1294.760')]
[36m[2025-07-02 08:23:26,002][166323] Fps is (10 sec: 1632.8, 60 sec: 545.8, 300 sec: 333.2). Total num frames: 11960320. Throughput: 0: 276.3. Samples: 11960112. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 08:23:26,002][166323] Avg episode reward: [(0, '1262.414')]
[36m[2025-07-02 08:23:30,966][166323] Fps is (10 sec: 1640.1, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11960320. Throughput: 0: 278.1. Samples: 11961728. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 08:23:30,966][166323] Avg episode reward: [(0, '1260.956')]
[36m[2025-07-02 08:23:35,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11960320. Throughput: 0: 275.8. Samples: 11962480. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 08:23:35,993][166323] Avg episode reward: [(0, '1250.983')]
[37m[1m[2025-07-02 08:23:36,078][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023352_11960320.pth...
[36m[2025-07-02 08:23:36,083][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023224_11894784.pth
[36m[2025-07-02 08:23:40,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11960320. Throughput: 0: 274.4. Samples: 11964144. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 08:23:40,954][166323] Avg episode reward: [(0, '1235.769')]
[36m[2025-07-02 08:23:45,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11960320. Throughput: 0: 269.8. Samples: 11965728. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 08:23:45,978][166323] Avg episode reward: [(0, '1188.589')]
[36m[2025-07-02 08:23:50,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11960320. Throughput: 0: 270.5. Samples: 11966624. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 08:23:50,981][166323] Avg episode reward: [(0, '1175.886')]
[36m[2025-07-02 08:23:55,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11960320. Throughput: 0: 275.7. Samples: 11968352. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 08:23:55,949][166323] Avg episode reward: [(0, '1230.954')]
[36m[2025-07-02 08:24:00,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11960320. Throughput: 0: 277.9. Samples: 11970016. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 08:24:00,990][166323] Avg episode reward: [(0, '1222.152')]
[36m[2025-07-02 08:24:05,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11960320. Throughput: 0: 276.4. Samples: 11970848. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 08:24:05,956][166323] Avg episode reward: [(0, '1173.524')]
[36m[2025-07-02 08:24:10,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11960320. Throughput: 0: 275.8. Samples: 11972512. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 08:24:10,957][166323] Avg episode reward: [(0, '1208.179')]
[36m[2025-07-02 08:24:15,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11960320. Throughput: 0: 276.7. Samples: 11974176. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 08:24:15,952][166323] Avg episode reward: [(0, '1243.505')]
[36m[2025-07-02 08:24:20,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11960320. Throughput: 0: 279.9. Samples: 11975072. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 08:24:20,985][166323] Avg episode reward: [(0, '1300.126')]
[36m[2025-07-02 08:24:26,012][166323] Fps is (10 sec: 1628.6, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 11976704. Throughput: 0: 277.3. Samples: 11976640. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 08:24:26,012][166323] Avg episode reward: [(0, '1312.062')]
[36m[2025-07-02 08:24:30,991][166323] Fps is (10 sec: 1637.4, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11976704. Throughput: 0: 281.9. Samples: 11978416. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 08:24:30,991][166323] Avg episode reward: [(0, '1343.551')]
[31m[42660549 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[42660550 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[42660550 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:24:35,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11976704. Throughput: 0: 279.8. Samples: 11979216. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 08:24:35,980][166323] Avg episode reward: [(0, '1323.993')]
[36m[2025-07-02 08:24:40,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11976704. Throughput: 0: 279.7. Samples: 11980944. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 08:24:40,971][166323] Avg episode reward: [(0, '1329.568')]
[36m[2025-07-02 08:24:45,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11976704. Throughput: 0: 280.0. Samples: 11982608. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 08:24:45,953][166323] Avg episode reward: [(0, '1284.371')]
[36m[2025-07-02 08:24:50,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11976704. Throughput: 0: 281.0. Samples: 11983488. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 08:24:50,944][166323] Avg episode reward: [(0, '1311.520')]
[36m[2025-07-02 08:24:55,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11976704. Throughput: 0: 283.6. Samples: 11985280. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 08:24:55,975][166323] Avg episode reward: [(0, '1277.625')]
[36m[2025-07-02 08:25:00,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11976704. Throughput: 0: 284.3. Samples: 11986976. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 08:25:00,969][166323] Avg episode reward: [(0, '1265.367')]
[36m[2025-07-02 08:25:05,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11976704. Throughput: 0: 283.6. Samples: 11987824. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 08:25:05,944][166323] Avg episode reward: [(0, '1251.193')]
[36m[2025-07-02 08:25:10,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11976704. Throughput: 0: 283.9. Samples: 11989408. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 08:25:10,982][166323] Avg episode reward: [(0, '1318.542')]
[33m[42700604 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[42700604 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.8671875
[33mCrash Rate: 0.119140625
[33mTimeout Rate: 0.013671875 (navigation_task.py:265)
[33m[42700604 ms][navigation_task] - WARNING : 
[33mSuccesses: 1776
[33mCrashes : 244
[33mTimeouts: 28 (navigation_task.py:268)
[36m[2025-07-02 08:25:15,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11976704. Throughput: 0: 280.7. Samples: 11991040. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 08:25:15,972][166323] Avg episode reward: [(0, '1250.702')]
[36m[2025-07-02 08:25:20,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11976704. Throughput: 0: 282.0. Samples: 11991904. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-02 08:25:20,971][166323] Avg episode reward: [(0, '1227.237')]
[36m[2025-07-02 08:25:25,989][166323] Fps is (10 sec: 1635.7, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11993088. Throughput: 0: 284.0. Samples: 11993728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:25:25,989][166323] Avg episode reward: [(0, '1262.711')]
[36m[2025-07-02 08:25:30,959][166323] Fps is (10 sec: 1640.3, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11993088. Throughput: 0: 289.7. Samples: 11995648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:25:30,959][166323] Avg episode reward: [(0, '1229.288')]
[36m[2025-07-02 08:25:35,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11993088. Throughput: 0: 287.7. Samples: 11996448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:25:35,996][166323] Avg episode reward: [(0, '1218.082')]
[37m[1m[2025-07-02 08:25:36,062][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023416_11993088.pth...
[36m[2025-07-02 08:25:36,066][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023288_11927552.pth
[36m[2025-07-02 08:25:40,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11993088. Throughput: 0: 284.3. Samples: 11998064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:25:40,946][166323] Avg episode reward: [(0, '1199.252')]
[36m[2025-07-02 08:25:45,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11993088. Throughput: 0: 282.9. Samples: 11999712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:25:45,988][166323] Avg episode reward: [(0, '1182.766')]
[36m[2025-07-02 08:25:50,975][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 11993088. Throughput: 0: 286.0. Samples: 12000704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:25:50,976][166323] Avg episode reward: [(0, '1248.283')]
[36m[2025-07-02 08:25:55,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 11993088. Throughput: 0: 287.5. Samples: 12002336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:25:55,955][166323] Avg episode reward: [(0, '1209.969')]
[36m[2025-07-02 08:26:00,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 11993088. Throughput: 0: 284.8. Samples: 12003856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:26:00,973][166323] Avg episode reward: [(0, '1205.712')]
[36m[2025-07-02 08:26:05,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11993088. Throughput: 0: 285.3. Samples: 12004736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:26:05,955][166323] Avg episode reward: [(0, '1180.427')]
[36m[2025-07-02 08:26:10,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11993088. Throughput: 0: 282.3. Samples: 12006432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:26:10,986][166323] Avg episode reward: [(0, '1204.096')]
[36m[2025-07-02 08:26:15,995][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 11993088. Throughput: 0: 277.1. Samples: 12008128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:26:15,995][166323] Avg episode reward: [(0, '1164.214')]
[36m[2025-07-02 08:26:20,975][166323] Fps is (10 sec: 1640.2, 60 sec: 546.1, 300 sec: 333.2). Total num frames: 12009472. Throughput: 0: 277.5. Samples: 12008928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:26:20,975][166323] Avg episode reward: [(0, '1284.388')]
[36m[2025-07-02 08:26:25,945][166323] Fps is (10 sec: 1646.6, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12009472. Throughput: 0: 274.8. Samples: 12010432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:26:25,946][166323] Avg episode reward: [(0, '1250.114')]
[36m[2025-07-02 08:26:30,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12009472. Throughput: 0: 272.4. Samples: 12011968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:26:30,978][166323] Avg episode reward: [(0, '1269.571')]
[36m[2025-07-02 08:26:35,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12009472. Throughput: 0: 269.2. Samples: 12012816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:26:35,962][166323] Avg episode reward: [(0, '1292.585')]
[36m[2025-07-02 08:26:40,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 12009472. Throughput: 0: 272.5. Samples: 12014608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:26:40,995][166323] Avg episode reward: [(0, '1284.823')]
[36m[2025-07-02 08:26:45,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12009472. Throughput: 0: 277.1. Samples: 12016320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:26:45,954][166323] Avg episode reward: [(0, '1311.944')]
[36m[2025-07-02 08:26:50,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12009472. Throughput: 0: 277.0. Samples: 12017200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:26:50,953][166323] Avg episode reward: [(0, '1350.729')]
[31m[42799668 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[42799669 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[42799669 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[42803171 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[42803171 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[42803172 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:26:55,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12009472. Throughput: 0: 276.5. Samples: 12018864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:26:55,951][166323] Avg episode reward: [(0, '1286.303')]
[36m[2025-07-02 08:27:00,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12009472. Throughput: 0: 273.6. Samples: 12020432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:27:00,973][166323] Avg episode reward: [(0, '1327.738')]
[36m[2025-07-02 08:27:05,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12009472. Throughput: 0: 277.7. Samples: 12021424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:27:05,968][166323] Avg episode reward: [(0, '1304.228')]
[36m[2025-07-02 08:27:10,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12009472. Throughput: 0: 282.2. Samples: 12023136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:27:10,970][166323] Avg episode reward: [(0, '1308.580')]
[36m[2025-07-02 08:27:15,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12009472. Throughput: 0: 289.5. Samples: 12024992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:27:15,961][166323] Avg episode reward: [(0, '1293.436')]
[36m[2025-07-02 08:27:20,972][166323] Fps is (10 sec: 1638.1, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 12025856. Throughput: 0: 286.9. Samples: 12025728. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 08:27:20,972][166323] Avg episode reward: [(0, '1283.306')]
[36m[2025-07-02 08:27:25,952][166323] Fps is (10 sec: 1639.8, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12025856. Throughput: 0: 282.9. Samples: 12027328. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 08:27:25,952][166323] Avg episode reward: [(0, '1219.305')]
[36m[2025-07-02 08:27:30,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12025856. Throughput: 0: 283.7. Samples: 12029088. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 08:27:30,960][166323] Avg episode reward: [(0, '1244.654')]
[36m[2025-07-02 08:27:35,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12025856. Throughput: 0: 282.2. Samples: 12029904. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 08:27:35,969][166323] Avg episode reward: [(0, '1292.599')]
[37m[1m[2025-07-02 08:27:36,042][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023480_12025856.pth...
[36m[2025-07-02 08:27:36,046][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023352_11960320.pth
[36m[2025-07-02 08:27:40,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12025856. Throughput: 0: 280.5. Samples: 12031488. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 08:27:40,950][166323] Avg episode reward: [(0, '1276.783')]
[31m[42850365 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[42850366 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[42850367 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:27:45,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12025856. Throughput: 0: 283.0. Samples: 12033168. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 08:27:45,976][166323] Avg episode reward: [(0, '1250.197')]
[36m[2025-07-02 08:27:50,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12025856. Throughput: 0: 282.5. Samples: 12034144. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 08:27:50,991][166323] Avg episode reward: [(0, '1289.237')]
[36m[2025-07-02 08:27:55,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12025856. Throughput: 0: 280.3. Samples: 12035744. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 08:27:55,954][166323] Avg episode reward: [(0, '1277.181')]
[36m[2025-07-02 08:28:00,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12025856. Throughput: 0: 278.5. Samples: 12037520. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 08:28:00,949][166323] Avg episode reward: [(0, '1323.902')]
[36m[2025-07-02 08:28:05,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12025856. Throughput: 0: 281.7. Samples: 12038400. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 08:28:05,960][166323] Avg episode reward: [(0, '1310.527')]
[36m[2025-07-02 08:28:11,004][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12025856. Throughput: 0: 286.2. Samples: 12040224. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 08:28:11,004][166323] Avg episode reward: [(0, '1290.311')]
[36m[2025-07-02 08:28:15,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12025856. Throughput: 0: 281.2. Samples: 12041744. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 08:28:15,970][166323] Avg episode reward: [(0, '1312.731')]
[36m[2025-07-02 08:28:20,976][166323] Fps is (10 sec: 1643.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12042240. Throughput: 0: 281.2. Samples: 12042560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:28:20,976][166323] Avg episode reward: [(0, '1286.881')]
[36m[2025-07-02 08:28:25,944][166323] Fps is (10 sec: 1642.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12042240. Throughput: 0: 286.3. Samples: 12044368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:28:25,944][166323] Avg episode reward: [(0, '1230.460')]
[36m[2025-07-02 08:28:30,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12042240. Throughput: 0: 286.4. Samples: 12046048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:28:30,956][166323] Avg episode reward: [(0, '1249.442')]
[36m[2025-07-02 08:28:36,003][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 12042240. Throughput: 0: 281.9. Samples: 12046832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:28:36,004][166323] Avg episode reward: [(0, '1245.977')]
[36m[2025-07-02 08:28:40,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12042240. Throughput: 0: 284.1. Samples: 12048528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:28:40,946][166323] Avg episode reward: [(0, '1207.112')]
[36m[2025-07-02 08:28:46,008][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12042240. Throughput: 0: 280.5. Samples: 12050160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:28:46,009][166323] Avg episode reward: [(0, '1223.735')]
[36m[2025-07-02 08:28:50,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12042240. Throughput: 0: 280.0. Samples: 12051008. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:28:50,989][166323] Avg episode reward: [(0, '1176.167')]
[36m[2025-07-02 08:28:55,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12042240. Throughput: 0: 278.0. Samples: 12052720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:28:55,946][166323] Avg episode reward: [(0, '1241.112')]
[36m[2025-07-02 08:29:00,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12042240. Throughput: 0: 281.2. Samples: 12054400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:29:00,982][166323] Avg episode reward: [(0, '1254.524')]
[31m[42932103 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[42932103 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[42932103 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:29:05,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12042240. Throughput: 0: 279.3. Samples: 12055120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:29:05,951][166323] Avg episode reward: [(0, '1232.573')]
[36m[2025-07-02 08:29:10,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12042240. Throughput: 0: 277.3. Samples: 12056848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:29:10,944][166323] Avg episode reward: [(0, '1256.700')]
[31m[42942141 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[42942141 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[42942142 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:29:15,962][166323] Fps is (10 sec: 1636.6, 60 sec: 546.2, 300 sec: 333.3). Total num frames: 12058624. Throughput: 0: 278.4. Samples: 12058576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:29:15,962][166323] Avg episode reward: [(0, '1289.924')]
[31m[42948950 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[42948951 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[42948951 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:29:20,965][166323] Fps is (10 sec: 1635.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12058624. Throughput: 0: 277.6. Samples: 12059312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:29:20,965][166323] Avg episode reward: [(0, '1215.975')]
[36m[2025-07-02 08:29:26,000][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 12058624. Throughput: 0: 276.3. Samples: 12060976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:29:26,001][166323] Avg episode reward: [(0, '1253.809')]
[31m[42959334 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[42959335 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[42959335 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:29:31,020][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 12058624. Throughput: 0: 277.6. Samples: 12062656. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:29:31,021][166323] Avg episode reward: [(0, '1244.983')]
[36m[2025-07-02 08:29:35,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12058624. Throughput: 0: 277.4. Samples: 12063488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:29:35,982][166323] Avg episode reward: [(0, '1295.348')]
[37m[1m[2025-07-02 08:29:36,044][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023544_12058624.pth...
[36m[2025-07-02 08:29:36,047][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023416_11993088.pth
[36m[2025-07-02 08:29:40,980][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12058624. Throughput: 0: 277.5. Samples: 12065216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:29:40,980][166323] Avg episode reward: [(0, '1264.647')]
[36m[2025-07-02 08:29:45,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12058624. Throughput: 0: 276.8. Samples: 12066848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:29:45,959][166323] Avg episode reward: [(0, '1243.084')]
[36m[2025-07-02 08:29:50,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12058624. Throughput: 0: 279.1. Samples: 12067680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:29:50,953][166323] Avg episode reward: [(0, '1286.598')]
[36m[2025-07-02 08:29:55,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12058624. Throughput: 0: 278.2. Samples: 12069376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:29:55,970][166323] Avg episode reward: [(0, '1257.539')]
[36m[2025-07-02 08:30:00,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12058624. Throughput: 0: 278.7. Samples: 12071120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:30:00,963][166323] Avg episode reward: [(0, '1212.990')]
[36m[2025-07-02 08:30:05,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12058624. Throughput: 0: 281.5. Samples: 12071984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:30:05,985][166323] Avg episode reward: [(0, '1153.082')]
[36m[2025-07-02 08:30:10,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12058624. Throughput: 0: 283.1. Samples: 12073712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:30:10,983][166323] Avg episode reward: [(0, '1166.918')]
[31m[43003984 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[43003984 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[43003984 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:30:15,962][166323] Fps is (10 sec: 1642.2, 60 sec: 273.1, 300 sec: 333.2). Total num frames: 12075008. Throughput: 0: 282.0. Samples: 12075328. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 08:30:15,962][166323] Avg episode reward: [(0, '1164.153')]
[36m[2025-07-02 08:30:20,955][166323] Fps is (10 sec: 1642.9, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12075008. Throughput: 0: 282.5. Samples: 12076192. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 08:30:20,955][166323] Avg episode reward: [(0, '1167.763')]
[36m[2025-07-02 08:30:25,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12075008. Throughput: 0: 277.6. Samples: 12077712. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 08:30:25,987][166323] Avg episode reward: [(0, '1154.507')]
[36m[2025-07-02 08:30:30,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12075008. Throughput: 0: 281.8. Samples: 12079536. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 08:30:30,979][166323] Avg episode reward: [(0, '1210.172')]
[36m[2025-07-02 08:30:35,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12075008. Throughput: 0: 282.6. Samples: 12080400. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 08:30:35,963][166323] Avg episode reward: [(0, '1259.443')]
[36m[2025-07-02 08:30:40,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12075008. Throughput: 0: 281.9. Samples: 12082064. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 08:30:40,983][166323] Avg episode reward: [(0, '1307.882')]
[36m[2025-07-02 08:30:45,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12075008. Throughput: 0: 284.8. Samples: 12083936. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 08:30:45,959][166323] Avg episode reward: [(0, '1250.431')]
[31m[43038553 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[43038553 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[43038554 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:30:50,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12075008. Throughput: 0: 283.0. Samples: 12084720. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 08:30:50,993][166323] Avg episode reward: [(0, '1268.643')]
[36m[2025-07-02 08:30:55,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12075008. Throughput: 0: 279.6. Samples: 12086288. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 08:30:55,964][166323] Avg episode reward: [(0, '1275.586')]
[36m[2025-07-02 08:31:00,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12075008. Throughput: 0: 286.2. Samples: 12088208. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 08:31:00,966][166323] Avg episode reward: [(0, '1225.773')]
[36m[2025-07-02 08:31:05,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12075008. Throughput: 0: 284.6. Samples: 12089008. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 08:31:05,979][166323] Avg episode reward: [(0, '1190.125')]
[36m[2025-07-02 08:31:10,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12075008. Throughput: 0: 290.0. Samples: 12090752. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 08:31:10,958][166323] Avg episode reward: [(0, '1221.017')]
[36m[2025-07-02 08:31:15,947][166323] Fps is (10 sec: 1643.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12091392. Throughput: 0: 285.4. Samples: 12092368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:31:15,947][166323] Avg episode reward: [(0, '1176.332')]
[36m[2025-07-02 08:31:20,976][166323] Fps is (10 sec: 1635.4, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12091392. Throughput: 0: 284.4. Samples: 12093200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:31:20,976][166323] Avg episode reward: [(0, '1223.446')]
[36m[2025-07-02 08:31:25,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12091392. Throughput: 0: 285.2. Samples: 12094896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:31:25,984][166323] Avg episode reward: [(0, '1201.870')]
[36m[2025-07-02 08:31:30,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12091392. Throughput: 0: 280.6. Samples: 12096560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:31:30,946][166323] Avg episode reward: [(0, '1183.603')]
[36m[2025-07-02 08:31:35,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12091392. Throughput: 0: 282.3. Samples: 12097408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:31:35,945][166323] Avg episode reward: [(0, '1198.663')]
[37m[1m[2025-07-02 08:31:36,025][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023608_12091392.pth...
[36m[2025-07-02 08:31:36,029][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023480_12025856.pth
[36m[2025-07-02 08:31:40,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12091392. Throughput: 0: 284.8. Samples: 12099104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:31:40,968][166323] Avg episode reward: [(0, '1150.654')]
[36m[2025-07-02 08:31:45,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12091392. Throughput: 0: 278.1. Samples: 12100720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:31:45,950][166323] Avg episode reward: [(0, '1144.094')]
[36m[2025-07-02 08:31:50,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12091392. Throughput: 0: 277.4. Samples: 12101488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:31:50,968][166323] Avg episode reward: [(0, '1194.559')]
[36m[2025-07-02 08:31:55,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12091392. Throughput: 0: 276.0. Samples: 12103168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:31:55,946][166323] Avg episode reward: [(0, '1247.458')]
[36m[2025-07-02 08:32:00,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12091392. Throughput: 0: 277.4. Samples: 12104864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:32:00,995][166323] Avg episode reward: [(0, '1275.229')]
[36m[2025-07-02 08:32:05,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12091392. Throughput: 0: 278.0. Samples: 12105712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:32:05,992][166323] Avg episode reward: [(0, '1283.896')]
[36m[2025-07-02 08:32:10,970][166323] Fps is (10 sec: 1642.5, 60 sec: 546.0, 300 sec: 333.2). Total num frames: 12107776. Throughput: 0: 277.8. Samples: 12107392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:32:10,970][166323] Avg episode reward: [(0, '1288.110')]
[36m[2025-07-02 08:32:15,971][166323] Fps is (10 sec: 1641.7, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12107776. Throughput: 0: 277.2. Samples: 12109040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:32:15,971][166323] Avg episode reward: [(0, '1300.447')]
[31m[43127245 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[43127245 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[43127246 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:32:20,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12107776. Throughput: 0: 275.8. Samples: 12109824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:32:20,963][166323] Avg episode reward: [(0, '1252.620')]
[36m[2025-07-02 08:32:25,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12107776. Throughput: 0: 275.1. Samples: 12111488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:32:25,984][166323] Avg episode reward: [(0, '1253.066')]
[36m[2025-07-02 08:32:30,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12107776. Throughput: 0: 280.3. Samples: 12113344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:32:30,993][166323] Avg episode reward: [(0, '1213.938')]
[36m[2025-07-02 08:32:35,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12107776. Throughput: 0: 282.0. Samples: 12114176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:32:35,963][166323] Avg episode reward: [(0, '1221.990')]
[36m[2025-07-02 08:32:40,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12107776. Throughput: 0: 279.6. Samples: 12115760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:32:40,984][166323] Avg episode reward: [(0, '1244.445')]
[36m[2025-07-02 08:32:45,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12107776. Throughput: 0: 281.8. Samples: 12117536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:32:45,959][166323] Avg episode reward: [(0, '1231.722')]
[36m[2025-07-02 08:32:50,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12107776. Throughput: 0: 279.9. Samples: 12118304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:32:50,984][166323] Avg episode reward: [(0, '1289.766')]
[36m[2025-07-02 08:32:55,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12107776. Throughput: 0: 280.9. Samples: 12120032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:32:55,961][166323] Avg episode reward: [(0, '1308.027')]
[36m[2025-07-02 08:33:00,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12107776. Throughput: 0: 281.3. Samples: 12121696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:33:00,962][166323] Avg episode reward: [(0, '1259.544')]
[36m[2025-07-02 08:33:05,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12107776. Throughput: 0: 280.3. Samples: 12122432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:33:05,947][166323] Avg episode reward: [(0, '1285.509')]
[36m[2025-07-02 08:33:10,986][166323] Fps is (10 sec: 1634.5, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 12124160. Throughput: 0: 276.6. Samples: 12123936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:33:10,986][166323] Avg episode reward: [(0, '1259.092')]
[36m[2025-07-02 08:33:15,952][166323] Fps is (10 sec: 1637.6, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12124160. Throughput: 0: 270.5. Samples: 12125504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:33:15,952][166323] Avg episode reward: [(0, '1207.330')]
[36m[2025-07-02 08:33:20,997][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 12124160. Throughput: 0: 271.1. Samples: 12126384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:33:20,997][166323] Avg episode reward: [(0, '1144.882')]
[36m[2025-07-02 08:33:25,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12124160. Throughput: 0: 269.7. Samples: 12127888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:33:25,958][166323] Avg episode reward: [(0, '1148.519')]
[36m[2025-07-02 08:33:30,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12124160. Throughput: 0: 267.7. Samples: 12129584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:33:30,961][166323] Avg episode reward: [(0, '1198.002')]
[36m[2025-07-02 08:33:36,014][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 12124160. Throughput: 0: 268.6. Samples: 12130400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:33:36,014][166323] Avg episode reward: [(0, '1253.811')]
[37m[1m[2025-07-02 08:33:36,020][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023672_12124160.pth...
[36m[2025-07-02 08:33:36,028][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023544_12058624.pth
[36m[2025-07-02 08:33:40,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12124160. Throughput: 0: 268.4. Samples: 12132112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:33:40,961][166323] Avg episode reward: [(0, '1236.709')]
[36m[2025-07-02 08:33:45,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12124160. Throughput: 0: 270.8. Samples: 12133888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:33:45,989][166323] Avg episode reward: [(0, '1307.527')]
[36m[2025-07-02 08:33:50,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12124160. Throughput: 0: 274.3. Samples: 12134784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:33:50,975][166323] Avg episode reward: [(0, '1372.896')]
[36m[2025-07-02 08:33:55,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12124160. Throughput: 0: 280.9. Samples: 12136576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:33:55,990][166323] Avg episode reward: [(0, '1377.100')]
[36m[2025-07-02 08:34:00,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12124160. Throughput: 0: 282.9. Samples: 12138240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:34:00,973][166323] Avg episode reward: [(0, '1341.379')]
[36m[2025-07-02 08:34:05,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12124160. Throughput: 0: 281.2. Samples: 12139024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:34:05,945][166323] Avg episode reward: [(0, '1357.869')]
[36m[2025-07-02 08:34:10,996][166323] Fps is (10 sec: 1634.5, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12140544. Throughput: 0: 285.6. Samples: 12140752. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 08:34:10,997][166323] Avg episode reward: [(0, '1335.784')]
[36m[2025-07-02 08:34:15,979][166323] Fps is (10 sec: 1632.8, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12140544. Throughput: 0: 287.5. Samples: 12142528. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 08:34:15,979][166323] Avg episode reward: [(0, '1319.157')]
[36m[2025-07-02 08:34:21,003][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12140544. Throughput: 0: 288.8. Samples: 12143392. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 08:34:21,003][166323] Avg episode reward: [(0, '1244.736')]
[31m[43254493 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[43254493 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[43254493 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:34:25,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12140544. Throughput: 0: 289.6. Samples: 12145152. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 08:34:25,985][166323] Avg episode reward: [(0, '1240.585')]
[31m[43257492 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[43257492 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[43257492 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:34:30,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12140544. Throughput: 0: 291.4. Samples: 12146992. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 08:34:30,960][166323] Avg episode reward: [(0, '1263.529')]
[36m[2025-07-02 08:34:35,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12140544. Throughput: 0: 293.1. Samples: 12147968. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 08:34:35,954][166323] Avg episode reward: [(0, '1230.255')]
[36m[2025-07-02 08:34:41,012][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 12140544. Throughput: 0: 289.6. Samples: 12149616. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 08:34:41,012][166323] Avg episode reward: [(0, '1217.378')]
[36m[2025-07-02 08:34:45,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12140544. Throughput: 0: 288.6. Samples: 12151232. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 08:34:45,985][166323] Avg episode reward: [(0, '1206.988')]
[36m[2025-07-02 08:34:50,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12140544. Throughput: 0: 290.5. Samples: 12152112. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 08:34:50,993][166323] Avg episode reward: [(0, '1250.061')]
[36m[2025-07-02 08:34:55,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12140544. Throughput: 0: 288.7. Samples: 12153744. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 08:34:55,991][166323] Avg episode reward: [(0, '1259.093')]
[36m[2025-07-02 08:35:00,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12140544. Throughput: 0: 288.8. Samples: 12155520. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 08:35:00,960][166323] Avg episode reward: [(0, '1300.321')]
[36m[2025-07-02 08:35:05,986][166323] Fps is (10 sec: 1639.2, 60 sec: 545.8, 300 sec: 333.2). Total num frames: 12156928. Throughput: 0: 288.5. Samples: 12156368. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 08:35:05,986][166323] Avg episode reward: [(0, '1259.427')]
[36m[2025-07-02 08:35:10,955][166323] Fps is (10 sec: 1639.2, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12156928. Throughput: 0: 284.6. Samples: 12157952. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 08:35:10,955][166323] Avg episode reward: [(0, '1304.592')]
[36m[2025-07-02 08:35:15,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12156928. Throughput: 0: 281.5. Samples: 12159664. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 08:35:15,979][166323] Avg episode reward: [(0, '1305.481')]
[36m[2025-07-02 08:35:20,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12156928. Throughput: 0: 278.3. Samples: 12160496. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 08:35:20,971][166323] Avg episode reward: [(0, '1366.439')]
[36m[2025-07-02 08:35:25,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12156928. Throughput: 0: 279.4. Samples: 12162176. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 08:35:25,958][166323] Avg episode reward: [(0, '1315.250')]
[36m[2025-07-02 08:35:30,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12156928. Throughput: 0: 284.4. Samples: 12164032. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 08:35:30,984][166323] Avg episode reward: [(0, '1300.978')]
[36m[2025-07-02 08:35:35,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12156928. Throughput: 0: 284.3. Samples: 12164896. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 08:35:35,957][166323] Avg episode reward: [(0, '1286.506')]
[37m[1m[2025-07-02 08:35:36,008][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023736_12156928.pth...
[36m[2025-07-02 08:35:36,012][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023608_12091392.pth
[36m[2025-07-02 08:35:40,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12156928. Throughput: 0: 284.2. Samples: 12166528. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 08:35:40,978][166323] Avg episode reward: [(0, '1342.634')]
[36m[2025-07-02 08:35:45,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12156928. Throughput: 0: 281.6. Samples: 12168192. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 08:35:45,954][166323] Avg episode reward: [(0, '1366.816')]
[31m[43334664 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[43334664 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[43334664 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:35:50,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12156928. Throughput: 0: 283.0. Samples: 12169104. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 08:35:50,987][166323] Avg episode reward: [(0, '1308.069')]
[36m[2025-07-02 08:35:55,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12156928. Throughput: 0: 284.7. Samples: 12170768. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 08:35:55,976][166323] Avg episode reward: [(0, '1298.581')]
[36m[2025-07-02 08:36:00,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12156928. Throughput: 0: 283.8. Samples: 12172432. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 08:36:00,971][166323] Avg episode reward: [(0, '1284.580')]
[36m[2025-07-02 08:36:05,948][166323] Fps is (10 sec: 1643.0, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 12173312. Throughput: 0: 286.4. Samples: 12173376. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 08:36:05,949][166323] Avg episode reward: [(0, '1286.609')]
[36m[2025-07-02 08:36:10,960][166323] Fps is (10 sec: 1640.2, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12173312. Throughput: 0: 287.6. Samples: 12175120. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 08:36:10,960][166323] Avg episode reward: [(0, '1262.967')]
[36m[2025-07-02 08:36:15,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12173312. Throughput: 0: 282.7. Samples: 12176752. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 08:36:15,972][166323] Avg episode reward: [(0, '1249.842')]
[36m[2025-07-02 08:36:20,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12173312. Throughput: 0: 281.6. Samples: 12177568. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 08:36:20,950][166323] Avg episode reward: [(0, '1298.478')]
[36m[2025-07-02 08:36:25,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12173312. Throughput: 0: 286.0. Samples: 12179392. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 08:36:25,951][166323] Avg episode reward: [(0, '1239.432')]
[36m[2025-07-02 08:36:30,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12173312. Throughput: 0: 282.8. Samples: 12180928. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 08:36:30,988][166323] Avg episode reward: [(0, '1215.861')]
[33m[43379742 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[43379742 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.86279296875
[33mCrash Rate: 0.12060546875
[33mTimeout Rate: 0.0166015625 (navigation_task.py:265)
[33m[43379742 ms][navigation_task] - WARNING : 
[33mSuccesses: 1767
[33mCrashes : 247
[33mTimeouts: 34 (navigation_task.py:268)
[36m[2025-07-02 08:36:35,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12173312. Throughput: 0: 284.8. Samples: 12181920. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 08:36:35,990][166323] Avg episode reward: [(0, '1260.384')]
[36m[2025-07-02 08:36:40,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12173312. Throughput: 0: 284.8. Samples: 12183584. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 08:36:40,969][166323] Avg episode reward: [(0, '1252.081')]
[31m[43391170 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[43391170 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[43391171 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[43391496 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[43391496 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[43391496 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:36:45,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12173312. Throughput: 0: 285.4. Samples: 12185280. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 08:36:45,984][166323] Avg episode reward: [(0, '1204.321')]
[36m[2025-07-02 08:36:50,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12173312. Throughput: 0: 282.2. Samples: 12186080. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 08:36:50,965][166323] Avg episode reward: [(0, '1198.766')]
[36m[2025-07-02 08:36:55,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12173312. Throughput: 0: 280.0. Samples: 12187728. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 08:36:55,990][166323] Avg episode reward: [(0, '1244.929')]
[36m[2025-07-02 08:37:00,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12173312. Throughput: 0: 281.8. Samples: 12189424. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 08:37:00,947][166323] Avg episode reward: [(0, '1272.482')]
[36m[2025-07-02 08:37:05,956][166323] Fps is (10 sec: 1643.9, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12189696. Throughput: 0: 281.9. Samples: 12190256. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 08:37:05,957][166323] Avg episode reward: [(0, '1314.117')]
[36m[2025-07-02 08:37:10,978][166323] Fps is (10 sec: 1633.2, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12189696. Throughput: 0: 277.2. Samples: 12191872. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 08:37:10,978][166323] Avg episode reward: [(0, '1305.725')]
[36m[2025-07-02 08:37:16,001][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12189696. Throughput: 0: 279.4. Samples: 12193504. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 08:37:16,001][166323] Avg episode reward: [(0, '1307.657')]
[36m[2025-07-02 08:37:20,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12189696. Throughput: 0: 276.9. Samples: 12194368. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 08:37:20,953][166323] Avg episode reward: [(0, '1331.690')]
[36m[2025-07-02 08:37:25,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12189696. Throughput: 0: 279.4. Samples: 12196160. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 08:37:25,973][166323] Avg episode reward: [(0, '1272.329')]
[36m[2025-07-02 08:37:30,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12189696. Throughput: 0: 281.8. Samples: 12197952. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 08:37:30,951][166323] Avg episode reward: [(0, '1326.201')]
[36m[2025-07-02 08:37:35,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12189696. Throughput: 0: 286.0. Samples: 12198944. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 08:37:35,951][166323] Avg episode reward: [(0, '1274.160')]
[37m[1m[2025-07-02 08:37:36,038][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023800_12189696.pth...
[36m[2025-07-02 08:37:36,049][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023672_12124160.pth
[36m[2025-07-02 08:37:40,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12189696. Throughput: 0: 283.9. Samples: 12200496. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 08:37:40,962][166323] Avg episode reward: [(0, '1302.769')]
[31m[43454306 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[43454307 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[43454307 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:37:45,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12189696. Throughput: 0: 284.0. Samples: 12202208. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 08:37:45,966][166323] Avg episode reward: [(0, '1208.129')]
[36m[2025-07-02 08:37:50,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12189696. Throughput: 0: 284.3. Samples: 12203056. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 08:37:50,985][166323] Avg episode reward: [(0, '1245.938')]
[36m[2025-07-02 08:37:55,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12189696. Throughput: 0: 284.1. Samples: 12204656. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 08:37:55,983][166323] Avg episode reward: [(0, '1294.111')]
[36m[2025-07-02 08:38:00,960][166323] Fps is (10 sec: 1642.6, 60 sec: 546.0, 300 sec: 333.2). Total num frames: 12206080. Throughput: 0: 285.1. Samples: 12206320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:38:00,960][166323] Avg episode reward: [(0, '1292.857')]
[36m[2025-07-02 08:38:06,017][166323] Fps is (10 sec: 1632.7, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 12206080. Throughput: 0: 282.3. Samples: 12207088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:38:06,018][166323] Avg episode reward: [(0, '1276.135')]
[36m[2025-07-02 08:38:10,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12206080. Throughput: 0: 280.9. Samples: 12208800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:38:10,966][166323] Avg episode reward: [(0, '1282.571')]
[36m[2025-07-02 08:38:15,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12206080. Throughput: 0: 280.5. Samples: 12210576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:38:15,961][166323] Avg episode reward: [(0, '1275.010')]
[36m[2025-07-02 08:38:20,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12206080. Throughput: 0: 279.0. Samples: 12211504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:38:20,963][166323] Avg episode reward: [(0, '1340.023')]
[36m[2025-07-02 08:38:25,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12206080. Throughput: 0: 284.1. Samples: 12213280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:38:25,956][166323] Avg episode reward: [(0, '1313.261')]
[36m[2025-07-02 08:38:30,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 12206080. Throughput: 0: 285.2. Samples: 12215040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:38:30,953][166323] Avg episode reward: [(0, '1316.987')]
[36m[2025-07-02 08:38:35,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12206080. Throughput: 0: 284.9. Samples: 12215872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:38:35,973][166323] Avg episode reward: [(0, '1343.299')]
[36m[2025-07-02 08:38:41,009][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12206080. Throughput: 0: 285.0. Samples: 12217488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:38:41,009][166323] Avg episode reward: [(0, '1335.630')]
[36m[2025-07-02 08:38:45,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12206080. Throughput: 0: 284.8. Samples: 12219136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:38:45,962][166323] Avg episode reward: [(0, '1359.366')]
[36m[2025-07-02 08:38:50,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12206080. Throughput: 0: 289.7. Samples: 12220112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:38:50,978][166323] Avg episode reward: [(0, '1398.201')]
[36m[2025-07-02 08:38:55,995][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12206080. Throughput: 0: 286.4. Samples: 12221696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:38:55,995][166323] Avg episode reward: [(0, '1377.230')]
[36m[2025-07-02 08:39:00,985][166323] Fps is (10 sec: 1637.3, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 12222464. Throughput: 0: 284.3. Samples: 12223376. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 08:39:00,985][166323] Avg episode reward: [(0, '1385.959')]
[36m[2025-07-02 08:39:05,943][166323] Fps is (10 sec: 1647.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 12222464. Throughput: 0: 283.9. Samples: 12224272. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 08:39:05,943][166323] Avg episode reward: [(0, '1386.816')]
[36m[2025-07-02 08:39:10,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12222464. Throughput: 0: 282.1. Samples: 12225984. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 08:39:10,997][166323] Avg episode reward: [(0, '1352.383')]
[36m[2025-07-02 08:39:16,019][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 12222464. Throughput: 0: 277.6. Samples: 12227552. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 08:39:16,019][166323] Avg episode reward: [(0, '1355.332')]
[36m[2025-07-02 08:39:20,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12222464. Throughput: 0: 278.1. Samples: 12228384. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 08:39:20,960][166323] Avg episode reward: [(0, '1280.246')]
[36m[2025-07-02 08:39:25,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12222464. Throughput: 0: 277.5. Samples: 12229968. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 08:39:25,975][166323] Avg episode reward: [(0, '1269.651')]
[36m[2025-07-02 08:39:30,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12222464. Throughput: 0: 276.4. Samples: 12231568. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 08:39:30,944][166323] Avg episode reward: [(0, '1263.955')]
[36m[2025-07-02 08:39:35,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12222464. Throughput: 0: 273.9. Samples: 12232432. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 08:39:35,957][166323] Avg episode reward: [(0, '1339.871')]
[37m[1m[2025-07-02 08:39:36,036][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023864_12222464.pth...
[36m[2025-07-02 08:39:36,040][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023736_12156928.pth
[36m[2025-07-02 08:39:40,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12222464. Throughput: 0: 275.9. Samples: 12234096. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 08:39:40,947][166323] Avg episode reward: [(0, '1292.058')]
[31m[43569610 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[43569610 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[43569610 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:39:45,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12222464. Throughput: 0: 276.7. Samples: 12235824. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 08:39:45,965][166323] Avg episode reward: [(0, '1277.997')]
[36m[2025-07-02 08:39:50,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12222464. Throughput: 0: 277.3. Samples: 12236752. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 08:39:50,951][166323] Avg episode reward: [(0, '1257.377')]
[36m[2025-07-02 08:39:55,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12222464. Throughput: 0: 276.6. Samples: 12238432. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 08:39:55,997][166323] Avg episode reward: [(0, '1275.099')]
[31m[43588449 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[43588450 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[43588450 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:40:00,974][166323] Fps is (10 sec: 1634.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12238848. Throughput: 0: 277.6. Samples: 12240032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:40:00,974][166323] Avg episode reward: [(0, '1236.090')]
[36m[2025-07-02 08:40:05,978][166323] Fps is (10 sec: 1641.4, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12238848. Throughput: 0: 277.2. Samples: 12240864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:40:05,978][166323] Avg episode reward: [(0, '1199.321')]
[31m[43594545 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[43594545 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[43594545 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:40:10,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12238848. Throughput: 0: 280.3. Samples: 12242576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:40:10,949][166323] Avg episode reward: [(0, '1184.581')]
[36m[2025-07-02 08:40:15,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12238848. Throughput: 0: 282.5. Samples: 12244288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:40:15,966][166323] Avg episode reward: [(0, '1183.560')]
[36m[2025-07-02 08:40:20,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12238848. Throughput: 0: 282.1. Samples: 12245136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:40:20,982][166323] Avg episode reward: [(0, '1188.068')]
[36m[2025-07-02 08:40:25,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12238848. Throughput: 0: 282.5. Samples: 12246816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:40:25,982][166323] Avg episode reward: [(0, '1227.603')]
[31m[43615558 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[43615558 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[43615558 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:40:30,973][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12238848. Throughput: 0: 280.1. Samples: 12248432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:40:30,973][166323] Avg episode reward: [(0, '1173.893')]
[31m[43621571 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[43621572 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[43621572 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:40:35,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12238848. Throughput: 0: 278.2. Samples: 12249280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:40:35,990][166323] Avg episode reward: [(0, '1204.522')]
[36m[2025-07-02 08:40:40,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12238848. Throughput: 0: 277.9. Samples: 12250928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:40:40,957][166323] Avg episode reward: [(0, '1185.480')]
[36m[2025-07-02 08:40:45,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12238848. Throughput: 0: 280.9. Samples: 12252672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:40:45,975][166323] Avg episode reward: [(0, '1213.062')]
[31m[43638534 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[43638535 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[43638535 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:40:50,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12238848. Throughput: 0: 282.9. Samples: 12253584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:40:50,945][166323] Avg episode reward: [(0, '1210.226')]
[36m[2025-07-02 08:40:55,975][166323] Fps is (10 sec: 1638.4, 60 sec: 546.3, 300 sec: 333.2). Total num frames: 12255232. Throughput: 0: 281.8. Samples: 12255264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:40:55,975][166323] Avg episode reward: [(0, '1192.926')]
[36m[2025-07-02 08:41:00,991][166323] Fps is (10 sec: 1630.9, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12255232. Throughput: 0: 281.1. Samples: 12256944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:41:00,991][166323] Avg episode reward: [(0, '1202.624')]
[36m[2025-07-02 08:41:05,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12255232. Throughput: 0: 278.1. Samples: 12257648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:41:05,969][166323] Avg episode reward: [(0, '1233.259')]
[36m[2025-07-02 08:41:10,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12255232. Throughput: 0: 279.1. Samples: 12259376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:41:10,983][166323] Avg episode reward: [(0, '1255.453')]
[36m[2025-07-02 08:41:15,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12255232. Throughput: 0: 281.2. Samples: 12261088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:41:15,977][166323] Avg episode reward: [(0, '1246.119')]
[36m[2025-07-02 08:41:20,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12255232. Throughput: 0: 283.4. Samples: 12262032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:41:20,980][166323] Avg episode reward: [(0, '1256.802')]
[36m[2025-07-02 08:41:25,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12255232. Throughput: 0: 284.5. Samples: 12263728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:41:25,945][166323] Avg episode reward: [(0, '1305.318')]
[36m[2025-07-02 08:41:30,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12255232. Throughput: 0: 282.6. Samples: 12265392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:41:30,981][166323] Avg episode reward: [(0, '1249.756')]
[36m[2025-07-02 08:41:35,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12255232. Throughput: 0: 280.6. Samples: 12266224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:41:35,999][166323] Avg episode reward: [(0, '1319.355')]
[37m[1m[2025-07-02 08:41:36,056][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023928_12255232.pth...
[36m[2025-07-02 08:41:36,060][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023800_12189696.pth
[36m[2025-07-02 08:41:40,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12255232. Throughput: 0: 282.1. Samples: 12267952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:41:40,952][166323] Avg episode reward: [(0, '1306.342')]
[36m[2025-07-02 08:41:45,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12255232. Throughput: 0: 283.7. Samples: 12269696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:41:45,946][166323] Avg episode reward: [(0, '1343.571')]
[36m[2025-07-02 08:41:50,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12255232. Throughput: 0: 286.7. Samples: 12270544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:41:50,955][166323] Avg episode reward: [(0, '1339.067')]
[36m[2025-07-02 08:41:55,985][166323] Fps is (10 sec: 1631.9, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 12271616. Throughput: 0: 283.0. Samples: 12272112. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 08:41:55,985][166323] Avg episode reward: [(0, '1341.219')]
[36m[2025-07-02 08:42:00,980][166323] Fps is (10 sec: 1634.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12271616. Throughput: 0: 282.6. Samples: 12273808. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 08:42:00,980][166323] Avg episode reward: [(0, '1353.983')]
[36m[2025-07-02 08:42:05,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12271616. Throughput: 0: 279.1. Samples: 12274592. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 08:42:05,984][166323] Avg episode reward: [(0, '1382.678')]
[36m[2025-07-02 08:42:11,013][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12271616. Throughput: 0: 280.1. Samples: 12276352. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 08:42:11,014][166323] Avg episode reward: [(0, '1375.463')]
[36m[2025-07-02 08:42:16,012][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 12271616. Throughput: 0: 282.5. Samples: 12278112. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 08:42:16,013][166323] Avg episode reward: [(0, '1373.552')]
[36m[2025-07-02 08:42:21,024][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 12271616. Throughput: 0: 282.9. Samples: 12278960. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 08:42:21,024][166323] Avg episode reward: [(0, '1346.209')]
[31m[43731137 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[43731138 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[43731139 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:42:25,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12271616. Throughput: 0: 283.4. Samples: 12280704. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 08:42:25,946][166323] Avg episode reward: [(0, '1270.552')]
[36m[2025-07-02 08:42:30,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12271616. Throughput: 0: 284.3. Samples: 12282496. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 08:42:30,965][166323] Avg episode reward: [(0, '1227.919')]
[36m[2025-07-02 08:42:35,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12271616. Throughput: 0: 283.7. Samples: 12283312. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 08:42:35,962][166323] Avg episode reward: [(0, '1154.536')]
[36m[2025-07-02 08:42:40,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12271616. Throughput: 0: 286.0. Samples: 12284976. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 08:42:40,963][166323] Avg episode reward: [(0, '1146.937')]
[36m[2025-07-02 08:42:45,983][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12271616. Throughput: 0: 285.8. Samples: 12286672. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 08:42:45,983][166323] Avg episode reward: [(0, '1122.768')]
[36m[2025-07-02 08:42:50,968][166323] Fps is (10 sec: 1637.7, 60 sec: 546.0, 300 sec: 333.3). Total num frames: 12288000. Throughput: 0: 287.7. Samples: 12287536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:42:50,968][166323] Avg episode reward: [(0, '1113.099')]
[36m[2025-07-02 08:42:55,983][166323] Fps is (10 sec: 1638.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12288000. Throughput: 0: 290.0. Samples: 12289392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:42:55,984][166323] Avg episode reward: [(0, '1168.448')]
[36m[2025-07-02 08:43:00,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.8). Total num frames: 12288000. Throughput: 0: 288.4. Samples: 12291072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:43:00,950][166323] Avg episode reward: [(0, '1203.511')]
[36m[2025-07-02 08:43:05,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12288000. Throughput: 0: 288.3. Samples: 12291920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:43:05,973][166323] Avg episode reward: [(0, '1202.375')]
[36m[2025-07-02 08:43:10,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12288000. Throughput: 0: 288.0. Samples: 12293680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:43:10,996][166323] Avg episode reward: [(0, '1284.177')]
[36m[2025-07-02 08:43:16,006][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12288000. Throughput: 0: 285.3. Samples: 12295344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:43:16,006][166323] Avg episode reward: [(0, '1254.176')]
[36m[2025-07-02 08:43:20,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12288000. Throughput: 0: 285.7. Samples: 12296176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:43:20,983][166323] Avg episode reward: [(0, '1241.584')]
[36m[2025-07-02 08:43:25,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12288000. Throughput: 0: 286.5. Samples: 12297872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:43:25,981][166323] Avg episode reward: [(0, '1227.916')]
[36m[2025-07-02 08:43:30,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12288000. Throughput: 0: 288.3. Samples: 12299632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:43:30,944][166323] Avg episode reward: [(0, '1197.815')]
[36m[2025-07-02 08:43:35,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12288000. Throughput: 0: 287.0. Samples: 12300448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:43:35,957][166323] Avg episode reward: [(0, '1211.230')]
[37m[1m[2025-07-02 08:43:36,023][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023992_12288000.pth...
[36m[2025-07-02 08:43:36,031][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023864_12222464.pth
[36m[2025-07-02 08:43:40,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12288000. Throughput: 0: 280.7. Samples: 12302016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:43:40,955][166323] Avg episode reward: [(0, '1194.262')]
[36m[2025-07-02 08:43:45,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12288000. Throughput: 0: 282.9. Samples: 12303808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:43:45,976][166323] Avg episode reward: [(0, '1238.099')]
[36m[2025-07-02 08:43:50,962][166323] Fps is (10 sec: 1637.2, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 12304384. Throughput: 0: 281.3. Samples: 12304576. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:43:50,962][166323] Avg episode reward: [(0, '1301.893')]
[36m[2025-07-02 08:43:55,949][166323] Fps is (10 sec: 1642.8, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12304384. Throughput: 0: 282.3. Samples: 12306368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:43:55,949][166323] Avg episode reward: [(0, '1337.698')]
[36m[2025-07-02 08:44:00,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12304384. Throughput: 0: 281.9. Samples: 12308016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:44:00,962][166323] Avg episode reward: [(0, '1375.610')]
[36m[2025-07-02 08:44:05,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12304384. Throughput: 0: 281.3. Samples: 12308832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:44:05,979][166323] Avg episode reward: [(0, '1389.063')]
[36m[2025-07-02 08:44:10,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.8). Total num frames: 12304384. Throughput: 0: 279.7. Samples: 12310448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:44:10,946][166323] Avg episode reward: [(0, '1343.957')]
[36m[2025-07-02 08:44:15,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12304384. Throughput: 0: 278.4. Samples: 12312160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:44:15,949][166323] Avg episode reward: [(0, '1367.815')]
[36m[2025-07-02 08:44:20,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12304384. Throughput: 0: 278.1. Samples: 12312960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:44:20,946][166323] Avg episode reward: [(0, '1360.189')]
[36m[2025-07-02 08:44:25,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12304384. Throughput: 0: 281.2. Samples: 12314672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:44:25,956][166323] Avg episode reward: [(0, '1339.603')]
[36m[2025-07-02 08:44:30,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12304384. Throughput: 0: 279.2. Samples: 12316368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:44:30,960][166323] Avg episode reward: [(0, '1282.389')]
[36m[2025-07-02 08:44:35,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12304384. Throughput: 0: 281.8. Samples: 12317264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:44:35,990][166323] Avg episode reward: [(0, '1189.666')]
[36m[2025-07-02 08:44:40,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12304384. Throughput: 0: 277.2. Samples: 12318848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:44:40,976][166323] Avg episode reward: [(0, '1193.338')]
[36m[2025-07-02 08:44:45,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12304384. Throughput: 0: 276.6. Samples: 12320464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:44:45,961][166323] Avg episode reward: [(0, '1201.586')]
[33m[2025-07-02 08:44:46,874][166323] KL-divergence is very high: 124.1722
[36m[2025-07-02 08:44:50,965][166323] Fps is (10 sec: 1640.2, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 12320768. Throughput: 0: 276.4. Samples: 12321264. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 08:44:50,965][166323] Avg episode reward: [(0, '1188.482')]
[36m[2025-07-02 08:44:55,989][166323] Fps is (10 sec: 1633.7, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12320768. Throughput: 0: 278.1. Samples: 12322976. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 08:44:55,989][166323] Avg episode reward: [(0, '1187.651')]
[36m[2025-07-02 08:45:00,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12320768. Throughput: 0: 276.9. Samples: 12324624. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 08:45:00,965][166323] Avg episode reward: [(0, '1179.892')]
[31m[43891252 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[43891252 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[43891252 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:45:06,008][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 12320768. Throughput: 0: 278.0. Samples: 12325488. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 08:45:06,008][166323] Avg episode reward: [(0, '1225.384')]
[36m[2025-07-02 08:45:10,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12320768. Throughput: 0: 276.3. Samples: 12327104. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 08:45:10,944][166323] Avg episode reward: [(0, '1245.730')]
[36m[2025-07-02 08:45:15,977][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12320768. Throughput: 0: 275.4. Samples: 12328768. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 08:45:15,977][166323] Avg episode reward: [(0, '1247.339')]
[36m[2025-07-02 08:45:20,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12320768. Throughput: 0: 275.7. Samples: 12329664. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 08:45:20,959][166323] Avg episode reward: [(0, '1240.549')]
[36m[2025-07-02 08:45:25,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12320768. Throughput: 0: 279.7. Samples: 12331424. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 08:45:25,945][166323] Avg episode reward: [(0, '1264.659')]
[36m[2025-07-02 08:45:30,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12320768. Throughput: 0: 280.8. Samples: 12333104. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 08:45:30,969][166323] Avg episode reward: [(0, '1243.003')]
[36m[2025-07-02 08:45:35,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12320768. Throughput: 0: 280.3. Samples: 12333872. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 08:45:35,952][166323] Avg episode reward: [(0, '1264.152')]
[37m[1m[2025-07-02 08:45:36,039][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000024056_12320768.pth...
[36m[2025-07-02 08:45:36,043][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023928_12255232.pth
[36m[2025-07-02 08:45:40,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12320768. Throughput: 0: 280.3. Samples: 12335584. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 08:45:40,972][166323] Avg episode reward: [(0, '1216.530')]
[36m[2025-07-02 08:45:45,947][166323] Fps is (10 sec: 1639.1, 60 sec: 546.3, 300 sec: 333.2). Total num frames: 12337152. Throughput: 0: 281.0. Samples: 12337264. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 08:45:45,948][166323] Avg episode reward: [(0, '1307.785')]
[36m[2025-07-02 08:45:50,969][166323] Fps is (10 sec: 1638.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12337152. Throughput: 0: 280.8. Samples: 12338112. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 08:45:50,969][166323] Avg episode reward: [(0, '1346.765')]
[36m[2025-07-02 08:45:55,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12337152. Throughput: 0: 281.8. Samples: 12339792. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 08:45:55,966][166323] Avg episode reward: [(0, '1313.650')]
[36m[2025-07-02 08:46:01,020][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 12337152. Throughput: 0: 284.5. Samples: 12341584. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 08:46:01,020][166323] Avg episode reward: [(0, '1314.434')]
[36m[2025-07-02 08:46:05,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12337152. Throughput: 0: 283.2. Samples: 12342416. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 08:46:05,984][166323] Avg episode reward: [(0, '1342.436')]
[36m[2025-07-02 08:46:10,979][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12337152. Throughput: 0: 282.1. Samples: 12344128. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 08:46:10,979][166323] Avg episode reward: [(0, '1319.763')]
[36m[2025-07-02 08:46:15,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12337152. Throughput: 0: 284.2. Samples: 12345888. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 08:46:15,945][166323] Avg episode reward: [(0, '1318.057')]
[36m[2025-07-02 08:46:20,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12337152. Throughput: 0: 286.9. Samples: 12346784. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 08:46:20,964][166323] Avg episode reward: [(0, '1311.701')]
[36m[2025-07-02 08:46:25,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12337152. Throughput: 0: 284.9. Samples: 12348400. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 08:46:25,958][166323] Avg episode reward: [(0, '1277.312')]
[36m[2025-07-02 08:46:30,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12337152. Throughput: 0: 286.5. Samples: 12350160. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 08:46:30,953][166323] Avg episode reward: [(0, '1262.580')]
[36m[2025-07-02 08:46:35,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12337152. Throughput: 0: 286.4. Samples: 12351008. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 08:46:35,999][166323] Avg episode reward: [(0, '1259.101')]
[36m[2025-07-02 08:46:40,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 12337152. Throughput: 0: 283.2. Samples: 12352544. Policy #0 lag: (min: 19.0, avg: 19.0, max: 19.0)
[36m[2025-07-02 08:46:40,997][166323] Avg episode reward: [(0, '1236.104')]
[36m[2025-07-02 08:46:45,959][166323] Fps is (10 sec: 1644.9, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 12353536. Throughput: 0: 279.8. Samples: 12354160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:46:45,960][166323] Avg episode reward: [(0, '1253.933')]
[36m[2025-07-02 08:46:50,949][166323] Fps is (10 sec: 1646.2, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12353536. Throughput: 0: 279.7. Samples: 12354992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:46:50,950][166323] Avg episode reward: [(0, '1268.004')]
[36m[2025-07-02 08:46:55,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12353536. Throughput: 0: 280.7. Samples: 12356752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:46:55,953][166323] Avg episode reward: [(0, '1283.868')]
[36m[2025-07-02 08:47:00,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12353536. Throughput: 0: 275.8. Samples: 12358304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:47:00,965][166323] Avg episode reward: [(0, '1263.076')]
[36m[2025-07-02 08:47:05,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12353536. Throughput: 0: 276.9. Samples: 12359248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:47:05,973][166323] Avg episode reward: [(0, '1161.362')]
[36m[2025-07-02 08:47:11,000][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12353536. Throughput: 0: 281.7. Samples: 12361088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:47:11,001][166323] Avg episode reward: [(0, '1184.235')]
[36m[2025-07-02 08:47:15,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.8). Total num frames: 12353536. Throughput: 0: 280.1. Samples: 12362768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:47:15,961][166323] Avg episode reward: [(0, '1175.527')]
[36m[2025-07-02 08:47:21,005][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 12353536. Throughput: 0: 280.1. Samples: 12363616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:47:21,005][166323] Avg episode reward: [(0, '1191.834')]
[36m[2025-07-02 08:47:25,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12353536. Throughput: 0: 281.1. Samples: 12365184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:47:25,968][166323] Avg episode reward: [(0, '1136.582')]
[36m[2025-07-02 08:47:30,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12353536. Throughput: 0: 280.6. Samples: 12366800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:47:31,000][166323] Avg episode reward: [(0, '1165.328')]
[36m[2025-07-02 08:47:35,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12353536. Throughput: 0: 278.5. Samples: 12367536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:47:35,991][166323] Avg episode reward: [(0, '1188.789')]
[37m[1m[2025-07-02 08:47:36,043][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000024120_12353536.pth...
[36m[2025-07-02 08:47:36,047][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000023992_12288000.pth
[36m[2025-07-02 08:47:40,997][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12353536. Throughput: 0: 274.2. Samples: 12369104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:47:40,998][166323] Avg episode reward: [(0, '1223.640')]
[36m[2025-07-02 08:47:45,964][166323] Fps is (10 sec: 1642.8, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12369920. Throughput: 0: 273.8. Samples: 12370624. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 08:47:45,964][166323] Avg episode reward: [(0, '1229.525')]
[36m[2025-07-02 08:47:50,989][166323] Fps is (10 sec: 1639.7, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12369920. Throughput: 0: 271.2. Samples: 12371456. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 08:47:50,990][166323] Avg episode reward: [(0, '1271.092')]
[33m[44060829 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[44060829 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.86767578125
[33mCrash Rate: 0.119140625
[33mTimeout Rate: 0.01318359375 (navigation_task.py:265)
[33m[44060829 ms][navigation_task] - WARNING : 
[33mSuccesses: 1777
[33mCrashes : 244
[33mTimeouts: 27 (navigation_task.py:268)
[36m[2025-07-02 08:47:55,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12369920. Throughput: 0: 269.8. Samples: 12373216. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 08:47:55,950][166323] Avg episode reward: [(0, '1226.848')]
[36m[2025-07-02 08:48:00,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12369920. Throughput: 0: 270.5. Samples: 12374944. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 08:48:00,976][166323] Avg episode reward: [(0, '1260.582')]
[36m[2025-07-02 08:48:05,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12369920. Throughput: 0: 271.8. Samples: 12375840. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 08:48:05,979][166323] Avg episode reward: [(0, '1240.938')]
[36m[2025-07-02 08:48:10,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12369920. Throughput: 0: 275.9. Samples: 12377600. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 08:48:10,964][166323] Avg episode reward: [(0, '1260.733')]
[36m[2025-07-02 08:48:15,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12369920. Throughput: 0: 280.8. Samples: 12379424. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 08:48:15,956][166323] Avg episode reward: [(0, '1302.907')]
[36m[2025-07-02 08:48:20,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12369920. Throughput: 0: 283.6. Samples: 12380288. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 08:48:20,960][166323] Avg episode reward: [(0, '1291.281')]
[36m[2025-07-02 08:48:25,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12369920. Throughput: 0: 289.7. Samples: 12382128. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 08:48:25,950][166323] Avg episode reward: [(0, '1268.479')]
[36m[2025-07-02 08:48:30,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12369920. Throughput: 0: 294.6. Samples: 12383888. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 08:48:30,981][166323] Avg episode reward: [(0, '1268.162')]
[36m[2025-07-02 08:48:35,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12369920. Throughput: 0: 294.4. Samples: 12384704. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 08:48:35,984][166323] Avg episode reward: [(0, '1268.596')]
[36m[2025-07-02 08:48:40,988][166323] Fps is (10 sec: 1637.3, 60 sec: 546.2, 300 sec: 333.2). Total num frames: 12386304. Throughput: 0: 296.6. Samples: 12386576. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 08:48:40,988][166323] Avg episode reward: [(0, '1246.568')]
[36m[2025-07-02 08:48:45,946][166323] Fps is (10 sec: 1644.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12386304. Throughput: 0: 300.6. Samples: 12388464. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 08:48:45,946][166323] Avg episode reward: [(0, '1235.203')]
[36m[2025-07-02 08:48:50,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12386304. Throughput: 0: 300.3. Samples: 12389344. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 08:48:50,951][166323] Avg episode reward: [(0, '1246.811')]
[36m[2025-07-02 08:48:56,001][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 12386304. Throughput: 0: 296.6. Samples: 12390960. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 08:48:56,001][166323] Avg episode reward: [(0, '1231.586')]
[31m[44127813 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[44127814 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[44127814 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:49:00,995][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12386304. Throughput: 0: 295.2. Samples: 12392720. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 08:49:00,995][166323] Avg episode reward: [(0, '1244.650')]
[36m[2025-07-02 08:49:06,012][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 12386304. Throughput: 0: 294.8. Samples: 12393568. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 08:49:06,013][166323] Avg episode reward: [(0, '1254.908')]
[31m[44135341 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[44135341 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[44135341 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:49:10,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12386304. Throughput: 0: 293.8. Samples: 12395360. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 08:49:10,990][166323] Avg episode reward: [(0, '1218.992')]
[36m[2025-07-02 08:49:15,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12386304. Throughput: 0: 291.5. Samples: 12397008. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 08:49:15,986][166323] Avg episode reward: [(0, '1246.120')]
[36m[2025-07-02 08:49:20,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12386304. Throughput: 0: 293.2. Samples: 12397888. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 08:49:20,948][166323] Avg episode reward: [(0, '1210.000')]
[36m[2025-07-02 08:49:26,018][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 12386304. Throughput: 0: 288.2. Samples: 12399552. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 08:49:26,019][166323] Avg episode reward: [(0, '1167.118')]
[36m[2025-07-02 08:49:30,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12386304. Throughput: 0: 283.0. Samples: 12401200. Policy #0 lag: (min: 24.0, avg: 24.2, max: 56.0)
[36m[2025-07-02 08:49:30,947][166323] Avg episode reward: [(0, '1131.214')]
[31m[44161037 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[44161038 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[44161038 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:49:35,991][166323] Fps is (10 sec: 1642.9, 60 sec: 546.1, 300 sec: 333.2). Total num frames: 12402688. Throughput: 0: 283.5. Samples: 12402112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:49:35,991][166323] Avg episode reward: [(0, '1122.352')]
[37m[1m[2025-07-02 08:49:36,045][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000024216_12402688.pth...
[36m[2025-07-02 08:49:36,049][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000024056_12320768.pth
[36m[2025-07-02 08:49:40,962][166323] Fps is (10 sec: 1635.9, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 12402688. Throughput: 0: 282.9. Samples: 12403680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:49:40,962][166323] Avg episode reward: [(0, '1189.075')]
[36m[2025-07-02 08:49:45,974][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12402688. Throughput: 0: 282.4. Samples: 12405424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:49:45,974][166323] Avg episode reward: [(0, '1203.687')]
[36m[2025-07-02 08:49:50,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12402688. Throughput: 0: 284.8. Samples: 12406368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:49:50,958][166323] Avg episode reward: [(0, '1140.150')]
[36m[2025-07-02 08:49:55,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12402688. Throughput: 0: 285.4. Samples: 12408192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:49:55,957][166323] Avg episode reward: [(0, '1189.325')]
[36m[2025-07-02 08:50:00,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12402688. Throughput: 0: 285.9. Samples: 12409872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:50:00,982][166323] Avg episode reward: [(0, '1246.930')]
[36m[2025-07-02 08:50:05,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12402688. Throughput: 0: 283.6. Samples: 12410656. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:50:05,973][166323] Avg episode reward: [(0, '1286.393')]
[36m[2025-07-02 08:50:10,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12402688. Throughput: 0: 284.1. Samples: 12412320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:50:10,956][166323] Avg episode reward: [(0, '1186.437')]
[36m[2025-07-02 08:50:15,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12402688. Throughput: 0: 283.2. Samples: 12413952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:50:15,976][166323] Avg episode reward: [(0, '1248.944')]
[36m[2025-07-02 08:50:20,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12402688. Throughput: 0: 280.0. Samples: 12414704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:50:20,957][166323] Avg episode reward: [(0, '1193.817')]
[36m[2025-07-02 08:50:25,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 12402688. Throughput: 0: 282.0. Samples: 12416368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:50:25,955][166323] Avg episode reward: [(0, '1237.164')]
[36m[2025-07-02 08:50:30,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12402688. Throughput: 0: 280.7. Samples: 12418048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:50:30,954][166323] Avg episode reward: [(0, '1221.508')]
[31m[44222593 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[44222593 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[44222593 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:50:35,954][166323] Fps is (10 sec: 1638.6, 60 sec: 273.2, 300 sec: 333.3). Total num frames: 12419072. Throughput: 0: 278.8. Samples: 12418912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:50:35,954][166323] Avg episode reward: [(0, '1137.163')]
[36m[2025-07-02 08:50:40,979][166323] Fps is (10 sec: 1634.4, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12419072. Throughput: 0: 274.7. Samples: 12420560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:50:40,979][166323] Avg episode reward: [(0, '1112.759')]
[31m[44233490 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[44233490 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[44233490 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:50:45,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12419072. Throughput: 0: 274.8. Samples: 12422240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:50:45,989][166323] Avg episode reward: [(0, '1104.661')]
[36m[2025-07-02 08:50:50,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12419072. Throughput: 0: 273.8. Samples: 12422976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:50:50,975][166323] Avg episode reward: [(0, '1100.434')]
[31m[44241685 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[44241686 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[44241686 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:50:55,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12419072. Throughput: 0: 270.4. Samples: 12424496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:50:55,988][166323] Avg episode reward: [(0, '1188.839')]
[36m[2025-07-02 08:51:01,022][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12419072. Throughput: 0: 270.3. Samples: 12426128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:51:01,023][166323] Avg episode reward: [(0, '1154.643')]
[36m[2025-07-02 08:51:05,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12419072. Throughput: 0: 271.9. Samples: 12426944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:51:05,968][166323] Avg episode reward: [(0, '1181.396')]
[36m[2025-07-02 08:51:10,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12419072. Throughput: 0: 268.3. Samples: 12428448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:51:10,985][166323] Avg episode reward: [(0, '1208.048')]
[36m[2025-07-02 08:51:15,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12419072. Throughput: 0: 266.9. Samples: 12430064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:51:15,969][166323] Avg episode reward: [(0, '1301.283')]
[36m[2025-07-02 08:51:20,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12419072. Throughput: 0: 266.3. Samples: 12430896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:51:20,955][166323] Avg episode reward: [(0, '1386.797')]
[36m[2025-07-02 08:51:25,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12419072. Throughput: 0: 264.6. Samples: 12432464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:51:25,964][166323] Avg episode reward: [(0, '1369.807')]
[36m[2025-07-02 08:51:30,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12419072. Throughput: 0: 265.8. Samples: 12434192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:51:30,947][166323] Avg episode reward: [(0, '1375.495')]
[36m[2025-07-02 08:51:35,946][166323] Fps is (10 sec: 1641.3, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 12435456. Throughput: 0: 268.3. Samples: 12435040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:51:35,947][166323] Avg episode reward: [(0, '1384.435')]
[37m[1m[2025-07-02 08:51:35,997][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000024280_12435456.pth...
[36m[2025-07-02 08:51:36,001][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000024120_12353536.pth
[36m[2025-07-02 08:51:40,948][166323] Fps is (10 sec: 1638.3, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12435456. Throughput: 0: 270.8. Samples: 12436672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:51:40,948][166323] Avg episode reward: [(0, '1376.652')]
[31m[44293173 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[44293174 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[44293174 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:51:45,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12435456. Throughput: 0: 273.1. Samples: 12438400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:51:45,965][166323] Avg episode reward: [(0, '1339.657')]
[31m[44296696 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[44296697 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[44296697 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:51:50,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12435456. Throughput: 0: 274.6. Samples: 12439296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:51:50,957][166323] Avg episode reward: [(0, '1323.186')]
[31m[44303150 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[44303150 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[44303150 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:51:55,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12435456. Throughput: 0: 279.8. Samples: 12441040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:51:55,989][166323] Avg episode reward: [(0, '1271.521')]
[36m[2025-07-02 08:52:00,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12435456. Throughput: 0: 278.3. Samples: 12442592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:52:00,992][166323] Avg episode reward: [(0, '1283.316')]
[36m[2025-07-02 08:52:05,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12435456. Throughput: 0: 277.9. Samples: 12443408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:52:05,986][166323] Avg episode reward: [(0, '1306.542')]
[31m[44318001 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[44318001 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[44318002 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:52:10,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12435456. Throughput: 0: 282.1. Samples: 12445168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:52:10,991][166323] Avg episode reward: [(0, '1286.062')]
[36m[2025-07-02 08:52:15,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12435456. Throughput: 0: 278.2. Samples: 12446720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:52:15,985][166323] Avg episode reward: [(0, '1321.754')]
[36m[2025-07-02 08:52:21,016][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 12435456. Throughput: 0: 279.4. Samples: 12447632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:52:21,016][166323] Avg episode reward: [(0, '1370.668')]
[36m[2025-07-02 08:52:25,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12435456. Throughput: 0: 279.4. Samples: 12449248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:52:25,953][166323] Avg episode reward: [(0, '1350.450')]
[36m[2025-07-02 08:52:30,973][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12435456. Throughput: 0: 281.2. Samples: 12451056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:52:30,973][166323] Avg episode reward: [(0, '1332.336')]
[33m[2025-07-02 08:52:33,827][166323] KL-divergence is very high: 118.2662
[36m[2025-07-02 08:52:35,948][166323] Fps is (10 sec: 1639.1, 60 sec: 273.1, 300 sec: 333.3). Total num frames: 12451840. Throughput: 0: 278.5. Samples: 12451824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:52:35,949][166323] Avg episode reward: [(0, '1308.966')]
[36m[2025-07-02 08:52:40,977][166323] Fps is (10 sec: 1637.6, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12451840. Throughput: 0: 277.4. Samples: 12453520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:52:40,978][166323] Avg episode reward: [(0, '1294.965')]
[36m[2025-07-02 08:52:45,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12451840. Throughput: 0: 284.3. Samples: 12455376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:52:45,959][166323] Avg episode reward: [(0, '1278.625')]
[36m[2025-07-02 08:52:50,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12451840. Throughput: 0: 286.6. Samples: 12456304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:52:50,979][166323] Avg episode reward: [(0, '1303.118')]
[36m[2025-07-02 08:52:55,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12451840. Throughput: 0: 286.4. Samples: 12458048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:52:55,963][166323] Avg episode reward: [(0, '1280.056')]
[36m[2025-07-02 08:53:00,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12451840. Throughput: 0: 287.5. Samples: 12459648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:53:00,947][166323] Avg episode reward: [(0, '1276.675')]
[36m[2025-07-02 08:53:05,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12451840. Throughput: 0: 286.2. Samples: 12460496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:53:05,966][166323] Avg episode reward: [(0, '1370.677')]
[36m[2025-07-02 08:53:10,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12451840. Throughput: 0: 287.1. Samples: 12462176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:53:10,978][166323] Avg episode reward: [(0, '1368.471')]
[36m[2025-07-02 08:53:15,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12451840. Throughput: 0: 286.2. Samples: 12463936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:53:15,979][166323] Avg episode reward: [(0, '1403.972')]
[36m[2025-07-02 08:53:20,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12451840. Throughput: 0: 287.8. Samples: 12464784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:53:20,977][166323] Avg episode reward: [(0, '1351.555')]
[36m[2025-07-02 08:53:25,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12451840. Throughput: 0: 285.6. Samples: 12466368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:53:25,960][166323] Avg episode reward: [(0, '1331.410')]
[31m[44396982 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[44396983 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[44396983 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:53:30,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12451840. Throughput: 0: 279.4. Samples: 12467952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:53:30,976][166323] Avg episode reward: [(0, '1299.590')]
[36m[2025-07-02 08:53:35,966][166323] Fps is (10 sec: 1637.4, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12468224. Throughput: 0: 278.1. Samples: 12468816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:53:35,966][166323] Avg episode reward: [(0, '1310.603')]
[37m[1m[2025-07-02 08:53:36,020][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000024344_12468224.pth...
[36m[2025-07-02 08:53:36,025][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000024216_12402688.pth
[36m[2025-07-02 08:53:41,005][166323] Fps is (10 sec: 1633.7, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 12468224. Throughput: 0: 275.7. Samples: 12470464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:53:41,005][166323] Avg episode reward: [(0, '1303.819')]
[36m[2025-07-02 08:53:45,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12468224. Throughput: 0: 278.2. Samples: 12472176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:53:45,984][166323] Avg episode reward: [(0, '1287.495')]
[36m[2025-07-02 08:53:50,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12468224. Throughput: 0: 277.0. Samples: 12472960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:53:50,965][166323] Avg episode reward: [(0, '1219.681')]
[36m[2025-07-02 08:53:55,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12468224. Throughput: 0: 274.1. Samples: 12474512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:53:55,986][166323] Avg episode reward: [(0, '1280.327')]
[36m[2025-07-02 08:54:00,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12468224. Throughput: 0: 274.4. Samples: 12476288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:54:00,986][166323] Avg episode reward: [(0, '1227.290')]
[36m[2025-07-02 08:54:05,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12468224. Throughput: 0: 272.7. Samples: 12477056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:54:05,984][166323] Avg episode reward: [(0, '1267.411')]
[36m[2025-07-02 08:54:10,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12468224. Throughput: 0: 277.7. Samples: 12478864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:54:10,965][166323] Avg episode reward: [(0, '1203.805')]
[36m[2025-07-02 08:54:15,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 12468224. Throughput: 0: 278.6. Samples: 12480496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:54:15,996][166323] Avg episode reward: [(0, '1179.131')]
[36m[2025-07-02 08:54:20,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12468224. Throughput: 0: 280.4. Samples: 12481440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:54:20,987][166323] Avg episode reward: [(0, '1269.647')]
[36m[2025-07-02 08:54:25,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12468224. Throughput: 0: 284.4. Samples: 12483248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:54:25,950][166323] Avg episode reward: [(0, '1208.269')]
[36m[2025-07-02 08:54:30,956][166323] Fps is (10 sec: 1643.4, 60 sec: 546.3, 300 sec: 277.7). Total num frames: 12484608. Throughput: 0: 279.3. Samples: 12484736. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 08:54:30,956][166323] Avg episode reward: [(0, '1244.680')]
[36m[2025-07-02 08:54:36,019][166323] Fps is (10 sec: 1627.2, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 12484608. Throughput: 0: 280.6. Samples: 12485600. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 08:54:36,019][166323] Avg episode reward: [(0, '1207.131')]
[31m[44464749 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[44464749 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[44464749 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:54:40,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12484608. Throughput: 0: 284.9. Samples: 12487328. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 08:54:40,968][166323] Avg episode reward: [(0, '1202.978')]
[36m[2025-07-02 08:54:45,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12484608. Throughput: 0: 282.2. Samples: 12488976. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 08:54:45,951][166323] Avg episode reward: [(0, '1271.436')]
[36m[2025-07-02 08:54:51,001][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12484608. Throughput: 0: 284.7. Samples: 12489872. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 08:54:51,001][166323] Avg episode reward: [(0, '1237.711')]
[36m[2025-07-02 08:54:55,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12484608. Throughput: 0: 282.4. Samples: 12491568. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 08:54:55,955][166323] Avg episode reward: [(0, '1202.133')]
[36m[2025-07-02 08:55:00,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12484608. Throughput: 0: 284.1. Samples: 12493280. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 08:55:00,997][166323] Avg episode reward: [(0, '1209.214')]
[36m[2025-07-02 08:55:06,018][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 12484608. Throughput: 0: 283.9. Samples: 12494224. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 08:55:06,019][166323] Avg episode reward: [(0, '1244.551')]
[36m[2025-07-02 08:55:10,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12484608. Throughput: 0: 280.0. Samples: 12495856. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 08:55:10,986][166323] Avg episode reward: [(0, '1285.636')]
[36m[2025-07-02 08:55:15,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12484608. Throughput: 0: 282.9. Samples: 12497472. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 08:55:15,978][166323] Avg episode reward: [(0, '1285.261')]
[36m[2025-07-02 08:55:20,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12484608. Throughput: 0: 282.0. Samples: 12498272. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 08:55:20,958][166323] Avg episode reward: [(0, '1282.011')]
[36m[2025-07-02 08:55:25,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12484608. Throughput: 0: 276.7. Samples: 12499776. Policy #0 lag: (min: 5.0, avg: 5.0, max: 5.0)
[36m[2025-07-02 08:55:25,958][166323] Avg episode reward: [(0, '1255.801')]
[36m[2025-07-02 08:55:30,979][166323] Fps is (10 sec: 1634.9, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12500992. Throughput: 0: 276.1. Samples: 12501408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:55:30,979][166323] Avg episode reward: [(0, '1318.981')]
[36m[2025-07-02 08:55:35,968][166323] Fps is (10 sec: 1636.7, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12500992. Throughput: 0: 276.1. Samples: 12502288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:55:35,968][166323] Avg episode reward: [(0, '1288.339')]
[37m[1m[2025-07-02 08:55:36,019][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000024408_12500992.pth...
[36m[2025-07-02 08:55:36,023][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000024280_12435456.pth
[36m[2025-07-02 08:55:40,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12500992. Throughput: 0: 274.7. Samples: 12503936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:55:40,978][166323] Avg episode reward: [(0, '1273.596')]
[36m[2025-07-02 08:55:45,977][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12500992. Throughput: 0: 276.0. Samples: 12505696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:55:45,977][166323] Avg episode reward: [(0, '1260.099')]
[36m[2025-07-02 08:55:50,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12500992. Throughput: 0: 274.4. Samples: 12506560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:55:50,976][166323] Avg episode reward: [(0, '1186.634')]
[36m[2025-07-02 08:55:55,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 12500992. Throughput: 0: 277.6. Samples: 12508336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:55:55,950][166323] Avg episode reward: [(0, '1156.618')]
[36m[2025-07-02 08:56:00,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12500992. Throughput: 0: 277.5. Samples: 12509952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:56:00,946][166323] Avg episode reward: [(0, '1131.178')]
[36m[2025-07-02 08:56:05,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12500992. Throughput: 0: 279.4. Samples: 12510848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:56:05,967][166323] Avg episode reward: [(0, '1164.504')]
[36m[2025-07-02 08:56:10,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12500992. Throughput: 0: 282.1. Samples: 12512480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:56:10,999][166323] Avg episode reward: [(0, '1234.727')]
[36m[2025-07-02 08:56:15,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12500992. Throughput: 0: 283.3. Samples: 12514160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:56:15,994][166323] Avg episode reward: [(0, '1219.490')]
[36m[2025-07-02 08:56:20,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12500992. Throughput: 0: 279.3. Samples: 12514864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:56:20,989][166323] Avg episode reward: [(0, '1248.373')]
[36m[2025-07-02 08:56:25,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12500992. Throughput: 0: 278.8. Samples: 12516480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 08:56:25,967][166323] Avg episode reward: [(0, '1237.485')]
[36m[2025-07-02 08:56:30,989][166323] Fps is (10 sec: 1638.4, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12517376. Throughput: 0: 276.9. Samples: 12518160. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 08:56:30,989][166323] Avg episode reward: [(0, '1273.666')]
[36m[2025-07-02 08:56:35,982][166323] Fps is (10 sec: 1636.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12517376. Throughput: 0: 276.2. Samples: 12518992. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 08:56:35,982][166323] Avg episode reward: [(0, '1312.991')]
[36m[2025-07-02 08:56:40,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12517376. Throughput: 0: 274.7. Samples: 12520704. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 08:56:40,974][166323] Avg episode reward: [(0, '1248.688')]
[36m[2025-07-02 08:56:45,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12517376. Throughput: 0: 278.6. Samples: 12522496. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 08:56:45,976][166323] Avg episode reward: [(0, '1272.140')]
[31m[44596818 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[44596819 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[44596819 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:56:50,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12517376. Throughput: 0: 277.3. Samples: 12523328. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 08:56:50,977][166323] Avg episode reward: [(0, '1252.550')]
[36m[2025-07-02 08:56:55,980][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12517376. Throughput: 0: 279.6. Samples: 12525056. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 08:56:55,981][166323] Avg episode reward: [(0, '1306.355')]
[36m[2025-07-02 08:57:00,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12517376. Throughput: 0: 279.0. Samples: 12526704. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 08:57:00,959][166323] Avg episode reward: [(0, '1287.512')]
[36m[2025-07-02 08:57:06,001][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12517376. Throughput: 0: 280.8. Samples: 12527504. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 08:57:06,002][166323] Avg episode reward: [(0, '1321.906')]
[36m[2025-07-02 08:57:10,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12517376. Throughput: 0: 281.8. Samples: 12529168. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 08:57:10,986][166323] Avg episode reward: [(0, '1258.221')]
[31m[44620178 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[44620178 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[44620178 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:57:15,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12517376. Throughput: 0: 281.0. Samples: 12530800. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 08:57:15,966][166323] Avg episode reward: [(0, '1291.333')]
[36m[2025-07-02 08:57:20,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12517376. Throughput: 0: 282.2. Samples: 12531680. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 08:57:20,950][166323] Avg episode reward: [(0, '1291.754')]
[36m[2025-07-02 08:57:25,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12517376. Throughput: 0: 279.6. Samples: 12533280. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 08:57:25,958][166323] Avg episode reward: [(0, '1347.520')]
[36m[2025-07-02 08:57:31,003][166323] Fps is (10 sec: 1629.8, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 12533760. Throughput: 0: 273.6. Samples: 12534816. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 08:57:31,003][166323] Avg episode reward: [(0, '1324.099')]
[36m[2025-07-02 08:57:36,019][166323] Fps is (10 sec: 1628.4, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12533760. Throughput: 0: 273.5. Samples: 12535648. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 08:57:36,019][166323] Avg episode reward: [(0, '1351.076')]
[37m[1m[2025-07-02 08:57:36,093][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000024472_12533760.pth...
[36m[2025-07-02 08:57:36,097][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000024344_12468224.pth
[36m[2025-07-02 08:57:40,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12533760. Throughput: 0: 270.7. Samples: 12537232. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 08:57:40,968][166323] Avg episode reward: [(0, '1359.858')]
[36m[2025-07-02 08:57:46,019][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12533760. Throughput: 0: 269.2. Samples: 12538832. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 08:57:46,019][166323] Avg episode reward: [(0, '1380.107')]
[36m[2025-07-02 08:57:50,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12533760. Throughput: 0: 269.8. Samples: 12539632. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 08:57:50,959][166323] Avg episode reward: [(0, '1356.898')]
[36m[2025-07-02 08:57:55,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12533760. Throughput: 0: 273.1. Samples: 12541456. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 08:57:55,985][166323] Avg episode reward: [(0, '1306.613')]
[31m[44669363 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[44669364 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[44669364 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:58:00,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12533760. Throughput: 0: 274.9. Samples: 12543168. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 08:58:00,964][166323] Avg episode reward: [(0, '1262.768')]
[36m[2025-07-02 08:58:05,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12533760. Throughput: 0: 276.5. Samples: 12544128. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 08:58:05,964][166323] Avg episode reward: [(0, '1259.573')]
[36m[2025-07-02 08:58:10,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12533760. Throughput: 0: 276.1. Samples: 12545712. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 08:58:10,988][166323] Avg episode reward: [(0, '1241.526')]
[36m[2025-07-02 08:58:15,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12533760. Throughput: 0: 279.9. Samples: 12547408. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 08:58:15,985][166323] Avg episode reward: [(0, '1199.937')]
[31m[44685220 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[44685221 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[44685221 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:58:20,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12533760. Throughput: 0: 280.6. Samples: 12548256. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 08:58:20,954][166323] Avg episode reward: [(0, '1241.484')]
[31m[44693602 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[44693603 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[44693603 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:58:25,953][166323] Fps is (10 sec: 1643.8, 60 sec: 546.2, 300 sec: 333.3). Total num frames: 12550144. Throughput: 0: 282.4. Samples: 12549936. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 08:58:25,953][166323] Avg episode reward: [(0, '1273.702')]
[36m[2025-07-02 08:58:30,960][166323] Fps is (10 sec: 1637.4, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12550144. Throughput: 0: 284.8. Samples: 12551632. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 08:58:30,960][166323] Avg episode reward: [(0, '1293.954')]
[36m[2025-07-02 08:58:35,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 12550144. Throughput: 0: 285.9. Samples: 12552496. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 08:58:35,956][166323] Avg episode reward: [(0, '1311.639')]
[36m[2025-07-02 08:58:40,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12550144. Throughput: 0: 281.4. Samples: 12554112. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 08:58:40,963][166323] Avg episode reward: [(0, '1241.288')]
[36m[2025-07-02 08:58:46,013][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.6). Total num frames: 12550144. Throughput: 0: 279.2. Samples: 12555744. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 08:58:46,014][166323] Avg episode reward: [(0, '1236.086')]
[36m[2025-07-02 08:58:50,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12550144. Throughput: 0: 278.5. Samples: 12556656. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 08:58:50,953][166323] Avg episode reward: [(0, '1294.369')]
[36m[2025-07-02 08:58:55,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12550144. Throughput: 0: 281.5. Samples: 12558368. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 08:58:55,947][166323] Avg episode reward: [(0, '1219.625')]
[36m[2025-07-02 08:59:00,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12550144. Throughput: 0: 281.0. Samples: 12560048. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 08:59:00,964][166323] Avg episode reward: [(0, '1206.601')]
[31m[44733736 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[44733737 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[44733737 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 08:59:05,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12550144. Throughput: 0: 282.5. Samples: 12560976. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 08:59:05,975][166323] Avg episode reward: [(0, '1140.722')]
[33m[44738560 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[44738560 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.86083984375
[33mCrash Rate: 0.12939453125
[33mTimeout Rate: 0.009765625 (navigation_task.py:265)
[33m[44738560 ms][navigation_task] - WARNING : 
[33mSuccesses: 1763
[33mCrashes : 265
[33mTimeouts: 20 (navigation_task.py:268)
[36m[2025-07-02 08:59:10,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12550144. Throughput: 0: 284.1. Samples: 12562720. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 08:59:10,946][166323] Avg episode reward: [(0, '1188.339')]
[36m[2025-07-02 08:59:15,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12550144. Throughput: 0: 281.8. Samples: 12564320. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 08:59:15,982][166323] Avg episode reward: [(0, '1238.218')]
[36m[2025-07-02 08:59:20,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12550144. Throughput: 0: 281.7. Samples: 12565168. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 08:59:20,946][166323] Avg episode reward: [(0, '1242.551')]
[36m[2025-07-02 08:59:25,951][166323] Fps is (10 sec: 1643.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12566528. Throughput: 0: 282.7. Samples: 12566832. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 08:59:25,952][166323] Avg episode reward: [(0, '1247.047')]
[36m[2025-07-02 08:59:30,949][166323] Fps is (10 sec: 1637.9, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 12566528. Throughput: 0: 280.6. Samples: 12568352. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 08:59:30,949][166323] Avg episode reward: [(0, '1266.224')]
[36m[2025-07-02 08:59:35,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12566528. Throughput: 0: 277.8. Samples: 12569168. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 08:59:35,988][166323] Avg episode reward: [(0, '1292.845')]
[37m[1m[2025-07-02 08:59:36,041][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000024536_12566528.pth...
[36m[2025-07-02 08:59:36,045][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000024408_12500992.pth
[36m[2025-07-02 08:59:40,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12566528. Throughput: 0: 278.3. Samples: 12570896. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 08:59:40,959][166323] Avg episode reward: [(0, '1313.925')]
[36m[2025-07-02 08:59:45,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12566528. Throughput: 0: 279.1. Samples: 12572608. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 08:59:45,963][166323] Avg episode reward: [(0, '1236.811')]
[36m[2025-07-02 08:59:50,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12566528. Throughput: 0: 278.5. Samples: 12573504. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 08:59:50,953][166323] Avg episode reward: [(0, '1171.855')]
[36m[2025-07-02 08:59:55,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12566528. Throughput: 0: 275.3. Samples: 12575120. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 08:59:55,991][166323] Avg episode reward: [(0, '1193.734')]
[36m[2025-07-02 09:00:00,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 12566528. Throughput: 0: 278.6. Samples: 12576848. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 09:00:00,948][166323] Avg episode reward: [(0, '1200.559')]
[36m[2025-07-02 09:00:05,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12566528. Throughput: 0: 278.6. Samples: 12577712. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 09:00:05,979][166323] Avg episode reward: [(0, '1236.047')]
[36m[2025-07-02 09:00:11,007][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 12566528. Throughput: 0: 282.3. Samples: 12579552. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 09:00:11,007][166323] Avg episode reward: [(0, '1203.059')]
[36m[2025-07-02 09:00:15,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12566528. Throughput: 0: 286.6. Samples: 12581248. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 09:00:15,946][166323] Avg episode reward: [(0, '1180.052')]
[36m[2025-07-02 09:00:20,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12566528. Throughput: 0: 288.0. Samples: 12582128. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 09:00:20,991][166323] Avg episode reward: [(0, '1150.517')]
[36m[2025-07-02 09:00:25,957][166323] Fps is (10 sec: 1636.5, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12582912. Throughput: 0: 283.0. Samples: 12583632. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 09:00:25,958][166323] Avg episode reward: [(0, '1151.618')]
[36m[2025-07-02 09:00:30,955][166323] Fps is (10 sec: 1644.3, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12582912. Throughput: 0: 283.4. Samples: 12585360. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 09:00:30,955][166323] Avg episode reward: [(0, '1145.092')]
[36m[2025-07-02 09:00:35,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12582912. Throughput: 0: 280.2. Samples: 12586112. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 09:00:35,951][166323] Avg episode reward: [(0, '1155.784')]
[36m[2025-07-02 09:00:40,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12582912. Throughput: 0: 281.7. Samples: 12587792. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 09:00:40,978][166323] Avg episode reward: [(0, '1182.503')]
[36m[2025-07-02 09:00:45,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12582912. Throughput: 0: 280.2. Samples: 12589472. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 09:00:45,999][166323] Avg episode reward: [(0, '1221.231')]
[31m[44838652 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[44838653 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[44838653 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:00:51,006][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 12582912. Throughput: 0: 280.4. Samples: 12590336. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 09:00:51,007][166323] Avg episode reward: [(0, '1241.310')]
[36m[2025-07-02 09:00:55,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12582912. Throughput: 0: 273.6. Samples: 12591856. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 09:00:55,985][166323] Avg episode reward: [(0, '1294.284')]
[31m[44846964 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[44846964 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[44846964 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:01:00,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12582912. Throughput: 0: 273.9. Samples: 12593584. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 09:01:00,989][166323] Avg episode reward: [(0, '1226.198')]
[36m[2025-07-02 09:01:05,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12582912. Throughput: 0: 274.5. Samples: 12594480. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 09:01:05,981][166323] Avg episode reward: [(0, '1282.813')]
[31m[44855755 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[44855755 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[44855755 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[44856279 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[44856279 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[44856280 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:01:10,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12582912. Throughput: 0: 280.2. Samples: 12596240. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 09:01:10,950][166323] Avg episode reward: [(0, '1174.518')]
[36m[2025-07-02 09:01:15,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12582912. Throughput: 0: 276.5. Samples: 12597808. Policy #0 lag: (min: 20.0, avg: 20.0, max: 20.0)
[36m[2025-07-02 09:01:15,970][166323] Avg episode reward: [(0, '1206.961')]
[36m[2025-07-02 09:01:21,153][166323] Fps is (10 sec: 1605.8, 60 sec: 544.7, 300 sec: 333.0). Total num frames: 12599296. Throughput: 0: 277.5. Samples: 12598656. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:01:21,153][166323] Avg episode reward: [(0, '1195.607')]
[36m[2025-07-02 09:01:25,982][166323] Fps is (10 sec: 1636.4, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12599296. Throughput: 0: 276.6. Samples: 12600240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:01:25,982][166323] Avg episode reward: [(0, '1210.269')]
[36m[2025-07-02 09:01:30,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12599296. Throughput: 0: 273.7. Samples: 12601776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:01:30,948][166323] Avg episode reward: [(0, '1184.795')]
[36m[2025-07-02 09:01:35,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12599296. Throughput: 0: 271.8. Samples: 12602560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:01:35,977][166323] Avg episode reward: [(0, '1260.772')]
[37m[1m[2025-07-02 09:01:36,039][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000024600_12599296.pth...
[36m[2025-07-02 09:01:36,047][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000024472_12533760.pth
[36m[2025-07-02 09:01:40,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12599296. Throughput: 0: 273.8. Samples: 12604176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:01:40,988][166323] Avg episode reward: [(0, '1189.517')]
[36m[2025-07-02 09:01:45,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12599296. Throughput: 0: 272.8. Samples: 12605856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:01:45,977][166323] Avg episode reward: [(0, '1158.792')]
[36m[2025-07-02 09:01:50,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12599296. Throughput: 0: 271.6. Samples: 12606704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:01:50,989][166323] Avg episode reward: [(0, '1142.895')]
[31m[44901473 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[44901474 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[44901474 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:01:55,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12599296. Throughput: 0: 267.6. Samples: 12608288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:01:55,970][166323] Avg episode reward: [(0, '1134.979')]
[36m[2025-07-02 09:02:00,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12599296. Throughput: 0: 269.3. Samples: 12609920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:02:00,950][166323] Avg episode reward: [(0, '1133.798')]
[36m[2025-07-02 09:02:05,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12599296. Throughput: 0: 270.6. Samples: 12610784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:02:05,975][166323] Avg episode reward: [(0, '1161.918')]
[36m[2025-07-02 09:02:10,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12599296. Throughput: 0: 274.0. Samples: 12612560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:02:10,950][166323] Avg episode reward: [(0, '1183.851')]
[36m[2025-07-02 09:02:15,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12599296. Throughput: 0: 279.1. Samples: 12614336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:02:15,949][166323] Avg episode reward: [(0, '1226.750')]
[36m[2025-07-02 09:02:21,101][166323] Fps is (10 sec: 1613.9, 60 sec: 273.3, 300 sec: 333.1). Total num frames: 12615680. Throughput: 0: 278.0. Samples: 12615104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:02:21,102][166323] Avg episode reward: [(0, '1312.018')]
[36m[2025-07-02 09:02:25,990][166323] Fps is (10 sec: 1631.7, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12615680. Throughput: 0: 279.8. Samples: 12616768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:02:25,990][166323] Avg episode reward: [(0, '1290.441')]
[36m[2025-07-02 09:02:30,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.8). Total num frames: 12615680. Throughput: 0: 283.5. Samples: 12618608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:02:30,953][166323] Avg episode reward: [(0, '1305.724')]
[36m[2025-07-02 09:02:35,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12615680. Throughput: 0: 283.1. Samples: 12619440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:02:35,970][166323] Avg episode reward: [(0, '1241.406')]
[36m[2025-07-02 09:02:40,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12615680. Throughput: 0: 283.3. Samples: 12621040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:02:40,978][166323] Avg episode reward: [(0, '1192.836')]
[36m[2025-07-02 09:02:45,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12615680. Throughput: 0: 283.5. Samples: 12622688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:02:45,987][166323] Avg episode reward: [(0, '1191.708')]
[36m[2025-07-02 09:02:51,021][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12615680. Throughput: 0: 281.3. Samples: 12623456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:02:51,021][166323] Avg episode reward: [(0, '1194.768')]
[36m[2025-07-02 09:02:55,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12615680. Throughput: 0: 280.0. Samples: 12625168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:02:55,973][166323] Avg episode reward: [(0, '1231.163')]
[36m[2025-07-02 09:03:00,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12615680. Throughput: 0: 278.0. Samples: 12626848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:03:00,961][166323] Avg episode reward: [(0, '1209.861')]
[36m[2025-07-02 09:03:05,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12615680. Throughput: 0: 283.1. Samples: 12627808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:03:05,970][166323] Avg episode reward: [(0, '1247.021')]
[36m[2025-07-02 09:03:10,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12615680. Throughput: 0: 280.5. Samples: 12629392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:03:10,993][166323] Avg episode reward: [(0, '1290.184')]
[36m[2025-07-02 09:03:15,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12615680. Throughput: 0: 278.8. Samples: 12631152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:03:15,949][166323] Avg episode reward: [(0, '1314.056')]
[36m[2025-07-02 09:03:20,944][166323] Fps is (10 sec: 1646.4, 60 sec: 273.8, 300 sec: 277.7). Total num frames: 12632064. Throughput: 0: 281.4. Samples: 12632096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:03:20,944][166323] Avg episode reward: [(0, '1335.523')]
[36m[2025-07-02 09:03:25,952][166323] Fps is (10 sec: 1637.8, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12632064. Throughput: 0: 281.8. Samples: 12633712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:03:25,952][166323] Avg episode reward: [(0, '1327.236')]
[36m[2025-07-02 09:03:30,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12632064. Throughput: 0: 282.6. Samples: 12635408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:03:30,994][166323] Avg episode reward: [(0, '1292.717')]
[36m[2025-07-02 09:03:36,002][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12632064. Throughput: 0: 284.9. Samples: 12636272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:03:36,002][166323] Avg episode reward: [(0, '1287.391')]
[37m[1m[2025-07-02 09:03:36,007][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000024664_12632064.pth...
[36m[2025-07-02 09:03:36,014][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000024536_12566528.pth
[36m[2025-07-02 09:03:40,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12632064. Throughput: 0: 284.1. Samples: 12637952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:03:40,968][166323] Avg episode reward: [(0, '1271.676')]
[36m[2025-07-02 09:03:46,003][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 12632064. Throughput: 0: 284.2. Samples: 12639648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:03:46,004][166323] Avg episode reward: [(0, '1263.586')]
[31m[45015198 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[45015198 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[45015198 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:03:50,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12632064. Throughput: 0: 283.0. Samples: 12640544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:03:50,968][166323] Avg episode reward: [(0, '1276.636')]
[36m[2025-07-02 09:03:55,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12632064. Throughput: 0: 286.7. Samples: 12642288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:03:55,973][166323] Avg episode reward: [(0, '1260.242')]
[36m[2025-07-02 09:04:00,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12632064. Throughput: 0: 285.8. Samples: 12644016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:04:00,965][166323] Avg episode reward: [(0, '1282.670')]
[36m[2025-07-02 09:04:05,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12632064. Throughput: 0: 284.0. Samples: 12644880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:04:05,956][166323] Avg episode reward: [(0, '1332.066')]
[36m[2025-07-02 09:04:10,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12632064. Throughput: 0: 284.2. Samples: 12646512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:04:10,984][166323] Avg episode reward: [(0, '1265.686')]
[36m[2025-07-02 09:04:15,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12632064. Throughput: 0: 285.2. Samples: 12648240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:04:15,981][166323] Avg episode reward: [(0, '1265.849')]
[36m[2025-07-02 09:04:20,971][166323] Fps is (10 sec: 1640.5, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12648448. Throughput: 0: 283.6. Samples: 12649024. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 09:04:20,971][166323] Avg episode reward: [(0, '1262.302')]
[36m[2025-07-02 09:04:25,964][166323] Fps is (10 sec: 1641.1, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12648448. Throughput: 0: 282.7. Samples: 12650672. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 09:04:25,965][166323] Avg episode reward: [(0, '1265.872')]
[36m[2025-07-02 09:04:30,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12648448. Throughput: 0: 282.3. Samples: 12652336. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 09:04:30,944][166323] Avg episode reward: [(0, '1254.441')]
[36m[2025-07-02 09:04:35,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12648448. Throughput: 0: 280.1. Samples: 12653152. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 09:04:35,977][166323] Avg episode reward: [(0, '1253.947')]
[36m[2025-07-02 09:04:40,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12648448. Throughput: 0: 278.5. Samples: 12654816. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 09:04:40,954][166323] Avg episode reward: [(0, '1272.586')]
[36m[2025-07-02 09:04:45,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12648448. Throughput: 0: 277.8. Samples: 12656512. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 09:04:45,953][166323] Avg episode reward: [(0, '1260.450')]
[31m[45076327 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[45076328 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[45076328 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:04:50,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12648448. Throughput: 0: 277.0. Samples: 12657344. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 09:04:50,953][166323] Avg episode reward: [(0, '1233.038')]
[36m[2025-07-02 09:04:55,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12648448. Throughput: 0: 277.5. Samples: 12658992. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 09:04:55,952][166323] Avg episode reward: [(0, '1229.362')]
[36m[2025-07-02 09:05:00,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12648448. Throughput: 0: 275.5. Samples: 12660640. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 09:05:00,985][166323] Avg episode reward: [(0, '1201.800')]
[36m[2025-07-02 09:05:05,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12648448. Throughput: 0: 276.7. Samples: 12661472. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 09:05:05,955][166323] Avg episode reward: [(0, '1162.824')]
[36m[2025-07-02 09:05:10,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12648448. Throughput: 0: 277.2. Samples: 12663152. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-02 09:05:10,985][166323] Avg episode reward: [(0, '1189.539')]
[31m[45101320 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[45101321 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[45101321 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:05:15,969][166323] Fps is (10 sec: 1636.1, 60 sec: 546.2, 300 sec: 333.3). Total num frames: 12664832. Throughput: 0: 279.7. Samples: 12664928. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 09:05:15,969][166323] Avg episode reward: [(0, '1160.134')]
[36m[2025-07-02 09:05:20,972][166323] Fps is (10 sec: 1640.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12664832. Throughput: 0: 281.3. Samples: 12665808. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 09:05:20,972][166323] Avg episode reward: [(0, '1206.876')]
[36m[2025-07-02 09:05:25,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12664832. Throughput: 0: 280.8. Samples: 12667456. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 09:05:25,965][166323] Avg episode reward: [(0, '1264.178')]
[36m[2025-07-02 09:05:30,970][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12664832. Throughput: 0: 283.6. Samples: 12669280. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 09:05:30,970][166323] Avg episode reward: [(0, '1267.459')]
[36m[2025-07-02 09:05:35,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12664832. Throughput: 0: 285.5. Samples: 12670192. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 09:05:35,958][166323] Avg episode reward: [(0, '1282.043')]
[37m[1m[2025-07-02 09:05:36,009][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000024728_12664832.pth...
[36m[2025-07-02 09:05:36,014][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000024600_12599296.pth
[36m[2025-07-02 09:05:40,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12664832. Throughput: 0: 284.1. Samples: 12671776. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 09:05:40,950][166323] Avg episode reward: [(0, '1240.802')]
[36m[2025-07-02 09:05:46,004][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 12664832. Throughput: 0: 288.2. Samples: 12673616. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 09:05:46,005][166323] Avg episode reward: [(0, '1202.484')]
[36m[2025-07-02 09:05:50,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12664832. Throughput: 0: 288.3. Samples: 12674448. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 09:05:50,961][166323] Avg episode reward: [(0, '1268.264')]
[36m[2025-07-02 09:05:55,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12664832. Throughput: 0: 290.1. Samples: 12676208. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 09:05:55,985][166323] Avg episode reward: [(0, '1201.525')]
[36m[2025-07-02 09:06:00,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12664832. Throughput: 0: 290.0. Samples: 12677984. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 09:06:00,983][166323] Avg episode reward: [(0, '1260.028')]
[36m[2025-07-02 09:06:06,010][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 12664832. Throughput: 0: 289.2. Samples: 12678832. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 09:06:06,010][166323] Avg episode reward: [(0, '1286.756')]
[36m[2025-07-02 09:06:10,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12664832. Throughput: 0: 289.4. Samples: 12680480. Policy #0 lag: (min: 0.0, avg: 0.2, max: 32.0)
[36m[2025-07-02 09:06:10,969][166323] Avg episode reward: [(0, '1307.873')]
[36m[2025-07-02 09:06:15,962][166323] Fps is (10 sec: 1646.3, 60 sec: 273.1, 300 sec: 277.9). Total num frames: 12681216. Throughput: 0: 285.6. Samples: 12682128. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 09:06:15,962][166323] Avg episode reward: [(0, '1319.472')]
[36m[2025-07-02 09:06:20,956][166323] Fps is (10 sec: 1640.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12681216. Throughput: 0: 285.2. Samples: 12683024. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 09:06:20,956][166323] Avg episode reward: [(0, '1318.798')]
[36m[2025-07-02 09:06:25,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12681216. Throughput: 0: 287.6. Samples: 12684720. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 09:06:25,954][166323] Avg episode reward: [(0, '1326.221')]
[36m[2025-07-02 09:06:30,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12681216. Throughput: 0: 283.4. Samples: 12686352. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 09:06:30,943][166323] Avg episode reward: [(0, '1370.513')]
[36m[2025-07-02 09:06:35,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12681216. Throughput: 0: 282.2. Samples: 12687152. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 09:06:35,977][166323] Avg episode reward: [(0, '1327.562')]
[36m[2025-07-02 09:06:40,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12681216. Throughput: 0: 281.1. Samples: 12688848. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 09:06:40,959][166323] Avg episode reward: [(0, '1277.210')]
[36m[2025-07-02 09:06:45,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12681216. Throughput: 0: 279.8. Samples: 12690576. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 09:06:45,989][166323] Avg episode reward: [(0, '1289.178')]
[36m[2025-07-02 09:06:50,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12681216. Throughput: 0: 278.1. Samples: 12691328. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 09:06:50,949][166323] Avg episode reward: [(0, '1339.596')]
[36m[2025-07-02 09:06:55,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12681216. Throughput: 0: 277.0. Samples: 12692944. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 09:06:55,961][166323] Avg episode reward: [(0, '1326.839')]
[36m[2025-07-02 09:07:00,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12681216. Throughput: 0: 277.2. Samples: 12694608. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 09:07:00,986][166323] Avg episode reward: [(0, '1293.364')]
[36m[2025-07-02 09:07:05,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12681216. Throughput: 0: 275.7. Samples: 12695440. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 09:07:05,988][166323] Avg episode reward: [(0, '1224.802')]
[36m[2025-07-02 09:07:10,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12681216. Throughput: 0: 276.6. Samples: 12697168. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-07-02 09:07:10,955][166323] Avg episode reward: [(0, '1255.878')]
[36m[2025-07-02 09:07:15,956][166323] Fps is (10 sec: 1643.5, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 12697600. Throughput: 0: 275.1. Samples: 12698736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:07:15,956][166323] Avg episode reward: [(0, '1233.994')]
[36m[2025-07-02 09:07:20,977][166323] Fps is (10 sec: 1634.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12697600. Throughput: 0: 277.7. Samples: 12699648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:07:20,977][166323] Avg episode reward: [(0, '1271.414')]
[36m[2025-07-02 09:07:25,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12697600. Throughput: 0: 277.7. Samples: 12701344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:07:25,957][166323] Avg episode reward: [(0, '1234.682')]
[36m[2025-07-02 09:07:30,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12697600. Throughput: 0: 277.5. Samples: 12703056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:07:30,966][166323] Avg episode reward: [(0, '1182.671')]
[36m[2025-07-02 09:07:35,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12697600. Throughput: 0: 280.7. Samples: 12703968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:07:35,973][166323] Avg episode reward: [(0, '1189.228')]
[37m[1m[2025-07-02 09:07:36,050][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000024792_12697600.pth...
[36m[2025-07-02 09:07:36,056][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000024664_12632064.pth
[36m[2025-07-02 09:07:40,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12697600. Throughput: 0: 281.3. Samples: 12705600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:07:40,953][166323] Avg episode reward: [(0, '1224.044')]
[36m[2025-07-02 09:07:45,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12697600. Throughput: 0: 282.0. Samples: 12707296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:07:45,972][166323] Avg episode reward: [(0, '1226.849')]
[36m[2025-07-02 09:07:50,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12697600. Throughput: 0: 282.8. Samples: 12708160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:07:50,966][166323] Avg episode reward: [(0, '1268.066')]
[36m[2025-07-02 09:07:55,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12697600. Throughput: 0: 282.2. Samples: 12709872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:07:55,977][166323] Avg episode reward: [(0, '1247.119')]
[31m[45268898 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[45268898 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[45268898 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[45268949 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[45268950 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[45268950 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:08:00,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12697600. Throughput: 0: 286.0. Samples: 12711616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:08:00,985][166323] Avg episode reward: [(0, '1245.129')]
[36m[2025-07-02 09:08:05,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12697600. Throughput: 0: 282.7. Samples: 12712368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:08:05,977][166323] Avg episode reward: [(0, '1272.463')]
[36m[2025-07-02 09:08:11,001][166323] Fps is (10 sec: 1635.7, 60 sec: 545.7, 300 sec: 333.2). Total num frames: 12713984. Throughput: 0: 281.7. Samples: 12714032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:08:11,001][166323] Avg episode reward: [(0, '1301.579')]
[36m[2025-07-02 09:08:15,979][166323] Fps is (10 sec: 1638.1, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12713984. Throughput: 0: 282.2. Samples: 12715760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:08:15,979][166323] Avg episode reward: [(0, '1321.259')]
[36m[2025-07-02 09:08:20,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12713984. Throughput: 0: 280.9. Samples: 12716608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:08:20,978][166323] Avg episode reward: [(0, '1292.546')]
[36m[2025-07-02 09:08:25,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12713984. Throughput: 0: 282.9. Samples: 12718336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:08:25,974][166323] Avg episode reward: [(0, '1288.275')]
[36m[2025-07-02 09:08:30,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12713984. Throughput: 0: 284.4. Samples: 12720096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:08:30,973][166323] Avg episode reward: [(0, '1330.275')]
[36m[2025-07-02 09:08:35,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12713984. Throughput: 0: 283.0. Samples: 12720896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:08:35,966][166323] Avg episode reward: [(0, '1416.580')]
[36m[2025-07-02 09:08:40,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 12713984. Throughput: 0: 283.2. Samples: 12722608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:08:40,944][166323] Avg episode reward: [(0, '1408.735')]
[36m[2025-07-02 09:08:45,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12713984. Throughput: 0: 281.4. Samples: 12724272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:08:45,960][166323] Avg episode reward: [(0, '1373.504')]
[36m[2025-07-02 09:08:50,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12713984. Throughput: 0: 284.7. Samples: 12725168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:08:50,944][166323] Avg episode reward: [(0, '1348.375')]
[36m[2025-07-02 09:08:55,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12713984. Throughput: 0: 281.3. Samples: 12726688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:08:55,997][166323] Avg episode reward: [(0, '1312.849')]
[36m[2025-07-02 09:09:00,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12713984. Throughput: 0: 277.4. Samples: 12728240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:09:00,961][166323] Avg episode reward: [(0, '1253.576')]
[36m[2025-07-02 09:09:06,025][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12713984. Throughput: 0: 278.5. Samples: 12729152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:09:06,025][166323] Avg episode reward: [(0, '1268.598')]
[36m[2025-07-02 09:09:10,943][166323] Fps is (10 sec: 1641.3, 60 sec: 273.3, 300 sec: 333.3). Total num frames: 12730368. Throughput: 0: 277.2. Samples: 12730800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:09:10,943][166323] Avg episode reward: [(0, '1207.660')]
[36m[2025-07-02 09:09:15,974][166323] Fps is (10 sec: 1646.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12730368. Throughput: 0: 275.2. Samples: 12732480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:09:15,974][166323] Avg episode reward: [(0, '1208.675')]
[36m[2025-07-02 09:09:20,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12730368. Throughput: 0: 277.9. Samples: 12733408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:09:20,986][166323] Avg episode reward: [(0, '1255.107')]
[36m[2025-07-02 09:09:25,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12730368. Throughput: 0: 276.7. Samples: 12735072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:09:25,987][166323] Avg episode reward: [(0, '1278.696')]
[36m[2025-07-02 09:09:30,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12730368. Throughput: 0: 277.9. Samples: 12736784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:09:30,980][166323] Avg episode reward: [(0, '1290.866')]
[36m[2025-07-02 09:09:35,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12730368. Throughput: 0: 275.4. Samples: 12737568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:09:35,968][166323] Avg episode reward: [(0, '1345.976')]
[37m[1m[2025-07-02 09:09:36,037][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000024856_12730368.pth...
[36m[2025-07-02 09:09:36,041][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000024728_12664832.pth
[36m[2025-07-02 09:09:40,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12730368. Throughput: 0: 280.5. Samples: 12739296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:09:40,946][166323] Avg episode reward: [(0, '1336.160')]
[36m[2025-07-02 09:09:45,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12730368. Throughput: 0: 286.9. Samples: 12741152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:09:45,972][166323] Avg episode reward: [(0, '1397.424')]
[36m[2025-07-02 09:09:50,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12730368. Throughput: 0: 286.3. Samples: 12742016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:09:50,954][166323] Avg episode reward: [(0, '1387.384')]
[36m[2025-07-02 09:09:55,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12730368. Throughput: 0: 281.7. Samples: 12743488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:09:55,980][166323] Avg episode reward: [(0, '1381.590')]
[36m[2025-07-02 09:10:00,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12730368. Throughput: 0: 283.4. Samples: 12745232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:10:00,969][166323] Avg episode reward: [(0, '1389.589')]
[36m[2025-07-02 09:10:05,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12730368. Throughput: 0: 280.2. Samples: 12746016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:10:05,990][166323] Avg episode reward: [(0, '1355.616')]
[36m[2025-07-02 09:10:10,984][166323] Fps is (10 sec: 1635.8, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12746752. Throughput: 0: 279.5. Samples: 12747648. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 09:10:10,984][166323] Avg episode reward: [(0, '1323.932')]
[36m[2025-07-02 09:10:15,970][166323] Fps is (10 sec: 1641.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12746752. Throughput: 0: 279.9. Samples: 12749376. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 09:10:15,970][166323] Avg episode reward: [(0, '1351.718')]
[36m[2025-07-02 09:10:20,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12746752. Throughput: 0: 280.2. Samples: 12750176. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 09:10:20,959][166323] Avg episode reward: [(0, '1296.618')]
[36m[2025-07-02 09:10:25,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12746752. Throughput: 0: 281.5. Samples: 12751968. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 09:10:25,955][166323] Avg episode reward: [(0, '1290.684')]
[36m[2025-07-02 09:10:30,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12746752. Throughput: 0: 279.9. Samples: 12753744. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 09:10:30,962][166323] Avg episode reward: [(0, '1274.082')]
[33m[45420052 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[45420052 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.865234375
[33mCrash Rate: 0.12158203125
[33mTimeout Rate: 0.01318359375 (navigation_task.py:265)
[33m[45420052 ms][navigation_task] - WARNING : 
[33mSuccesses: 1772
[33mCrashes : 249
[33mTimeouts: 27 (navigation_task.py:268)
[31m[45420219 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[45420219 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[45420220 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:10:35,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12746752. Throughput: 0: 280.0. Samples: 12754624. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 09:10:35,989][166323] Avg episode reward: [(0, '1227.257')]
[36m[2025-07-02 09:10:40,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12746752. Throughput: 0: 286.0. Samples: 12756352. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 09:10:40,955][166323] Avg episode reward: [(0, '1288.493')]
[36m[2025-07-02 09:10:45,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12746752. Throughput: 0: 286.4. Samples: 12758112. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 09:10:45,944][166323] Avg episode reward: [(0, '1274.507')]
[36m[2025-07-02 09:10:50,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12746752. Throughput: 0: 288.6. Samples: 12758992. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 09:10:50,956][166323] Avg episode reward: [(0, '1267.762')]
[36m[2025-07-02 09:10:55,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12746752. Throughput: 0: 291.6. Samples: 12760768. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 09:10:55,976][166323] Avg episode reward: [(0, '1306.294')]
[36m[2025-07-02 09:11:00,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12746752. Throughput: 0: 293.4. Samples: 12762576. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 09:11:00,959][166323] Avg episode reward: [(0, '1279.724')]
[36m[2025-07-02 09:11:05,968][166323] Fps is (10 sec: 1639.7, 60 sec: 546.3, 300 sec: 333.2). Total num frames: 12763136. Throughput: 0: 295.1. Samples: 12763456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:11:05,968][166323] Avg episode reward: [(0, '1282.164')]
[36m[2025-07-02 09:11:10,977][166323] Fps is (10 sec: 1635.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12763136. Throughput: 0: 290.7. Samples: 12765056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:11:10,977][166323] Avg episode reward: [(0, '1304.486')]
[36m[2025-07-02 09:11:15,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12763136. Throughput: 0: 290.2. Samples: 12766800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:11:15,953][166323] Avg episode reward: [(0, '1349.560')]
[36m[2025-07-02 09:11:20,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12763136. Throughput: 0: 290.0. Samples: 12767664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:11:20,951][166323] Avg episode reward: [(0, '1357.579')]
[36m[2025-07-02 09:11:25,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12763136. Throughput: 0: 287.5. Samples: 12769296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:11:25,970][166323] Avg episode reward: [(0, '1319.519')]
[36m[2025-07-02 09:11:30,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12763136. Throughput: 0: 285.9. Samples: 12770992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:11:30,988][166323] Avg episode reward: [(0, '1283.148')]
[36m[2025-07-02 09:11:35,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12763136. Throughput: 0: 283.7. Samples: 12771760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:11:35,961][166323] Avg episode reward: [(0, '1204.598')]
[37m[1m[2025-07-02 09:11:36,012][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000024920_12763136.pth...
[36m[2025-07-02 09:11:36,016][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000024792_12697600.pth
[36m[2025-07-02 09:11:40,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12763136. Throughput: 0: 282.1. Samples: 12773456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:11:40,951][166323] Avg episode reward: [(0, '1176.085')]
[36m[2025-07-02 09:11:46,028][166323] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 277.6). Total num frames: 12763136. Throughput: 0: 279.4. Samples: 12775168. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:11:46,029][166323] Avg episode reward: [(0, '1124.844')]
[36m[2025-07-02 09:11:50,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12763136. Throughput: 0: 278.3. Samples: 12775984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:11:50,981][166323] Avg episode reward: [(0, '1115.537')]
[36m[2025-07-02 09:11:55,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12763136. Throughput: 0: 276.2. Samples: 12777488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:11:55,982][166323] Avg episode reward: [(0, '1115.860')]
[36m[2025-07-02 09:12:00,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12763136. Throughput: 0: 271.9. Samples: 12779040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:12:00,977][166323] Avg episode reward: [(0, '1140.345')]
[36m[2025-07-02 09:12:05,943][166323] Fps is (10 sec: 1644.9, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 12779520. Throughput: 0: 270.6. Samples: 12779840. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:12:05,943][166323] Avg episode reward: [(0, '1178.192')]
[36m[2025-07-02 09:12:10,988][166323] Fps is (10 sec: 1636.5, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12779520. Throughput: 0: 273.7. Samples: 12781616. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:12:10,989][166323] Avg episode reward: [(0, '1278.613')]
[36m[2025-07-02 09:12:15,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12779520. Throughput: 0: 275.8. Samples: 12783392. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:12:15,948][166323] Avg episode reward: [(0, '1337.791')]
[36m[2025-07-02 09:12:20,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12779520. Throughput: 0: 276.1. Samples: 12784192. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:12:20,991][166323] Avg episode reward: [(0, '1361.432')]
[36m[2025-07-02 09:12:25,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12779520. Throughput: 0: 275.7. Samples: 12785872. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:12:25,984][166323] Avg episode reward: [(0, '1365.261')]
[36m[2025-07-02 09:12:30,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12779520. Throughput: 0: 274.7. Samples: 12787520. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:12:30,987][166323] Avg episode reward: [(0, '1412.938')]
[36m[2025-07-02 09:12:35,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12779520. Throughput: 0: 276.2. Samples: 12788416. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:12:35,992][166323] Avg episode reward: [(0, '1418.897')]
[36m[2025-07-02 09:12:40,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12779520. Throughput: 0: 276.3. Samples: 12789920. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:12:40,975][166323] Avg episode reward: [(0, '1345.265')]
[36m[2025-07-02 09:12:45,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12779520. Throughput: 0: 279.5. Samples: 12791616. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:12:45,967][166323] Avg episode reward: [(0, '1338.006')]
[36m[2025-07-02 09:12:50,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12779520. Throughput: 0: 278.4. Samples: 12792368. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:12:50,950][166323] Avg episode reward: [(0, '1343.295')]
[36m[2025-07-02 09:12:55,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12779520. Throughput: 0: 277.2. Samples: 12794080. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:12:55,955][166323] Avg episode reward: [(0, '1347.043')]
[36m[2025-07-02 09:13:01,000][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12779520. Throughput: 0: 272.4. Samples: 12795664. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:13:01,000][166323] Avg episode reward: [(0, '1339.671')]
[36m[2025-07-02 09:13:05,959][166323] Fps is (10 sec: 1637.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12795904. Throughput: 0: 270.8. Samples: 12796368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:13:05,959][166323] Avg episode reward: [(0, '1353.863')]
[36m[2025-07-02 09:13:10,989][166323] Fps is (10 sec: 1640.1, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12795904. Throughput: 0: 270.9. Samples: 12798064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:13:10,990][166323] Avg episode reward: [(0, '1297.428')]
[36m[2025-07-02 09:13:15,979][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12795904. Throughput: 0: 273.1. Samples: 12799808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:13:15,979][166323] Avg episode reward: [(0, '1351.988')]
[36m[2025-07-02 09:13:20,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12795904. Throughput: 0: 272.7. Samples: 12800688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:13:20,987][166323] Avg episode reward: [(0, '1360.791')]
[36m[2025-07-02 09:13:25,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12795904. Throughput: 0: 275.6. Samples: 12802320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:13:25,963][166323] Avg episode reward: [(0, '1339.057')]
[36m[2025-07-02 09:13:31,003][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12795904. Throughput: 0: 278.2. Samples: 12804144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:13:31,004][166323] Avg episode reward: [(0, '1344.011')]
[31m[45602115 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[45602116 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[45602116 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:13:35,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12795904. Throughput: 0: 279.0. Samples: 12804928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:13:35,975][166323] Avg episode reward: [(0, '1301.786')]
[37m[1m[2025-07-02 09:13:36,029][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000024984_12795904.pth...
[36m[2025-07-02 09:13:36,033][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000024856_12730368.pth
[36m[2025-07-02 09:13:40,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12795904. Throughput: 0: 279.2. Samples: 12806640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:13:40,944][166323] Avg episode reward: [(0, '1260.383')]
[36m[2025-07-02 09:13:45,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12795904. Throughput: 0: 282.8. Samples: 12808384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:13:45,983][166323] Avg episode reward: [(0, '1271.669')]
[36m[2025-07-02 09:13:50,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12795904. Throughput: 0: 286.9. Samples: 12809280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:13:50,962][166323] Avg episode reward: [(0, '1249.774')]
[36m[2025-07-02 09:13:55,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12795904. Throughput: 0: 286.2. Samples: 12810928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:13:55,945][166323] Avg episode reward: [(0, '1252.693')]
[36m[2025-07-02 09:14:01,010][166323] Fps is (10 sec: 1630.6, 60 sec: 546.0, 300 sec: 333.3). Total num frames: 12812288. Throughput: 0: 281.4. Samples: 12812480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:14:01,010][166323] Avg episode reward: [(0, '1230.742')]
[36m[2025-07-02 09:14:05,947][166323] Fps is (10 sec: 1638.1, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12812288. Throughput: 0: 279.0. Samples: 12813232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:14:05,947][166323] Avg episode reward: [(0, '1261.266')]
[36m[2025-07-02 09:14:10,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12812288. Throughput: 0: 282.1. Samples: 12815008. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:14:10,945][166323] Avg episode reward: [(0, '1275.815')]
[36m[2025-07-02 09:14:16,001][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12812288. Throughput: 0: 279.5. Samples: 12816720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:14:16,002][166323] Avg episode reward: [(0, '1275.939')]
[36m[2025-07-02 09:14:20,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12812288. Throughput: 0: 281.4. Samples: 12817584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:14:20,954][166323] Avg episode reward: [(0, '1269.621')]
[36m[2025-07-02 09:14:25,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12812288. Throughput: 0: 279.7. Samples: 12819232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:14:25,963][166323] Avg episode reward: [(0, '1269.353')]
[36m[2025-07-02 09:14:30,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12812288. Throughput: 0: 277.2. Samples: 12820848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:14:30,947][166323] Avg episode reward: [(0, '1267.284')]
[36m[2025-07-02 09:14:35,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12812288. Throughput: 0: 274.8. Samples: 12821648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:14:35,971][166323] Avg episode reward: [(0, '1261.094')]
[36m[2025-07-02 09:14:40,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12812288. Throughput: 0: 277.4. Samples: 12823424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:14:40,986][166323] Avg episode reward: [(0, '1250.335')]
[36m[2025-07-02 09:14:45,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12812288. Throughput: 0: 281.7. Samples: 12825152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:14:45,999][166323] Avg episode reward: [(0, '1280.719')]
[36m[2025-07-02 09:14:50,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12812288. Throughput: 0: 281.0. Samples: 12825888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:14:50,987][166323] Avg episode reward: [(0, '1304.644')]
[36m[2025-07-02 09:14:55,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12812288. Throughput: 0: 278.3. Samples: 12827536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:14:55,968][166323] Avg episode reward: [(0, '1335.296')]
[36m[2025-07-02 09:15:00,959][166323] Fps is (10 sec: 1642.9, 60 sec: 273.3, 300 sec: 333.3). Total num frames: 12828672. Throughput: 0: 275.5. Samples: 12829104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:15:00,959][166323] Avg episode reward: [(0, '1350.552')]
[36m[2025-07-02 09:15:05,976][166323] Fps is (10 sec: 1637.1, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12828672. Throughput: 0: 274.7. Samples: 12829952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:15:05,977][166323] Avg episode reward: [(0, '1344.684')]
[36m[2025-07-02 09:15:11,009][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 12828672. Throughput: 0: 274.2. Samples: 12831584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:15:11,009][166323] Avg episode reward: [(0, '1376.760')]
[36m[2025-07-02 09:15:15,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12828672. Throughput: 0: 277.3. Samples: 12833328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:15:15,954][166323] Avg episode reward: [(0, '1355.400')]
[36m[2025-07-02 09:15:20,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12828672. Throughput: 0: 279.2. Samples: 12834208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:15:20,954][166323] Avg episode reward: [(0, '1331.370')]
[36m[2025-07-02 09:15:25,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12828672. Throughput: 0: 275.3. Samples: 12835808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:15:25,968][166323] Avg episode reward: [(0, '1265.085')]
[36m[2025-07-02 09:15:31,004][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 12828672. Throughput: 0: 274.5. Samples: 12837504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:15:31,005][166323] Avg episode reward: [(0, '1240.290')]
[36m[2025-07-02 09:15:36,001][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12828672. Throughput: 0: 275.1. Samples: 12838272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:15:36,001][166323] Avg episode reward: [(0, '1260.269')]
[37m[1m[2025-07-02 09:15:36,083][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000025048_12828672.pth...
[36m[2025-07-02 09:15:36,087][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000024920_12763136.pth
[36m[2025-07-02 09:15:40,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12828672. Throughput: 0: 273.3. Samples: 12839840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:15:40,988][166323] Avg episode reward: [(0, '1243.516')]
[36m[2025-07-02 09:15:45,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12828672. Throughput: 0: 276.6. Samples: 12841552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:15:45,955][166323] Avg episode reward: [(0, '1218.935')]
[36m[2025-07-02 09:15:50,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12828672. Throughput: 0: 275.9. Samples: 12842368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:15:50,985][166323] Avg episode reward: [(0, '1278.979')]
[36m[2025-07-02 09:15:55,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12828672. Throughput: 0: 273.6. Samples: 12843888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:15:55,980][166323] Avg episode reward: [(0, '1289.663')]
[36m[2025-07-02 09:16:00,997][166323] Fps is (10 sec: 1636.3, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12845056. Throughput: 0: 277.1. Samples: 12845808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:16:00,997][166323] Avg episode reward: [(0, '1319.375')]
[36m[2025-07-02 09:16:05,983][166323] Fps is (10 sec: 1637.8, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12845056. Throughput: 0: 277.2. Samples: 12846688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:16:05,983][166323] Avg episode reward: [(0, '1334.883')]
[36m[2025-07-02 09:16:10,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12845056. Throughput: 0: 279.4. Samples: 12848384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:16:10,981][166323] Avg episode reward: [(0, '1309.385')]
[36m[2025-07-02 09:16:15,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12845056. Throughput: 0: 277.4. Samples: 12849984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:16:15,995][166323] Avg episode reward: [(0, '1380.521')]
[36m[2025-07-02 09:16:20,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12845056. Throughput: 0: 279.1. Samples: 12850816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:16:20,944][166323] Avg episode reward: [(0, '1323.984')]
[36m[2025-07-02 09:16:25,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12845056. Throughput: 0: 283.3. Samples: 12852592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:16:25,995][166323] Avg episode reward: [(0, '1333.949')]
[36m[2025-07-02 09:16:30,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12845056. Throughput: 0: 283.8. Samples: 12854320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:16:30,943][166323] Avg episode reward: [(0, '1246.114')]
[36m[2025-07-02 09:16:35,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12845056. Throughput: 0: 285.8. Samples: 12855232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:16:35,994][166323] Avg episode reward: [(0, '1245.821')]
[36m[2025-07-02 09:16:40,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12845056. Throughput: 0: 288.9. Samples: 12856896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:16:40,999][166323] Avg episode reward: [(0, '1230.105')]
[31m[45791924 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[45791924 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[45791924 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:16:45,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12845056. Throughput: 0: 283.5. Samples: 12858560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:16:45,975][166323] Avg episode reward: [(0, '1198.364')]
[36m[2025-07-02 09:16:51,005][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12845056. Throughput: 0: 281.5. Samples: 12859360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:16:51,005][166323] Avg episode reward: [(0, '1179.486')]
[36m[2025-07-02 09:16:56,067][166323] Fps is (10 sec: 1623.4, 60 sec: 545.3, 300 sec: 333.1). Total num frames: 12861440. Throughput: 0: 281.8. Samples: 12861088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:16:56,068][166323] Avg episode reward: [(0, '1255.893')]
[36m[2025-07-02 09:17:00,964][166323] Fps is (10 sec: 1645.1, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12861440. Throughput: 0: 286.8. Samples: 12862880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:17:00,964][166323] Avg episode reward: [(0, '1271.932')]
[36m[2025-07-02 09:17:05,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12861440. Throughput: 0: 287.9. Samples: 12863776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:17:05,961][166323] Avg episode reward: [(0, '1314.096')]
[36m[2025-07-02 09:17:10,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12861440. Throughput: 0: 288.0. Samples: 12865536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:17:10,947][166323] Avg episode reward: [(0, '1346.435')]
[36m[2025-07-02 09:17:15,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12861440. Throughput: 0: 286.6. Samples: 12867232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:17:15,989][166323] Avg episode reward: [(0, '1334.400')]
[36m[2025-07-02 09:17:20,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12861440. Throughput: 0: 285.7. Samples: 12868080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:17:20,969][166323] Avg episode reward: [(0, '1386.624')]
[36m[2025-07-02 09:17:25,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12861440. Throughput: 0: 288.6. Samples: 12869872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:17:25,956][166323] Avg episode reward: [(0, '1418.482')]
[36m[2025-07-02 09:17:30,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12861440. Throughput: 0: 288.4. Samples: 12871536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:17:30,966][166323] Avg episode reward: [(0, '1324.570')]
[36m[2025-07-02 09:17:35,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12861440. Throughput: 0: 289.0. Samples: 12872352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:17:35,958][166323] Avg episode reward: [(0, '1306.697')]
[37m[1m[2025-07-02 09:17:36,018][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000025112_12861440.pth...
[36m[2025-07-02 09:17:36,022][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000024984_12795904.pth
[36m[2025-07-02 09:17:40,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12861440. Throughput: 0: 287.9. Samples: 12874016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:17:40,965][166323] Avg episode reward: [(0, '1302.707')]
[36m[2025-07-02 09:17:45,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12861440. Throughput: 0: 284.8. Samples: 12875696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:17:45,959][166323] Avg episode reward: [(0, '1277.135')]
[36m[2025-07-02 09:17:50,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12861440. Throughput: 0: 284.0. Samples: 12876560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:17:50,967][166323] Avg episode reward: [(0, '1292.475')]
[36m[2025-07-02 09:17:55,995][166323] Fps is (10 sec: 1632.4, 60 sec: 273.4, 300 sec: 333.2). Total num frames: 12877824. Throughput: 0: 280.6. Samples: 12878176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:17:55,996][166323] Avg episode reward: [(0, '1255.317')]
[36m[2025-07-02 09:18:00,962][166323] Fps is (10 sec: 1639.1, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12877824. Throughput: 0: 280.7. Samples: 12879856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:18:00,963][166323] Avg episode reward: [(0, '1239.788')]
[36m[2025-07-02 09:18:05,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12877824. Throughput: 0: 278.9. Samples: 12880624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:18:05,948][166323] Avg episode reward: [(0, '1278.699')]
[36m[2025-07-02 09:18:10,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12877824. Throughput: 0: 274.7. Samples: 12882240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:18:10,986][166323] Avg episode reward: [(0, '1273.133')]
[36m[2025-07-02 09:18:15,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12877824. Throughput: 0: 270.3. Samples: 12883696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:18:15,956][166323] Avg episode reward: [(0, '1265.508')]
[36m[2025-07-02 09:18:20,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12877824. Throughput: 0: 270.2. Samples: 12884512. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:18:20,966][166323] Avg episode reward: [(0, '1244.083')]
[36m[2025-07-02 09:18:25,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12877824. Throughput: 0: 272.2. Samples: 12886272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:18:25,983][166323] Avg episode reward: [(0, '1240.030')]
[36m[2025-07-02 09:18:30,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12877824. Throughput: 0: 271.3. Samples: 12887904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:18:30,962][166323] Avg episode reward: [(0, '1268.505')]
[36m[2025-07-02 09:18:36,003][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 12877824. Throughput: 0: 270.7. Samples: 12888752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:18:36,004][166323] Avg episode reward: [(0, '1260.507')]
[36m[2025-07-02 09:18:40,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12877824. Throughput: 0: 270.9. Samples: 12890352. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:18:40,950][166323] Avg episode reward: [(0, '1302.567')]
[36m[2025-07-02 09:18:45,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12877824. Throughput: 0: 273.2. Samples: 12892144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:18:45,948][166323] Avg episode reward: [(0, '1292.703')]
[36m[2025-07-02 09:18:50,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 12877824. Throughput: 0: 276.0. Samples: 12893056. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:18:50,994][166323] Avg episode reward: [(0, '1309.084')]
[36m[2025-07-02 09:18:55,963][166323] Fps is (10 sec: 1635.9, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12894208. Throughput: 0: 276.1. Samples: 12894656. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 09:18:55,963][166323] Avg episode reward: [(0, '1349.449')]
[36m[2025-07-02 09:19:00,991][166323] Fps is (10 sec: 1638.8, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12894208. Throughput: 0: 278.9. Samples: 12896256. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 09:19:00,991][166323] Avg episode reward: [(0, '1299.690')]
[36m[2025-07-02 09:19:05,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12894208. Throughput: 0: 279.9. Samples: 12897104. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 09:19:05,955][166323] Avg episode reward: [(0, '1229.438')]
[36m[2025-07-02 09:19:10,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12894208. Throughput: 0: 277.0. Samples: 12898736. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 09:19:10,978][166323] Avg episode reward: [(0, '1206.416')]
[36m[2025-07-02 09:19:15,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12894208. Throughput: 0: 279.1. Samples: 12900464. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 09:19:15,956][166323] Avg episode reward: [(0, '1215.086')]
[36m[2025-07-02 09:19:20,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12894208. Throughput: 0: 281.0. Samples: 12901392. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 09:19:20,985][166323] Avg episode reward: [(0, '1264.046')]
[36m[2025-07-02 09:19:25,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12894208. Throughput: 0: 284.1. Samples: 12903136. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 09:19:25,947][166323] Avg episode reward: [(0, '1219.492')]
[36m[2025-07-02 09:19:30,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12894208. Throughput: 0: 280.5. Samples: 12904768. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 09:19:30,946][166323] Avg episode reward: [(0, '1219.422')]
[36m[2025-07-02 09:19:35,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12894208. Throughput: 0: 280.4. Samples: 12905664. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 09:19:35,955][166323] Avg episode reward: [(0, '1278.401')]
[37m[1m[2025-07-02 09:19:36,024][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000025176_12894208.pth...
[36m[2025-07-02 09:19:36,031][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000025048_12828672.pth
[36m[2025-07-02 09:19:40,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12894208. Throughput: 0: 281.8. Samples: 12907344. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 09:19:40,990][166323] Avg episode reward: [(0, '1312.980')]
[36m[2025-07-02 09:19:45,997][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 12894208. Throughput: 0: 284.1. Samples: 12909040. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 09:19:45,997][166323] Avg episode reward: [(0, '1308.375')]
[36m[2025-07-02 09:19:50,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12894208. Throughput: 0: 281.6. Samples: 12909776. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 09:19:50,958][166323] Avg episode reward: [(0, '1262.777')]
[31m[45981796 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[45981797 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[45981797 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:19:55,962][166323] Fps is (10 sec: 1644.1, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12910592. Throughput: 0: 279.9. Samples: 12911328. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 09:19:55,962][166323] Avg episode reward: [(0, '1239.734')]
[36m[2025-07-02 09:20:00,957][166323] Fps is (10 sec: 1638.4, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12910592. Throughput: 0: 278.0. Samples: 12912976. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 09:20:00,958][166323] Avg episode reward: [(0, '1247.362')]
[36m[2025-07-02 09:20:05,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12910592. Throughput: 0: 275.5. Samples: 12913792. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 09:20:05,988][166323] Avg episode reward: [(0, '1271.542')]
[36m[2025-07-02 09:20:10,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12910592. Throughput: 0: 271.9. Samples: 12915376. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 09:20:10,958][166323] Avg episode reward: [(0, '1192.964')]
[36m[2025-07-02 09:20:15,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12910592. Throughput: 0: 270.7. Samples: 12916960. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 09:20:15,979][166323] Avg episode reward: [(0, '1190.403')]
[36m[2025-07-02 09:20:20,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12910592. Throughput: 0: 269.1. Samples: 12917776. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 09:20:20,963][166323] Avg episode reward: [(0, '1171.803')]
[36m[2025-07-02 09:20:25,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12910592. Throughput: 0: 266.7. Samples: 12919344. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 09:20:25,987][166323] Avg episode reward: [(0, '1213.706')]
[36m[2025-07-02 09:20:30,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12910592. Throughput: 0: 267.6. Samples: 12921072. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 09:20:30,957][166323] Avg episode reward: [(0, '1283.055')]
[36m[2025-07-02 09:20:35,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12910592. Throughput: 0: 269.9. Samples: 12921920. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 09:20:35,946][166323] Avg episode reward: [(0, '1290.820')]
[36m[2025-07-02 09:20:40,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12910592. Throughput: 0: 272.2. Samples: 12923584. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 09:20:40,991][166323] Avg episode reward: [(0, '1289.870')]
[36m[2025-07-02 09:20:45,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12910592. Throughput: 0: 274.0. Samples: 12925312. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 09:20:45,982][166323] Avg episode reward: [(0, '1304.721')]
[36m[2025-07-02 09:20:50,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12910592. Throughput: 0: 273.6. Samples: 12926096. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 09:20:50,963][166323] Avg episode reward: [(0, '1287.640')]
[36m[2025-07-02 09:20:55,966][166323] Fps is (10 sec: 1641.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12926976. Throughput: 0: 271.2. Samples: 12927584. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 09:20:55,966][166323] Avg episode reward: [(0, '1276.946')]
[36m[2025-07-02 09:21:00,982][166323] Fps is (10 sec: 1635.2, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12926976. Throughput: 0: 272.3. Samples: 12929216. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 09:21:00,982][166323] Avg episode reward: [(0, '1236.520')]
[36m[2025-07-02 09:21:05,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12926976. Throughput: 0: 273.0. Samples: 12930064. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 09:21:05,977][166323] Avg episode reward: [(0, '1211.727')]
[36m[2025-07-02 09:21:10,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12926976. Throughput: 0: 275.9. Samples: 12931760. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 09:21:10,983][166323] Avg episode reward: [(0, '1159.788')]
[36m[2025-07-02 09:21:15,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12926976. Throughput: 0: 275.2. Samples: 12933456. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 09:21:15,960][166323] Avg episode reward: [(0, '1166.223')]
[36m[2025-07-02 09:21:20,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12926976. Throughput: 0: 275.0. Samples: 12934304. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 09:21:20,974][166323] Avg episode reward: [(0, '1224.068')]
[36m[2025-07-02 09:21:25,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12926976. Throughput: 0: 271.8. Samples: 12935808. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 09:21:25,965][166323] Avg episode reward: [(0, '1237.040')]
[36m[2025-07-02 09:21:31,006][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 12926976. Throughput: 0: 269.0. Samples: 12937424. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 09:21:31,006][166323] Avg episode reward: [(0, '1195.133')]
[36m[2025-07-02 09:21:35,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12926976. Throughput: 0: 267.7. Samples: 12938144. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 09:21:35,961][166323] Avg episode reward: [(0, '1212.764')]
[37m[1m[2025-07-02 09:21:36,023][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000025240_12926976.pth...
[36m[2025-07-02 09:21:36,029][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000025112_12861440.pth
[31m[46085667 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[46085667 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[46085668 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:21:40,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12926976. Throughput: 0: 272.4. Samples: 12939840. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 09:21:40,954][166323] Avg episode reward: [(0, '1190.292')]
[36m[2025-07-02 09:21:45,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12926976. Throughput: 0: 272.1. Samples: 12941456. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 09:21:45,958][166323] Avg episode reward: [(0, '1261.071')]
[36m[2025-07-02 09:21:50,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 12926976. Throughput: 0: 272.3. Samples: 12942320. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 09:21:50,979][166323] Avg episode reward: [(0, '1294.263')]
[36m[2025-07-02 09:21:55,952][166323] Fps is (10 sec: 1639.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12943360. Throughput: 0: 269.0. Samples: 12943856. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 09:21:55,952][166323] Avg episode reward: [(0, '1270.012')]
[33m[46108425 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[46108425 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.8701171875
[33mCrash Rate: 0.11474609375
[33mTimeout Rate: 0.01513671875 (navigation_task.py:265)
[33m[46108425 ms][navigation_task] - WARNING : 
[33mSuccesses: 1782
[33mCrashes : 235
[33mTimeouts: 31 (navigation_task.py:268)
[36m[2025-07-02 09:22:00,955][166323] Fps is (10 sec: 1642.3, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12943360. Throughput: 0: 271.3. Samples: 12945664. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 09:22:00,955][166323] Avg episode reward: [(0, '1274.009')]
[36m[2025-07-02 09:22:05,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12943360. Throughput: 0: 271.2. Samples: 12946512. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 09:22:05,988][166323] Avg episode reward: [(0, '1323.874')]
[36m[2025-07-02 09:22:10,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12943360. Throughput: 0: 275.6. Samples: 12948208. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 09:22:10,960][166323] Avg episode reward: [(0, '1361.164')]
[36m[2025-07-02 09:22:15,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12943360. Throughput: 0: 274.5. Samples: 12949760. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 09:22:15,952][166323] Avg episode reward: [(0, '1338.034')]
[36m[2025-07-02 09:22:20,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12943360. Throughput: 0: 278.2. Samples: 12950656. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 09:22:20,943][166323] Avg episode reward: [(0, '1307.364')]
[31m[46132122 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[46132123 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[46132123 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:22:25,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12943360. Throughput: 0: 279.7. Samples: 12952432. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 09:22:25,967][166323] Avg episode reward: [(0, '1286.476')]
[36m[2025-07-02 09:22:30,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12943360. Throughput: 0: 280.0. Samples: 12954064. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 09:22:30,983][166323] Avg episode reward: [(0, '1299.416')]
[36m[2025-07-02 09:22:35,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12943360. Throughput: 0: 280.4. Samples: 12954944. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 09:22:35,996][166323] Avg episode reward: [(0, '1349.129')]
[36m[2025-07-02 09:22:40,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12943360. Throughput: 0: 280.3. Samples: 12956480. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 09:22:40,996][166323] Avg episode reward: [(0, '1297.296')]
[36m[2025-07-02 09:22:45,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12943360. Throughput: 0: 278.6. Samples: 12958208. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 09:22:45,979][166323] Avg episode reward: [(0, '1283.577')]
[36m[2025-07-02 09:22:51,051][166323] Fps is (10 sec: 1629.3, 60 sec: 545.5, 300 sec: 277.6). Total num frames: 12959744. Throughput: 0: 279.4. Samples: 12959104. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 09:22:51,052][166323] Avg episode reward: [(0, '1279.169')]
[36m[2025-07-02 09:22:55,967][166323] Fps is (10 sec: 1640.4, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12959744. Throughput: 0: 275.9. Samples: 12960624. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 09:22:55,967][166323] Avg episode reward: [(0, '1283.854')]
[36m[2025-07-02 09:23:01,000][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 12959744. Throughput: 0: 277.0. Samples: 12962240. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 09:23:01,000][166323] Avg episode reward: [(0, '1300.578')]
[36m[2025-07-02 09:23:05,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12959744. Throughput: 0: 274.4. Samples: 12963008. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 09:23:05,955][166323] Avg episode reward: [(0, '1289.750')]
[36m[2025-07-02 09:23:10,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12959744. Throughput: 0: 269.7. Samples: 12964576. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 09:23:10,993][166323] Avg episode reward: [(0, '1244.318')]
[36m[2025-07-02 09:23:15,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12959744. Throughput: 0: 270.3. Samples: 12966224. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 09:23:15,966][166323] Avg episode reward: [(0, '1293.142')]
[36m[2025-07-02 09:23:20,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12959744. Throughput: 0: 273.3. Samples: 12967232. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 09:23:20,955][166323] Avg episode reward: [(0, '1323.639')]
[36m[2025-07-02 09:23:25,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12959744. Throughput: 0: 277.0. Samples: 12968944. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 09:23:25,995][166323] Avg episode reward: [(0, '1354.959')]
[36m[2025-07-02 09:23:30,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12959744. Throughput: 0: 272.8. Samples: 12970480. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 09:23:30,958][166323] Avg episode reward: [(0, '1373.091')]
[36m[2025-07-02 09:23:35,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12959744. Throughput: 0: 273.6. Samples: 12971392. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 09:23:35,964][166323] Avg episode reward: [(0, '1348.788')]
[37m[1m[2025-07-02 09:23:36,045][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000025304_12959744.pth...
[36m[2025-07-02 09:23:36,049][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000025176_12894208.pth
[36m[2025-07-02 09:23:40,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12959744. Throughput: 0: 273.5. Samples: 12972928. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 09:23:40,950][166323] Avg episode reward: [(0, '1317.508')]
[36m[2025-07-02 09:23:45,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12959744. Throughput: 0: 272.8. Samples: 12974512. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 09:23:45,990][166323] Avg episode reward: [(0, '1337.814')]
[36m[2025-07-02 09:23:50,982][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 222.1). Total num frames: 12959744. Throughput: 0: 273.3. Samples: 12975312. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 09:23:50,982][166323] Avg episode reward: [(0, '1265.832')]
[36m[2025-07-02 09:23:55,981][166323] Fps is (10 sec: 1639.8, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12976128. Throughput: 0: 271.7. Samples: 12976800. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 09:23:55,981][166323] Avg episode reward: [(0, '1186.276')]
[36m[2025-07-02 09:24:00,965][166323] Fps is (10 sec: 1641.3, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12976128. Throughput: 0: 270.9. Samples: 12978416. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 09:24:00,965][166323] Avg episode reward: [(0, '1160.833')]
[36m[2025-07-02 09:24:05,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12976128. Throughput: 0: 265.6. Samples: 12979184. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 09:24:05,946][166323] Avg episode reward: [(0, '1145.466')]
[36m[2025-07-02 09:24:10,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12976128. Throughput: 0: 265.0. Samples: 12980864. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 09:24:10,973][166323] Avg episode reward: [(0, '1165.191')]
[36m[2025-07-02 09:24:15,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12976128. Throughput: 0: 266.2. Samples: 12982464. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 09:24:15,984][166323] Avg episode reward: [(0, '1149.941')]
[31m[46244738 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[46244738 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[46244738 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:24:20,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12976128. Throughput: 0: 264.2. Samples: 12983280. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 09:24:20,953][166323] Avg episode reward: [(0, '1116.438')]
[36m[2025-07-02 09:24:25,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12976128. Throughput: 0: 268.1. Samples: 12984992. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 09:24:25,954][166323] Avg episode reward: [(0, '1154.085')]
[36m[2025-07-02 09:24:30,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12976128. Throughput: 0: 267.9. Samples: 12986560. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 09:24:30,960][166323] Avg episode reward: [(0, '1220.975')]
[36m[2025-07-02 09:24:35,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12976128. Throughput: 0: 267.9. Samples: 12987360. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 09:24:35,948][166323] Avg episode reward: [(0, '1247.508')]
[36m[2025-07-02 09:24:40,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12976128. Throughput: 0: 271.6. Samples: 12989024. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 09:24:40,985][166323] Avg episode reward: [(0, '1269.603')]
[36m[2025-07-02 09:24:45,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 12976128. Throughput: 0: 273.5. Samples: 12990720. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 09:24:45,947][166323] Avg episode reward: [(0, '1305.507')]
[36m[2025-07-02 09:24:50,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 12976128. Throughput: 0: 274.2. Samples: 12991536. Policy #0 lag: (min: 23.0, avg: 23.0, max: 23.0)
[36m[2025-07-02 09:24:50,990][166323] Avg episode reward: [(0, '1355.823')]
[36m[2025-07-02 09:24:55,958][166323] Fps is (10 sec: 1636.4, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12992512. Throughput: 0: 271.4. Samples: 12993072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:24:55,959][166323] Avg episode reward: [(0, '1378.963')]
[36m[2025-07-02 09:25:00,976][166323] Fps is (10 sec: 1640.6, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12992512. Throughput: 0: 272.8. Samples: 12994736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:25:00,977][166323] Avg episode reward: [(0, '1345.806')]
[31m[46294498 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[46294498 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[46294499 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:25:05,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12992512. Throughput: 0: 273.9. Samples: 12995616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:25:05,989][166323] Avg episode reward: [(0, '1290.393')]
[36m[2025-07-02 09:25:10,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12992512. Throughput: 0: 273.1. Samples: 12997280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:25:10,950][166323] Avg episode reward: [(0, '1268.812')]
[31m[46302812 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[46302812 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[46302812 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:25:15,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 12992512. Throughput: 0: 274.9. Samples: 12998928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:25:15,945][166323] Avg episode reward: [(0, '1248.346')]
[36m[2025-07-02 09:25:20,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12992512. Throughput: 0: 274.8. Samples: 12999728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:25:20,955][166323] Avg episode reward: [(0, '1282.383')]
[36m[2025-07-02 09:25:25,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12992512. Throughput: 0: 271.0. Samples: 13001216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:25:25,974][166323] Avg episode reward: [(0, '1230.793')]
[36m[2025-07-02 09:25:30,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 12992512. Throughput: 0: 271.2. Samples: 13002928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:25:30,963][166323] Avg episode reward: [(0, '1263.856')]
[31m[46323680 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[46323680 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[46323680 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:25:35,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12992512. Throughput: 0: 270.8. Samples: 13003712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:25:35,946][166323] Avg episode reward: [(0, '1263.753')]
[37m[1m[2025-07-02 09:25:36,022][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000025368_12992512.pth...
[36m[2025-07-02 09:25:36,028][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000025240_12926976.pth
[36m[2025-07-02 09:25:40,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 12992512. Throughput: 0: 272.2. Samples: 13005328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:25:40,985][166323] Avg episode reward: [(0, '1255.743')]
[36m[2025-07-02 09:25:45,973][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 12992512. Throughput: 0: 269.9. Samples: 13006880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:25:45,973][166323] Avg episode reward: [(0, '1298.852')]
[36m[2025-07-02 09:25:50,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 222.2). Total num frames: 12992512. Throughput: 0: 269.1. Samples: 13007712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:25:50,943][166323] Avg episode reward: [(0, '1261.940')]
[36m[2025-07-02 09:25:55,962][166323] Fps is (10 sec: 1640.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13008896. Throughput: 0: 267.7. Samples: 13009328. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 09:25:55,962][166323] Avg episode reward: [(0, '1232.419')]
[31m[46349033 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[46349034 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[46349034 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:26:00,967][166323] Fps is (10 sec: 1634.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13008896. Throughput: 0: 267.2. Samples: 13010960. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 09:26:00,967][166323] Avg episode reward: [(0, '1261.005')]
[36m[2025-07-02 09:26:05,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13008896. Throughput: 0: 269.6. Samples: 13011872. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 09:26:05,999][166323] Avg episode reward: [(0, '1265.869')]
[31m[46358260 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[46358260 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[46358260 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:26:10,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13008896. Throughput: 0: 273.6. Samples: 13013520. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 09:26:10,946][166323] Avg episode reward: [(0, '1235.586')]
[36m[2025-07-02 09:26:15,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13008896. Throughput: 0: 276.5. Samples: 13015376. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 09:26:15,989][166323] Avg episode reward: [(0, '1213.385')]
[36m[2025-07-02 09:26:20,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13008896. Throughput: 0: 278.0. Samples: 13016224. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 09:26:20,957][166323] Avg episode reward: [(0, '1240.216')]
[36m[2025-07-02 09:26:25,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13008896. Throughput: 0: 277.1. Samples: 13017792. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 09:26:25,962][166323] Avg episode reward: [(0, '1222.930')]
[36m[2025-07-02 09:26:30,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13008896. Throughput: 0: 280.2. Samples: 13019488. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 09:26:30,969][166323] Avg episode reward: [(0, '1211.185')]
[36m[2025-07-02 09:26:36,021][166323] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 277.6). Total num frames: 13008896. Throughput: 0: 279.7. Samples: 13020320. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 09:26:36,022][166323] Avg episode reward: [(0, '1237.863')]
[36m[2025-07-02 09:26:40,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13008896. Throughput: 0: 279.9. Samples: 13021920. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 09:26:40,958][166323] Avg episode reward: [(0, '1249.656')]
[36m[2025-07-02 09:26:45,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13008896. Throughput: 0: 280.5. Samples: 13023584. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 09:26:45,973][166323] Avg episode reward: [(0, '1280.468')]
[36m[2025-07-02 09:26:50,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 13008896. Throughput: 0: 278.8. Samples: 13024416. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 09:26:50,985][166323] Avg episode reward: [(0, '1320.087')]
[36m[2025-07-02 09:26:55,985][166323] Fps is (10 sec: 1636.3, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13025280. Throughput: 0: 278.2. Samples: 13026048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:26:55,986][166323] Avg episode reward: [(0, '1258.345')]
[36m[2025-07-02 09:27:00,995][166323] Fps is (10 sec: 1636.8, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13025280. Throughput: 0: 272.3. Samples: 13027632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:27:00,995][166323] Avg episode reward: [(0, '1334.569')]
[36m[2025-07-02 09:27:05,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13025280. Throughput: 0: 271.9. Samples: 13028464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:27:05,967][166323] Avg episode reward: [(0, '1307.732')]
[36m[2025-07-02 09:27:10,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13025280. Throughput: 0: 272.7. Samples: 13030064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:27:10,971][166323] Avg episode reward: [(0, '1268.605')]
[36m[2025-07-02 09:27:15,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13025280. Throughput: 0: 273.8. Samples: 13031808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:27:15,970][166323] Avg episode reward: [(0, '1256.754')]
[36m[2025-07-02 09:27:20,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13025280. Throughput: 0: 276.6. Samples: 13032752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:27:20,973][166323] Avg episode reward: [(0, '1221.641')]
[31m[46431795 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[46431795 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[46431795 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:27:25,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13025280. Throughput: 0: 279.4. Samples: 13034496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:27:25,964][166323] Avg episode reward: [(0, '1214.497')]
[36m[2025-07-02 09:27:30,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13025280. Throughput: 0: 279.5. Samples: 13036160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:27:30,962][166323] Avg episode reward: [(0, '1249.470')]
[36m[2025-07-02 09:27:35,995][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13025280. Throughput: 0: 278.3. Samples: 13036944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:27:35,996][166323] Avg episode reward: [(0, '1253.055')]
[37m[1m[2025-07-02 09:27:36,089][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000025432_13025280.pth...
[36m[2025-07-02 09:27:36,093][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000025304_12959744.pth
[36m[2025-07-02 09:27:40,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13025280. Throughput: 0: 279.7. Samples: 13038624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:27:40,947][166323] Avg episode reward: [(0, '1224.656')]
[36m[2025-07-02 09:27:45,998][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 13025280. Throughput: 0: 278.0. Samples: 13040144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:27:45,998][166323] Avg episode reward: [(0, '1184.509')]
[36m[2025-07-02 09:27:50,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.1). Total num frames: 13025280. Throughput: 0: 276.9. Samples: 13040928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:27:50,982][166323] Avg episode reward: [(0, '1173.964')]
[36m[2025-07-02 09:27:55,993][166323] Fps is (10 sec: 1639.2, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13041664. Throughput: 0: 273.3. Samples: 13042368. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:27:55,993][166323] Avg episode reward: [(0, '1180.892')]
[36m[2025-07-02 09:28:00,944][166323] Fps is (10 sec: 1644.6, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13041664. Throughput: 0: 272.5. Samples: 13044064. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:28:00,945][166323] Avg episode reward: [(0, '1223.099')]
[36m[2025-07-02 09:28:05,998][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13041664. Throughput: 0: 268.3. Samples: 13044832. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:28:05,998][166323] Avg episode reward: [(0, '1260.559')]
[36m[2025-07-02 09:28:10,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13041664. Throughput: 0: 266.4. Samples: 13046480. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:28:10,952][166323] Avg episode reward: [(0, '1278.608')]
[36m[2025-07-02 09:28:16,012][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 13041664. Throughput: 0: 267.1. Samples: 13048192. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:28:16,012][166323] Avg episode reward: [(0, '1327.247')]
[36m[2025-07-02 09:28:20,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13041664. Throughput: 0: 268.0. Samples: 13048992. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:28:20,958][166323] Avg episode reward: [(0, '1378.405')]
[36m[2025-07-02 09:28:25,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13041664. Throughput: 0: 265.9. Samples: 13050592. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:28:25,953][166323] Avg episode reward: [(0, '1410.990')]
[36m[2025-07-02 09:28:30,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13041664. Throughput: 0: 270.3. Samples: 13052304. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:28:30,990][166323] Avg episode reward: [(0, '1412.627')]
[36m[2025-07-02 09:28:35,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13041664. Throughput: 0: 271.4. Samples: 13053136. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:28:35,968][166323] Avg episode reward: [(0, '1463.917')]
[37m[1m[2025-07-02 09:28:36,037][166323] Saving new best policy, reward=1463.917!
[36m[2025-07-02 09:28:40,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13041664. Throughput: 0: 273.7. Samples: 13054672. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:28:40,953][166323] Avg episode reward: [(0, '1415.873')]
[36m[2025-07-02 09:28:45,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13041664. Throughput: 0: 271.2. Samples: 13056272. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:28:45,955][166323] Avg episode reward: [(0, '1408.658')]
[36m[2025-07-02 09:28:51,005][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 13041664. Throughput: 0: 273.7. Samples: 13057152. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:28:51,005][166323] Avg episode reward: [(0, '1379.605')]
[36m[2025-07-02 09:28:55,977][166323] Fps is (10 sec: 1634.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13058048. Throughput: 0: 269.7. Samples: 13058624. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 09:28:55,977][166323] Avg episode reward: [(0, '1367.257')]
[36m[2025-07-02 09:29:00,959][166323] Fps is (10 sec: 1646.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13058048. Throughput: 0: 270.2. Samples: 13060336. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 09:29:00,959][166323] Avg episode reward: [(0, '1347.720')]
[36m[2025-07-02 09:29:05,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13058048. Throughput: 0: 272.4. Samples: 13061248. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 09:29:05,944][166323] Avg episode reward: [(0, '1360.898')]
[36m[2025-07-02 09:29:10,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13058048. Throughput: 0: 275.3. Samples: 13062992. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 09:29:10,996][166323] Avg episode reward: [(0, '1326.230')]
[36m[2025-07-02 09:29:15,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13058048. Throughput: 0: 276.6. Samples: 13064752. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 09:29:15,996][166323] Avg episode reward: [(0, '1344.750')]
[36m[2025-07-02 09:29:20,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13058048. Throughput: 0: 276.5. Samples: 13065584. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 09:29:20,986][166323] Avg episode reward: [(0, '1340.271')]
[36m[2025-07-02 09:29:25,983][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13058048. Throughput: 0: 280.7. Samples: 13067312. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 09:29:25,984][166323] Avg episode reward: [(0, '1340.622')]
[36m[2025-07-02 09:29:30,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13058048. Throughput: 0: 283.1. Samples: 13069008. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 09:29:30,944][166323] Avg episode reward: [(0, '1366.112')]
[36m[2025-07-02 09:29:35,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13058048. Throughput: 0: 282.3. Samples: 13069840. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 09:29:35,946][166323] Avg episode reward: [(0, '1371.620')]
[37m[1m[2025-07-02 09:29:36,004][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000025496_13058048.pth...
[36m[2025-07-02 09:29:36,009][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000025368_12992512.pth
[36m[2025-07-02 09:29:41,009][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 13058048. Throughput: 0: 283.5. Samples: 13071392. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 09:29:41,010][166323] Avg episode reward: [(0, '1353.546')]
[36m[2025-07-02 09:29:45,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13058048. Throughput: 0: 281.6. Samples: 13073008. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-02 09:29:45,956][166323] Avg episode reward: [(0, '1371.286')]
[36m[2025-07-02 09:29:51,125][166323] Fps is (10 sec: 1619.6, 60 sec: 545.0, 300 sec: 277.5). Total num frames: 13074432. Throughput: 0: 278.0. Samples: 13073808. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 09:29:51,126][166323] Avg episode reward: [(0, '1346.008')]
[36m[2025-07-02 09:29:55,969][166323] Fps is (10 sec: 1636.1, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13074432. Throughput: 0: 276.4. Samples: 13075424. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 09:29:55,970][166323] Avg episode reward: [(0, '1328.271')]
[36m[2025-07-02 09:30:00,995][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13074432. Throughput: 0: 271.6. Samples: 13076976. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 09:30:00,996][166323] Avg episode reward: [(0, '1298.031')]
[36m[2025-07-02 09:30:06,008][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 13074432. Throughput: 0: 270.1. Samples: 13077744. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 09:30:06,008][166323] Avg episode reward: [(0, '1271.162')]
[36m[2025-07-02 09:30:11,007][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 13074432. Throughput: 0: 269.0. Samples: 13079424. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 09:30:11,008][166323] Avg episode reward: [(0, '1277.633')]
[36m[2025-07-02 09:30:15,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13074432. Throughput: 0: 266.3. Samples: 13080992. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 09:30:15,950][166323] Avg episode reward: [(0, '1296.048')]
[36m[2025-07-02 09:30:20,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13074432. Throughput: 0: 268.4. Samples: 13081920. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 09:30:20,951][166323] Avg episode reward: [(0, '1267.577')]
[36m[2025-07-02 09:30:26,000][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13074432. Throughput: 0: 268.1. Samples: 13083456. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 09:30:26,001][166323] Avg episode reward: [(0, '1258.765')]
[36m[2025-07-02 09:30:30,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13074432. Throughput: 0: 267.5. Samples: 13085056. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 09:30:30,987][166323] Avg episode reward: [(0, '1273.640')]
[36m[2025-07-02 09:30:35,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13074432. Throughput: 0: 267.8. Samples: 13085824. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 09:30:35,992][166323] Avg episode reward: [(0, '1312.131')]
[36m[2025-07-02 09:30:40,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13074432. Throughput: 0: 267.6. Samples: 13087472. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 09:30:40,987][166323] Avg episode reward: [(0, '1345.314')]
[36m[2025-07-02 09:30:45,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13074432. Throughput: 0: 272.3. Samples: 13089216. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 09:30:45,951][166323] Avg episode reward: [(0, '1318.695')]
[36m[2025-07-02 09:30:50,977][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 222.1). Total num frames: 13074432. Throughput: 0: 272.9. Samples: 13090016. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 09:30:50,978][166323] Avg episode reward: [(0, '1306.699')]
[36m[2025-07-02 09:30:55,966][166323] Fps is (10 sec: 1635.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13090816. Throughput: 0: 269.8. Samples: 13091552. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 09:30:55,967][166323] Avg episode reward: [(0, '1292.999')]
[36m[2025-07-02 09:31:00,971][166323] Fps is (10 sec: 1639.3, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13090816. Throughput: 0: 270.4. Samples: 13093168. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 09:31:00,972][166323] Avg episode reward: [(0, '1310.657')]
[36m[2025-07-02 09:31:05,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13090816. Throughput: 0: 267.6. Samples: 13093968. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 09:31:05,970][166323] Avg episode reward: [(0, '1318.500')]
[36m[2025-07-02 09:31:10,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13090816. Throughput: 0: 272.3. Samples: 13095696. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 09:31:10,954][166323] Avg episode reward: [(0, '1274.436')]
[36m[2025-07-02 09:31:15,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13090816. Throughput: 0: 275.3. Samples: 13097440. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 09:31:15,963][166323] Avg episode reward: [(0, '1289.050')]
[36m[2025-07-02 09:31:20,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13090816. Throughput: 0: 276.3. Samples: 13098256. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 09:31:20,988][166323] Avg episode reward: [(0, '1298.809')]
[36m[2025-07-02 09:31:25,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13090816. Throughput: 0: 274.7. Samples: 13099824. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 09:31:25,956][166323] Avg episode reward: [(0, '1272.470')]
[36m[2025-07-02 09:31:30,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13090816. Throughput: 0: 271.9. Samples: 13101456. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 09:31:30,974][166323] Avg episode reward: [(0, '1309.629')]
[36m[2025-07-02 09:31:35,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13090816. Throughput: 0: 274.5. Samples: 13102368. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 09:31:35,978][166323] Avg episode reward: [(0, '1354.075')]
[37m[1m[2025-07-02 09:31:36,035][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000025560_13090816.pth...
[36m[2025-07-02 09:31:36,042][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000025432_13025280.pth
[31m[46688767 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[46688768 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[46688768 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:31:40,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13090816. Throughput: 0: 277.6. Samples: 13104048. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 09:31:40,977][166323] Avg episode reward: [(0, '1311.787')]
[36m[2025-07-02 09:31:45,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13090816. Throughput: 0: 277.7. Samples: 13105664. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 09:31:45,966][166323] Avg episode reward: [(0, '1322.430')]
[36m[2025-07-02 09:31:50,994][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 13090816. Throughput: 0: 278.6. Samples: 13106512. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-02 09:31:50,995][166323] Avg episode reward: [(0, '1311.492')]
[31m[46702334 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[46702335 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[46702335 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:31:55,984][166323] Fps is (10 sec: 1635.3, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13107200. Throughput: 0: 273.2. Samples: 13108000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:31:55,984][166323] Avg episode reward: [(0, '1307.738')]
[36m[2025-07-02 09:32:00,955][166323] Fps is (10 sec: 1644.9, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13107200. Throughput: 0: 272.0. Samples: 13109680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:32:00,956][166323] Avg episode reward: [(0, '1303.585')]
[36m[2025-07-02 09:32:05,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13107200. Throughput: 0: 273.6. Samples: 13110560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:32:05,957][166323] Avg episode reward: [(0, '1258.205')]
[36m[2025-07-02 09:32:10,983][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13107200. Throughput: 0: 275.4. Samples: 13112224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:32:10,983][166323] Avg episode reward: [(0, '1229.178')]
[31m[46723241 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[46723241 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[46723242 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:32:15,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13107200. Throughput: 0: 275.1. Samples: 13113840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:32:15,988][166323] Avg episode reward: [(0, '1209.139')]
[31m[46729248 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[46729249 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[46729249 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:32:20,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13107200. Throughput: 0: 272.4. Samples: 13114624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:32:20,962][166323] Avg episode reward: [(0, '1251.953')]
[36m[2025-07-02 09:32:25,983][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13107200. Throughput: 0: 271.6. Samples: 13116272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:32:25,984][166323] Avg episode reward: [(0, '1246.536')]
[36m[2025-07-02 09:32:30,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13107200. Throughput: 0: 272.5. Samples: 13117920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:32:30,945][166323] Avg episode reward: [(0, '1300.649')]
[36m[2025-07-02 09:32:35,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13107200. Throughput: 0: 273.1. Samples: 13118800. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:32:35,985][166323] Avg episode reward: [(0, '1333.941')]
[36m[2025-07-02 09:32:40,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13107200. Throughput: 0: 276.0. Samples: 13120416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:32:40,964][166323] Avg episode reward: [(0, '1326.298')]
[36m[2025-07-02 09:32:45,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13107200. Throughput: 0: 277.1. Samples: 13122160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:32:45,989][166323] Avg episode reward: [(0, '1303.127')]
[36m[2025-07-02 09:32:51,033][166323] Fps is (10 sec: 1627.1, 60 sec: 545.8, 300 sec: 277.7). Total num frames: 13123584. Throughput: 0: 275.4. Samples: 13122976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:32:51,034][166323] Avg episode reward: [(0, '1315.825')]
[36m[2025-07-02 09:32:55,995][166323] Fps is (10 sec: 1637.4, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 13123584. Throughput: 0: 275.5. Samples: 13124624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:32:55,995][166323] Avg episode reward: [(0, '1304.670')]
[36m[2025-07-02 09:33:00,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13123584. Throughput: 0: 276.7. Samples: 13126288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:33:00,975][166323] Avg episode reward: [(0, '1268.173')]
[36m[2025-07-02 09:33:05,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13123584. Throughput: 0: 277.8. Samples: 13127120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:33:05,948][166323] Avg episode reward: [(0, '1240.469')]
[36m[2025-07-02 09:33:10,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13123584. Throughput: 0: 274.8. Samples: 13128640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:33:10,990][166323] Avg episode reward: [(0, '1239.921')]
[36m[2025-07-02 09:33:15,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13123584. Throughput: 0: 277.4. Samples: 13130416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:33:15,988][166323] Avg episode reward: [(0, '1280.021')]
[36m[2025-07-02 09:33:20,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13123584. Throughput: 0: 275.9. Samples: 13131216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:33:20,991][166323] Avg episode reward: [(0, '1263.295')]
[36m[2025-07-02 09:33:25,996][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13123584. Throughput: 0: 278.6. Samples: 13132960. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:33:25,996][166323] Avg episode reward: [(0, '1287.227')]
[36m[2025-07-02 09:33:30,970][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13123584. Throughput: 0: 277.1. Samples: 13134624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:33:30,970][166323] Avg episode reward: [(0, '1278.340')]
[36m[2025-07-02 09:33:35,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13123584. Throughput: 0: 277.6. Samples: 13135456. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:33:35,986][166323] Avg episode reward: [(0, '1253.295')]
[37m[1m[2025-07-02 09:33:36,043][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000025624_13123584.pth...
[36m[2025-07-02 09:33:36,047][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000025496_13058048.pth
[33m[46807602 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[46807602 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.875
[33mCrash Rate: 0.11474609375
[33mTimeout Rate: 0.01025390625 (navigation_task.py:265)
[33m[46807602 ms][navigation_task] - WARNING : 
[33mSuccesses: 1792
[33mCrashes : 235
[33mTimeouts: 21 (navigation_task.py:268)
[36m[2025-07-02 09:33:40,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13123584. Throughput: 0: 275.4. Samples: 13137008. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:33:40,966][166323] Avg episode reward: [(0, '1306.845')]
[36m[2025-07-02 09:33:45,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13123584. Throughput: 0: 274.7. Samples: 13138640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:33:45,949][166323] Avg episode reward: [(0, '1297.814')]
[31m[46814581 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[46814581 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[46814582 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:33:50,959][166323] Fps is (10 sec: 1639.5, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 13139968. Throughput: 0: 275.8. Samples: 13139536. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 09:33:50,959][166323] Avg episode reward: [(0, '1294.936')]
[36m[2025-07-02 09:33:55,963][166323] Fps is (10 sec: 1636.1, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13139968. Throughput: 0: 278.2. Samples: 13141152. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 09:33:55,963][166323] Avg episode reward: [(0, '1310.236')]
[36m[2025-07-02 09:34:00,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13139968. Throughput: 0: 274.4. Samples: 13142752. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 09:34:00,947][166323] Avg episode reward: [(0, '1289.286')]
[36m[2025-07-02 09:34:05,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13139968. Throughput: 0: 276.2. Samples: 13143632. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 09:34:05,948][166323] Avg episode reward: [(0, '1253.887')]
[36m[2025-07-02 09:34:10,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13139968. Throughput: 0: 273.8. Samples: 13145280. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 09:34:10,990][166323] Avg episode reward: [(0, '1313.231')]
[36m[2025-07-02 09:34:15,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13139968. Throughput: 0: 276.6. Samples: 13147072. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 09:34:15,977][166323] Avg episode reward: [(0, '1268.643')]
[36m[2025-07-02 09:34:20,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13139968. Throughput: 0: 278.0. Samples: 13147968. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 09:34:20,985][166323] Avg episode reward: [(0, '1295.897')]
[36m[2025-07-02 09:34:25,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13139968. Throughput: 0: 276.7. Samples: 13149456. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 09:34:25,956][166323] Avg episode reward: [(0, '1236.352')]
[36m[2025-07-02 09:34:30,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13139968. Throughput: 0: 278.2. Samples: 13151168. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 09:34:30,975][166323] Avg episode reward: [(0, '1249.089')]
[36m[2025-07-02 09:34:35,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13139968. Throughput: 0: 276.8. Samples: 13152000. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 09:34:35,989][166323] Avg episode reward: [(0, '1324.059')]
[36m[2025-07-02 09:34:40,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13139968. Throughput: 0: 276.1. Samples: 13153584. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 09:34:40,983][166323] Avg episode reward: [(0, '1339.762')]
[36m[2025-07-02 09:34:45,996][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 222.3). Total num frames: 13139968. Throughput: 0: 278.1. Samples: 13155280. Policy #0 lag: (min: 12.0, avg: 12.0, max: 12.0)
[36m[2025-07-02 09:34:45,997][166323] Avg episode reward: [(0, '1336.251')]
[36m[2025-07-02 09:34:50,945][166323] Fps is (10 sec: 1644.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13156352. Throughput: 0: 277.0. Samples: 13156096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:34:50,945][166323] Avg episode reward: [(0, '1369.068')]
[36m[2025-07-02 09:34:55,963][166323] Fps is (10 sec: 1644.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13156352. Throughput: 0: 274.3. Samples: 13157616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:34:55,963][166323] Avg episode reward: [(0, '1389.508')]
[36m[2025-07-02 09:35:01,005][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 13156352. Throughput: 0: 271.1. Samples: 13159280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:35:01,005][166323] Avg episode reward: [(0, '1413.249')]
[36m[2025-07-02 09:35:05,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13156352. Throughput: 0: 268.4. Samples: 13160048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:35:05,984][166323] Avg episode reward: [(0, '1442.081')]
[36m[2025-07-02 09:35:10,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13156352. Throughput: 0: 270.1. Samples: 13161616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:35:10,978][166323] Avg episode reward: [(0, '1394.275')]
[36m[2025-07-02 09:35:15,997][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13156352. Throughput: 0: 269.7. Samples: 13163312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:35:15,998][166323] Avg episode reward: [(0, '1324.291')]
[36m[2025-07-02 09:35:20,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13156352. Throughput: 0: 267.8. Samples: 13164048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:35:20,981][166323] Avg episode reward: [(0, '1273.843')]
[36m[2025-07-02 09:35:25,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13156352. Throughput: 0: 270.8. Samples: 13165760. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:35:25,951][166323] Avg episode reward: [(0, '1254.141')]
[31m[46917597 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[46917597 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[46917598 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:35:30,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13156352. Throughput: 0: 271.9. Samples: 13167504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:35:30,954][166323] Avg episode reward: [(0, '1210.726')]
[36m[2025-07-02 09:35:35,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13156352. Throughput: 0: 269.8. Samples: 13168240. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:35:35,949][166323] Avg episode reward: [(0, '1201.943')]
[37m[1m[2025-07-02 09:35:36,010][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000025688_13156352.pth...
[36m[2025-07-02 09:35:36,015][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000025560_13090816.pth
[36m[2025-07-02 09:35:40,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13156352. Throughput: 0: 272.7. Samples: 13169888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:35:40,966][166323] Avg episode reward: [(0, '1196.366')]
[36m[2025-07-02 09:35:45,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13156352. Throughput: 0: 273.5. Samples: 13171584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:35:45,985][166323] Avg episode reward: [(0, '1240.525')]
[36m[2025-07-02 09:35:50,951][166323] Fps is (10 sec: 1640.7, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13172736. Throughput: 0: 275.4. Samples: 13172432. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 09:35:50,952][166323] Avg episode reward: [(0, '1288.124')]
[36m[2025-07-02 09:35:55,996][166323] Fps is (10 sec: 1636.6, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13172736. Throughput: 0: 274.0. Samples: 13173952. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 09:35:55,996][166323] Avg episode reward: [(0, '1272.080')]
[36m[2025-07-02 09:36:00,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13172736. Throughput: 0: 269.4. Samples: 13175424. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 09:36:00,949][166323] Avg episode reward: [(0, '1290.212')]
[36m[2025-07-02 09:36:05,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13172736. Throughput: 0: 270.6. Samples: 13176224. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 09:36:05,980][166323] Avg episode reward: [(0, '1265.548')]
[36m[2025-07-02 09:36:10,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13172736. Throughput: 0: 272.4. Samples: 13178016. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 09:36:10,945][166323] Avg episode reward: [(0, '1222.094')]
[36m[2025-07-02 09:36:15,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13172736. Throughput: 0: 273.3. Samples: 13179808. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 09:36:15,976][166323] Avg episode reward: [(0, '1258.203')]
[36m[2025-07-02 09:36:20,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13172736. Throughput: 0: 276.8. Samples: 13180704. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 09:36:20,982][166323] Avg episode reward: [(0, '1245.112')]
[31m[46974316 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[46974317 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[46974317 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:36:25,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13172736. Throughput: 0: 275.6. Samples: 13182288. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 09:36:25,953][166323] Avg episode reward: [(0, '1247.896')]
[36m[2025-07-02 09:36:30,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13172736. Throughput: 0: 272.7. Samples: 13183856. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 09:36:30,985][166323] Avg episode reward: [(0, '1266.663')]
[36m[2025-07-02 09:36:35,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13172736. Throughput: 0: 272.7. Samples: 13184704. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 09:36:35,948][166323] Avg episode reward: [(0, '1287.963')]
[36m[2025-07-02 09:36:40,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13172736. Throughput: 0: 276.7. Samples: 13186400. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 09:36:40,977][166323] Avg episode reward: [(0, '1305.185')]
[36m[2025-07-02 09:36:45,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13172736. Throughput: 0: 282.0. Samples: 13188112. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-02 09:36:45,948][166323] Avg episode reward: [(0, '1363.369')]
[36m[2025-07-02 09:36:50,989][166323] Fps is (10 sec: 1636.3, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13189120. Throughput: 0: 282.2. Samples: 13188928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:36:50,990][166323] Avg episode reward: [(0, '1371.592')]
[36m[2025-07-02 09:36:56,016][166323] Fps is (10 sec: 1627.4, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 13189120. Throughput: 0: 277.6. Samples: 13190528. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:36:56,016][166323] Avg episode reward: [(0, '1382.128')]
[36m[2025-07-02 09:37:00,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13189120. Throughput: 0: 275.6. Samples: 13192208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:37:00,973][166323] Avg episode reward: [(0, '1371.800')]
[36m[2025-07-02 09:37:06,020][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13189120. Throughput: 0: 275.0. Samples: 13193088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:37:06,020][166323] Avg episode reward: [(0, '1324.674')]
[36m[2025-07-02 09:37:10,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13189120. Throughput: 0: 274.9. Samples: 13194656. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:37:10,946][166323] Avg episode reward: [(0, '1321.012')]
[36m[2025-07-02 09:37:15,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13189120. Throughput: 0: 277.3. Samples: 13196336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:37:15,990][166323] Avg episode reward: [(0, '1316.249')]
[36m[2025-07-02 09:37:20,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13189120. Throughput: 0: 275.2. Samples: 13197088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:37:20,948][166323] Avg episode reward: [(0, '1328.300')]
[36m[2025-07-02 09:37:25,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13189120. Throughput: 0: 272.7. Samples: 13198672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:37:25,984][166323] Avg episode reward: [(0, '1297.673')]
[36m[2025-07-02 09:37:30,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13189120. Throughput: 0: 270.2. Samples: 13200272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:37:30,947][166323] Avg episode reward: [(0, '1266.112')]
[36m[2025-07-02 09:37:35,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13189120. Throughput: 0: 271.6. Samples: 13201136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:37:35,946][166323] Avg episode reward: [(0, '1290.834')]
[37m[1m[2025-07-02 09:37:35,998][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000025752_13189120.pth...
[36m[2025-07-02 09:37:36,003][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000025624_13123584.pth
[36m[2025-07-02 09:37:40,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13189120. Throughput: 0: 274.1. Samples: 13202848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:37:40,971][166323] Avg episode reward: [(0, '1372.028')]
[36m[2025-07-02 09:37:45,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 13189120. Throughput: 0: 269.5. Samples: 13204336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:37:45,970][166323] Avg episode reward: [(0, '1363.429')]
[36m[2025-07-02 09:37:50,978][166323] Fps is (10 sec: 1637.3, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13205504. Throughput: 0: 268.3. Samples: 13205152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:37:50,978][166323] Avg episode reward: [(0, '1380.350')]
[36m[2025-07-02 09:37:55,948][166323] Fps is (10 sec: 1641.9, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 13205504. Throughput: 0: 268.8. Samples: 13206752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:37:55,949][166323] Avg episode reward: [(0, '1356.304')]
[36m[2025-07-02 09:38:00,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13205504. Throughput: 0: 265.3. Samples: 13208272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:38:00,973][166323] Avg episode reward: [(0, '1373.335')]
[36m[2025-07-02 09:38:05,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13205504. Throughput: 0: 266.9. Samples: 13209104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:38:05,965][166323] Avg episode reward: [(0, '1414.292')]
[36m[2025-07-02 09:38:10,983][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13205504. Throughput: 0: 268.8. Samples: 13210768. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:38:10,984][166323] Avg episode reward: [(0, '1429.699')]
[36m[2025-07-02 09:38:15,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13205504. Throughput: 0: 271.5. Samples: 13212496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:38:15,975][166323] Avg episode reward: [(0, '1418.632')]
[36m[2025-07-02 09:38:20,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13205504. Throughput: 0: 271.9. Samples: 13213376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:38:20,967][166323] Avg episode reward: [(0, '1374.323')]
[36m[2025-07-02 09:38:25,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13205504. Throughput: 0: 270.9. Samples: 13215040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:38:25,982][166323] Avg episode reward: [(0, '1355.063')]
[36m[2025-07-02 09:38:30,998][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 13205504. Throughput: 0: 273.6. Samples: 13216656. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:38:30,999][166323] Avg episode reward: [(0, '1326.621')]
[36m[2025-07-02 09:38:35,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13205504. Throughput: 0: 272.8. Samples: 13217424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:38:35,960][166323] Avg episode reward: [(0, '1318.467')]
[31m[47106938 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[47106938 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[47106938 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:38:40,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13205504. Throughput: 0: 272.0. Samples: 13218992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:38:40,945][166323] Avg episode reward: [(0, '1257.522')]
[36m[2025-07-02 09:38:45,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 13205504. Throughput: 0: 273.0. Samples: 13220560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:38:45,986][166323] Avg episode reward: [(0, '1252.576')]
[36m[2025-07-02 09:38:50,990][166323] Fps is (10 sec: 1630.9, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13221888. Throughput: 0: 273.3. Samples: 13221408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:38:50,991][166323] Avg episode reward: [(0, '1208.620')]
[36m[2025-07-02 09:38:55,954][166323] Fps is (10 sec: 1643.7, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13221888. Throughput: 0: 271.5. Samples: 13222976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:38:55,954][166323] Avg episode reward: [(0, '1229.710')]
[36m[2025-07-02 09:39:00,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13221888. Throughput: 0: 271.1. Samples: 13224688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:39:00,954][166323] Avg episode reward: [(0, '1227.505')]
[36m[2025-07-02 09:39:05,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13221888. Throughput: 0: 270.2. Samples: 13225536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:39:05,969][166323] Avg episode reward: [(0, '1269.014')]
[36m[2025-07-02 09:39:10,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13221888. Throughput: 0: 270.5. Samples: 13227216. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:39:10,988][166323] Avg episode reward: [(0, '1273.253')]
[36m[2025-07-02 09:39:16,000][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13221888. Throughput: 0: 272.0. Samples: 13228896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:39:16,000][166323] Avg episode reward: [(0, '1297.633')]
[36m[2025-07-02 09:39:20,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13221888. Throughput: 0: 273.0. Samples: 13229712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:39:20,976][166323] Avg episode reward: [(0, '1305.645')]
[36m[2025-07-02 09:39:25,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13221888. Throughput: 0: 274.0. Samples: 13231328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:39:25,967][166323] Avg episode reward: [(0, '1358.014')]
[36m[2025-07-02 09:39:30,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13221888. Throughput: 0: 276.0. Samples: 13232976. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:39:30,978][166323] Avg episode reward: [(0, '1353.511')]
[36m[2025-07-02 09:39:35,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13221888. Throughput: 0: 276.4. Samples: 13233840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:39:35,962][166323] Avg episode reward: [(0, '1377.834')]
[37m[1m[2025-07-02 09:39:36,012][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000025816_13221888.pth...
[36m[2025-07-02 09:39:36,016][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000025688_13156352.pth
[36m[2025-07-02 09:39:40,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13221888. Throughput: 0: 276.2. Samples: 13235408. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:39:40,970][166323] Avg episode reward: [(0, '1379.348')]
[36m[2025-07-02 09:39:45,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.1). Total num frames: 13221888. Throughput: 0: 272.2. Samples: 13236944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:39:45,985][166323] Avg episode reward: [(0, '1316.767')]
[36m[2025-07-02 09:39:51,011][166323] Fps is (10 sec: 1631.6, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 13238272. Throughput: 0: 273.5. Samples: 13237856. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 09:39:51,012][166323] Avg episode reward: [(0, '1289.368')]
[36m[2025-07-02 09:39:55,995][166323] Fps is (10 sec: 1636.6, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13238272. Throughput: 0: 272.7. Samples: 13239488. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 09:39:55,996][166323] Avg episode reward: [(0, '1305.851')]
[36m[2025-07-02 09:40:00,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13238272. Throughput: 0: 271.8. Samples: 13241120. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 09:40:00,980][166323] Avg episode reward: [(0, '1271.122')]
[36m[2025-07-02 09:40:05,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13238272. Throughput: 0: 274.3. Samples: 13242048. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 09:40:05,955][166323] Avg episode reward: [(0, '1307.749')]
[36m[2025-07-02 09:40:10,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13238272. Throughput: 0: 275.0. Samples: 13243696. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 09:40:10,946][166323] Avg episode reward: [(0, '1280.962')]
[36m[2025-07-02 09:40:15,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13238272. Throughput: 0: 277.1. Samples: 13245440. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 09:40:15,952][166323] Avg episode reward: [(0, '1252.931')]
[36m[2025-07-02 09:40:20,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13238272. Throughput: 0: 276.9. Samples: 13246304. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 09:40:20,973][166323] Avg episode reward: [(0, '1343.437')]
[36m[2025-07-02 09:40:25,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13238272. Throughput: 0: 280.2. Samples: 13248016. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 09:40:25,972][166323] Avg episode reward: [(0, '1299.588')]
[36m[2025-07-02 09:40:30,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13238272. Throughput: 0: 278.5. Samples: 13249472. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 09:40:30,964][166323] Avg episode reward: [(0, '1316.267')]
[36m[2025-07-02 09:40:35,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13238272. Throughput: 0: 278.0. Samples: 13250352. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 09:40:35,960][166323] Avg episode reward: [(0, '1296.635')]
[36m[2025-07-02 09:40:40,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13238272. Throughput: 0: 275.7. Samples: 13251888. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 09:40:40,973][166323] Avg episode reward: [(0, '1295.570')]
[36m[2025-07-02 09:40:45,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.1). Total num frames: 13238272. Throughput: 0: 276.0. Samples: 13253536. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 09:40:45,969][166323] Avg episode reward: [(0, '1312.445')]
[36m[2025-07-02 09:40:50,948][166323] Fps is (10 sec: 1642.4, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 13254656. Throughput: 0: 275.6. Samples: 13254448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:40:50,948][166323] Avg episode reward: [(0, '1273.511')]
[36m[2025-07-02 09:40:55,973][166323] Fps is (10 sec: 1637.7, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13254656. Throughput: 0: 270.8. Samples: 13255888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:40:55,973][166323] Avg episode reward: [(0, '1253.506')]
[31m[47248804 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[47248804 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[47248805 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:41:00,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13254656. Throughput: 0: 269.3. Samples: 13257568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:41:00,988][166323] Avg episode reward: [(0, '1269.605')]
[36m[2025-07-02 09:41:05,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13254656. Throughput: 0: 270.6. Samples: 13258480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:41:05,968][166323] Avg episode reward: [(0, '1232.368')]
[36m[2025-07-02 09:41:10,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13254656. Throughput: 0: 269.8. Samples: 13260160. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:41:10,987][166323] Avg episode reward: [(0, '1278.777')]
[36m[2025-07-02 09:41:15,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13254656. Throughput: 0: 276.9. Samples: 13261936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:41:15,974][166323] Avg episode reward: [(0, '1274.807')]
[36m[2025-07-02 09:41:20,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13254656. Throughput: 0: 273.5. Samples: 13262656. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:41:20,947][166323] Avg episode reward: [(0, '1288.201')]
[36m[2025-07-02 09:41:26,023][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 13254656. Throughput: 0: 277.0. Samples: 13264368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:41:26,024][166323] Avg episode reward: [(0, '1375.567')]
[36m[2025-07-02 09:41:30,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13254656. Throughput: 0: 273.5. Samples: 13265840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:41:30,951][166323] Avg episode reward: [(0, '1365.581')]
[36m[2025-07-02 09:41:35,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13254656. Throughput: 0: 270.9. Samples: 13266640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:41:35,962][166323] Avg episode reward: [(0, '1373.668')]
[37m[1m[2025-07-02 09:41:36,050][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000025880_13254656.pth...
[36m[2025-07-02 09:41:36,055][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000025752_13189120.pth
[36m[2025-07-02 09:41:41,014][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 13254656. Throughput: 0: 273.9. Samples: 13268224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:41:41,015][166323] Avg episode reward: [(0, '1346.021')]
[36m[2025-07-02 09:41:45,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 13254656. Throughput: 0: 275.0. Samples: 13269936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:41:45,965][166323] Avg episode reward: [(0, '1338.783')]
[33m[2025-07-02 09:41:49,625][166323] KL-divergence is very high: 102.0311
[36m[2025-07-02 09:41:51,001][166323] Fps is (10 sec: 1640.6, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 13271040. Throughput: 0: 273.2. Samples: 13270784. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 09:41:51,001][166323] Avg episode reward: [(0, '1337.213')]
[36m[2025-07-02 09:41:55,978][166323] Fps is (10 sec: 1636.2, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13271040. Throughput: 0: 274.9. Samples: 13272528. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 09:41:55,978][166323] Avg episode reward: [(0, '1286.735')]
[36m[2025-07-02 09:42:00,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13271040. Throughput: 0: 271.9. Samples: 13274176. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 09:42:00,992][166323] Avg episode reward: [(0, '1234.893')]
[36m[2025-07-02 09:42:05,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13271040. Throughput: 0: 274.9. Samples: 13275040. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 09:42:05,991][166323] Avg episode reward: [(0, '1176.823')]
[36m[2025-07-02 09:42:10,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13271040. Throughput: 0: 272.1. Samples: 13276592. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 09:42:10,950][166323] Avg episode reward: [(0, '1215.078')]
[36m[2025-07-02 09:42:15,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13271040. Throughput: 0: 274.2. Samples: 13278176. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 09:42:15,946][166323] Avg episode reward: [(0, '1234.242')]
[36m[2025-07-02 09:42:20,977][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13271040. Throughput: 0: 275.1. Samples: 13279024. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 09:42:20,977][166323] Avg episode reward: [(0, '1239.363')]
[36m[2025-07-02 09:42:25,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13271040. Throughput: 0: 271.9. Samples: 13280448. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 09:42:25,972][166323] Avg episode reward: [(0, '1284.050')]
[36m[2025-07-02 09:42:30,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13271040. Throughput: 0: 275.5. Samples: 13282336. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 09:42:30,966][166323] Avg episode reward: [(0, '1306.367')]
[36m[2025-07-02 09:42:35,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13271040. Throughput: 0: 275.3. Samples: 13283168. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 09:42:35,988][166323] Avg episode reward: [(0, '1335.500')]
[36m[2025-07-02 09:42:40,971][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13271040. Throughput: 0: 271.3. Samples: 13284736. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 09:42:40,972][166323] Avg episode reward: [(0, '1359.132')]
[36m[2025-07-02 09:42:45,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 13271040. Throughput: 0: 272.5. Samples: 13286432. Policy #0 lag: (min: 22.0, avg: 22.0, max: 22.0)
[36m[2025-07-02 09:42:45,969][166323] Avg episode reward: [(0, '1300.406')]
[36m[2025-07-02 09:42:51,002][166323] Fps is (10 sec: 1633.4, 60 sec: 273.1, 300 sec: 277.6). Total num frames: 13287424. Throughput: 0: 272.6. Samples: 13287312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:42:51,002][166323] Avg episode reward: [(0, '1270.023')]
[36m[2025-07-02 09:42:56,016][166323] Fps is (10 sec: 1630.8, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13287424. Throughput: 0: 271.6. Samples: 13288832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:42:56,016][166323] Avg episode reward: [(0, '1269.364')]
[36m[2025-07-02 09:43:00,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13287424. Throughput: 0: 273.6. Samples: 13290496. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:43:00,984][166323] Avg episode reward: [(0, '1256.940')]
[31m[47370477 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[47370477 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[47370478 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:43:05,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13287424. Throughput: 0: 274.2. Samples: 13291360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:43:05,960][166323] Avg episode reward: [(0, '1231.389')]
[36m[2025-07-02 09:43:10,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13287424. Throughput: 0: 281.0. Samples: 13293088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:43:10,956][166323] Avg episode reward: [(0, '1210.666')]
[36m[2025-07-02 09:43:15,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13287424. Throughput: 0: 278.7. Samples: 13294880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:43:15,983][166323] Avg episode reward: [(0, '1232.408')]
[36m[2025-07-02 09:43:20,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13287424. Throughput: 0: 278.8. Samples: 13295712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:43:20,975][166323] Avg episode reward: [(0, '1263.549')]
[36m[2025-07-02 09:43:25,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13287424. Throughput: 0: 277.6. Samples: 13297232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:43:25,985][166323] Avg episode reward: [(0, '1246.415')]
[36m[2025-07-02 09:43:31,012][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 13287424. Throughput: 0: 275.6. Samples: 13298848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:43:31,013][166323] Avg episode reward: [(0, '1268.426')]
[36m[2025-07-02 09:43:35,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13287424. Throughput: 0: 273.5. Samples: 13299616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:43:35,989][166323] Avg episode reward: [(0, '1240.774')]
[37m[1m[2025-07-02 09:43:36,045][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000025944_13287424.pth...
[36m[2025-07-02 09:43:36,052][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000025816_13221888.pth
[36m[2025-07-02 09:43:40,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13287424. Throughput: 0: 278.1. Samples: 13301328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:43:40,957][166323] Avg episode reward: [(0, '1267.204')]
[36m[2025-07-02 09:43:45,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 13287424. Throughput: 0: 278.6. Samples: 13303024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:43:45,951][166323] Avg episode reward: [(0, '1238.798')]
[36m[2025-07-02 09:43:50,963][166323] Fps is (10 sec: 1637.3, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13303808. Throughput: 0: 277.3. Samples: 13303840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:43:50,964][166323] Avg episode reward: [(0, '1220.582')]
[36m[2025-07-02 09:43:55,996][166323] Fps is (10 sec: 1631.1, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13303808. Throughput: 0: 273.2. Samples: 13305392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:43:55,996][166323] Avg episode reward: [(0, '1230.595')]
[36m[2025-07-02 09:44:01,001][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13303808. Throughput: 0: 267.6. Samples: 13306928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:44:01,002][166323] Avg episode reward: [(0, '1191.126')]
[36m[2025-07-02 09:44:05,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13303808. Throughput: 0: 266.8. Samples: 13307712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:44:05,960][166323] Avg episode reward: [(0, '1143.517')]
[36m[2025-07-02 09:44:10,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13303808. Throughput: 0: 267.4. Samples: 13309264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:44:10,979][166323] Avg episode reward: [(0, '1178.257')]
[36m[2025-07-02 09:44:15,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13303808. Throughput: 0: 267.7. Samples: 13310880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:44:15,952][166323] Avg episode reward: [(0, '1132.856')]
[36m[2025-07-02 09:44:20,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13303808. Throughput: 0: 268.7. Samples: 13311696. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:44:20,943][166323] Avg episode reward: [(0, '1176.350')]
[36m[2025-07-02 09:44:25,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13303808. Throughput: 0: 266.8. Samples: 13313344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:44:25,992][166323] Avg episode reward: [(0, '1189.781')]
[36m[2025-07-02 09:44:30,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13303808. Throughput: 0: 267.0. Samples: 13315040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:44:30,955][166323] Avg episode reward: [(0, '1133.853')]
[36m[2025-07-02 09:44:35,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13303808. Throughput: 0: 270.1. Samples: 13316000. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:44:35,986][166323] Avg episode reward: [(0, '1153.878')]
[36m[2025-07-02 09:44:40,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13303808. Throughput: 0: 275.6. Samples: 13317792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:44:40,986][166323] Avg episode reward: [(0, '1130.065')]
[36m[2025-07-02 09:44:45,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 13303808. Throughput: 0: 278.4. Samples: 13319440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:44:45,952][166323] Avg episode reward: [(0, '1110.761')]
[36m[2025-07-02 09:44:50,967][166323] Fps is (10 sec: 1641.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13320192. Throughput: 0: 278.7. Samples: 13320256. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 09:44:50,967][166323] Avg episode reward: [(0, '1155.769')]
[36m[2025-07-02 09:44:56,020][166323] Fps is (10 sec: 1627.3, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13320192. Throughput: 0: 279.9. Samples: 13321872. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 09:44:56,020][166323] Avg episode reward: [(0, '1142.812')]
[36m[2025-07-02 09:45:00,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13320192. Throughput: 0: 279.6. Samples: 13323472. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 09:45:00,980][166323] Avg episode reward: [(0, '1235.975')]
[31m[47492690 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[47492691 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[47492691 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:45:05,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13320192. Throughput: 0: 281.2. Samples: 13324352. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 09:45:05,953][166323] Avg episode reward: [(0, '1153.726')]
[36m[2025-07-02 09:45:10,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13320192. Throughput: 0: 279.2. Samples: 13325904. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 09:45:10,979][166323] Avg episode reward: [(0, '1212.882')]
[36m[2025-07-02 09:45:15,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13320192. Throughput: 0: 280.2. Samples: 13327648. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 09:45:15,956][166323] Avg episode reward: [(0, '1221.588')]
[36m[2025-07-02 09:45:20,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 13320192. Throughput: 0: 278.3. Samples: 13328528. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 09:45:20,999][166323] Avg episode reward: [(0, '1218.409')]
[33m[47510742 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[47510742 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.87158203125
[33mCrash Rate: 0.11474609375
[33mTimeout Rate: 0.013671875 (navigation_task.py:265)
[33m[47510742 ms][navigation_task] - WARNING : 
[33mSuccesses: 1785
[33mCrashes : 235
[33mTimeouts: 28 (navigation_task.py:268)
[36m[2025-07-02 09:45:25,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13320192. Throughput: 0: 274.8. Samples: 13330160. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 09:45:25,987][166323] Avg episode reward: [(0, '1152.121')]
[36m[2025-07-02 09:45:30,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13320192. Throughput: 0: 274.7. Samples: 13331808. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 09:45:30,973][166323] Avg episode reward: [(0, '1207.362')]
[36m[2025-07-02 09:45:35,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13320192. Throughput: 0: 276.4. Samples: 13332688. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 09:45:35,950][166323] Avg episode reward: [(0, '1237.768')]
[37m[1m[2025-07-02 09:45:36,009][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000026008_13320192.pth...
[36m[2025-07-02 09:45:36,013][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000025880_13254656.pth
[36m[2025-07-02 09:45:40,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13320192. Throughput: 0: 275.6. Samples: 13334256. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 09:45:40,952][166323] Avg episode reward: [(0, '1278.038')]
[36m[2025-07-02 09:45:45,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 13320192. Throughput: 0: 275.5. Samples: 13335872. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 09:45:45,989][166323] Avg episode reward: [(0, '1224.130')]
[36m[2025-07-02 09:45:50,955][166323] Fps is (10 sec: 1637.9, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13336576. Throughput: 0: 273.8. Samples: 13336672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:45:50,955][166323] Avg episode reward: [(0, '1155.657')]
[36m[2025-07-02 09:45:55,952][166323] Fps is (10 sec: 1644.3, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 13336576. Throughput: 0: 275.7. Samples: 13338304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:45:55,953][166323] Avg episode reward: [(0, '1167.254')]
[36m[2025-07-02 09:46:00,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13336576. Throughput: 0: 272.4. Samples: 13339904. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:46:00,950][166323] Avg episode reward: [(0, '1239.489')]
[36m[2025-07-02 09:46:05,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13336576. Throughput: 0: 271.8. Samples: 13340752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:46:05,977][166323] Avg episode reward: [(0, '1252.416')]
[36m[2025-07-02 09:46:10,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13336576. Throughput: 0: 272.6. Samples: 13342416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:46:10,951][166323] Avg episode reward: [(0, '1211.966')]
[31m[47562199 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[47562200 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[47562200 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:46:15,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13336576. Throughput: 0: 271.2. Samples: 13344016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:46:15,992][166323] Avg episode reward: [(0, '1234.221')]
[36m[2025-07-02 09:46:20,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13336576. Throughput: 0: 269.3. Samples: 13344816. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:46:20,986][166323] Avg episode reward: [(0, '1316.632')]
[36m[2025-07-02 09:46:25,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13336576. Throughput: 0: 271.0. Samples: 13346448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:46:25,944][166323] Avg episode reward: [(0, '1352.392')]
[36m[2025-07-02 09:46:30,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13336576. Throughput: 0: 271.2. Samples: 13348064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:46:30,944][166323] Avg episode reward: [(0, '1289.777')]
[36m[2025-07-02 09:46:35,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 13336576. Throughput: 0: 270.6. Samples: 13348848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:46:35,946][166323] Avg episode reward: [(0, '1328.576')]
[36m[2025-07-02 09:46:40,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13336576. Throughput: 0: 269.7. Samples: 13350448. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:46:40,973][166323] Avg episode reward: [(0, '1272.741')]
[36m[2025-07-02 09:46:46,025][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 13336576. Throughput: 0: 273.3. Samples: 13352224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:46:46,025][166323] Avg episode reward: [(0, '1291.681')]
[36m[2025-07-02 09:46:50,967][166323] Fps is (10 sec: 1639.4, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13352960. Throughput: 0: 272.1. Samples: 13352992. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 09:46:50,968][166323] Avg episode reward: [(0, '1285.937')]
[36m[2025-07-02 09:46:55,956][166323] Fps is (10 sec: 1649.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13352960. Throughput: 0: 273.4. Samples: 13354720. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 09:46:55,956][166323] Avg episode reward: [(0, '1293.920')]
[31m[47608079 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[47608079 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[47608080 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:47:01,016][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 13352960. Throughput: 0: 276.8. Samples: 13356480. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 09:47:01,017][166323] Avg episode reward: [(0, '1245.965')]
[31m[47610859 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[47610859 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[47610859 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:47:06,001][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 13352960. Throughput: 0: 276.5. Samples: 13357264. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 09:47:06,001][166323] Avg episode reward: [(0, '1277.454')]
[36m[2025-07-02 09:47:10,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13352960. Throughput: 0: 279.0. Samples: 13359008. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 09:47:10,960][166323] Avg episode reward: [(0, '1280.107')]
[36m[2025-07-02 09:47:15,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13352960. Throughput: 0: 278.3. Samples: 13360592. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 09:47:15,962][166323] Avg episode reward: [(0, '1261.210')]
[36m[2025-07-02 09:47:20,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13352960. Throughput: 0: 278.6. Samples: 13361392. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 09:47:20,977][166323] Avg episode reward: [(0, '1219.978')]
[36m[2025-07-02 09:47:25,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13352960. Throughput: 0: 279.2. Samples: 13363008. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 09:47:25,960][166323] Avg episode reward: [(0, '1188.587')]
[36m[2025-07-02 09:47:31,004][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 13352960. Throughput: 0: 280.3. Samples: 13364832. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 09:47:31,004][166323] Avg episode reward: [(0, '1186.933')]
[36m[2025-07-02 09:47:35,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13352960. Throughput: 0: 280.7. Samples: 13365632. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 09:47:35,990][166323] Avg episode reward: [(0, '1217.635')]
[37m[1m[2025-07-02 09:47:36,078][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000026072_13352960.pth...
[36m[2025-07-02 09:47:36,082][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000025944_13287424.pth
[36m[2025-07-02 09:47:40,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13352960. Throughput: 0: 277.4. Samples: 13367200. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 09:47:40,950][166323] Avg episode reward: [(0, '1137.996')]
[36m[2025-07-02 09:47:45,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 222.2). Total num frames: 13352960. Throughput: 0: 273.8. Samples: 13368784. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-02 09:47:45,954][166323] Avg episode reward: [(0, '1164.183')]
[31m[47657705 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[47657705 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[47657705 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:47:50,980][166323] Fps is (10 sec: 1633.5, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13369344. Throughput: 0: 274.3. Samples: 13369600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:47:50,980][166323] Avg episode reward: [(0, '1172.634')]
[36m[2025-07-02 09:47:55,988][166323] Fps is (10 sec: 1632.8, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13369344. Throughput: 0: 271.8. Samples: 13371248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:47:55,988][166323] Avg episode reward: [(0, '1245.175')]
[36m[2025-07-02 09:48:00,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13369344. Throughput: 0: 269.8. Samples: 13372736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:48:00,974][166323] Avg episode reward: [(0, '1196.392')]
[36m[2025-07-02 09:48:05,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13369344. Throughput: 0: 272.3. Samples: 13373648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:48:05,985][166323] Avg episode reward: [(0, '1217.822')]
[36m[2025-07-02 09:48:10,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13369344. Throughput: 0: 274.2. Samples: 13375344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:48:10,952][166323] Avg episode reward: [(0, '1205.966')]
[36m[2025-07-02 09:48:16,004][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13369344. Throughput: 0: 266.7. Samples: 13376832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:48:16,004][166323] Avg episode reward: [(0, '1229.839')]
[36m[2025-07-02 09:48:21,025][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 13369344. Throughput: 0: 266.8. Samples: 13377648. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:48:21,026][166323] Avg episode reward: [(0, '1253.180')]
[36m[2025-07-02 09:48:25,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 13369344. Throughput: 0: 264.9. Samples: 13379120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:48:25,953][166323] Avg episode reward: [(0, '1244.710')]
[36m[2025-07-02 09:48:30,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13369344. Throughput: 0: 268.4. Samples: 13380864. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:48:30,968][166323] Avg episode reward: [(0, '1215.112')]
[36m[2025-07-02 09:48:35,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13369344. Throughput: 0: 269.3. Samples: 13381712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:48:35,950][166323] Avg episode reward: [(0, '1266.096')]
[36m[2025-07-02 09:48:40,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13369344. Throughput: 0: 269.0. Samples: 13383344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:48:40,961][166323] Avg episode reward: [(0, '1259.276')]
[36m[2025-07-02 09:48:45,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 13369344. Throughput: 0: 269.8. Samples: 13384880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:48:45,986][166323] Avg episode reward: [(0, '1285.958')]
[36m[2025-07-02 09:48:50,986][166323] Fps is (10 sec: 1634.3, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13385728. Throughput: 0: 267.4. Samples: 13385680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:48:50,986][166323] Avg episode reward: [(0, '1332.795')]
[36m[2025-07-02 09:48:55,985][166323] Fps is (10 sec: 1638.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13385728. Throughput: 0: 264.7. Samples: 13387264. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:48:55,985][166323] Avg episode reward: [(0, '1350.525')]
[36m[2025-07-02 09:49:00,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13385728. Throughput: 0: 266.8. Samples: 13388832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:49:00,990][166323] Avg episode reward: [(0, '1353.913')]
[36m[2025-07-02 09:49:05,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13385728. Throughput: 0: 270.9. Samples: 13389824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:49:05,965][166323] Avg episode reward: [(0, '1359.105')]
[36m[2025-07-02 09:49:11,005][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 13385728. Throughput: 0: 275.9. Samples: 13391552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:49:11,006][166323] Avg episode reward: [(0, '1388.063')]
[36m[2025-07-02 09:49:15,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13385728. Throughput: 0: 271.3. Samples: 13393072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:49:15,971][166323] Avg episode reward: [(0, '1283.751')]
[36m[2025-07-02 09:49:20,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13385728. Throughput: 0: 271.2. Samples: 13393920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:49:20,964][166323] Avg episode reward: [(0, '1286.029')]
[36m[2025-07-02 09:49:25,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13385728. Throughput: 0: 272.6. Samples: 13395616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:49:25,981][166323] Avg episode reward: [(0, '1285.444')]
[36m[2025-07-02 09:49:30,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13385728. Throughput: 0: 276.4. Samples: 13397312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:49:30,963][166323] Avg episode reward: [(0, '1256.203')]
[36m[2025-07-02 09:49:35,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13385728. Throughput: 0: 277.1. Samples: 13398144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:49:35,959][166323] Avg episode reward: [(0, '1279.019')]
[37m[1m[2025-07-02 09:49:36,010][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000026136_13385728.pth...
[36m[2025-07-02 09:49:36,014][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000026008_13320192.pth
[36m[2025-07-02 09:49:40,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13385728. Throughput: 0: 278.9. Samples: 13399808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:49:40,962][166323] Avg episode reward: [(0, '1288.717')]
[36m[2025-07-02 09:49:45,997][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 13385728. Throughput: 0: 279.8. Samples: 13401424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:49:45,998][166323] Avg episode reward: [(0, '1263.319')]
[36m[2025-07-02 09:49:50,977][166323] Fps is (10 sec: 1635.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13402112. Throughput: 0: 274.1. Samples: 13402160. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 09:49:50,978][166323] Avg episode reward: [(0, '1313.188')]
[36m[2025-07-02 09:49:55,970][166323] Fps is (10 sec: 1643.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13402112. Throughput: 0: 273.6. Samples: 13403856. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 09:49:55,970][166323] Avg episode reward: [(0, '1314.820')]
[31m[47786737 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[47786738 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[47786738 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:50:00,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13402112. Throughput: 0: 276.3. Samples: 13405504. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 09:50:00,963][166323] Avg episode reward: [(0, '1277.854')]
[36m[2025-07-02 09:50:05,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13402112. Throughput: 0: 278.3. Samples: 13406448. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 09:50:05,980][166323] Avg episode reward: [(0, '1325.635')]
[36m[2025-07-02 09:50:10,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13402112. Throughput: 0: 277.9. Samples: 13408112. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 09:50:10,949][166323] Avg episode reward: [(0, '1325.981')]
[36m[2025-07-02 09:50:15,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13402112. Throughput: 0: 279.3. Samples: 13409888. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 09:50:15,985][166323] Avg episode reward: [(0, '1330.313')]
[36m[2025-07-02 09:50:20,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13402112. Throughput: 0: 279.5. Samples: 13410720. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 09:50:20,947][166323] Avg episode reward: [(0, '1291.936')]
[36m[2025-07-02 09:50:25,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13402112. Throughput: 0: 280.7. Samples: 13412448. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 09:50:25,984][166323] Avg episode reward: [(0, '1341.531')]
[36m[2025-07-02 09:50:30,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13402112. Throughput: 0: 281.0. Samples: 13414064. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 09:50:30,974][166323] Avg episode reward: [(0, '1341.877')]
[36m[2025-07-02 09:50:35,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13402112. Throughput: 0: 281.5. Samples: 13414832. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 09:50:35,991][166323] Avg episode reward: [(0, '1368.219')]
[36m[2025-07-02 09:50:40,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13402112. Throughput: 0: 283.1. Samples: 13416592. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 09:50:40,952][166323] Avg episode reward: [(0, '1335.636')]
[36m[2025-07-02 09:50:45,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.1). Total num frames: 13402112. Throughput: 0: 283.7. Samples: 13418272. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 09:50:45,975][166323] Avg episode reward: [(0, '1267.514')]
[36m[2025-07-02 09:50:50,978][166323] Fps is (10 sec: 1634.1, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13418496. Throughput: 0: 282.3. Samples: 13419152. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:50:50,978][166323] Avg episode reward: [(0, '1297.329')]
[36m[2025-07-02 09:50:55,954][166323] Fps is (10 sec: 1641.8, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13418496. Throughput: 0: 280.5. Samples: 13420736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:50:55,954][166323] Avg episode reward: [(0, '1246.025')]
[36m[2025-07-02 09:51:00,989][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13418496. Throughput: 0: 277.3. Samples: 13422368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:51:00,989][166323] Avg episode reward: [(0, '1273.303')]
[36m[2025-07-02 09:51:05,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13418496. Throughput: 0: 278.3. Samples: 13423248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:51:05,961][166323] Avg episode reward: [(0, '1275.076')]
[36m[2025-07-02 09:51:10,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13418496. Throughput: 0: 275.7. Samples: 13424848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:51:10,962][166323] Avg episode reward: [(0, '1266.110')]
[36m[2025-07-02 09:51:15,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13418496. Throughput: 0: 277.3. Samples: 13426544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:51:15,978][166323] Avg episode reward: [(0, '1244.649')]
[31m[47865762 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[47865762 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[47865763 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:51:20,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13418496. Throughput: 0: 277.6. Samples: 13427312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:51:20,950][166323] Avg episode reward: [(0, '1315.784')]
[36m[2025-07-02 09:51:25,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13418496. Throughput: 0: 274.5. Samples: 13428944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:51:25,954][166323] Avg episode reward: [(0, '1355.221')]
[36m[2025-07-02 09:51:30,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13418496. Throughput: 0: 273.9. Samples: 13430592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:51:30,959][166323] Avg episode reward: [(0, '1360.358')]
[31m[47880672 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[47880673 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[47880673 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:51:35,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13418496. Throughput: 0: 273.2. Samples: 13431440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:51:35,964][166323] Avg episode reward: [(0, '1305.003')]
[37m[1m[2025-07-02 09:51:36,028][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000026200_13418496.pth...
[36m[2025-07-02 09:51:36,032][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000026072_13352960.pth
[36m[2025-07-02 09:51:40,978][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13418496. Throughput: 0: 276.8. Samples: 13433200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:51:40,978][166323] Avg episode reward: [(0, '1305.986')]
[36m[2025-07-02 09:51:46,006][166323] Fps is (10 sec: 1631.5, 60 sec: 545.8, 300 sec: 277.7). Total num frames: 13434880. Throughput: 0: 280.8. Samples: 13435008. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:51:46,006][166323] Avg episode reward: [(0, '1342.069')]
[36m[2025-07-02 09:51:50,986][166323] Fps is (10 sec: 1637.2, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13434880. Throughput: 0: 279.0. Samples: 13435808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:51:50,986][166323] Avg episode reward: [(0, '1306.151')]
[36m[2025-07-02 09:51:55,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 13434880. Throughput: 0: 277.4. Samples: 13437328. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:51:55,946][166323] Avg episode reward: [(0, '1239.957')]
[36m[2025-07-02 09:52:00,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13434880. Throughput: 0: 271.5. Samples: 13438752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:52:00,946][166323] Avg episode reward: [(0, '1242.792')]
[36m[2025-07-02 09:52:05,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13434880. Throughput: 0: 271.6. Samples: 13439536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:52:05,955][166323] Avg episode reward: [(0, '1229.656')]
[31m[47918887 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[47918887 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[47918887 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:52:10,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13434880. Throughput: 0: 274.2. Samples: 13441280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:52:10,951][166323] Avg episode reward: [(0, '1238.534')]
[36m[2025-07-02 09:52:15,999][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13434880. Throughput: 0: 275.3. Samples: 13442992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:52:15,999][166323] Avg episode reward: [(0, '1236.894')]
[36m[2025-07-02 09:52:21,000][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 13434880. Throughput: 0: 275.0. Samples: 13443824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:52:21,000][166323] Avg episode reward: [(0, '1186.602')]
[36m[2025-07-02 09:52:25,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13434880. Throughput: 0: 274.1. Samples: 13445536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:52:25,982][166323] Avg episode reward: [(0, '1211.124')]
[36m[2025-07-02 09:52:30,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13434880. Throughput: 0: 271.9. Samples: 13447232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:52:30,961][166323] Avg episode reward: [(0, '1251.606')]
[36m[2025-07-02 09:52:36,010][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 13434880. Throughput: 0: 271.5. Samples: 13448032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:52:36,020][166323] Avg episode reward: [(0, '1266.686')]
[36m[2025-07-02 09:52:40,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13434880. Throughput: 0: 274.2. Samples: 13449680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:52:40,986][166323] Avg episode reward: [(0, '1278.503')]
[36m[2025-07-02 09:52:45,983][166323] Fps is (10 sec: 1642.8, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13451264. Throughput: 0: 283.1. Samples: 13451504. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:52:45,983][166323] Avg episode reward: [(0, '1319.798')]
[36m[2025-07-02 09:52:50,946][166323] Fps is (10 sec: 1644.9, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13451264. Throughput: 0: 284.5. Samples: 13452336. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:52:50,946][166323] Avg episode reward: [(0, '1287.458')]
[36m[2025-07-02 09:52:56,003][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 13451264. Throughput: 0: 281.6. Samples: 13453968. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:52:56,004][166323] Avg episode reward: [(0, '1300.680')]
[36m[2025-07-02 09:53:00,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13451264. Throughput: 0: 280.4. Samples: 13455600. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:53:00,957][166323] Avg episode reward: [(0, '1285.961')]
[36m[2025-07-02 09:53:05,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13451264. Throughput: 0: 281.5. Samples: 13456480. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:53:05,964][166323] Avg episode reward: [(0, '1281.896')]
[36m[2025-07-02 09:53:10,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13451264. Throughput: 0: 280.9. Samples: 13458176. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:53:10,986][166323] Avg episode reward: [(0, '1258.205')]
[36m[2025-07-02 09:53:16,009][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13451264. Throughput: 0: 280.9. Samples: 13459888. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:53:16,009][166323] Avg episode reward: [(0, '1232.358')]
[36m[2025-07-02 09:53:21,007][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 13451264. Throughput: 0: 282.3. Samples: 13460736. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:53:21,007][166323] Avg episode reward: [(0, '1177.293')]
[36m[2025-07-02 09:53:25,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13451264. Throughput: 0: 282.9. Samples: 13462400. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:53:25,947][166323] Avg episode reward: [(0, '1180.895')]
[31m[47995599 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[47995599 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[47995599 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:53:30,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13451264. Throughput: 0: 283.1. Samples: 13464240. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:53:30,973][166323] Avg episode reward: [(0, '1209.126')]
[36m[2025-07-02 09:53:35,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13451264. Throughput: 0: 285.3. Samples: 13465184. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:53:35,973][166323] Avg episode reward: [(0, '1234.924')]
[37m[1m[2025-07-02 09:53:36,024][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000026264_13451264.pth...
[36m[2025-07-02 09:53:36,028][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000026136_13385728.pth
[36m[2025-07-02 09:53:40,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13451264. Throughput: 0: 287.9. Samples: 13466912. Policy #0 lag: (min: 21.0, avg: 21.0, max: 21.0)
[36m[2025-07-02 09:53:40,957][166323] Avg episode reward: [(0, '1235.771')]
[36m[2025-07-02 09:53:45,984][166323] Fps is (10 sec: 1636.4, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13467648. Throughput: 0: 288.2. Samples: 13468576. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 09:53:45,985][166323] Avg episode reward: [(0, '1256.833')]
[36m[2025-07-02 09:53:50,965][166323] Fps is (10 sec: 1637.1, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13467648. Throughput: 0: 288.7. Samples: 13469472. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 09:53:50,965][166323] Avg episode reward: [(0, '1301.461')]
[36m[2025-07-02 09:53:55,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13467648. Throughput: 0: 287.4. Samples: 13471104. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 09:53:55,975][166323] Avg episode reward: [(0, '1291.656')]
[36m[2025-07-02 09:54:00,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13467648. Throughput: 0: 286.5. Samples: 13472768. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 09:54:00,967][166323] Avg episode reward: [(0, '1318.843')]
[36m[2025-07-02 09:54:05,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13467648. Throughput: 0: 284.7. Samples: 13473536. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 09:54:05,960][166323] Avg episode reward: [(0, '1250.340')]
[36m[2025-07-02 09:54:10,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13467648. Throughput: 0: 285.1. Samples: 13475232. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 09:54:10,958][166323] Avg episode reward: [(0, '1267.384')]
[36m[2025-07-02 09:54:15,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 277.7). Total num frames: 13467648. Throughput: 0: 281.8. Samples: 13476912. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 09:54:15,945][166323] Avg episode reward: [(0, '1288.168')]
[36m[2025-07-02 09:54:20,991][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13467648. Throughput: 0: 278.6. Samples: 13477728. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 09:54:20,991][166323] Avg episode reward: [(0, '1249.224')]
[36m[2025-07-02 09:54:25,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13467648. Throughput: 0: 276.8. Samples: 13479376. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 09:54:25,982][166323] Avg episode reward: [(0, '1249.994')]
[36m[2025-07-02 09:54:30,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13467648. Throughput: 0: 273.5. Samples: 13480880. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 09:54:30,978][166323] Avg episode reward: [(0, '1287.411')]
[36m[2025-07-02 09:54:35,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13467648. Throughput: 0: 273.1. Samples: 13481760. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 09:54:35,957][166323] Avg episode reward: [(0, '1298.666')]
[36m[2025-07-02 09:54:40,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13467648. Throughput: 0: 277.5. Samples: 13483584. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-02 09:54:40,949][166323] Avg episode reward: [(0, '1327.104')]
[36m[2025-07-02 09:54:45,960][166323] Fps is (10 sec: 1637.9, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13484032. Throughput: 0: 277.4. Samples: 13485248. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 09:54:45,960][166323] Avg episode reward: [(0, '1254.398')]
[36m[2025-07-02 09:54:50,991][166323] Fps is (10 sec: 1631.5, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13484032. Throughput: 0: 278.9. Samples: 13486096. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 09:54:50,991][166323] Avg episode reward: [(0, '1264.189')]
[31m[48081708 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[48081708 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[48081708 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:54:55,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13484032. Throughput: 0: 274.5. Samples: 13487584. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 09:54:55,954][166323] Avg episode reward: [(0, '1193.608')]
[36m[2025-07-02 09:55:00,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13484032. Throughput: 0: 272.2. Samples: 13489168. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 09:55:00,973][166323] Avg episode reward: [(0, '1121.087')]
[36m[2025-07-02 09:55:05,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13484032. Throughput: 0: 274.4. Samples: 13490064. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 09:55:05,946][166323] Avg episode reward: [(0, '1136.179')]
[36m[2025-07-02 09:55:10,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13484032. Throughput: 0: 275.1. Samples: 13491744. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 09:55:10,948][166323] Avg episode reward: [(0, '1142.621')]
[31m[48104323 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[48104323 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[48104323 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:55:15,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13484032. Throughput: 0: 278.5. Samples: 13493408. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 09:55:15,968][166323] Avg episode reward: [(0, '1192.686')]
[36m[2025-07-02 09:55:21,020][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13484032. Throughput: 0: 279.8. Samples: 13494368. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 09:55:21,020][166323] Avg episode reward: [(0, '1137.469')]
[36m[2025-07-02 09:55:25,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13484032. Throughput: 0: 274.9. Samples: 13495968. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 09:55:25,991][166323] Avg episode reward: [(0, '1206.000')]
[36m[2025-07-02 09:55:30,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13484032. Throughput: 0: 277.7. Samples: 13497744. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 09:55:30,956][166323] Avg episode reward: [(0, '1298.815')]
[36m[2025-07-02 09:55:35,950][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13484032. Throughput: 0: 279.4. Samples: 13498656. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-02 09:55:35,950][166323] Avg episode reward: [(0, '1256.484')]
[37m[1m[2025-07-02 09:55:36,001][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000026328_13484032.pth...
[36m[2025-07-02 09:55:36,005][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000026200_13418496.pth
[31m[48127682 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[48127682 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[48127682 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:55:40,993][166323] Fps is (10 sec: 1632.2, 60 sec: 545.7, 300 sec: 333.2). Total num frames: 13500416. Throughput: 0: 285.3. Samples: 13500432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:55:40,994][166323] Avg episode reward: [(0, '1240.908')]
[36m[2025-07-02 09:55:45,987][166323] Fps is (10 sec: 1632.3, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13500416. Throughput: 0: 289.0. Samples: 13502176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:55:45,988][166323] Avg episode reward: [(0, '1267.349')]
[36m[2025-07-02 09:55:50,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13500416. Throughput: 0: 287.4. Samples: 13503008. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:55:50,977][166323] Avg episode reward: [(0, '1351.743')]
[36m[2025-07-02 09:55:55,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13500416. Throughput: 0: 288.1. Samples: 13504720. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:55:55,995][166323] Avg episode reward: [(0, '1283.272')]
[36m[2025-07-02 09:56:00,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13500416. Throughput: 0: 290.0. Samples: 13506464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:56:00,985][166323] Avg episode reward: [(0, '1296.371')]
[36m[2025-07-02 09:56:05,973][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13500416. Throughput: 0: 288.7. Samples: 13507344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:56:05,973][166323] Avg episode reward: [(0, '1285.908')]
[36m[2025-07-02 09:56:10,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13500416. Throughput: 0: 291.2. Samples: 13509072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:56:10,992][166323] Avg episode reward: [(0, '1279.560')]
[36m[2025-07-02 09:56:15,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13500416. Throughput: 0: 286.5. Samples: 13510640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:56:15,973][166323] Avg episode reward: [(0, '1247.152')]
[36m[2025-07-02 09:56:20,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13500416. Throughput: 0: 286.4. Samples: 13511552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:56:20,981][166323] Avg episode reward: [(0, '1192.806')]
[36m[2025-07-02 09:56:25,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13500416. Throughput: 0: 282.6. Samples: 13513136. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:56:25,955][166323] Avg episode reward: [(0, '1225.348')]
[31m[48178181 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[48178182 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[48178182 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:56:30,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13500416. Throughput: 0: 280.4. Samples: 13514784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:56:30,946][166323] Avg episode reward: [(0, '1193.747')]
[36m[2025-07-02 09:56:35,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13500416. Throughput: 0: 279.8. Samples: 13515600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:56:35,982][166323] Avg episode reward: [(0, '1173.661')]
[36m[2025-07-02 09:56:40,955][166323] Fps is (10 sec: 1636.8, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13516800. Throughput: 0: 280.4. Samples: 13517328. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 09:56:40,955][166323] Avg episode reward: [(0, '1191.779')]
[36m[2025-07-02 09:56:45,989][166323] Fps is (10 sec: 1637.2, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13516800. Throughput: 0: 278.4. Samples: 13518992. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 09:56:45,989][166323] Avg episode reward: [(0, '1222.697')]
[33m[48197733 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[48197733 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.8642578125
[33mCrash Rate: 0.1240234375
[33mTimeout Rate: 0.01171875 (navigation_task.py:265)
[33m[48197733 ms][navigation_task] - WARNING : 
[33mSuccesses: 1770
[33mCrashes : 254
[33mTimeouts: 24 (navigation_task.py:268)
[36m[2025-07-02 09:56:50,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13516800. Throughput: 0: 278.5. Samples: 13519872. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 09:56:50,949][166323] Avg episode reward: [(0, '1275.551')]
[36m[2025-07-02 09:56:55,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13516800. Throughput: 0: 274.4. Samples: 13521408. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 09:56:55,952][166323] Avg episode reward: [(0, '1289.973')]
[36m[2025-07-02 09:57:01,008][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 13516800. Throughput: 0: 274.6. Samples: 13523008. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 09:57:01,009][166323] Avg episode reward: [(0, '1317.125')]
[31m[48213289 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[48213289 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[48213289 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[48213346 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[48213346 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[48213347 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:57:05,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13516800. Throughput: 0: 273.5. Samples: 13523856. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 09:57:05,967][166323] Avg episode reward: [(0, '1268.503')]
[36m[2025-07-02 09:57:10,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13516800. Throughput: 0: 275.1. Samples: 13525520. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 09:57:10,968][166323] Avg episode reward: [(0, '1269.255')]
[36m[2025-07-02 09:57:15,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13516800. Throughput: 0: 276.7. Samples: 13527248. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 09:57:15,990][166323] Avg episode reward: [(0, '1336.182')]
[36m[2025-07-02 09:57:20,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13516800. Throughput: 0: 279.6. Samples: 13528176. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 09:57:20,965][166323] Avg episode reward: [(0, '1300.370')]
[36m[2025-07-02 09:57:25,993][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13516800. Throughput: 0: 276.7. Samples: 13529792. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 09:57:25,993][166323] Avg episode reward: [(0, '1258.852')]
[36m[2025-07-02 09:57:30,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13516800. Throughput: 0: 278.1. Samples: 13531504. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 09:57:30,981][166323] Avg episode reward: [(0, '1304.785')]
[36m[2025-07-02 09:57:36,009][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13516800. Throughput: 0: 277.0. Samples: 13532352. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 09:57:36,009][166323] Avg episode reward: [(0, '1328.669')]
[37m[1m[2025-07-02 09:57:36,069][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000026392_13516800.pth...
[36m[2025-07-02 09:57:36,073][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000026264_13451264.pth
[36m[2025-07-02 09:57:40,989][166323] Fps is (10 sec: 1637.1, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13533184. Throughput: 0: 275.7. Samples: 13533824. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:57:40,989][166323] Avg episode reward: [(0, '1372.866')]
[36m[2025-07-02 09:57:45,968][166323] Fps is (10 sec: 1645.2, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13533184. Throughput: 0: 275.4. Samples: 13535392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:57:45,968][166323] Avg episode reward: [(0, '1371.883')]
[36m[2025-07-02 09:57:50,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 13533184. Throughput: 0: 274.3. Samples: 13536192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:57:50,944][166323] Avg episode reward: [(0, '1346.045')]
[36m[2025-07-02 09:57:55,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13533184. Throughput: 0: 278.0. Samples: 13538032. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:57:55,976][166323] Avg episode reward: [(0, '1335.772')]
[36m[2025-07-02 09:58:00,997][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13533184. Throughput: 0: 279.1. Samples: 13539808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:58:00,997][166323] Avg episode reward: [(0, '1327.883')]
[36m[2025-07-02 09:58:05,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13533184. Throughput: 0: 276.3. Samples: 13540608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:58:05,955][166323] Avg episode reward: [(0, '1302.801')]
[36m[2025-07-02 09:58:10,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13533184. Throughput: 0: 276.3. Samples: 13542224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:58:10,989][166323] Avg episode reward: [(0, '1278.509')]
[36m[2025-07-02 09:58:16,010][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13533184. Throughput: 0: 272.9. Samples: 13543792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:58:16,010][166323] Avg episode reward: [(0, '1213.389')]
[36m[2025-07-02 09:58:20,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13533184. Throughput: 0: 272.4. Samples: 13544592. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:58:20,946][166323] Avg episode reward: [(0, '1244.051')]
[36m[2025-07-02 09:58:25,982][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13533184. Throughput: 0: 274.9. Samples: 13546192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:58:25,982][166323] Avg episode reward: [(0, '1217.288')]
[36m[2025-07-02 09:58:30,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13533184. Throughput: 0: 278.8. Samples: 13547936. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:58:30,967][166323] Avg episode reward: [(0, '1206.734')]
[36m[2025-07-02 09:58:35,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13533184. Throughput: 0: 281.0. Samples: 13548848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 09:58:35,982][166323] Avg episode reward: [(0, '1186.367')]
[36m[2025-07-02 09:58:40,970][166323] Fps is (10 sec: 1637.9, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13549568. Throughput: 0: 277.0. Samples: 13550496. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 09:58:40,970][166323] Avg episode reward: [(0, '1197.574')]
[31m[48312090 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[48312091 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[48312091 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:58:45,981][166323] Fps is (10 sec: 1638.5, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13549568. Throughput: 0: 273.9. Samples: 13552128. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 09:58:45,981][166323] Avg episode reward: [(0, '1222.355')]
[36m[2025-07-02 09:58:50,973][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13549568. Throughput: 0: 275.1. Samples: 13552992. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 09:58:50,973][166323] Avg episode reward: [(0, '1224.795')]
[36m[2025-07-02 09:58:55,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13549568. Throughput: 0: 279.1. Samples: 13554784. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 09:58:55,989][166323] Avg episode reward: [(0, '1225.138')]
[36m[2025-07-02 09:59:00,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13549568. Throughput: 0: 282.1. Samples: 13556480. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 09:59:00,991][166323] Avg episode reward: [(0, '1240.563')]
[36m[2025-07-02 09:59:05,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13549568. Throughput: 0: 282.9. Samples: 13557328. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 09:59:05,963][166323] Avg episode reward: [(0, '1303.804')]
[36m[2025-07-02 09:59:10,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13549568. Throughput: 0: 285.6. Samples: 13559040. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 09:59:10,968][166323] Avg episode reward: [(0, '1282.834')]
[36m[2025-07-02 09:59:15,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13549568. Throughput: 0: 284.1. Samples: 13560720. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 09:59:15,970][166323] Avg episode reward: [(0, '1249.823')]
[36m[2025-07-02 09:59:20,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13549568. Throughput: 0: 282.4. Samples: 13561552. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 09:59:20,962][166323] Avg episode reward: [(0, '1150.825')]
[31m[48353453 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[48353454 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[48353454 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:59:25,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13549568. Throughput: 0: 285.5. Samples: 13563344. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 09:59:25,974][166323] Avg episode reward: [(0, '1188.311')]
[36m[2025-07-02 09:59:30,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13549568. Throughput: 0: 285.4. Samples: 13564960. Policy #0 lag: (min: 14.0, avg: 14.0, max: 14.0)
[36m[2025-07-02 09:59:30,944][166323] Avg episode reward: [(0, '1219.088')]
[36m[2025-07-02 09:59:35,995][166323] Fps is (10 sec: 1634.9, 60 sec: 546.0, 300 sec: 333.2). Total num frames: 13565952. Throughput: 0: 284.7. Samples: 13565808. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 09:59:35,995][166323] Avg episode reward: [(0, '1260.140')]
[37m[1m[2025-07-02 09:59:36,067][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000026488_13565952.pth...
[36m[2025-07-02 09:59:36,071][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000026328_13484032.pth
[31m[48366206 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[48366206 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[48366206 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 09:59:40,994][166323] Fps is (10 sec: 1630.2, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13565952. Throughput: 0: 276.2. Samples: 13567216. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 09:59:40,994][166323] Avg episode reward: [(0, '1203.388')]
[36m[2025-07-02 09:59:45,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13565952. Throughput: 0: 273.3. Samples: 13568768. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 09:59:45,946][166323] Avg episode reward: [(0, '1193.106')]
[36m[2025-07-02 09:59:50,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13565952. Throughput: 0: 273.4. Samples: 13569632. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 09:59:50,970][166323] Avg episode reward: [(0, '1194.804')]
[36m[2025-07-02 09:59:55,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13565952. Throughput: 0: 268.5. Samples: 13571120. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 09:59:55,966][166323] Avg episode reward: [(0, '1228.741')]
[31m[48386078 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[48386079 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[48386079 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 10:00:00,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13565952. Throughput: 0: 267.5. Samples: 13572752. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 10:00:00,947][166323] Avg episode reward: [(0, '1221.876')]
[36m[2025-07-02 10:00:05,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13565952. Throughput: 0: 268.2. Samples: 13573616. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 10:00:05,944][166323] Avg episode reward: [(0, '1221.943')]
[36m[2025-07-02 10:00:10,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13565952. Throughput: 0: 265.6. Samples: 13575296. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 10:00:10,968][166323] Avg episode reward: [(0, '1207.156')]
[36m[2025-07-02 10:00:15,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.8). Total num frames: 13565952. Throughput: 0: 267.6. Samples: 13577008. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 10:00:15,960][166323] Avg episode reward: [(0, '1187.181')]
[36m[2025-07-02 10:00:20,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13565952. Throughput: 0: 268.6. Samples: 13577888. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 10:00:20,963][166323] Avg episode reward: [(0, '1235.322')]
[36m[2025-07-02 10:00:25,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13565952. Throughput: 0: 277.2. Samples: 13579680. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 10:00:25,959][166323] Avg episode reward: [(0, '1210.336')]
[36m[2025-07-02 10:00:30,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13565952. Throughput: 0: 279.9. Samples: 13581376. Policy #0 lag: (min: 25.0, avg: 25.0, max: 25.0)
[36m[2025-07-02 10:00:30,990][166323] Avg episode reward: [(0, '1270.038')]
[36m[2025-07-02 10:00:35,975][166323] Fps is (10 sec: 1635.7, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13582336. Throughput: 0: 279.8. Samples: 13582224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:00:35,976][166323] Avg episode reward: [(0, '1252.615')]
[36m[2025-07-02 10:00:41,006][166323] Fps is (10 sec: 1635.8, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13582336. Throughput: 0: 279.9. Samples: 13583728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:00:41,006][166323] Avg episode reward: [(0, '1242.164')]
[36m[2025-07-02 10:00:45,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13582336. Throughput: 0: 278.8. Samples: 13585312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:00:45,991][166323] Avg episode reward: [(0, '1266.936')]
[31m[48438337 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[48438338 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[48438338 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 10:00:51,016][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13582336. Throughput: 0: 277.2. Samples: 13586112. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:00:51,017][166323] Avg episode reward: [(0, '1262.360')]
[36m[2025-07-02 10:00:55,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13582336. Throughput: 0: 279.2. Samples: 13587856. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:00:55,955][166323] Avg episode reward: [(0, '1279.157')]
[36m[2025-07-02 10:01:00,974][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13582336. Throughput: 0: 277.6. Samples: 13589504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:01:00,975][166323] Avg episode reward: [(0, '1292.338')]
[36m[2025-07-02 10:01:05,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13582336. Throughput: 0: 276.0. Samples: 13590304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:01:05,953][166323] Avg episode reward: [(0, '1239.026')]
[31m[48457850 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[48457851 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[48457851 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 10:01:10,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13582336. Throughput: 0: 269.5. Samples: 13591808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:01:10,959][166323] Avg episode reward: [(0, '1249.432')]
[36m[2025-07-02 10:01:15,981][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13582336. Throughput: 0: 270.3. Samples: 13593536. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:01:15,982][166323] Avg episode reward: [(0, '1284.713')]
[36m[2025-07-02 10:01:20,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13582336. Throughput: 0: 271.0. Samples: 13594416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:01:20,971][166323] Avg episode reward: [(0, '1279.886')]
[36m[2025-07-02 10:01:25,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13582336. Throughput: 0: 275.8. Samples: 13596128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:01:25,959][166323] Avg episode reward: [(0, '1322.260')]
[36m[2025-07-02 10:01:30,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13582336. Throughput: 0: 278.6. Samples: 13597840. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:01:30,961][166323] Avg episode reward: [(0, '1334.827')]
[36m[2025-07-02 10:01:36,008][166323] Fps is (10 sec: 1630.4, 60 sec: 272.9, 300 sec: 277.6). Total num frames: 13598720. Throughput: 0: 279.9. Samples: 13598704. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 10:01:36,008][166323] Avg episode reward: [(0, '1400.468')]
[37m[1m[2025-07-02 10:01:36,074][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000026552_13598720.pth...
[36m[2025-07-02 10:01:36,079][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000026392_13516800.pth
[36m[2025-07-02 10:01:41,014][166323] Fps is (10 sec: 1629.8, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13598720. Throughput: 0: 275.9. Samples: 13600288. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 10:01:41,014][166323] Avg episode reward: [(0, '1342.755')]
[31m[48491389 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[48491390 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[48491390 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 10:01:45,988][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13598720. Throughput: 0: 277.2. Samples: 13601984. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 10:01:45,988][166323] Avg episode reward: [(0, '1352.365')]
[36m[2025-07-02 10:01:50,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13598720. Throughput: 0: 275.9. Samples: 13602720. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 10:01:50,959][166323] Avg episode reward: [(0, '1344.377')]
[36m[2025-07-02 10:01:55,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13598720. Throughput: 0: 278.8. Samples: 13604352. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 10:01:55,959][166323] Avg episode reward: [(0, '1294.106')]
[36m[2025-07-02 10:02:00,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13598720. Throughput: 0: 277.5. Samples: 13606016. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 10:02:00,962][166323] Avg episode reward: [(0, '1288.397')]
[36m[2025-07-02 10:02:05,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13598720. Throughput: 0: 276.0. Samples: 13606832. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 10:02:05,959][166323] Avg episode reward: [(0, '1255.602')]
[36m[2025-07-02 10:02:10,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13598720. Throughput: 0: 275.0. Samples: 13608512. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 10:02:10,989][166323] Avg episode reward: [(0, '1278.092')]
[31m[48523213 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[48523213 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[48523213 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 10:02:16,001][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13598720. Throughput: 0: 276.0. Samples: 13610272. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 10:02:16,001][166323] Avg episode reward: [(0, '1279.529')]
[36m[2025-07-02 10:02:20,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13598720. Throughput: 0: 275.5. Samples: 13611088. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 10:02:20,955][166323] Avg episode reward: [(0, '1232.305')]
[31m[48531386 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[48531387 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[48531388 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 10:02:25,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13598720. Throughput: 0: 276.2. Samples: 13612704. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 10:02:25,965][166323] Avg episode reward: [(0, '1185.116')]
[36m[2025-07-02 10:02:30,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13598720. Throughput: 0: 274.0. Samples: 13614304. Policy #0 lag: (min: 10.0, avg: 10.0, max: 10.0)
[36m[2025-07-02 10:02:30,955][166323] Avg episode reward: [(0, '1190.834')]
[33m[2025-07-02 10:02:33,196][166323] KL-divergence is very high: 498.2476
[33m[2025-07-02 10:02:33,236][166323] KL-divergence is very high: 100.6851
[36m[2025-07-02 10:02:36,000][166323] Fps is (10 sec: 1632.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13615104. Throughput: 0: 277.4. Samples: 13615216. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 10:02:36,001][166323] Avg episode reward: [(0, '1236.802')]
[36m[2025-07-02 10:02:40,962][166323] Fps is (10 sec: 1637.2, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13615104. Throughput: 0: 278.0. Samples: 13616864. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 10:02:40,963][166323] Avg episode reward: [(0, '1303.508')]
[36m[2025-07-02 10:02:45,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13615104. Throughput: 0: 277.6. Samples: 13618512. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 10:02:45,972][166323] Avg episode reward: [(0, '1228.733')]
[36m[2025-07-02 10:02:50,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13615104. Throughput: 0: 280.1. Samples: 13619440. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 10:02:50,966][166323] Avg episode reward: [(0, '1274.526')]
[36m[2025-07-02 10:02:55,966][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13615104. Throughput: 0: 281.0. Samples: 13621152. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 10:02:55,966][166323] Avg episode reward: [(0, '1346.033')]
[36m[2025-07-02 10:03:00,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13615104. Throughput: 0: 281.3. Samples: 13622928. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 10:03:00,987][166323] Avg episode reward: [(0, '1407.991')]
[36m[2025-07-02 10:03:05,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13615104. Throughput: 0: 283.0. Samples: 13623824. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 10:03:05,953][166323] Avg episode reward: [(0, '1415.878')]
[36m[2025-07-02 10:03:10,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13615104. Throughput: 0: 285.2. Samples: 13625536. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 10:03:10,956][166323] Avg episode reward: [(0, '1374.442')]
[36m[2025-07-02 10:03:15,975][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13615104. Throughput: 0: 289.3. Samples: 13627328. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 10:03:15,975][166323] Avg episode reward: [(0, '1351.754')]
[36m[2025-07-02 10:03:20,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13615104. Throughput: 0: 288.0. Samples: 13628176. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 10:03:20,999][166323] Avg episode reward: [(0, '1381.507')]
[36m[2025-07-02 10:03:26,002][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13615104. Throughput: 0: 286.0. Samples: 13629744. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 10:03:26,002][166323] Avg episode reward: [(0, '1395.704')]
[36m[2025-07-02 10:03:30,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13615104. Throughput: 0: 284.0. Samples: 13631296. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-02 10:03:30,978][166323] Avg episode reward: [(0, '1372.001')]
[36m[2025-07-02 10:03:35,953][166323] Fps is (10 sec: 1646.5, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13631488. Throughput: 0: 283.5. Samples: 13632192. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 10:03:35,953][166323] Avg episode reward: [(0, '1302.891')]
[37m[1m[2025-07-02 10:03:36,022][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000026616_13631488.pth...
[36m[2025-07-02 10:03:36,029][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000026488_13565952.pth
[36m[2025-07-02 10:03:40,945][166323] Fps is (10 sec: 1643.9, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13631488. Throughput: 0: 285.6. Samples: 13634000. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 10:03:40,945][166323] Avg episode reward: [(0, '1285.567')]
[36m[2025-07-02 10:03:45,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13631488. Throughput: 0: 282.8. Samples: 13635648. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 10:03:45,970][166323] Avg episode reward: [(0, '1286.681')]
[36m[2025-07-02 10:03:50,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13631488. Throughput: 0: 280.7. Samples: 13636464. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 10:03:50,980][166323] Avg episode reward: [(0, '1310.577')]
[36m[2025-07-02 10:03:55,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13631488. Throughput: 0: 277.5. Samples: 13638032. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 10:03:55,989][166323] Avg episode reward: [(0, '1295.090')]
[36m[2025-07-02 10:04:01,001][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13631488. Throughput: 0: 275.0. Samples: 13639712. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 10:04:01,001][166323] Avg episode reward: [(0, '1288.583')]
[36m[2025-07-02 10:04:05,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13631488. Throughput: 0: 275.2. Samples: 13640544. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 10:04:05,944][166323] Avg episode reward: [(0, '1290.177')]
[36m[2025-07-02 10:04:10,954][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13631488. Throughput: 0: 278.0. Samples: 13642240. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 10:04:10,955][166323] Avg episode reward: [(0, '1324.159')]
[36m[2025-07-02 10:04:15,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13631488. Throughput: 0: 277.5. Samples: 13643776. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 10:04:15,948][166323] Avg episode reward: [(0, '1371.543')]
[36m[2025-07-02 10:04:20,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13631488. Throughput: 0: 277.7. Samples: 13644688. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 10:04:20,952][166323] Avg episode reward: [(0, '1370.741')]
[36m[2025-07-02 10:04:25,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13631488. Throughput: 0: 275.1. Samples: 13646384. Policy #0 lag: (min: 8.0, avg: 8.2, max: 40.0)
[36m[2025-07-02 10:04:25,965][166323] Avg episode reward: [(0, '1344.662')]
[36m[2025-07-02 10:04:30,977][166323] Fps is (10 sec: 1634.2, 60 sec: 546.1, 300 sec: 277.7). Total num frames: 13647872. Throughput: 0: 275.9. Samples: 13648064. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 10:04:30,978][166323] Avg episode reward: [(0, '1406.703')]
[36m[2025-07-02 10:04:35,978][166323] Fps is (10 sec: 1636.1, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13647872. Throughput: 0: 275.6. Samples: 13648864. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 10:04:35,979][166323] Avg episode reward: [(0, '1408.987')]
[36m[2025-07-02 10:04:40,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13647872. Throughput: 0: 276.5. Samples: 13650464. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 10:04:40,959][166323] Avg episode reward: [(0, '1441.411')]
[36m[2025-07-02 10:04:46,010][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13647872. Throughput: 0: 274.4. Samples: 13652064. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 10:04:46,010][166323] Avg episode reward: [(0, '1403.041')]
[36m[2025-07-02 10:04:50,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13647872. Throughput: 0: 275.1. Samples: 13652928. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 10:04:50,956][166323] Avg episode reward: [(0, '1362.672')]
[36m[2025-07-02 10:04:55,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13647872. Throughput: 0: 275.9. Samples: 13654656. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 10:04:55,949][166323] Avg episode reward: [(0, '1356.312')]
[31m[48689304 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[48689304 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[48689304 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 10:05:00,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13647872. Throughput: 0: 278.8. Samples: 13656320. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 10:05:00,947][166323] Avg episode reward: [(0, '1347.934')]
[36m[2025-07-02 10:05:05,984][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13647872. Throughput: 0: 278.2. Samples: 13657216. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 10:05:05,984][166323] Avg episode reward: [(0, '1339.989')]
[36m[2025-07-02 10:05:10,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13647872. Throughput: 0: 279.1. Samples: 13658944. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 10:05:10,973][166323] Avg episode reward: [(0, '1316.672')]
[36m[2025-07-02 10:05:15,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13647872. Throughput: 0: 279.0. Samples: 13660624. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 10:05:15,992][166323] Avg episode reward: [(0, '1266.501')]
[36m[2025-07-02 10:05:20,982][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13647872. Throughput: 0: 277.7. Samples: 13661360. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 10:05:20,982][166323] Avg episode reward: [(0, '1337.821')]
[36m[2025-07-02 10:05:25,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13647872. Throughput: 0: 278.8. Samples: 13663008. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 10:05:25,959][166323] Avg episode reward: [(0, '1326.099')]
[36m[2025-07-02 10:05:31,028][166323] Fps is (10 sec: 1630.8, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 13664256. Throughput: 0: 278.6. Samples: 13664608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:05:31,029][166323] Avg episode reward: [(0, '1331.601')]
[36m[2025-07-02 10:05:35,968][166323] Fps is (10 sec: 1637.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13664256. Throughput: 0: 276.2. Samples: 13665360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:05:35,968][166323] Avg episode reward: [(0, '1332.227')]
[37m[1m[2025-07-02 10:05:36,069][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000026680_13664256.pth...
[36m[2025-07-02 10:05:36,074][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000026552_13598720.pth
[36m[2025-07-02 10:05:40,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13664256. Throughput: 0: 272.8. Samples: 13666944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:05:40,987][166323] Avg episode reward: [(0, '1306.792')]
[36m[2025-07-02 10:05:45,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13664256. Throughput: 0: 272.8. Samples: 13668608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:05:45,984][166323] Avg episode reward: [(0, '1294.880')]
[36m[2025-07-02 10:05:50,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13664256. Throughput: 0: 272.7. Samples: 13669488. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:05:50,986][166323] Avg episode reward: [(0, '1349.566')]
[36m[2025-07-02 10:05:55,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13664256. Throughput: 0: 269.8. Samples: 13671088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:05:55,988][166323] Avg episode reward: [(0, '1298.923')]
[36m[2025-07-02 10:06:00,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13664256. Throughput: 0: 267.6. Samples: 13672656. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:06:00,960][166323] Avg episode reward: [(0, '1284.720')]
[36m[2025-07-02 10:06:05,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13664256. Throughput: 0: 268.4. Samples: 13673440. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:06:05,985][166323] Avg episode reward: [(0, '1284.313')]
[36m[2025-07-02 10:06:10,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13664256. Throughput: 0: 273.4. Samples: 13675312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:06:10,962][166323] Avg episode reward: [(0, '1292.814')]
[36m[2025-07-02 10:06:15,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13664256. Throughput: 0: 271.9. Samples: 13676832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:06:15,990][166323] Avg episode reward: [(0, '1238.804')]
[36m[2025-07-02 10:06:20,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13664256. Throughput: 0: 271.3. Samples: 13677568. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:06:20,969][166323] Avg episode reward: [(0, '1239.767')]
[36m[2025-07-02 10:06:25,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13664256. Throughput: 0: 269.7. Samples: 13679072. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:06:25,957][166323] Avg episode reward: [(0, '1214.722')]
[36m[2025-07-02 10:06:30,978][166323] Fps is (10 sec: 1637.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13680640. Throughput: 0: 268.8. Samples: 13680704. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:06:30,978][166323] Avg episode reward: [(0, '1229.065')]
[36m[2025-07-02 10:06:35,955][166323] Fps is (10 sec: 1638.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13680640. Throughput: 0: 265.4. Samples: 13681424. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:06:35,956][166323] Avg episode reward: [(0, '1221.659')]
[36m[2025-07-02 10:06:40,951][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13680640. Throughput: 0: 265.8. Samples: 13683040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:06:40,952][166323] Avg episode reward: [(0, '1211.479')]
[36m[2025-07-02 10:06:45,943][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13680640. Throughput: 0: 267.5. Samples: 13684688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:06:45,944][166323] Avg episode reward: [(0, '1225.111')]
[36m[2025-07-02 10:06:50,992][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13680640. Throughput: 0: 269.1. Samples: 13685552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:06:50,992][166323] Avg episode reward: [(0, '1262.918')]
[36m[2025-07-02 10:06:55,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13680640. Throughput: 0: 263.7. Samples: 13687184. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:06:55,985][166323] Avg episode reward: [(0, '1300.844')]
[36m[2025-07-02 10:07:00,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13680640. Throughput: 0: 268.7. Samples: 13688912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:07:00,955][166323] Avg episode reward: [(0, '1278.887')]
[36m[2025-07-02 10:07:05,978][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13680640. Throughput: 0: 271.2. Samples: 13689776. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:07:05,978][166323] Avg episode reward: [(0, '1304.635')]
[36m[2025-07-02 10:07:10,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13680640. Throughput: 0: 271.6. Samples: 13691296. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:07:10,957][166323] Avg episode reward: [(0, '1296.213')]
[36m[2025-07-02 10:07:15,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13680640. Throughput: 0: 273.7. Samples: 13693024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:07:15,985][166323] Avg episode reward: [(0, '1352.052')]
[36m[2025-07-02 10:07:20,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13680640. Throughput: 0: 274.7. Samples: 13693792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:07:20,985][166323] Avg episode reward: [(0, '1354.396')]
[36m[2025-07-02 10:07:25,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13680640. Throughput: 0: 274.0. Samples: 13695376. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:07:25,969][166323] Avg episode reward: [(0, '1358.151')]
[36m[2025-07-02 10:07:30,950][166323] Fps is (10 sec: 1644.1, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13697024. Throughput: 0: 273.7. Samples: 13697008. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:07:30,951][166323] Avg episode reward: [(0, '1319.304')]
[36m[2025-07-02 10:07:35,986][166323] Fps is (10 sec: 1635.6, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13697024. Throughput: 0: 272.4. Samples: 13697808. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:07:35,986][166323] Avg episode reward: [(0, '1390.865')]
[37m[1m[2025-07-02 10:07:36,037][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000026744_13697024.pth...
[36m[2025-07-02 10:07:36,041][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000026616_13631488.pth
[36m[2025-07-02 10:07:40,997][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13697024. Throughput: 0: 273.7. Samples: 13699504. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:07:40,998][166323] Avg episode reward: [(0, '1380.049')]
[36m[2025-07-02 10:07:45,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13697024. Throughput: 0: 270.4. Samples: 13701088. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:07:45,989][166323] Avg episode reward: [(0, '1392.834')]
[36m[2025-07-02 10:07:50,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13697024. Throughput: 0: 269.9. Samples: 13701920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:07:50,974][166323] Avg episode reward: [(0, '1350.068')]
[36m[2025-07-02 10:07:55,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13697024. Throughput: 0: 274.8. Samples: 13703664. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:07:55,957][166323] Avg episode reward: [(0, '1318.484')]
[31m[48866611 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[48866611 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[48866611 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 10:08:00,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13697024. Throughput: 0: 271.7. Samples: 13705248. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:08:00,980][166323] Avg episode reward: [(0, '1301.866')]
[36m[2025-07-02 10:08:05,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13697024. Throughput: 0: 272.9. Samples: 13706064. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:08:05,961][166323] Avg episode reward: [(0, '1330.767')]
[36m[2025-07-02 10:08:10,990][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13697024. Throughput: 0: 275.8. Samples: 13707792. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:08:10,990][166323] Avg episode reward: [(0, '1267.656')]
[36m[2025-07-02 10:08:15,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13697024. Throughput: 0: 278.7. Samples: 13709552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:08:15,953][166323] Avg episode reward: [(0, '1191.178')]
[36m[2025-07-02 10:08:20,974][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13697024. Throughput: 0: 279.9. Samples: 13710400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:08:20,974][166323] Avg episode reward: [(0, '1248.208')]
[33m[48892936 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[48892936 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.875
[33mCrash Rate: 0.1171875
[33mTimeout Rate: 0.0078125 (navigation_task.py:265)
[33m[48892937 ms][navigation_task] - WARNING : 
[33mSuccesses: 1792
[33mCrashes : 240
[33mTimeouts: 16 (navigation_task.py:268)
[36m[2025-07-02 10:08:26,008][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13697024. Throughput: 0: 278.7. Samples: 13712048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:08:26,008][166323] Avg episode reward: [(0, '1261.158')]
[36m[2025-07-02 10:08:30,999][166323] Fps is (10 sec: 1634.3, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 13713408. Throughput: 0: 281.5. Samples: 13713760. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 10:08:30,999][166323] Avg episode reward: [(0, '1288.166')]
[36m[2025-07-02 10:08:35,970][166323] Fps is (10 sec: 1644.6, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13713408. Throughput: 0: 282.3. Samples: 13714624. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 10:08:35,970][166323] Avg episode reward: [(0, '1248.178')]
[36m[2025-07-02 10:08:40,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13713408. Throughput: 0: 280.9. Samples: 13716304. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 10:08:40,963][166323] Avg episode reward: [(0, '1222.570')]
[31m[48912843 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[48912844 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[48912844 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 10:08:45,946][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13713408. Throughput: 0: 282.9. Samples: 13717968. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 10:08:45,946][166323] Avg episode reward: [(0, '1211.975')]
[36m[2025-07-02 10:08:50,973][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13713408. Throughput: 0: 282.2. Samples: 13718768. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 10:08:50,974][166323] Avg episode reward: [(0, '1218.897')]
[36m[2025-07-02 10:08:55,985][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13713408. Throughput: 0: 280.9. Samples: 13720432. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 10:08:55,985][166323] Avg episode reward: [(0, '1203.826')]
[36m[2025-07-02 10:09:00,984][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13713408. Throughput: 0: 274.7. Samples: 13721920. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 10:09:00,984][166323] Avg episode reward: [(0, '1171.193')]
[36m[2025-07-02 10:09:05,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13713408. Throughput: 0: 271.7. Samples: 13722624. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 10:09:05,967][166323] Avg episode reward: [(0, '1198.550')]
[36m[2025-07-02 10:09:11,000][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 13713408. Throughput: 0: 269.6. Samples: 13724176. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 10:09:11,000][166323] Avg episode reward: [(0, '1244.879')]
[36m[2025-07-02 10:09:15,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13713408. Throughput: 0: 269.4. Samples: 13725872. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 10:09:15,959][166323] Avg episode reward: [(0, '1237.042')]
[36m[2025-07-02 10:09:20,970][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13713408. Throughput: 0: 267.0. Samples: 13726640. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 10:09:20,970][166323] Avg episode reward: [(0, '1278.408')]
[36m[2025-07-02 10:09:25,960][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 222.2). Total num frames: 13713408. Throughput: 0: 265.3. Samples: 13728240. Policy #0 lag: (min: 18.0, avg: 18.0, max: 18.0)
[36m[2025-07-02 10:09:25,960][166323] Avg episode reward: [(0, '1311.239')]
[36m[2025-07-02 10:09:30,986][166323] Fps is (10 sec: 1635.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13729792. Throughput: 0: 264.3. Samples: 13729872. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:09:30,987][166323] Avg episode reward: [(0, '1341.178')]
[36m[2025-07-02 10:09:35,947][166323] Fps is (10 sec: 1640.5, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13729792. Throughput: 0: 264.7. Samples: 13730672. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:09:35,948][166323] Avg episode reward: [(0, '1380.421')]
[37m[1m[2025-07-02 10:09:36,001][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000026808_13729792.pth...
[36m[2025-07-02 10:09:36,006][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000026680_13664256.pth
[36m[2025-07-02 10:09:40,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13729792. Throughput: 0: 264.5. Samples: 13732336. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:09:40,983][166323] Avg episode reward: [(0, '1353.058')]
[36m[2025-07-02 10:09:45,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13729792. Throughput: 0: 268.8. Samples: 13734016. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:09:45,992][166323] Avg episode reward: [(0, '1328.006')]
[36m[2025-07-02 10:09:50,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13729792. Throughput: 0: 271.7. Samples: 13734848. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:09:50,959][166323] Avg episode reward: [(0, '1349.342')]
[31m[48983704 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[48983705 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[48983705 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 10:09:56,000][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.6). Total num frames: 13729792. Throughput: 0: 275.2. Samples: 13736560. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:09:56,000][166323] Avg episode reward: [(0, '1318.118')]
[36m[2025-07-02 10:10:00,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13729792. Throughput: 0: 277.6. Samples: 13738368. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:10:00,979][166323] Avg episode reward: [(0, '1323.146')]
[36m[2025-07-02 10:10:05,986][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13729792. Throughput: 0: 277.2. Samples: 13739120. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:10:05,986][166323] Avg episode reward: [(0, '1306.380')]
[36m[2025-07-02 10:10:10,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13729792. Throughput: 0: 277.7. Samples: 13740736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:10:10,955][166323] Avg episode reward: [(0, '1275.141')]
[36m[2025-07-02 10:10:15,977][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13729792. Throughput: 0: 278.1. Samples: 13742384. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:10:15,977][166323] Avg episode reward: [(0, '1283.468')]
[36m[2025-07-02 10:10:20,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13729792. Throughput: 0: 278.2. Samples: 13743200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:10:20,980][166323] Avg episode reward: [(0, '1283.604')]
[36m[2025-07-02 10:10:25,959][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 13729792. Throughput: 0: 277.8. Samples: 13744832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:10:25,959][166323] Avg episode reward: [(0, '1312.873')]
[36m[2025-07-02 10:10:31,007][166323] Fps is (10 sec: 1633.9, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13746176. Throughput: 0: 273.0. Samples: 13746304. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:10:31,007][166323] Avg episode reward: [(0, '1334.661')]
[36m[2025-07-02 10:10:35,963][166323] Fps is (10 sec: 1637.8, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13746176. Throughput: 0: 272.3. Samples: 13747104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:10:35,963][166323] Avg episode reward: [(0, '1370.604')]
[36m[2025-07-02 10:10:40,945][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13746176. Throughput: 0: 268.4. Samples: 13748624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:10:40,945][166323] Avg episode reward: [(0, '1382.034')]
[36m[2025-07-02 10:10:45,983][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13746176. Throughput: 0: 261.3. Samples: 13750128. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:10:45,983][166323] Avg episode reward: [(0, '1334.546')]
[36m[2025-07-02 10:10:50,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13746176. Throughput: 0: 264.0. Samples: 13750992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:10:50,955][166323] Avg episode reward: [(0, '1360.509')]
[36m[2025-07-02 10:10:55,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13746176. Throughput: 0: 264.0. Samples: 13752624. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:10:55,990][166323] Avg episode reward: [(0, '1303.433')]
[31m[49048595 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[49048595 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[49048595 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 10:11:00,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13746176. Throughput: 0: 263.2. Samples: 13754224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:11:00,962][166323] Avg episode reward: [(0, '1313.395')]
[36m[2025-07-02 10:11:05,965][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13746176. Throughput: 0: 261.1. Samples: 13754944. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:11:05,966][166323] Avg episode reward: [(0, '1246.831')]
[31m[49056750 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[49056750 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[49056751 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 10:11:10,972][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13746176. Throughput: 0: 260.2. Samples: 13756544. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:11:10,973][166323] Avg episode reward: [(0, '1192.787')]
[36m[2025-07-02 10:11:15,953][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13746176. Throughput: 0: 263.4. Samples: 13758144. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:11:15,953][166323] Avg episode reward: [(0, '1158.176')]
[31m[49066693 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[49066694 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[49066694 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 10:11:20,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13746176. Throughput: 0: 261.8. Samples: 13758880. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:11:20,948][166323] Avg episode reward: [(0, '1237.247')]
[36m[2025-07-02 10:11:25,968][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 13746176. Throughput: 0: 261.6. Samples: 13760400. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:11:25,968][166323] Avg episode reward: [(0, '1182.890')]
[36m[2025-07-02 10:11:30,957][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 222.2). Total num frames: 13746176. Throughput: 0: 262.9. Samples: 13761952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:11:30,958][166323] Avg episode reward: [(0, '1216.874')]
[36m[2025-07-02 10:11:35,951][166323] Fps is (10 sec: 1641.3, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13762560. Throughput: 0: 261.4. Samples: 13762752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:11:35,951][166323] Avg episode reward: [(0, '1163.514')]
[37m[1m[2025-07-02 10:11:36,028][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000026872_13762560.pth...
[36m[2025-07-02 10:11:36,034][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000026744_13697024.pth
[31m[49087137 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[49087138 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[49087138 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 10:11:40,997][166323] Fps is (10 sec: 1631.9, 60 sec: 272.8, 300 sec: 277.6). Total num frames: 13762560. Throughput: 0: 262.4. Samples: 13764432. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:11:40,998][166323] Avg episode reward: [(0, '1169.755')]
[36m[2025-07-02 10:11:45,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13762560. Throughput: 0: 265.8. Samples: 13766192. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:11:45,989][166323] Avg episode reward: [(0, '1176.552')]
[36m[2025-07-02 10:11:50,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13762560. Throughput: 0: 268.7. Samples: 13767040. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:11:50,987][166323] Avg episode reward: [(0, '1161.815')]
[31m[49099757 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[49099758 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[49099758 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 10:11:55,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13762560. Throughput: 0: 269.8. Samples: 13768688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:11:55,985][166323] Avg episode reward: [(0, '1149.599')]
[36m[2025-07-02 10:12:00,997][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13762560. Throughput: 0: 270.3. Samples: 13770320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:12:00,998][166323] Avg episode reward: [(0, '1177.338')]
[36m[2025-07-02 10:12:05,989][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13762560. Throughput: 0: 271.4. Samples: 13771104. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:12:05,989][166323] Avg episode reward: [(0, '1206.072')]
[36m[2025-07-02 10:12:10,952][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13762560. Throughput: 0: 275.3. Samples: 13772784. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:12:10,953][166323] Avg episode reward: [(0, '1251.784')]
[36m[2025-07-02 10:12:15,994][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13762560. Throughput: 0: 278.2. Samples: 13774480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:12:15,995][166323] Avg episode reward: [(0, '1318.049')]
[36m[2025-07-02 10:12:20,949][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13762560. Throughput: 0: 280.9. Samples: 13775392. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:12:20,949][166323] Avg episode reward: [(0, '1296.959')]
[36m[2025-07-02 10:12:25,976][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 13762560. Throughput: 0: 280.0. Samples: 13777024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:12:25,976][166323] Avg episode reward: [(0, '1382.826')]
[36m[2025-07-02 10:12:30,988][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.2). Total num frames: 13762560. Throughput: 0: 277.0. Samples: 13778656. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:12:30,989][166323] Avg episode reward: [(0, '1374.998')]
[31m[49142775 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[49142775 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[49142775 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 10:12:35,947][166323] Fps is (10 sec: 1643.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13778944. Throughput: 0: 274.4. Samples: 13779376. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 10:12:35,947][166323] Avg episode reward: [(0, '1299.491')]
[36m[2025-07-02 10:12:40,969][166323] Fps is (10 sec: 1641.5, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13778944. Throughput: 0: 271.7. Samples: 13780912. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 10:12:40,970][166323] Avg episode reward: [(0, '1275.461')]
[36m[2025-07-02 10:12:45,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13778944. Throughput: 0: 270.1. Samples: 13782464. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 10:12:45,962][166323] Avg episode reward: [(0, '1236.792')]
[36m[2025-07-02 10:12:50,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13778944. Throughput: 0: 269.8. Samples: 13783232. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 10:12:50,949][166323] Avg episode reward: [(0, '1199.387')]
[36m[2025-07-02 10:12:56,013][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13778944. Throughput: 0: 266.7. Samples: 13784800. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 10:12:56,014][166323] Avg episode reward: [(0, '1222.922')]
[36m[2025-07-02 10:13:00,990][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13778944. Throughput: 0: 262.4. Samples: 13786288. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 10:13:00,990][166323] Avg episode reward: [(0, '1193.362')]
[31m[49171420 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[49171421 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[49171421 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 10:13:05,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13778944. Throughput: 0: 257.7. Samples: 13786992. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 10:13:05,963][166323] Avg episode reward: [(0, '1184.741')]
[36m[2025-07-02 10:13:10,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13778944. Throughput: 0: 255.7. Samples: 13788528. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 10:13:10,964][166323] Avg episode reward: [(0, '1217.934')]
[36m[2025-07-02 10:13:15,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13778944. Throughput: 0: 255.7. Samples: 13790160. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 10:13:15,979][166323] Avg episode reward: [(0, '1244.519')]
[31m[49188579 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[49188580 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[49188581 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[49189497 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[49189497 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[49189497 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 10:13:20,986][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13778944. Throughput: 0: 258.3. Samples: 13791008. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 10:13:20,987][166323] Avg episode reward: [(0, '1251.685')]
[36m[2025-07-02 10:13:25,979][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 13778944. Throughput: 0: 259.5. Samples: 13792592. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 10:13:25,979][166323] Avg episode reward: [(0, '1282.744')]
[36m[2025-07-02 10:13:30,985][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.1). Total num frames: 13778944. Throughput: 0: 256.6. Samples: 13794016. Policy #0 lag: (min: 17.0, avg: 17.0, max: 17.0)
[36m[2025-07-02 10:13:30,986][166323] Avg episode reward: [(0, '1288.537')]
[36m[2025-07-02 10:13:36,114][166323] Fps is (10 sec: 1616.5, 60 sec: 272.3, 300 sec: 277.6). Total num frames: 13795328. Throughput: 0: 254.0. Samples: 13794704. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 10:13:36,115][166323] Avg episode reward: [(0, '1303.487')]
[37m[1m[2025-07-02 10:13:36,168][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000026936_13795328.pth...
[36m[2025-07-02 10:13:36,172][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000026808_13729792.pth
[36m[2025-07-02 10:13:40,991][166323] Fps is (10 sec: 1637.4, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13795328. Throughput: 0: 253.3. Samples: 13796192. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 10:13:40,991][166323] Avg episode reward: [(0, '1309.742')]
[36m[2025-07-02 10:13:45,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13795328. Throughput: 0: 255.3. Samples: 13797776. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 10:13:45,981][166323] Avg episode reward: [(0, '1322.203')]
[36m[2025-07-02 10:13:50,958][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13795328. Throughput: 0: 256.4. Samples: 13798528. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 10:13:50,958][166323] Avg episode reward: [(0, '1314.696')]
[36m[2025-07-02 10:13:55,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13795328. Throughput: 0: 251.7. Samples: 13799856. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 10:13:55,968][166323] Avg episode reward: [(0, '1284.444')]
[36m[2025-07-02 10:14:00,993][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13795328. Throughput: 0: 248.5. Samples: 13801344. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 10:14:00,993][166323] Avg episode reward: [(0, '1298.714')]
[36m[2025-07-02 10:14:05,964][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13795328. Throughput: 0: 246.2. Samples: 13802080. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 10:14:05,964][166323] Avg episode reward: [(0, '1302.042')]
[36m[2025-07-02 10:14:10,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13795328. Throughput: 0: 245.8. Samples: 13803648. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 10:14:10,968][166323] Avg episode reward: [(0, '1288.928')]
[36m[2025-07-02 10:14:15,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13795328. Throughput: 0: 246.1. Samples: 13805088. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 10:14:15,967][166323] Avg episode reward: [(0, '1253.655')]
[36m[2025-07-02 10:14:20,997][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13795328. Throughput: 0: 248.1. Samples: 13805840. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 10:14:20,997][166323] Avg episode reward: [(0, '1277.049')]
[36m[2025-07-02 10:14:25,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 13795328. Throughput: 0: 249.1. Samples: 13807392. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 10:14:25,957][166323] Avg episode reward: [(0, '1295.373')]
[36m[2025-07-02 10:14:31,002][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 13795328. Throughput: 0: 247.3. Samples: 13808912. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 10:14:31,003][166323] Avg episode reward: [(0, '1245.601')]
[31m[49262398 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[49262398 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[49262398 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 10:14:35,965][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 222.2). Total num frames: 13795328. Throughput: 0: 247.4. Samples: 13809664. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 10:14:35,966][166323] Avg episode reward: [(0, '1180.876')]
[36m[2025-07-02 10:14:40,958][166323] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 222.2). Total num frames: 13795328. Throughput: 0: 252.9. Samples: 13811232. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-02 10:14:40,958][166323] Avg episode reward: [(0, '1182.296')]
[36m[2025-07-02 10:14:45,988][166323] Fps is (10 sec: 1634.8, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13811712. Throughput: 0: 253.2. Samples: 13812736. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 10:14:45,988][166323] Avg episode reward: [(0, '1214.851')]
[36m[2025-07-02 10:14:50,961][166323] Fps is (10 sec: 1637.8, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13811712. Throughput: 0: 254.2. Samples: 13813520. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 10:14:50,962][166323] Avg episode reward: [(0, '1242.634')]
[36m[2025-07-02 10:14:55,955][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13811712. Throughput: 0: 254.6. Samples: 13815104. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 10:14:55,955][166323] Avg episode reward: [(0, '1187.216')]
[36m[2025-07-02 10:15:00,987][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13811712. Throughput: 0: 256.6. Samples: 13816640. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 10:15:00,988][166323] Avg episode reward: [(0, '1236.829')]
[31m[49290779 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[49290780 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[49290780 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 10:15:05,991][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13811712. Throughput: 0: 256.7. Samples: 13817392. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 10:15:05,991][166323] Avg episode reward: [(0, '1220.141')]
[36m[2025-07-02 10:15:10,947][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13811712. Throughput: 0: 256.8. Samples: 13818944. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 10:15:10,947][166323] Avg episode reward: [(0, '1246.481')]
[36m[2025-07-02 10:15:15,944][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13811712. Throughput: 0: 258.5. Samples: 13820528. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 10:15:15,944][166323] Avg episode reward: [(0, '1184.271')]
[36m[2025-07-02 10:15:21,004][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13811712. Throughput: 0: 259.3. Samples: 13821344. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 10:15:21,004][166323] Avg episode reward: [(0, '1222.625')]
[36m[2025-07-02 10:15:25,999][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.2). Total num frames: 13811712. Throughput: 0: 257.5. Samples: 13822832. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 10:15:25,999][166323] Avg episode reward: [(0, '1254.928')]
[36m[2025-07-02 10:15:30,961][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 222.2). Total num frames: 13811712. Throughput: 0: 259.4. Samples: 13824400. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 10:15:30,961][166323] Avg episode reward: [(0, '1275.275')]
[36m[2025-07-02 10:15:35,948][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 13811712. Throughput: 0: 259.3. Samples: 13825184. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 10:15:35,948][166323] Avg episode reward: [(0, '1254.341')]
[37m[1m[2025-07-02 10:15:35,998][166323] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000026968_13811712.pth...
[36m[2025-07-02 10:15:36,002][166323] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/HIGH_CONFIG_16ENV_2/checkpoint_p0/checkpoint_000026872_13762560.pth
[36m[2025-07-02 10:15:40,969][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 13811712. Throughput: 0: 258.8. Samples: 13826752. Policy #0 lag: (min: 13.0, avg: 13.0, max: 13.0)
[36m[2025-07-02 10:15:40,969][166323] Avg episode reward: [(0, '1239.703')]
[36m[2025-07-02 10:15:45,977][166323] Fps is (10 sec: 1633.7, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13828096. Throughput: 0: 258.6. Samples: 13828272. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:15:45,977][166323] Avg episode reward: [(0, '1324.177')]
[31m[49337109 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[49337109 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([6], device='cuda:0') (navigation_task.py:196)
[31m[49337109 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 10:15:50,969][166323] Fps is (10 sec: 1638.4, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13828096. Throughput: 0: 258.6. Samples: 13829024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:15:50,969][166323] Avg episode reward: [(0, '1248.667')]
[31m[49343974 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[49343975 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[49343975 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 10:15:55,981][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 277.7). Total num frames: 13828096. Throughput: 0: 255.8. Samples: 13830464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:15:55,981][166323] Avg episode reward: [(0, '1275.233')]
[36m[2025-07-02 10:16:00,967][166323] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 277.7). Total num frames: 13828096. Throughput: 0: 253.7. Samples: 13831952. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:16:00,968][166323] Avg episode reward: [(0, '1262.308')]
[36m[2025-07-02 10:16:05,980][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 13828096. Throughput: 0: 253.6. Samples: 13832752. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:16:05,980][166323] Avg episode reward: [(0, '1265.804')]
[36m[2025-07-02 10:16:10,963][166323] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 277.7). Total num frames: 13828096. Throughput: 0: 252.3. Samples: 13834176. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:16:10,964][166323] Avg episode reward: [(0, '1274.404')]
[36m[2025-07-02 10:16:15,992][166323] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 277.7). Total num frames: 13828096. Throughput: 0: 255.8. Samples: 13835920. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:16:15,992][166323] Avg episode reward: [(0, '1307.510')]
[36m[2025-07-02 10:16:20,956][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13828096. Throughput: 0: 255.6. Samples: 13836688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:16:20,956][166323] Avg episode reward: [(0, '1301.089')]
[36m[2025-07-02 10:16:25,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 277.7). Total num frames: 13828096. Throughput: 0: 255.7. Samples: 13838256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:16:25,957][166323] Avg episode reward: [(0, '1331.481')]
[36m[2025-07-02 10:16:30,962][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.1). Total num frames: 13828096. Throughput: 0: 254.3. Samples: 13839712. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:16:30,962][166323] Avg episode reward: [(0, '1331.330')]
[36m[2025-07-02 10:16:35,987][166323] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.2). Total num frames: 13828096. Throughput: 0: 254.1. Samples: 13840464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:16:35,988][166323] Avg episode reward: [(0, '1338.268')]
[31m[49385611 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[49385611 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[49385612 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-02 10:16:40,957][166323] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 13828096. Throughput: 0: 255.8. Samples: 13841968. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-02 10:16:40,957][166323] Avg episode reward: [(0, '1299.132')]
[37m[1m[2025-07-02 10:16:44,854][166323] Keyboard interrupt detected in the event loop EvtLoop [Runner_EvtLoop, process=main process 166323], exiting...
[37m[1m[2025-07-02 10:16:44,854][166323] Runner profile tree view:
[37m[1mmain_loop: 49382.8126
[37m[1m[2025-07-02 10:16:44,855][166323] Collected {0: 13828096}, FPS: 280.0