Importing module 'gym_38' (/home/ziyar/aerialgym/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_38.so)
Setting GYM_USD_PLUG_INFO_PATH to /home/ziyar/aerialgym/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json
[36m[2025-06-29 17:34:13,269][187912] Queried available GPUs: 0
[37m[1m[2025-06-29 17:34:13,269][187912] Environment var CUDA_VISIBLE_DEVICES is 0
PyTorch version 1.13.1
Device count 1
/home/ziyar/aerialgym/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/_bindings/src/gymtorch
ninja: no work to do.
Warp 1.0.0-beta.5 initialized:
   CUDA Toolkit: 11.5, Driver: 12.4
   Devices:
     "cpu"    | x86_64
     "cuda:0" | NVIDIA GeForce RTX 4080 Laptop GPU (sm_89)
   Kernel cache: /home/ziyar/.cache/warp/1.0.0-beta.5
Registered dce_navigation_task in subprocess
[isaacgym:gymutil.py] Unknown args:  ['--env=dce_navigation_task', '--train_for_env_steps=100000000', '--experiment=16env_first_test', '--async_rl=True', '--use_env_info_cache=False', '--normalize_input=True']
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:0
GPU Pipeline: enabled
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/torch/utils/cpp_extension.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
  from pkg_resources import packaging  # type: ignore[attr-defined]
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
Using /home/ziyar/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Emitting ninja build file /home/ziyar/.cache/torch_extensions/py38_cu117/gymtorch/build.ninja...
Building extension module gymtorch...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module gymtorch...
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/networkx/classes/graph.py:23: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  from collections import Mapping
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/networkx/classes/reportviews.py:95: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  from collections import Mapping, Set, Iterable
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/networkx/readwrite/graphml.py:346: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.int, "int"), (np.int8, "int"),
/home/ziyar/aerialgym/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/torch_utils.py:135: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  def get_axis_params(value, axis_idx, x_value=0., dtype=np.float, n_dims=3):
[31m[1801 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task] - CRITICAL : Setting number of environments to 16 for parallel training. (dce_navigation_task.py:18)
[31m[1801 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task] - CRITICAL : Enabling viewer for visual training feedback. (dce_navigation_task.py:34)
[37m[1801 ms][base_task] - INFO : Setting seed: 3994758580 (base_task.py:38)
[37m[1801 ms][navigation_task] - INFO : Building environment for navigation task. (navigation_task.py:44)
[37m[1801 ms][navigation_task] - INFO : Sim Name: base_sim, Env Name: env_with_obstacles, Robot Name: lmf2, Controller Name: lmf2_velocity_control (navigation_task.py:45)
[37m[1801 ms][env_manager] - INFO : Populating environments. (env_manager.py:73)
[37m[1801 ms][env_manager] - INFO : Creating simulation instance. (env_manager.py:87)
[37m[1801 ms][env_manager] - INFO : Instantiating IGE object. (env_manager.py:88)
[37m[1801 ms][IsaacGymEnvManager] - INFO : Creating Isaac Gym Environment (IGE_env_manager.py:41)
[37m[1801 ms][IsaacGymEnvManager] - INFO : Acquiring gym object (IGE_env_manager.py:73)
[37m[1801 ms][IsaacGymEnvManager] - INFO : Acquired gym object (IGE_env_manager.py:75)
[37m[1802 ms][IsaacGymEnvManager] - INFO : Fixing devices (IGE_env_manager.py:89)
[37m[1802 ms][IsaacGymEnvManager] - INFO : Using GPU pipeline for simulation. (IGE_env_manager.py:102)
[37m[1802 ms][IsaacGymEnvManager] - INFO : Sim Device type: cuda, Sim Device ID: 0 (IGE_env_manager.py:105)
[31m[1802 ms][IsaacGymEnvManager] - CRITICAL : 
[31m Setting graphics device to -1.
[31m This is done because the simulation is run in headless mode and no Isaac Gym cameras are used.
[31m No need to worry. The simulation and warp rendering will work as expected. (IGE_env_manager.py:112)
[37m[1802 ms][IsaacGymEnvManager] - INFO : Graphics Device ID: -1 (IGE_env_manager.py:119)
[37m[1802 ms][IsaacGymEnvManager] - INFO : Creating Isaac Gym Simulation Object (IGE_env_manager.py:120)
[33m[1802 ms][IsaacGymEnvManager] - WARNING : If you have set the CUDA_VISIBLE_DEVICES environment variable, please ensure that you set it
[33mto a particular one that works for your system to use the viewer or Isaac Gym cameras.
[33mIf you want to run parallel simulations on multiple GPUs with camera sensors,
[33mplease disable Isaac Gym and use warp (by setting use_warp=True), set the viewer to headless. (IGE_env_manager.py:127)
[33m[1802 ms][IsaacGymEnvManager] - WARNING : If you see a segfault in the next lines, it is because of the discrepancy between the CUDA device and the graphics device.
[33mPlease ensure that the CUDA device and the graphics device are the same. (IGE_env_manager.py:132)
*** Can't create empty tensor
WARNING: allocation matrix is not full rank. Rank: 4
creating render graph
Module warp.utils load on device 'cuda:0' took 1.36 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_camera_kernels load on device 'cuda:0' took 7.59 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_stereo_camera_kernels load on device 'cuda:0' took 11.94 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_lidar_kernels load on device 'cuda:0' took 5.51 ms
finishing capture of render graph
Encoder network initialized.
Defined encoder.
[ImgDecoder] Starting create_model
[ImgDecoder] Done with create_model
Defined decoder.
Loading weights from file:  /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/utils/vae/weights/ICRA_test_set_more_sim_data_kld_beta_3_LD_64_epoch_49.pth
[37m[2633 ms][IsaacGymEnvManager] - INFO : Created Isaac Gym Simulation Object (IGE_env_manager.py:136)
[37m[2633 ms][IsaacGymEnvManager] - INFO : Created Isaac Gym Environment (IGE_env_manager.py:43)
[37m[2838 ms][env_manager] - INFO : IGE object instantiated. (env_manager.py:109)
[37m[2838 ms][env_manager] - INFO : Creating warp environment. (env_manager.py:112)
[37m[2838 ms][env_manager] - INFO : Warp environment created. (env_manager.py:114)
[37m[2838 ms][env_manager] - INFO : Creating robot manager. (env_manager.py:118)
[37m[2838 ms][BaseRobot] - INFO : [DONE] Initializing controller (base_robot.py:26)
[37m[2838 ms][BaseRobot] - INFO : Initializing controller lmf2_velocity_control (base_robot.py:29)
[33m[2838 ms][base_multirotor] - WARNING : Creating 16 multirotors. (base_multirotor.py:32)
[37m[2838 ms][env_manager] - INFO : [DONE] Creating robot manager. (env_manager.py:123)
[37m[2838 ms][env_manager] - INFO : [DONE] Creating simulation instance. (env_manager.py:125)
[37m[2838 ms][asset_loader] - INFO : Loading asset: model.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2839 ms][asset_loader] - INFO : Loading asset: panel.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2840 ms][asset_loader] - INFO : Loading asset: cuboidal_rod.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2841 ms][asset_loader] - INFO : Loading asset: 0_5_x_0_5_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2842 ms][asset_loader] - INFO : Loading asset: small_cube.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2843 ms][asset_loader] - INFO : Loading asset: left_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2844 ms][asset_loader] - INFO : Loading asset: right_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2845 ms][asset_loader] - INFO : Loading asset: back_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2846 ms][asset_loader] - INFO : Loading asset: front_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2847 ms][asset_loader] - INFO : Loading asset: bottom_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2848 ms][asset_loader] - INFO : Loading asset: top_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2849 ms][asset_loader] - INFO : Loading asset: gate.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2851 ms][asset_loader] - INFO : Loading asset: 1_x_1_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2853 ms][env_manager] - INFO : Populating environment 0 (env_manager.py:179)
[33m[3228 ms][robot_manager] - WARNING : 
[33mRobot mass: 1.2400000467896461,
[33mInertia: tensor([[0.0134, 0.0000, 0.0000],
[33m        [0.0000, 0.0144, 0.0000],
[33m        [0.0000, 0.0000, 0.0138]], device='cuda:0'),
[33mRobot COM: tensor([[0., 0., 0., 1.]], device='cuda:0') (robot_manager.py:437)
[33m[3228 ms][robot_manager] - WARNING : Calculated robot mass and inertia for this robot. This code assumes that your robot is the same across environments. (robot_manager.py:440)
[31m[3228 ms][robot_manager] - CRITICAL : If your robot differs across environments you need to perform this computation for each different robot here. (robot_manager.py:443)
[37m[3252 ms][env_manager] - INFO : [DONE] Populating environments. (env_manager.py:75)
[33m[3259 ms][IsaacGymEnvManager] - WARNING : Headless: True (IGE_env_manager.py:424)
[37m[3259 ms][IsaacGymEnvManager] - INFO : Headless mode. Viewer not created. (IGE_env_manager.py:434)
[33m[3315 ms][asset_manager] - WARNING : Number of obstacles to be kept in the environment: 9 (asset_manager.py:32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/control/motor_model.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(self.min_thrust, device=self.device, dtype=torch.float32).expand(
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/control/motor_model.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(self.max_thrust, device=self.device, dtype=torch.float32).expand(
[33m[3505 ms][control_allocation] - WARNING : Control allocation does not account for actuator limits. This leads to suboptimal allocation (control_allocation.py:48)
[37m[3506 ms][WarpSensor] - INFO : Camera sensor initialized (warp_sensor.py:50)
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.
  logger.warn(
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.
  logger.warn(
[36m[2025-06-29 17:34:17,570][188040] Env info: EnvInfo(obs_space=Dict('image_obs': Box(-1.0, 1.0, (1, 135, 240), float32), 'observations': Box(-1.0, 1.0, (81,), float32)), action_space=Box(-1.0, 1.0, (4,), float32), num_agents=16, gpu_actions=True, gpu_observations=True, action_splits=None, all_discrete=None, frameskip=1, reward_shaping_scheme=None, env_info_protocol_version=1)
[36m[2025-06-29 17:34:18,250][187912] Automatically setting recurrence to 32
[33m[2025-06-29 17:34:18,251][187912] In serial mode all components run on the same process. Only use async_rl and serial mode together for debugging.
[36m[2025-06-29 17:34:18,251][187912] Starting experiment with the following configuration:
[36mhelp=False
[36malgo=APPO
[36menv=dce_navigation_task
[36mexperiment=16env_first_test
[36mtrain_dir=/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir
[36mrestart_behavior=overwrite
[36mdevice=gpu
[36mseed=None
[36mnum_policies=1
[36masync_rl=True
[36mserial_mode=True
[36mbatched_sampling=True
[36mnum_batches_to_accumulate=2
[36mworker_num_splits=1
[36mpolicy_workers_per_policy=1
[36mmax_policy_lag=1000
[36mnum_workers=1
[36mnum_envs_per_worker=1
[36mbatch_size=1024
[36mnum_batches_per_epoch=4
[36mnum_epochs=4
[36mrollout=32
[36mrecurrence=32
[36mshuffle_minibatches=True
[36mgamma=0.98
[36mreward_scale=0.1
[36mreward_clip=1000.0
[36mvalue_bootstrap=True
[36mnormalize_returns=True
[36mexploration_loss_coeff=0.0
[36mvalue_loss_coeff=2.0
[36mkl_loss_coeff=0.1
[36mexploration_loss=entropy
[36mgae_lambda=0.95
[36mppo_clip_ratio=0.2
[36mppo_clip_value=1.0
[36mwith_vtrace=False
[36mvtrace_rho=1.0
[36mvtrace_c=1.0
[36moptimizer=adam
[36madam_eps=1e-06
[36madam_beta1=0.9
[36madam_beta2=0.999
[36mmax_grad_norm=1.0
[36mlearning_rate=0.0001
[36mlr_schedule=kl_adaptive_epoch
[36mlr_schedule_kl_threshold=0.016
[36mlr_adaptive_min=1e-06
[36mlr_adaptive_max=0.01
[36mobs_subtract_mean=0.0
[36mobs_scale=1.0
[36mnormalize_input=True
[36mnormalize_input_keys=None
[36mdecorrelate_experience_max_seconds=0
[36mdecorrelate_envs_on_one_worker=True
[36mactor_worker_gpus=[0]
[36mset_workers_cpu_affinity=True
[36mforce_envs_single_thread=False
[36mdefault_niceness=0
[36mlog_to_file=True
[36mexperiment_summaries_interval=10
[36mflush_summaries_interval=30
[36mstats_avg=100
[36msummaries_use_frameskip=True
[36mheartbeat_interval=20
[36mheartbeat_reporting_interval=180
[36mtrain_for_env_steps=100000000
[36mtrain_for_seconds=10000000000
[36msave_every_sec=120
[36mkeep_checkpoints=2
[36mload_checkpoint_kind=latest
[36msave_milestones_sec=-1
[36msave_best_every_sec=5
[36msave_best_metric=reward
[36msave_best_after=1000000
[36mbenchmark=False
[36mencoder_mlp_layers=[256, 128, 64]
[36mencoder_conv_architecture=convnet_simple
[36mencoder_conv_mlp_layers=[512]
[36muse_rnn=True
[36mrnn_size=64
[36mrnn_type=gru
[36mrnn_num_layers=1
[36mdecoder_mlp_layers=[]
[36mnonlinearity=elu
[36mpolicy_initialization=torch_default
[36mpolicy_init_gain=1.0
[36mactor_critic_share_weights=True
[36madaptive_stddev=True
[36mcontinuous_tanh_scale=0.0
[36minitial_stddev=1.0
[36muse_env_info_cache=False
[36menv_gpu_actions=True
[36menv_gpu_observations=True
[36menv_frameskip=1
[36menv_framestack=1
[36mpixel_format=CHW
[36muse_record_episode_statistics=False
[36mwith_wandb=True
[36mwandb_user=ziya-ruso-ucl
[36mwandb_project=aerialgym-dce-navigation
[36mwandb_group=dce_navigation_training
[36mwandb_job_type=SF
[36mwandb_tags=['aerial_gym', 'dce', 'navigation', 'sample_factory']
[36mwith_pbt=False
[36mpbt_mix_policies_in_one_env=True
[36mpbt_period_env_steps=5000000
[36mpbt_start_mutation=20000000
[36mpbt_replace_fraction=0.3
[36mpbt_mutation_rate=0.15
[36mpbt_replace_reward_gap=0.1
[36mpbt_replace_reward_gap_absolute=1e-06
[36mpbt_optimize_gamma=False
[36mpbt_target_objective=true_objective
[36mpbt_perturb_min=1.1
[36mpbt_perturb_max=1.5
[36menv_agents=-1
[36mobs_key=obs
[36msubtask=None
[36mige_api_version=preview4
[36meval_stats=False
[36mcommand_line=--env=dce_navigation_task --train_for_env_steps=100000000 --experiment=16env_first_test --async_rl=True --use_env_info_cache=False --normalize_input=True
[36mcli_args={'env': 'dce_navigation_task', 'experiment': '16env_first_test', 'async_rl': True, 'normalize_input': True, 'train_for_env_steps': 100000000, 'use_env_info_cache': False}
[36mgit_hash=7f35eed17f2afcde33e3a7aec669b48e9e8e34cd
[36mgit_repo_name=https://github.com/ntnu-arl/aerial_gym_simulator.git
[36mwandb_unique_id=16env_first_test_20250629_173410_095735
[36m[2025-06-29 17:34:18,251][187912] Saving configuration to /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/config.json...
[36m[2025-06-29 17:34:18,303][187912] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[36m[2025-06-29 17:34:18,303][187912] Rollout worker 0 uses device cuda:0
[36m[2025-06-29 17:34:18,507][187912] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[37m[1m[2025-06-29 17:34:18,507][187912] InferenceWorker_p0-w0: min num requests: 1
[36m[2025-06-29 17:34:18,508][187912] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[37m[1m[2025-06-29 17:34:18,509][187912] Starting seed is not provided
[36m[2025-06-29 17:34:18,509][187912] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[36m[2025-06-29 17:34:18,509][187912] Initializing actor-critic model on device cuda:0
[36m[2025-06-29 17:34:18,510][187912] RunningMeanStd input shape: (1, 135, 240)
[36m[2025-06-29 17:34:18,510][187912] RunningMeanStd input shape: (81,)
[36m[2025-06-29 17:34:18,511][187912] RunningMeanStd input shape: (1,)
[36m[2025-06-29 17:34:18,524][187912] ConvEncoder: input_channels=1
[36m[2025-06-29 17:34:18,627][187912] Conv encoder output size: 512
[36m[2025-06-29 17:34:18,638][187912] Created Actor Critic model with architecture:
[36m[2025-06-29 17:34:18,639][187912] ActorCriticSharedWeights(
[36m  (obs_normalizer): ObservationNormalizer(
[36m    (running_mean_std): RunningMeanStdDictInPlace(
[36m      (running_mean_std): ModuleDict(
[36m        (image_obs): RunningMeanStdInPlace()
[36m        (observations): RunningMeanStdInPlace()
[36m      )
[36m    )
[36m  )
[36m  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)
[36m  (encoder): CustomEncoder(
[36m    (encoders): ModuleDict(
[36m      (image_obs): ConvEncoder(
[36m        (enc): RecursiveScriptModule(
[36m          original_name=ConvEncoderImpl
[36m          (conv_head): RecursiveScriptModule(
[36m            original_name=Sequential
[36m            (0): RecursiveScriptModule(original_name=Conv2d)
[36m            (1): RecursiveScriptModule(original_name=ELU)
[36m            (2): RecursiveScriptModule(original_name=Conv2d)
[36m            (3): RecursiveScriptModule(original_name=ELU)
[36m            (4): RecursiveScriptModule(original_name=Conv2d)
[36m            (5): RecursiveScriptModule(original_name=ELU)
[36m          )
[36m          (mlp_layers): RecursiveScriptModule(
[36m            original_name=Sequential
[36m            (0): RecursiveScriptModule(original_name=Linear)
[36m            (1): RecursiveScriptModule(original_name=ELU)
[36m          )
[36m        )
[36m      )
[36m    )
[36m    (mlp_head_custom): RecursiveScriptModule(
[36m      original_name=Sequential
[36m      (0): RecursiveScriptModule(original_name=Linear)
[36m      (1): RecursiveScriptModule(original_name=ELU)
[36m      (2): RecursiveScriptModule(original_name=Linear)
[36m      (3): RecursiveScriptModule(original_name=ELU)
[36m      (4): RecursiveScriptModule(original_name=Linear)
[36m      (5): RecursiveScriptModule(original_name=ELU)
[36m    )
[36m  )
[36m  (core): ModelCoreRNN(
[36m    (core): GRU(64, 64)
[36m  )
[36m  (decoder): MlpDecoder(
[36m    (mlp): Identity()
[36m  )
[36m  (critic_linear): Linear(in_features=64, out_features=1, bias=True)
[36m  (action_parameterization): ActionParameterizationDefault(
[36m    (distribution_linear): Linear(in_features=64, out_features=8, bias=True)
[36m  )
[36m)
[36m[2025-06-29 17:34:19,063][187912] Using optimizer <class 'torch.optim.adam.Adam'>
[33m[2025-06-29 17:34:19,064][187912] No checkpoints found
[36m[2025-06-29 17:34:19,064][187912] Did not load from checkpoint, starting from scratch!
[36m[2025-06-29 17:34:19,064][187912] Initialized policy 0 weights for model version 0
[36m[2025-06-29 17:34:19,064][187912] LearnerWorker_p0 finished initialization!
[36m[2025-06-29 17:34:19,064][187912] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[37m[1m[2025-06-29 17:34:19,262][187912] Inference worker 0-0 is ready!
[37m[1m[2025-06-29 17:34:19,263][187912] All inference workers are ready! Signal rollout workers to start!
[36m[2025-06-29 17:34:19,263][187912] EnvRunner 0-0 uses policy 0
[31m[12537 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task] - CRITICAL : Setting number of environments to 16 for parallel training. (dce_navigation_task.py:18)
[31m[12537 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task] - CRITICAL : Enabling viewer for visual training feedback. (dce_navigation_task.py:34)
[37m[12537 ms][base_task] - INFO : Setting seed: 3687691632 (base_task.py:38)
[37m[12538 ms][navigation_task] - INFO : Building environment for navigation task. (navigation_task.py:44)
[37m[12538 ms][navigation_task] - INFO : Sim Name: base_sim, Env Name: env_with_obstacles, Robot Name: lmf2, Controller Name: lmf2_velocity_control (navigation_task.py:45)
[37m[12538 ms][env_manager] - INFO : Populating environments. (env_manager.py:73)
[37m[12538 ms][env_manager] - INFO : Creating simulation instance. (env_manager.py:87)
[37m[12538 ms][env_manager] - INFO : Instantiating IGE object. (env_manager.py:88)
[37m[12538 ms][IsaacGymEnvManager] - INFO : Creating Isaac Gym Environment (IGE_env_manager.py:41)
[37m[12538 ms][IsaacGymEnvManager] - INFO : Acquiring gym object (IGE_env_manager.py:73)
[37m[12538 ms][IsaacGymEnvManager] - INFO : Acquired gym object (IGE_env_manager.py:75)
[37m[12540 ms][IsaacGymEnvManager] - INFO : Fixing devices (IGE_env_manager.py:89)
[37m[12540 ms][IsaacGymEnvManager] - INFO : Using GPU pipeline for simulation. (IGE_env_manager.py:102)
[37m[12540 ms][IsaacGymEnvManager] - INFO : Sim Device type: cuda, Sim Device ID: 0 (IGE_env_manager.py:105)
[31m[12540 ms][IsaacGymEnvManager] - CRITICAL : 
[31m Setting graphics device to -1.
[31m This is done because the simulation is run in headless mode and no Isaac Gym cameras are used.
[31m No need to worry. The simulation and warp rendering will work as expected. (IGE_env_manager.py:112)
[37m[12540 ms][IsaacGymEnvManager] - INFO : Graphics Device ID: -1 (IGE_env_manager.py:119)
[37m[12540 ms][IsaacGymEnvManager] - INFO : Creating Isaac Gym Simulation Object (IGE_env_manager.py:120)
[33m[12540 ms][IsaacGymEnvManager] - WARNING : If you have set the CUDA_VISIBLE_DEVICES environment variable, please ensure that you set it
[33mto a particular one that works for your system to use the viewer or Isaac Gym cameras.
[33mIf you want to run parallel simulations on multiple GPUs with camera sensors,
[33mplease disable Isaac Gym and use warp (by setting use_warp=True), set the viewer to headless. (IGE_env_manager.py:127)
[33m[12540 ms][IsaacGymEnvManager] - WARNING : If you see a segfault in the next lines, it is because of the discrepancy between the CUDA device and the graphics device.
[33mPlease ensure that the CUDA device and the graphics device are the same. (IGE_env_manager.py:132)
[isaacgym:gymutil.py] Unknown args:  ['--env=dce_navigation_task', '--train_for_env_steps=100000000', '--experiment=16env_first_test', '--async_rl=True', '--use_env_info_cache=False', '--normalize_input=True']
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:0
GPU Pipeline: enabled
*** Can't create empty tensor
WARNING: allocation matrix is not full rank. Rank: 4
creating render graph
Module warp.utils load on device 'cuda:0' took 1.60 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_camera_kernels load on device 'cuda:0' took 7.79 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_stereo_camera_kernels load on device 'cuda:0' took 11.37 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_lidar_kernels load on device 'cuda:0' took 5.63 ms
finishing capture of render graph
Encoder network initialized.
Defined encoder.
[ImgDecoder] Starting create_model
[ImgDecoder] Done with create_model
Defined decoder.
Loading weights from file:  /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/utils/vae/weights/ICRA_test_set_more_sim_data_kld_beta_3_LD_64_epoch_49.pth
[37m[13377 ms][IsaacGymEnvManager] - INFO : Created Isaac Gym Simulation Object (IGE_env_manager.py:136)
[37m[13377 ms][IsaacGymEnvManager] - INFO : Created Isaac Gym Environment (IGE_env_manager.py:43)
[37m[13579 ms][env_manager] - INFO : IGE object instantiated. (env_manager.py:109)
[37m[13580 ms][env_manager] - INFO : Creating warp environment. (env_manager.py:112)
[37m[13580 ms][env_manager] - INFO : Warp environment created. (env_manager.py:114)
[37m[13580 ms][env_manager] - INFO : Creating robot manager. (env_manager.py:118)
[37m[13580 ms][BaseRobot] - INFO : [DONE] Initializing controller (base_robot.py:26)
[37m[13580 ms][BaseRobot] - INFO : Initializing controller lmf2_velocity_control (base_robot.py:29)
[33m[13580 ms][base_multirotor] - WARNING : Creating 16 multirotors. (base_multirotor.py:32)
[37m[13580 ms][env_manager] - INFO : [DONE] Creating robot manager. (env_manager.py:123)
[37m[13580 ms][env_manager] - INFO : [DONE] Creating simulation instance. (env_manager.py:125)
[37m[13580 ms][asset_loader] - INFO : Loading asset: model.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[13581 ms][asset_loader] - INFO : Loading asset: panel.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[13583 ms][asset_loader] - INFO : Loading asset: gate.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[13586 ms][asset_loader] - INFO : Loading asset: small_cube.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[13587 ms][asset_loader] - INFO : Loading asset: 0_5_x_0_5_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[13588 ms][asset_loader] - INFO : Loading asset: cuboidal_rod.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[13589 ms][asset_loader] - INFO : Loading asset: left_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[13590 ms][asset_loader] - INFO : Loading asset: right_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[13591 ms][asset_loader] - INFO : Loading asset: back_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[13592 ms][asset_loader] - INFO : Loading asset: front_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[13592 ms][asset_loader] - INFO : Loading asset: bottom_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[13593 ms][asset_loader] - INFO : Loading asset: top_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[13595 ms][asset_loader] - INFO : Loading asset: 1_x_1_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[13596 ms][env_manager] - INFO : Populating environment 0 (env_manager.py:179)
[33m[13613 ms][robot_manager] - WARNING : 
[33mRobot mass: 1.2400000467896461,
[33mInertia: tensor([[0.0134, 0.0000, 0.0000],
[33m        [0.0000, 0.0144, 0.0000],
[33m        [0.0000, 0.0000, 0.0138]], device='cuda:0'),
[33mRobot COM: tensor([[0., 0., 0., 1.]], device='cuda:0') (robot_manager.py:437)
[33m[13613 ms][robot_manager] - WARNING : Calculated robot mass and inertia for this robot. This code assumes that your robot is the same across environments. (robot_manager.py:440)
[31m[13614 ms][robot_manager] - CRITICAL : If your robot differs across environments you need to perform this computation for each different robot here. (robot_manager.py:443)
[37m[13637 ms][env_manager] - INFO : [DONE] Populating environments. (env_manager.py:75)
[33m[13643 ms][IsaacGymEnvManager] - WARNING : Headless: True (IGE_env_manager.py:424)
[37m[13643 ms][IsaacGymEnvManager] - INFO : Headless mode. Viewer not created. (IGE_env_manager.py:434)
[33m[13671 ms][asset_manager] - WARNING : Number of obstacles to be kept in the environment: 9 (asset_manager.py:32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/control/motor_model.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(self.min_thrust, device=self.device, dtype=torch.float32).expand(
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/control/motor_model.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(self.max_thrust, device=self.device, dtype=torch.float32).expand(
[33m[13875 ms][control_allocation] - WARNING : Control allocation does not account for actuator limits. This leads to suboptimal allocation (control_allocation.py:48)
[37m[13876 ms][WarpSensor] - INFO : Camera sensor initialized (warp_sensor.py:50)
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.
  logger.warn(
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.
  logger.warn(
[31m[16535 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16536 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15],
[31m       device='cuda:0') (navigation_task.py:196)
[31m[16536 ms][navigation_task] - CRITICAL : Time at crash: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0',
[31m       dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 17:34:24,040][187912] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 0. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-06-29 17:34:24,040][187912] Avg episode reward: [(0, '-100.000')]
[31m[19997 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19997 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[19997 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 17:34:27,579][187912] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 49.7. Samples: 176. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-06-29 17:34:27,579][187912] Avg episode reward: [(0, '-84.541')]
[36m[2025-06-29 17:34:32,593][187912] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 261.9. Samples: 2240. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-06-29 17:34:32,593][187912] Avg episode reward: [(0, '-90.488')]
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/torch/nn/modules/module.py:1194: UserWarning: operator() profile_node %104 : int[] = prim::profile_ivalue(%102)
 does not have profile information (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
[36m[2025-06-29 17:34:37,592][187912] Fps is (10 sec: 409.1, 60 sec: 302.2, 300 sec: 302.2). Total num frames: 4096. Throughput: 0: 304.6. Samples: 4128. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-06-29 17:34:37,592][187912] Avg episode reward: [(0, '-98.464')]
[37m[1m[2025-06-29 17:34:38,545][187912] Heartbeat connected on Batcher_0
[37m[1m[2025-06-29 17:34:38,589][187912] Heartbeat connected on LearnerWorker_p0
[37m[1m[2025-06-29 17:34:38,590][187912] Heartbeat connected on InferenceWorker_p0-w0
[37m[1m[2025-06-29 17:34:38,590][187912] Heartbeat connected on RolloutWorker_w0
[36m[2025-06-29 17:34:42,608][187912] Fps is (10 sec: 409.0, 60 sec: 220.6, 300 sec: 220.6). Total num frames: 4096. Throughput: 0: 273.1. Samples: 5072. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-06-29 17:34:42,609][187912] Avg episode reward: [(0, '-98.585')]
[36m[2025-06-29 17:34:47,599][187912] Fps is (10 sec: 0.0, 60 sec: 173.9, 300 sec: 173.9). Total num frames: 4096. Throughput: 0: 292.7. Samples: 6896. Policy #0 lag: (min: 4.0, avg: 4.0, max: 4.0)
[36m[2025-06-29 17:34:47,599][187912] Avg episode reward: [(0, '-98.813')]
[31m[43750 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[43750 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[43751 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 17:34:52,558][187912] Fps is (10 sec: 411.7, 60 sec: 287.3, 300 sec: 287.3). Total num frames: 8192. Throughput: 0: 291.2. Samples: 8304. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 17:34:52,558][187912] Avg episode reward: [(0, '-95.539')]
[36m[2025-06-29 17:34:57,594][187912] Fps is (10 sec: 409.8, 60 sec: 244.1, 300 sec: 244.1). Total num frames: 8192. Throughput: 0: 271.8. Samples: 9120. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 17:34:57,594][187912] Avg episode reward: [(0, '-95.412')]
[36m[2025-06-29 17:35:02,583][187912] Fps is (10 sec: 0.0, 60 sec: 212.5, 300 sec: 212.5). Total num frames: 8192. Throughput: 0: 283.9. Samples: 10944. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 17:35:02,583][187912] Avg episode reward: [(0, '-97.384')]
[36m[2025-06-29 17:35:07,578][187912] Fps is (10 sec: 410.3, 60 sec: 282.2, 300 sec: 282.2). Total num frames: 12288. Throughput: 0: 293.6. Samples: 12784. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 17:35:07,578][187912] Avg episode reward: [(0, '-98.071')]
[36m[2025-06-29 17:35:12,559][187912] Fps is (10 sec: 410.6, 60 sec: 253.3, 300 sec: 253.3). Total num frames: 12288. Throughput: 0: 303.8. Samples: 13840. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 17:35:12,559][187912] Avg episode reward: [(0, '-98.744')]
[36m[2025-06-29 17:35:17,610][187912] Fps is (10 sec: 0.0, 60 sec: 229.4, 300 sec: 229.4). Total num frames: 12288. Throughput: 0: 301.4. Samples: 15808. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 17:35:17,611][187912] Avg episode reward: [(0, '-98.822')]
[36m[2025-06-29 17:35:22,586][187912] Fps is (10 sec: 408.5, 60 sec: 279.8, 300 sec: 279.8). Total num frames: 16384. Throughput: 0: 295.5. Samples: 17424. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 17:35:22,586][187912] Avg episode reward: [(0, '-95.907')]
[31m[76155 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[76155 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[76155 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 17:35:27,568][187912] Fps is (10 sec: 411.3, 60 sec: 273.1, 300 sec: 257.9). Total num frames: 16384. Throughput: 0: 298.6. Samples: 18496. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 17:35:27,568][187912] Avg episode reward: [(0, '-94.292')]
[36m[2025-06-29 17:35:32,596][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 298.7). Total num frames: 20480. Throughput: 0: 302.2. Samples: 20496. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 17:35:32,596][187912] Avg episode reward: [(0, '-97.402')]
[36m[2025-06-29 17:35:37,583][187912] Fps is (10 sec: 409.0, 60 sec: 273.1, 300 sec: 278.5). Total num frames: 20480. Throughput: 0: 313.1. Samples: 22400. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 17:35:37,583][187912] Avg episode reward: [(0, '-95.973')]
[36m[2025-06-29 17:35:42,571][187912] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 260.8). Total num frames: 20480. Throughput: 0: 319.8. Samples: 23504. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 17:35:42,571][187912] Avg episode reward: [(0, '-97.370')]
[36m[2025-06-29 17:35:47,597][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 294.1). Total num frames: 24576. Throughput: 0: 319.5. Samples: 25328. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:35:47,598][187912] Avg episode reward: [(0, '-95.459')]
[36m[2025-06-29 17:35:52,594][187912] Fps is (10 sec: 408.7, 60 sec: 272.9, 300 sec: 277.5). Total num frames: 24576. Throughput: 0: 321.3. Samples: 27248. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:35:52,594][187912] Avg episode reward: [(0, '-96.242')]
[36m[2025-06-29 17:35:57,613][187912] Fps is (10 sec: 409.0, 60 sec: 341.2, 300 sec: 306.4). Total num frames: 28672. Throughput: 0: 318.6. Samples: 28192. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 17:35:57,613][187912] Avg episode reward: [(0, '-96.305')]
[36m[2025-06-29 17:36:02,595][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 290.9). Total num frames: 28672. Throughput: 0: 315.5. Samples: 30000. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 17:36:02,595][187912] Avg episode reward: [(0, '-97.191')]
[36m[2025-06-29 17:36:07,600][187912] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 276.9). Total num frames: 28672. Throughput: 0: 324.9. Samples: 32048. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 17:36:07,601][187912] Avg episode reward: [(0, '-99.586')]
[31m[121652 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[121653 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[121653 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 17:36:12,612][187912] Fps is (10 sec: 408.9, 60 sec: 341.0, 300 sec: 301.8). Total num frames: 32768. Throughput: 0: 317.6. Samples: 32800. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 17:36:12,613][187912] Avg episode reward: [(0, '-97.632')]
[37m[1m[2025-06-29 17:36:12,618][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000000128_32768.pth...
[36m[2025-06-29 17:36:17,576][187912] Fps is (10 sec: 410.6, 60 sec: 341.5, 300 sec: 288.6). Total num frames: 32768. Throughput: 0: 316.2. Samples: 34720. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 17:36:17,577][187912] Avg episode reward: [(0, '-95.031')]
[36m[2025-06-29 17:36:22,564][187912] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 276.5). Total num frames: 32768. Throughput: 0: 317.3. Samples: 36672. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 17:36:22,564][187912] Avg episode reward: [(0, '-90.765')]
[36m[2025-06-29 17:36:27,591][187912] Fps is (10 sec: 409.0, 60 sec: 341.2, 300 sec: 298.4). Total num frames: 36864. Throughput: 0: 305.6. Samples: 37264. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 17:36:27,591][187912] Avg episode reward: [(0, '-89.066')]
[36m[2025-06-29 17:36:32,624][187912] Fps is (10 sec: 407.2, 60 sec: 272.9, 300 sec: 286.7). Total num frames: 36864. Throughput: 0: 304.9. Samples: 39056. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 17:36:32,624][187912] Avg episode reward: [(0, '-89.452')]
[36m[2025-06-29 17:36:37,612][187912] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 276.0). Total num frames: 36864. Throughput: 0: 296.8. Samples: 40608. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 17:36:37,613][187912] Avg episode reward: [(0, '-88.242')]
[36m[2025-06-29 17:36:42,619][187912] Fps is (10 sec: 409.8, 60 sec: 341.1, 300 sec: 295.6). Total num frames: 40960. Throughput: 0: 291.2. Samples: 41296. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 17:36:42,620][187912] Avg episode reward: [(0, '-86.154')]
[36m[2025-06-29 17:36:47,595][187912] Fps is (10 sec: 410.3, 60 sec: 273.1, 300 sec: 285.3). Total num frames: 40960. Throughput: 0: 283.4. Samples: 42752. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 17:36:47,595][187912] Avg episode reward: [(0, '-81.863')]
[36m[2025-06-29 17:36:52,566][187912] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 275.8). Total num frames: 40960. Throughput: 0: 275.1. Samples: 44416. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 17:36:52,566][187912] Avg episode reward: [(0, '-75.631')]
[36m[2025-06-29 17:36:57,572][187912] Fps is (10 sec: 410.5, 60 sec: 273.3, 300 sec: 293.5). Total num frames: 45056. Throughput: 0: 273.3. Samples: 45088. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 17:36:57,573][187912] Avg episode reward: [(0, '-75.946')]
[36m[2025-06-29 17:37:02,603][187912] Fps is (10 sec: 408.1, 60 sec: 273.0, 300 sec: 284.2). Total num frames: 45056. Throughput: 0: 268.6. Samples: 46816. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 17:37:02,604][187912] Avg episode reward: [(0, '-74.264')]
[36m[2025-06-29 17:37:07,616][187912] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 275.4). Total num frames: 45056. Throughput: 0: 269.9. Samples: 48832. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 17:37:07,616][187912] Avg episode reward: [(0, '-74.096')]
[36m[2025-06-29 17:37:12,562][187912] Fps is (10 sec: 411.3, 60 sec: 273.3, 300 sec: 291.7). Total num frames: 49152. Throughput: 0: 275.7. Samples: 49664. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 17:37:12,562][187912] Avg episode reward: [(0, '-65.561')]
[36m[2025-06-29 17:37:17,612][187912] Fps is (10 sec: 409.8, 60 sec: 272.9, 300 sec: 283.2). Total num frames: 49152. Throughput: 0: 284.9. Samples: 51872. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 17:37:17,612][187912] Avg episode reward: [(0, '-63.852')]
[36m[2025-06-29 17:37:22,565][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 298.3). Total num frames: 53248. Throughput: 0: 290.4. Samples: 53664. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:37:22,565][187912] Avg episode reward: [(0, '-58.516')]
[36m[2025-06-29 17:37:27,564][187912] Fps is (10 sec: 411.6, 60 sec: 273.2, 300 sec: 290.1). Total num frames: 53248. Throughput: 0: 302.2. Samples: 54880. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:37:27,564][187912] Avg episode reward: [(0, '-43.377')]
[36m[2025-06-29 17:37:32,635][187912] Fps is (10 sec: 406.8, 60 sec: 341.3, 300 sec: 304.1). Total num frames: 57344. Throughput: 0: 318.6. Samples: 57104. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 17:37:32,635][187912] Avg episode reward: [(0, '-34.347')]
[36m[2025-06-29 17:37:37,581][187912] Fps is (10 sec: 408.9, 60 sec: 341.5, 300 sec: 296.3). Total num frames: 57344. Throughput: 0: 325.6. Samples: 59072. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 17:37:37,581][187912] Avg episode reward: [(0, '-33.094')]
[36m[2025-06-29 17:37:42,589][187912] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 288.8). Total num frames: 57344. Throughput: 0: 335.5. Samples: 60192. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 17:37:42,589][187912] Avg episode reward: [(0, '-22.490')]
[36m[2025-06-29 17:37:47,587][187912] Fps is (10 sec: 409.4, 60 sec: 341.4, 300 sec: 301.8). Total num frames: 61440. Throughput: 0: 341.5. Samples: 62176. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:37:47,587][187912] Avg episode reward: [(0, '-18.406')]
[36m[2025-06-29 17:37:52,579][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 294.6). Total num frames: 61440. Throughput: 0: 345.9. Samples: 64384. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:37:52,579][187912] Avg episode reward: [(0, '-17.820')]
[36m[2025-06-29 17:37:57,590][187912] Fps is (10 sec: 409.5, 60 sec: 341.2, 300 sec: 306.9). Total num frames: 65536. Throughput: 0: 353.2. Samples: 65568. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:37:57,590][187912] Avg episode reward: [(0, '-13.239')]
[36m[2025-06-29 17:38:02,561][187912] Fps is (10 sec: 410.3, 60 sec: 341.6, 300 sec: 299.9). Total num frames: 65536. Throughput: 0: 352.0. Samples: 67696. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:38:02,561][187912] Avg episode reward: [(0, '-8.365')]
[36m[2025-06-29 17:38:07,579][187912] Fps is (10 sec: 410.0, 60 sec: 409.9, 300 sec: 311.5). Total num frames: 69632. Throughput: 0: 355.8. Samples: 69680. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:38:07,579][187912] Avg episode reward: [(0, '-3.569')]
[36m[2025-06-29 17:38:12,565][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 304.7). Total num frames: 69632. Throughput: 0: 355.9. Samples: 70896. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:38:12,565][187912] Avg episode reward: [(0, '-2.448')]
[37m[1m[2025-06-29 17:38:12,611][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000000272_69632.pth...
[36m[2025-06-29 17:38:18,186][187912] Fps is (10 sec: 386.2, 60 sec: 405.7, 300 sec: 314.9). Total num frames: 73728. Throughput: 0: 353.4. Samples: 73200. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 17:38:18,186][187912] Avg episode reward: [(0, '0.203')]
[36m[2025-06-29 17:38:22,565][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 309.1). Total num frames: 73728. Throughput: 0: 359.2. Samples: 75232. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 17:38:22,565][187912] Avg episode reward: [(0, '2.056')]
[36m[2025-06-29 17:38:27,569][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 302.7). Total num frames: 73728. Throughput: 0: 359.6. Samples: 76368. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 17:38:27,569][187912] Avg episode reward: [(0, '4.694')]
[36m[2025-06-29 17:38:32,564][187912] Fps is (10 sec: 409.6, 60 sec: 341.7, 300 sec: 313.1). Total num frames: 77824. Throughput: 0: 357.5. Samples: 78256. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:38:32,564][187912] Avg episode reward: [(0, '0.878')]
[36m[2025-06-29 17:38:37,561][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 307.0). Total num frames: 77824. Throughput: 0: 359.3. Samples: 80544. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:38:37,561][187912] Avg episode reward: [(0, '6.959')]
[36m[2025-06-29 17:38:42,584][187912] Fps is (10 sec: 408.8, 60 sec: 409.6, 300 sec: 316.9). Total num frames: 81920. Throughput: 0: 357.4. Samples: 81648. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 17:38:42,585][187912] Avg episode reward: [(0, '3.420')]
[36m[2025-06-29 17:38:47,592][187912] Fps is (10 sec: 408.3, 60 sec: 341.3, 300 sec: 310.8). Total num frames: 81920. Throughput: 0: 355.3. Samples: 83696. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 17:38:47,592][187912] Avg episode reward: [(0, '9.685')]
[36m[2025-06-29 17:38:53,174][187912] Fps is (10 sec: 386.8, 60 sec: 405.6, 300 sec: 319.6). Total num frames: 86016. Throughput: 0: 355.8. Samples: 85904. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 17:38:53,175][187912] Avg episode reward: [(0, '5.648')]
[36m[2025-06-29 17:38:57,594][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 314.4). Total num frames: 86016. Throughput: 0: 353.9. Samples: 86832. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 17:38:57,594][187912] Avg episode reward: [(0, '7.057')]
[36m[2025-06-29 17:39:02,571][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 308.8). Total num frames: 86016. Throughput: 0: 354.7. Samples: 88944. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 17:39:02,571][187912] Avg episode reward: [(0, '9.442')]
[36m[2025-06-29 17:39:07,595][187912] Fps is (10 sec: 409.6, 60 sec: 341.2, 300 sec: 317.8). Total num frames: 90112. Throughput: 0: 346.8. Samples: 90848. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 17:39:07,595][187912] Avg episode reward: [(0, '13.182')]
[31m[301680 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[301680 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[301680 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 17:39:12,567][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 312.3). Total num frames: 90112. Throughput: 0: 347.4. Samples: 92000. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 17:39:12,568][187912] Avg episode reward: [(0, '15.285')]
[36m[2025-06-29 17:39:17,578][187912] Fps is (10 sec: 410.3, 60 sec: 344.8, 300 sec: 320.9). Total num frames: 94208. Throughput: 0: 354.4. Samples: 94208. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 17:39:17,578][187912] Avg episode reward: [(0, '15.737')]
[36m[2025-06-29 17:39:22,587][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 319.3). Total num frames: 94208. Throughput: 0: 347.5. Samples: 96192. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 17:39:22,587][187912] Avg episode reward: [(0, '24.634')]
[36m[2025-06-29 17:39:28,302][187912] Fps is (10 sec: 381.9, 60 sec: 404.7, 300 sec: 332.4). Total num frames: 98304. Throughput: 0: 344.0. Samples: 97376. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:39:28,302][187912] Avg episode reward: [(0, '16.186')]
[36m[2025-06-29 17:39:32,576][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 319.4). Total num frames: 98304. Throughput: 0: 351.8. Samples: 99520. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:39:32,576][187912] Avg episode reward: [(0, '11.469')]
[31m[328111 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[328111 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[328111 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 17:39:37,603][187912] Fps is (10 sec: 0.0, 60 sec: 341.1, 300 sec: 319.4). Total num frames: 98304. Throughput: 0: 356.9. Samples: 101760. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:39:37,603][187912] Avg episode reward: [(0, '5.968')]
[36m[2025-06-29 17:39:42,607][187912] Fps is (10 sec: 408.3, 60 sec: 341.2, 300 sec: 333.2). Total num frames: 102400. Throughput: 0: 351.9. Samples: 102672. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:39:42,607][187912] Avg episode reward: [(0, '5.880')]
[36m[2025-06-29 17:39:47,590][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 319.3). Total num frames: 102400. Throughput: 0: 355.4. Samples: 104944. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:39:47,590][187912] Avg episode reward: [(0, '1.513')]
[36m[2025-06-29 17:39:52,565][187912] Fps is (10 sec: 411.3, 60 sec: 344.8, 300 sec: 333.3). Total num frames: 106496. Throughput: 0: 356.5. Samples: 106880. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 17:39:52,565][187912] Avg episode reward: [(0, '10.849')]
[36m[2025-06-29 17:39:57,594][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 333.2). Total num frames: 106496. Throughput: 0: 357.1. Samples: 108080. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 17:39:57,594][187912] Avg episode reward: [(0, '13.530')]
[36m[2025-06-29 17:40:02,695][187912] Fps is (10 sec: 404.3, 60 sec: 408.8, 300 sec: 333.1). Total num frames: 110592. Throughput: 0: 356.8. Samples: 110304. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:40:02,695][187912] Avg episode reward: [(0, '10.159')]
[31m[357656 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[357657 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[357657 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 17:40:07,568][187912] Fps is (10 sec: 410.6, 60 sec: 341.5, 300 sec: 333.2). Total num frames: 110592. Throughput: 0: 356.4. Samples: 112224. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:40:07,569][187912] Avg episode reward: [(0, '6.977')]
[36m[2025-06-29 17:40:12,558][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 333.3). Total num frames: 110592. Throughput: 0: 360.8. Samples: 113344. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:40:12,558][187912] Avg episode reward: [(0, '5.300')]
[37m[1m[2025-06-29 17:40:12,614][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000000432_110592.pth...
[36m[2025-06-29 17:40:12,683][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000000128_32768.pth
[36m[2025-06-29 17:40:17,590][187912] Fps is (10 sec: 408.7, 60 sec: 341.3, 300 sec: 333.2). Total num frames: 114688. Throughput: 0: 351.9. Samples: 115360. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:40:17,590][187912] Avg episode reward: [(0, '6.441')]
[36m[2025-06-29 17:40:22,574][187912] Fps is (10 sec: 408.9, 60 sec: 341.4, 300 sec: 333.2). Total num frames: 114688. Throughput: 0: 356.9. Samples: 117808. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:40:22,574][187912] Avg episode reward: [(0, '9.333')]
[36m[2025-06-29 17:40:27,574][187912] Fps is (10 sec: 410.2, 60 sec: 345.5, 300 sec: 333.3). Total num frames: 118784. Throughput: 0: 359.0. Samples: 118816. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 17:40:27,575][187912] Avg episode reward: [(0, '15.050')]
[33m[385004 ms][navigation_task] - WARNING : Curriculum Level: 15, Curriculum progress fraction: 0.0 (navigation_task.py:262)
[33m[385004 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.0
[33mCrash Rate: 0.69970703125
[33mTimeout Rate: 0.30029296875 (navigation_task.py:265)
[33m[385004 ms][navigation_task] - WARNING : 
[33mSuccesses: 0
[33mCrashes : 1433
[33mTimeouts: 615 (navigation_task.py:268)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/task/navigation_task/navigation_task.py:275: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.infos["curriculum/success_rate"] = torch.tensor(success_rate, dtype=torch.float32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/task/navigation_task/navigation_task.py:276: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.infos["curriculum/crash_rate"] = torch.tensor(crash_rate, dtype=torch.float32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/task/navigation_task/navigation_task.py:277: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.infos["curriculum/timeout_rate"] = torch.tensor(timeout_rate, dtype=torch.float32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/task/navigation_task/navigation_task.py:278: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.infos["curriculum/total_successes"] = torch.tensor(self.success_aggregate, dtype=torch.float32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/task/navigation_task/navigation_task.py:279: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.infos["curriculum/total_crashes"] = torch.tensor(self.crashes_aggregate, dtype=torch.float32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/task/navigation_task/navigation_task.py:280: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.infos["curriculum/total_timeouts"] = torch.tensor(self.timeouts_aggregate, dtype=torch.float32)
[36m[2025-06-29 17:40:32,568][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 333.3). Total num frames: 118784. Throughput: 0: 359.6. Samples: 121120. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 17:40:32,568][187912] Avg episode reward: [(0, '20.003')]
[36m[2025-06-29 17:40:37,580][187912] Fps is (10 sec: 409.4, 60 sec: 409.8, 300 sec: 347.1). Total num frames: 122880. Throughput: 0: 359.3. Samples: 123056. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:40:37,580][187912] Avg episode reward: [(0, '19.899')]
[36m[2025-06-29 17:40:42,585][187912] Fps is (10 sec: 408.9, 60 sec: 341.5, 300 sec: 333.2). Total num frames: 122880. Throughput: 0: 357.4. Samples: 124160. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:40:42,585][187912] Avg episode reward: [(0, '20.745')]
[36m[2025-06-29 17:40:48,289][187912] Fps is (10 sec: 382.5, 60 sec: 404.9, 300 sec: 346.3). Total num frames: 126976. Throughput: 0: 354.8. Samples: 126480. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 17:40:48,289][187912] Avg episode reward: [(0, '14.687')]
[36m[2025-06-29 17:40:52,566][187912] Fps is (10 sec: 410.4, 60 sec: 341.3, 300 sec: 333.3). Total num frames: 126976. Throughput: 0: 360.2. Samples: 128432. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 17:40:52,566][187912] Avg episode reward: [(0, '12.796')]
[36m[2025-06-29 17:40:57,592][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 333.2). Total num frames: 126976. Throughput: 0: 361.7. Samples: 129632. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 17:40:57,593][187912] Avg episode reward: [(0, '11.858')]
[36m[2025-06-29 17:41:02,594][187912] Fps is (10 sec: 408.4, 60 sec: 341.9, 300 sec: 347.1). Total num frames: 131072. Throughput: 0: 361.6. Samples: 131632. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 17:41:02,594][187912] Avg episode reward: [(0, '15.416')]
[36m[2025-06-29 17:41:07,566][187912] Fps is (10 sec: 410.7, 60 sec: 341.3, 300 sec: 333.3). Total num frames: 131072. Throughput: 0: 358.8. Samples: 133952. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 17:41:07,567][187912] Avg episode reward: [(0, '8.748')]
[36m[2025-06-29 17:41:12,564][187912] Fps is (10 sec: 410.8, 60 sec: 409.6, 300 sec: 347.1). Total num frames: 135168. Throughput: 0: 360.6. Samples: 135040. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:41:12,564][187912] Avg episode reward: [(0, '13.572')]
[36m[2025-06-29 17:41:17,591][187912] Fps is (10 sec: 408.6, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 135168. Throughput: 0: 353.2. Samples: 137024. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:41:17,591][187912] Avg episode reward: [(0, '12.500')]
[36m[2025-06-29 17:41:22,569][187912] Fps is (10 sec: 409.4, 60 sec: 409.6, 300 sec: 347.1). Total num frames: 139264. Throughput: 0: 361.0. Samples: 139296. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:41:22,570][187912] Avg episode reward: [(0, '11.090')]
[36m[2025-06-29 17:41:27,596][187912] Fps is (10 sec: 409.4, 60 sec: 341.2, 300 sec: 347.2). Total num frames: 139264. Throughput: 0: 361.2. Samples: 140416. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:41:27,596][187912] Avg episode reward: [(0, '5.984')]
[31m[444420 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[444421 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[444421 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 17:41:32,567][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.2). Total num frames: 139264. Throughput: 0: 366.4. Samples: 142704. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:41:32,567][187912] Avg episode reward: [(0, '11.099')]
[36m[2025-06-29 17:41:37,565][187912] Fps is (10 sec: 410.9, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 143360. Throughput: 0: 362.3. Samples: 144736. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 17:41:37,565][187912] Avg episode reward: [(0, '1.685')]
[36m[2025-06-29 17:41:42,588][187912] Fps is (10 sec: 408.7, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 143360. Throughput: 0: 359.9. Samples: 145824. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 17:41:42,588][187912] Avg episode reward: [(0, '2.638')]
[36m[2025-06-29 17:41:47,583][187912] Fps is (10 sec: 408.8, 60 sec: 345.4, 300 sec: 361.0). Total num frames: 147456. Throughput: 0: 359.2. Samples: 147792. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 17:41:47,584][187912] Avg episode reward: [(0, '-0.675')]
[36m[2025-06-29 17:41:52,578][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 147456. Throughput: 0: 355.8. Samples: 149968. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 17:41:52,579][187912] Avg episode reward: [(0, '-1.788')]
[36m[2025-06-29 17:41:57,580][187912] Fps is (10 sec: 409.8, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 151552. Throughput: 0: 356.5. Samples: 151088. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 17:41:57,580][187912] Avg episode reward: [(0, '-0.630')]
[36m[2025-06-29 17:42:02,582][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 151552. Throughput: 0: 355.6. Samples: 153024. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 17:42:02,582][187912] Avg episode reward: [(0, '2.397')]
[36m[2025-06-29 17:42:07,597][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 151552. Throughput: 0: 354.3. Samples: 155248. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 17:42:07,598][187912] Avg episode reward: [(0, '5.417')]
[36m[2025-06-29 17:42:12,566][187912] Fps is (10 sec: 410.3, 60 sec: 341.3, 300 sec: 361.1). Total num frames: 155648. Throughput: 0: 349.4. Samples: 156128. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:42:12,566][187912] Avg episode reward: [(0, '3.292')]
[37m[1m[2025-06-29 17:42:12,609][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000000608_155648.pth...
[36m[2025-06-29 17:42:12,676][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000000272_69632.pth
[36m[2025-06-29 17:42:17,576][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 155648. Throughput: 0: 348.7. Samples: 158400. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:42:17,576][187912] Avg episode reward: [(0, '9.355')]
[36m[2025-06-29 17:42:22,569][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 159744. Throughput: 0: 351.3. Samples: 160544. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 17:42:22,570][187912] Avg episode reward: [(0, '5.097')]
[36m[2025-06-29 17:42:27,592][187912] Fps is (10 sec: 409.0, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 159744. Throughput: 0: 352.0. Samples: 161664. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 17:42:27,592][187912] Avg episode reward: [(0, '9.069')]
[36m[2025-06-29 17:42:32,559][187912] Fps is (10 sec: 410.0, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 163840. Throughput: 0: 357.5. Samples: 163872. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:42:32,560][187912] Avg episode reward: [(0, '5.230')]
[36m[2025-06-29 17:42:37,591][187912] Fps is (10 sec: 409.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 163840. Throughput: 0: 353.7. Samples: 165888. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:42:37,592][187912] Avg episode reward: [(0, '5.062')]
[36m[2025-06-29 17:42:42,972][187912] Fps is (10 sec: 393.4, 60 sec: 407.0, 300 sec: 360.5). Total num frames: 167936. Throughput: 0: 352.8. Samples: 167104. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:42:42,973][187912] Avg episode reward: [(0, '2.911')]
[36m[2025-06-29 17:42:47,580][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 167936. Throughput: 0: 359.5. Samples: 169200. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:42:47,581][187912] Avg episode reward: [(0, '2.114')]
[36m[2025-06-29 17:42:52,561][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 167936. Throughput: 0: 363.7. Samples: 171600. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:42:52,561][187912] Avg episode reward: [(0, '0.596')]
[36m[2025-06-29 17:42:57,559][187912] Fps is (10 sec: 410.5, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 172032. Throughput: 0: 364.5. Samples: 172528. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 17:42:57,559][187912] Avg episode reward: [(0, '2.642')]
[36m[2025-06-29 17:43:02,569][187912] Fps is (10 sec: 409.3, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 172032. Throughput: 0: 360.9. Samples: 174640. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 17:43:02,569][187912] Avg episode reward: [(0, '6.605')]
[31m[538431 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[538431 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[538431 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 17:43:07,567][187912] Fps is (10 sec: 409.3, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 176128. Throughput: 0: 353.8. Samples: 176464. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:43:07,567][187912] Avg episode reward: [(0, '3.341')]
[36m[2025-06-29 17:43:12,580][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 347.8). Total num frames: 176128. Throughput: 0: 355.3. Samples: 177648. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:43:12,580][187912] Avg episode reward: [(0, '1.528')]
[36m[2025-06-29 17:43:17,756][187912] Fps is (10 sec: 402.0, 60 sec: 408.4, 300 sec: 360.8). Total num frames: 180224. Throughput: 0: 355.8. Samples: 179952. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 17:43:17,756][187912] Avg episode reward: [(0, '1.061')]
[36m[2025-06-29 17:43:22,601][187912] Fps is (10 sec: 408.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 180224. Throughput: 0: 359.0. Samples: 182048. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 17:43:22,601][187912] Avg episode reward: [(0, '4.041')]
[36m[2025-06-29 17:43:27,587][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 180224. Throughput: 0: 357.2. Samples: 183040. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 17:43:27,587][187912] Avg episode reward: [(0, '1.685')]
[36m[2025-06-29 17:43:32,589][187912] Fps is (10 sec: 410.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 184320. Throughput: 0: 350.9. Samples: 184992. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 17:43:32,589][187912] Avg episode reward: [(0, '4.903')]
[36m[2025-06-29 17:43:37,580][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 184320. Throughput: 0: 346.5. Samples: 187200. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 17:43:37,580][187912] Avg episode reward: [(0, '6.967')]
[36m[2025-06-29 17:43:42,574][187912] Fps is (10 sec: 410.2, 60 sec: 343.6, 300 sec: 361.0). Total num frames: 188416. Throughput: 0: 347.6. Samples: 188176. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:43:42,575][187912] Avg episode reward: [(0, '2.198')]
[36m[2025-06-29 17:43:47,592][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 347.8). Total num frames: 188416. Throughput: 0: 347.6. Samples: 190288. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:43:47,593][187912] Avg episode reward: [(0, '7.726')]
[31m[581301 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[581301 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[581301 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 17:43:52,742][187912] Fps is (10 sec: 402.8, 60 sec: 408.4, 300 sec: 360.8). Total num frames: 192512. Throughput: 0: 331.9. Samples: 191456. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:43:52,742][187912] Avg episode reward: [(0, '0.360')]
[36m[2025-06-29 17:43:57,577][187912] Fps is (10 sec: 410.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 192512. Throughput: 0: 352.7. Samples: 193520. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:43:57,578][187912] Avg episode reward: [(0, '-1.550')]
[36m[2025-06-29 17:44:02,587][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 192512. Throughput: 0: 354.4. Samples: 195840. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:44:02,587][187912] Avg episode reward: [(0, '-1.218')]
[36m[2025-06-29 17:44:07,583][187912] Fps is (10 sec: 409.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 196608. Throughput: 0: 353.2. Samples: 197936. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:44:07,583][187912] Avg episode reward: [(0, '-1.812')]
[36m[2025-06-29 17:44:12,588][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 196608. Throughput: 0: 357.0. Samples: 199104. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:44:12,589][187912] Avg episode reward: [(0, '2.292')]
[37m[1m[2025-06-29 17:44:12,632][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000000768_196608.pth...
[36m[2025-06-29 17:44:12,693][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000000432_110592.pth
[36m[2025-06-29 17:44:17,593][187912] Fps is (10 sec: 409.2, 60 sec: 342.3, 300 sec: 361.0). Total num frames: 200704. Throughput: 0: 356.6. Samples: 201040. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 17:44:17,594][187912] Avg episode reward: [(0, '8.525')]
[36m[2025-06-29 17:44:22,577][187912] Fps is (10 sec: 410.1, 60 sec: 341.5, 300 sec: 348.0). Total num frames: 200704. Throughput: 0: 360.6. Samples: 203424. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 17:44:22,577][187912] Avg episode reward: [(0, '1.772')]
[36m[2025-06-29 17:44:27,573][187912] Fps is (10 sec: 410.4, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 204800. Throughput: 0: 363.4. Samples: 204528. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 17:44:27,573][187912] Avg episode reward: [(0, '4.894')]
[36m[2025-06-29 17:44:32,582][187912] Fps is (10 sec: 409.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 204800. Throughput: 0: 362.0. Samples: 206576. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 17:44:32,582][187912] Avg episode reward: [(0, '0.595')]
[36m[2025-06-29 17:44:37,972][187912] Fps is (10 sec: 393.9, 60 sec: 406.9, 300 sec: 360.6). Total num frames: 208896. Throughput: 0: 384.9. Samples: 208864. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:44:37,972][187912] Avg episode reward: [(0, '-5.692')]
[36m[2025-06-29 17:44:42,558][187912] Fps is (10 sec: 410.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 208896. Throughput: 0: 362.5. Samples: 209824. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:44:42,558][187912] Avg episode reward: [(0, '-2.313')]
[36m[2025-06-29 17:44:47,589][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 208896. Throughput: 0: 359.8. Samples: 212032. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:44:47,589][187912] Avg episode reward: [(0, '-2.311')]
[36m[2025-06-29 17:44:52,588][187912] Fps is (10 sec: 408.3, 60 sec: 342.2, 300 sec: 361.0). Total num frames: 212992. Throughput: 0: 358.4. Samples: 214064. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:44:52,589][187912] Avg episode reward: [(0, '-2.012')]
[36m[2025-06-29 17:44:57,581][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 347.3). Total num frames: 212992. Throughput: 0: 357.0. Samples: 215168. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:44:57,581][187912] Avg episode reward: [(0, '-3.538')]
[36m[2025-06-29 17:45:02,575][187912] Fps is (10 sec: 410.2, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 217088. Throughput: 0: 357.5. Samples: 217120. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 17:45:02,575][187912] Avg episode reward: [(0, '4.845')]
[36m[2025-06-29 17:45:07,562][187912] Fps is (10 sec: 410.4, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 217088. Throughput: 0: 351.8. Samples: 219248. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 17:45:07,562][187912] Avg episode reward: [(0, '2.602')]
[36m[2025-06-29 17:45:13,266][187912] Fps is (10 sec: 383.1, 60 sec: 405.0, 300 sec: 360.2). Total num frames: 221184. Throughput: 0: 347.7. Samples: 220416. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:45:13,267][187912] Avg episode reward: [(0, '1.309')]
[36m[2025-06-29 17:45:17,562][187912] Fps is (10 sec: 409.6, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 221184. Throughput: 0: 348.6. Samples: 222256. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:45:17,563][187912] Avg episode reward: [(0, '-0.687')]
[36m[2025-06-29 17:45:22,560][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 221184. Throughput: 0: 347.4. Samples: 224352. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:45:22,560][187912] Avg episode reward: [(0, '6.695')]
[36m[2025-06-29 17:45:27,586][187912] Fps is (10 sec: 408.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 225280. Throughput: 0: 344.0. Samples: 225312. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 17:45:27,586][187912] Avg episode reward: [(0, '4.667')]
[36m[2025-06-29 17:45:32,594][187912] Fps is (10 sec: 408.2, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 225280. Throughput: 0: 340.6. Samples: 227360. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 17:45:32,594][187912] Avg episode reward: [(0, '1.562')]
[31m[688743 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[688743 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[688744 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 17:45:37,591][187912] Fps is (10 sec: 409.4, 60 sec: 343.5, 300 sec: 361.0). Total num frames: 229376. Throughput: 0: 341.3. Samples: 229424. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:45:37,591][187912] Avg episode reward: [(0, '2.468')]
[36m[2025-06-29 17:45:42,577][187912] Fps is (10 sec: 410.3, 60 sec: 341.2, 300 sec: 348.0). Total num frames: 229376. Throughput: 0: 343.5. Samples: 230624. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:45:42,578][187912] Avg episode reward: [(0, '-0.053')]
[36m[2025-06-29 17:45:47,591][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 229376. Throughput: 0: 346.9. Samples: 232736. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:45:47,591][187912] Avg episode reward: [(0, '0.288')]
[36m[2025-06-29 17:45:52,575][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 233472. Throughput: 0: 345.9. Samples: 234816. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:45:52,575][187912] Avg episode reward: [(0, '2.362')]
[36m[2025-06-29 17:45:57,594][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 233472. Throughput: 0: 350.5. Samples: 235952. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:45:57,594][187912] Avg episode reward: [(0, '8.809')]
[36m[2025-06-29 17:46:02,574][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 237568. Throughput: 0: 346.2. Samples: 237840. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:46:02,574][187912] Avg episode reward: [(0, '10.974')]
[36m[2025-06-29 17:46:07,605][187912] Fps is (10 sec: 409.1, 60 sec: 341.1, 300 sec: 347.1). Total num frames: 237568. Throughput: 0: 348.5. Samples: 240048. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:46:07,605][187912] Avg episode reward: [(0, '22.331')]
[36m[2025-06-29 17:46:12,559][187912] Fps is (10 sec: 410.2, 60 sec: 345.4, 300 sec: 361.0). Total num frames: 241664. Throughput: 0: 352.9. Samples: 241184. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:46:12,559][187912] Avg episode reward: [(0, '13.804')]
[37m[1m[2025-06-29 17:46:12,598][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000000944_241664.pth...
[36m[2025-06-29 17:46:12,663][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000000608_155648.pth
[36m[2025-06-29 17:46:17,597][187912] Fps is (10 sec: 409.9, 60 sec: 341.1, 300 sec: 347.1). Total num frames: 241664. Throughput: 0: 350.2. Samples: 243120. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:46:17,597][187912] Avg episode reward: [(0, '18.590')]
[36m[2025-06-29 17:46:22,594][187912] Fps is (10 sec: 0.0, 60 sec: 341.1, 300 sec: 347.1). Total num frames: 241664. Throughput: 0: 353.0. Samples: 245312. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:46:22,594][187912] Avg episode reward: [(0, '14.582')]
[36m[2025-06-29 17:46:27,567][187912] Fps is (10 sec: 410.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 245760. Throughput: 0: 345.7. Samples: 246176. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-06-29 17:46:27,567][187912] Avg episode reward: [(0, '13.058')]
[36m[2025-06-29 17:46:32,590][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 245760. Throughput: 0: 351.3. Samples: 248544. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-06-29 17:46:32,590][187912] Avg episode reward: [(0, '9.758')]
[36m[2025-06-29 17:46:37,583][187912] Fps is (10 sec: 408.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 249856. Throughput: 0: 349.4. Samples: 250544. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 17:46:37,583][187912] Avg episode reward: [(0, '8.590')]
[36m[2025-06-29 17:46:42,558][187912] Fps is (10 sec: 410.9, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 249856. Throughput: 0: 350.1. Samples: 251696. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 17:46:42,559][187912] Avg episode reward: [(0, '2.453')]
[36m[2025-06-29 17:46:47,568][187912] Fps is (10 sec: 410.2, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 253952. Throughput: 0: 357.7. Samples: 253936. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:46:47,568][187912] Avg episode reward: [(0, '1.063')]
[36m[2025-06-29 17:46:52,588][187912] Fps is (10 sec: 408.4, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 253952. Throughput: 0: 352.5. Samples: 255904. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:46:52,588][187912] Avg episode reward: [(0, '4.485')]
[36m[2025-06-29 17:46:57,573][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 253952. Throughput: 0: 351.5. Samples: 257008. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:46:57,574][187912] Avg episode reward: [(0, '2.423')]
[36m[2025-06-29 17:47:02,590][187912] Fps is (10 sec: 409.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 258048. Throughput: 0: 353.8. Samples: 259040. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:47:02,590][187912] Avg episode reward: [(0, '9.909')]
[36m[2025-06-29 17:47:07,578][187912] Fps is (10 sec: 409.4, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 258048. Throughput: 0: 353.9. Samples: 261232. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:47:07,578][187912] Avg episode reward: [(0, '7.208')]
[36m[2025-06-29 17:47:12,603][187912] Fps is (10 sec: 409.1, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 262144. Throughput: 0: 355.3. Samples: 262176. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:47:12,603][187912] Avg episode reward: [(0, '8.750')]
[36m[2025-06-29 17:47:17,559][187912] Fps is (10 sec: 410.4, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 262144. Throughput: 0: 355.4. Samples: 264528. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:47:17,559][187912] Avg episode reward: [(0, '5.461')]
[36m[2025-06-29 17:47:22,560][187912] Fps is (10 sec: 411.4, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 266240. Throughput: 0: 356.1. Samples: 266560. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 17:47:22,560][187912] Avg episode reward: [(0, '6.970')]
[36m[2025-06-29 17:47:27,570][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 266240. Throughput: 0: 354.0. Samples: 267632. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 17:47:27,571][187912] Avg episode reward: [(0, '10.959')]
[36m[2025-06-29 17:47:33,005][187912] Fps is (10 sec: 392.1, 60 sec: 406.8, 300 sec: 360.5). Total num frames: 270336. Throughput: 0: 351.8. Samples: 269920. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:47:33,005][187912] Avg episode reward: [(0, '6.681')]
[31m[807791 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[807791 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[807792 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 17:47:37,560][187912] Fps is (10 sec: 410.0, 60 sec: 341.5, 300 sec: 347.6). Total num frames: 270336. Throughput: 0: 354.7. Samples: 271856. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:47:37,561][187912] Avg episode reward: [(0, '8.565')]
[36m[2025-06-29 17:47:42,581][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 270336. Throughput: 0: 354.1. Samples: 272944. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:47:42,581][187912] Avg episode reward: [(0, '10.280')]
[36m[2025-06-29 17:47:47,559][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 274432. Throughput: 0: 355.4. Samples: 275024. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 17:47:47,559][187912] Avg episode reward: [(0, '7.205')]
[36m[2025-06-29 17:47:52,559][187912] Fps is (10 sec: 410.5, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 274432. Throughput: 0: 361.4. Samples: 277488. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 17:47:52,559][187912] Avg episode reward: [(0, '5.343')]
[36m[2025-06-29 17:47:57,562][187912] Fps is (10 sec: 409.5, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 278528. Throughput: 0: 364.4. Samples: 278560. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:47:57,563][187912] Avg episode reward: [(0, '7.313')]
[31m[833016 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[833017 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[833017 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 17:48:02,590][187912] Fps is (10 sec: 408.3, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 278528. Throughput: 0: 357.4. Samples: 280624. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:48:02,590][187912] Avg episode reward: [(0, '7.302')]
[36m[2025-06-29 17:48:07,589][187912] Fps is (10 sec: 408.5, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 282624. Throughput: 0: 357.5. Samples: 282656. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:48:07,590][187912] Avg episode reward: [(0, '9.939')]
[36m[2025-06-29 17:48:12,558][187912] Fps is (10 sec: 410.9, 60 sec: 341.6, 300 sec: 347.4). Total num frames: 282624. Throughput: 0: 354.9. Samples: 283600. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:48:12,558][187912] Avg episode reward: [(0, '13.106')]
[37m[1m[2025-06-29 17:48:12,607][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000001104_282624.pth...
[36m[2025-06-29 17:48:12,678][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000000768_196608.pth
[36m[2025-06-29 17:48:17,574][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.2). Total num frames: 282624. Throughput: 0: 355.0. Samples: 285744. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:48:17,574][187912] Avg episode reward: [(0, '11.185')]
[31m[854395 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[854395 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[854395 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 17:48:22,610][187912] Fps is (10 sec: 407.5, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 286720. Throughput: 0: 352.3. Samples: 287728. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:48:22,610][187912] Avg episode reward: [(0, '15.702')]
[36m[2025-06-29 17:48:27,590][187912] Fps is (10 sec: 409.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 286720. Throughput: 0: 353.7. Samples: 288864. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:48:27,590][187912] Avg episode reward: [(0, '10.030')]
[36m[2025-06-29 17:48:32,560][187912] Fps is (10 sec: 411.7, 60 sec: 343.9, 300 sec: 361.0). Total num frames: 290816. Throughput: 0: 351.6. Samples: 290848. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:48:32,560][187912] Avg episode reward: [(0, '3.571')]
[36m[2025-06-29 17:48:37,580][187912] Fps is (10 sec: 410.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 290816. Throughput: 0: 345.1. Samples: 293024. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:48:37,580][187912] Avg episode reward: [(0, '17.353')]
[36m[2025-06-29 17:48:42,888][187912] Fps is (10 sec: 396.6, 60 sec: 407.5, 300 sec: 360.6). Total num frames: 294912. Throughput: 0: 343.5. Samples: 294128. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 17:48:42,888][187912] Avg episode reward: [(0, '11.198')]
[36m[2025-06-29 17:48:47,558][187912] Fps is (10 sec: 410.5, 60 sec: 341.3, 300 sec: 347.3). Total num frames: 294912. Throughput: 0: 348.0. Samples: 296272. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 17:48:47,558][187912] Avg episode reward: [(0, '16.468')]
[36m[2025-06-29 17:48:52,561][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 294912. Throughput: 0: 352.2. Samples: 298496. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 17:48:52,561][187912] Avg episode reward: [(0, '19.035')]
[31m[886015 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[886015 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[886016 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 17:48:57,587][187912] Fps is (10 sec: 408.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 299008. Throughput: 0: 348.6. Samples: 299296. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 17:48:57,587][187912] Avg episode reward: [(0, '25.525')]
[36m[2025-06-29 17:49:02,564][187912] Fps is (10 sec: 409.5, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 299008. Throughput: 0: 354.9. Samples: 301712. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 17:49:02,564][187912] Avg episode reward: [(0, '26.070')]
[36m[2025-06-29 17:49:07,590][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 303104. Throughput: 0: 357.8. Samples: 303824. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:49:07,590][187912] Avg episode reward: [(0, '25.447')]
[36m[2025-06-29 17:49:12,583][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 303104. Throughput: 0: 359.2. Samples: 305024. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:49:12,583][187912] Avg episode reward: [(0, '18.433')]
[31m[906834 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[906835 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[906835 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 17:49:17,590][187912] Fps is (10 sec: 409.6, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 307200. Throughput: 0: 363.5. Samples: 307216. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 17:49:17,590][187912] Avg episode reward: [(0, '20.020')]
[36m[2025-06-29 17:49:22,573][187912] Fps is (10 sec: 410.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 307200. Throughput: 0: 361.3. Samples: 309280. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 17:49:22,573][187912] Avg episode reward: [(0, '18.825')]
[33m[916636 ms][navigation_task] - WARNING : Curriculum Level: 15, Curriculum progress fraction: 0.0 (navigation_task.py:262)
[33m[916636 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.00048828125
[33mCrash Rate: 0.19287109375
[33mTimeout Rate: 0.806640625 (navigation_task.py:265)
[33m[916636 ms][navigation_task] - WARNING : 
[33mSuccesses: 1
[33mCrashes : 395
[33mTimeouts: 1652 (navigation_task.py:268)
[36m[2025-06-29 17:49:28,076][187912] Fps is (10 sec: 390.6, 60 sec: 406.3, 300 sec: 360.4). Total num frames: 311296. Throughput: 0: 362.6. Samples: 310512. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:49:28,077][187912] Avg episode reward: [(0, '14.881')]
[36m[2025-06-29 17:49:32,586][187912] Fps is (10 sec: 409.1, 60 sec: 341.2, 300 sec: 347.6). Total num frames: 311296. Throughput: 0: 360.3. Samples: 312496. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:49:32,586][187912] Avg episode reward: [(0, '9.924')]
[36m[2025-06-29 17:49:37,572][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 311296. Throughput: 0: 361.2. Samples: 314752. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:49:37,572][187912] Avg episode reward: [(0, '16.298')]
[36m[2025-06-29 17:49:42,559][187912] Fps is (10 sec: 410.7, 60 sec: 343.2, 300 sec: 361.0). Total num frames: 315392. Throughput: 0: 364.0. Samples: 315664. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 17:49:42,559][187912] Avg episode reward: [(0, '11.731')]
[36m[2025-06-29 17:49:47,591][187912] Fps is (10 sec: 408.8, 60 sec: 341.1, 300 sec: 347.1). Total num frames: 315392. Throughput: 0: 363.2. Samples: 318064. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 17:49:47,591][187912] Avg episode reward: [(0, '13.812')]
[36m[2025-06-29 17:49:52,569][187912] Fps is (10 sec: 409.2, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 319488. Throughput: 0: 361.8. Samples: 320096. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:49:52,569][187912] Avg episode reward: [(0, '16.159')]
[36m[2025-06-29 17:49:57,593][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 319488. Throughput: 0: 361.2. Samples: 321280. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:49:57,593][187912] Avg episode reward: [(0, '22.232')]
[36m[2025-06-29 17:50:02,586][187912] Fps is (10 sec: 408.9, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 323584. Throughput: 0: 364.1. Samples: 323600. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:50:02,586][187912] Avg episode reward: [(0, '18.993')]
[36m[2025-06-29 17:50:07,589][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 347.9). Total num frames: 323584. Throughput: 0: 363.2. Samples: 325632. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:50:07,589][187912] Avg episode reward: [(0, '15.879')]
[36m[2025-06-29 17:50:12,558][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 323584. Throughput: 0: 365.1. Samples: 326752. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:50:12,558][187912] Avg episode reward: [(0, '16.536')]
[37m[1m[2025-06-29 17:50:12,602][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000001264_323584.pth...
[36m[2025-06-29 17:50:12,689][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000000944_241664.pth
[36m[2025-06-29 17:50:17,560][187912] Fps is (10 sec: 410.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 327680. Throughput: 0: 360.4. Samples: 328704. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:50:17,560][187912] Avg episode reward: [(0, '14.386')]
[36m[2025-06-29 17:50:22,579][187912] Fps is (10 sec: 408.7, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 327680. Throughput: 0: 363.3. Samples: 331104. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:50:22,579][187912] Avg episode reward: [(0, '13.098')]
[36m[2025-06-29 17:50:27,578][187912] Fps is (10 sec: 408.9, 60 sec: 344.2, 300 sec: 361.0). Total num frames: 331776. Throughput: 0: 362.2. Samples: 331968. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:50:27,578][187912] Avg episode reward: [(0, '21.679')]
[36m[2025-06-29 17:50:32,595][187912] Fps is (10 sec: 408.9, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 331776. Throughput: 0: 362.3. Samples: 334368. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:50:32,595][187912] Avg episode reward: [(0, '24.420')]
[36m[2025-06-29 17:50:37,594][187912] Fps is (10 sec: 408.9, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 335872. Throughput: 0: 359.3. Samples: 336272. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-06-29 17:50:37,594][187912] Avg episode reward: [(0, '31.480')]
[36m[2025-06-29 17:50:42,561][187912] Fps is (10 sec: 411.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 335872. Throughput: 0: 358.3. Samples: 337392. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-06-29 17:50:42,561][187912] Avg episode reward: [(0, '26.592')]
[36m[2025-06-29 17:50:47,942][187912] Fps is (10 sec: 395.8, 60 sec: 407.2, 300 sec: 360.6). Total num frames: 339968. Throughput: 0: 353.1. Samples: 339616. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 17:50:47,942][187912] Avg episode reward: [(0, '29.160')]
[36m[2025-06-29 17:50:52,565][187912] Fps is (10 sec: 409.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 339968. Throughput: 0: 355.4. Samples: 341616. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 17:50:52,566][187912] Avg episode reward: [(0, '36.554')]
[36m[2025-06-29 17:50:57,572][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 339968. Throughput: 0: 355.1. Samples: 342736. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 17:50:57,572][187912] Avg episode reward: [(0, '29.040')]
[36m[2025-06-29 17:51:02,588][187912] Fps is (10 sec: 408.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 344064. Throughput: 0: 356.4. Samples: 344752. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:51:02,588][187912] Avg episode reward: [(0, '34.413')]
[36m[2025-06-29 17:51:07,559][187912] Fps is (10 sec: 410.1, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 344064. Throughput: 0: 352.9. Samples: 346976. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:51:07,560][187912] Avg episode reward: [(0, '33.746')]
[36m[2025-06-29 17:51:12,576][187912] Fps is (10 sec: 410.1, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 348160. Throughput: 0: 358.8. Samples: 348112. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:51:12,576][187912] Avg episode reward: [(0, '25.693')]
[36m[2025-06-29 17:51:17,584][187912] Fps is (10 sec: 408.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 348160. Throughput: 0: 348.9. Samples: 350064. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:51:17,584][187912] Avg episode reward: [(0, '34.045')]
[36m[2025-06-29 17:51:22,568][187912] Fps is (10 sec: 409.9, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 352256. Throughput: 0: 356.1. Samples: 352288. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:51:22,568][187912] Avg episode reward: [(0, '32.225')]
[31m[1040639 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1040639 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[1040639 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 17:51:27,574][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 352256. Throughput: 0: 357.2. Samples: 353472. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:51:27,574][187912] Avg episode reward: [(0, '26.573')]
[36m[2025-06-29 17:51:32,593][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 352256. Throughput: 0: 360.8. Samples: 355728. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:51:32,593][187912] Avg episode reward: [(0, '17.158')]
[36m[2025-06-29 17:51:37,571][187912] Fps is (10 sec: 409.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 356352. Throughput: 0: 360.1. Samples: 357824. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:51:37,571][187912] Avg episode reward: [(0, '15.106')]
[36m[2025-06-29 17:51:42,593][187912] Fps is (10 sec: 409.6, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 356352. Throughput: 0: 358.9. Samples: 358896. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:51:42,593][187912] Avg episode reward: [(0, '13.429')]
[36m[2025-06-29 17:51:47,586][187912] Fps is (10 sec: 409.0, 60 sec: 343.4, 300 sec: 361.0). Total num frames: 360448. Throughput: 0: 360.2. Samples: 360960. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:51:47,586][187912] Avg episode reward: [(0, '20.517')]
[36m[2025-06-29 17:51:52,574][187912] Fps is (10 sec: 410.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 360448. Throughput: 0: 362.9. Samples: 363312. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:51:52,574][187912] Avg episode reward: [(0, '18.992')]
[36m[2025-06-29 17:51:57,591][187912] Fps is (10 sec: 409.4, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 364544. Throughput: 0: 363.6. Samples: 364480. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:51:57,592][187912] Avg episode reward: [(0, '31.346')]
[36m[2025-06-29 17:52:02,561][187912] Fps is (10 sec: 410.1, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 364544. Throughput: 0: 362.9. Samples: 366384. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:52:02,561][187912] Avg episode reward: [(0, '32.926')]
[36m[2025-06-29 17:52:07,604][187912] Fps is (10 sec: 0.0, 60 sec: 341.1, 300 sec: 347.1). Total num frames: 364544. Throughput: 0: 359.5. Samples: 368480. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:52:07,605][187912] Avg episode reward: [(0, '38.694')]
[36m[2025-06-29 17:52:12,602][187912] Fps is (10 sec: 407.9, 60 sec: 341.2, 300 sec: 360.9). Total num frames: 368640. Throughput: 0: 353.2. Samples: 369376. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:52:12,603][187912] Avg episode reward: [(0, '33.416')]
[37m[1m[2025-06-29 17:52:12,645][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000001440_368640.pth...
[36m[2025-06-29 17:52:12,773][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000001104_282624.pth
[36m[2025-06-29 17:52:17,570][187912] Fps is (10 sec: 411.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 368640. Throughput: 0: 354.7. Samples: 371680. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:52:17,570][187912] Avg episode reward: [(0, '39.487')]
[36m[2025-06-29 17:52:22,584][187912] Fps is (10 sec: 410.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 372736. Throughput: 0: 354.4. Samples: 373776. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:52:22,584][187912] Avg episode reward: [(0, '25.435')]
[36m[2025-06-29 17:52:27,575][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 347.6). Total num frames: 372736. Throughput: 0: 356.4. Samples: 374928. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:52:27,575][187912] Avg episode reward: [(0, '34.673')]
[31m[1104259 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1104259 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([12], device='cuda:0') (navigation_task.py:196)
[31m[1104260 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 17:52:32,579][187912] Fps is (10 sec: 409.8, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 376832. Throughput: 0: 354.9. Samples: 376928. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:52:32,580][187912] Avg episode reward: [(0, '26.629')]
[36m[2025-06-29 17:52:37,596][187912] Fps is (10 sec: 408.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 376832. Throughput: 0: 354.7. Samples: 379280. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:52:37,596][187912] Avg episode reward: [(0, '27.540')]
[36m[2025-06-29 17:52:42,579][187912] Fps is (10 sec: 409.6, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 380928. Throughput: 0: 353.2. Samples: 380368. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:52:42,580][187912] Avg episode reward: [(0, '14.051')]
[36m[2025-06-29 17:52:47,573][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 380928. Throughput: 0: 353.0. Samples: 382272. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:52:47,574][187912] Avg episode reward: [(0, '31.388')]
[31m[1121010 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1121010 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[1121011 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 17:52:52,572][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 380928. Throughput: 0: 352.6. Samples: 384336. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:52:52,572][187912] Avg episode reward: [(0, '32.029')]
[36m[2025-06-29 17:52:57,563][187912] Fps is (10 sec: 410.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 385024. Throughput: 0: 349.8. Samples: 385104. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:52:57,563][187912] Avg episode reward: [(0, '23.425')]
[36m[2025-06-29 17:53:02,576][187912] Fps is (10 sec: 409.4, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 385024. Throughput: 0: 350.2. Samples: 387440. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:53:02,576][187912] Avg episode reward: [(0, '11.548')]
[36m[2025-06-29 17:53:07,578][187912] Fps is (10 sec: 409.0, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 389120. Throughput: 0: 347.8. Samples: 389424. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:53:07,578][187912] Avg episode reward: [(0, '17.812')]
[36m[2025-06-29 17:53:12,574][187912] Fps is (10 sec: 409.7, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 389120. Throughput: 0: 349.5. Samples: 390656. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:53:12,574][187912] Avg episode reward: [(0, '11.444')]
[36m[2025-06-29 17:53:17,606][187912] Fps is (10 sec: 408.4, 60 sec: 409.3, 300 sec: 361.0). Total num frames: 393216. Throughput: 0: 356.4. Samples: 392976. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 17:53:17,607][187912] Avg episode reward: [(0, '11.984')]
[36m[2025-06-29 17:53:22,594][187912] Fps is (10 sec: 408.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 393216. Throughput: 0: 347.7. Samples: 394928. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 17:53:22,594][187912] Avg episode reward: [(0, '17.128')]
[36m[2025-06-29 17:53:27,583][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 393216. Throughput: 0: 350.2. Samples: 396128. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 17:53:27,583][187912] Avg episode reward: [(0, '28.132')]
[36m[2025-06-29 17:53:32,562][187912] Fps is (10 sec: 410.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 397312. Throughput: 0: 352.8. Samples: 398144. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:53:32,562][187912] Avg episode reward: [(0, '23.295')]
[36m[2025-06-29 17:53:37,559][187912] Fps is (10 sec: 410.6, 60 sec: 341.5, 300 sec: 347.5). Total num frames: 397312. Throughput: 0: 359.2. Samples: 400496. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:53:37,559][187912] Avg episode reward: [(0, '33.307')]
[36m[2025-06-29 17:53:42,584][187912] Fps is (10 sec: 408.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 401408. Throughput: 0: 362.9. Samples: 401440. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 17:53:42,584][187912] Avg episode reward: [(0, '40.839')]
[36m[2025-06-29 17:53:47,569][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 401408. Throughput: 0: 362.7. Samples: 403760. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 17:53:47,569][187912] Avg episode reward: [(0, '32.580')]
[36m[2025-06-29 17:53:52,578][187912] Fps is (10 sec: 409.9, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 405504. Throughput: 0: 365.2. Samples: 405856. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 17:53:52,578][187912] Avg episode reward: [(0, '28.773')]
[36m[2025-06-29 17:53:57,588][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 405504. Throughput: 0: 362.6. Samples: 406976. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 17:53:57,588][187912] Avg episode reward: [(0, '37.473')]
[36m[2025-06-29 17:54:02,591][187912] Fps is (10 sec: 409.1, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 409600. Throughput: 0: 363.9. Samples: 409344. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:54:02,591][187912] Avg episode reward: [(0, '32.792')]
[36m[2025-06-29 17:54:07,590][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 409600. Throughput: 0: 366.6. Samples: 411424. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:54:07,590][187912] Avg episode reward: [(0, '44.365')]
[36m[2025-06-29 17:54:12,583][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 409600. Throughput: 0: 365.9. Samples: 412592. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:54:12,583][187912] Avg episode reward: [(0, '50.746')]
[37m[1m[2025-06-29 17:54:12,625][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000001600_409600.pth...
[36m[2025-06-29 17:54:12,687][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000001264_323584.pth
[36m[2025-06-29 17:54:17,608][187912] Fps is (10 sec: 408.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 413696. Throughput: 0: 364.4. Samples: 414560. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 17:54:17,608][187912] Avg episode reward: [(0, '57.966')]
[36m[2025-06-29 17:54:22,565][187912] Fps is (10 sec: 410.3, 60 sec: 341.5, 300 sec: 347.7). Total num frames: 413696. Throughput: 0: 361.6. Samples: 416768. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 17:54:22,565][187912] Avg episode reward: [(0, '40.786')]
[36m[2025-06-29 17:54:27,565][187912] Fps is (10 sec: 411.4, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 417792. Throughput: 0: 364.2. Samples: 417824. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:54:27,565][187912] Avg episode reward: [(0, '26.091')]
[36m[2025-06-29 17:54:32,584][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 417792. Throughput: 0: 361.1. Samples: 420016. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:54:32,584][187912] Avg episode reward: [(0, '32.984')]
[31m[1228952 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1228953 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[1228953 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 17:54:37,584][187912] Fps is (10 sec: 408.8, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 421888. Throughput: 0: 359.4. Samples: 422032. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:54:37,584][187912] Avg episode reward: [(0, '18.305')]
[36m[2025-06-29 17:54:42,566][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 421888. Throughput: 0: 361.4. Samples: 423232. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:54:42,567][187912] Avg episode reward: [(0, '25.478')]
[36m[2025-06-29 17:54:48,131][187912] Fps is (10 sec: 388.3, 60 sec: 405.8, 300 sec: 360.3). Total num frames: 425984. Throughput: 0: 355.9. Samples: 425552. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:54:48,132][187912] Avg episode reward: [(0, '31.037')]
[36m[2025-06-29 17:54:52,595][187912] Fps is (10 sec: 408.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 425984. Throughput: 0: 360.5. Samples: 427648. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:54:52,595][187912] Avg episode reward: [(0, '44.198')]
[36m[2025-06-29 17:54:57,591][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 425984. Throughput: 0: 359.8. Samples: 428784. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:54:57,592][187912] Avg episode reward: [(0, '41.616')]
[36m[2025-06-29 17:55:02,578][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 430080. Throughput: 0: 362.9. Samples: 430880. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-06-29 17:55:02,579][187912] Avg episode reward: [(0, '52.951')]
[36m[2025-06-29 17:55:07,596][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 430080. Throughput: 0: 360.3. Samples: 432992. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-06-29 17:55:07,596][187912] Avg episode reward: [(0, '52.275')]
[36m[2025-06-29 17:55:12,574][187912] Fps is (10 sec: 409.8, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 434176. Throughput: 0: 361.9. Samples: 434112. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 17:55:12,574][187912] Avg episode reward: [(0, '49.062')]
[36m[2025-06-29 17:55:17,595][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 434176. Throughput: 0: 358.3. Samples: 436144. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 17:55:17,595][187912] Avg episode reward: [(0, '46.586')]
[36m[2025-06-29 17:55:22,693][187912] Fps is (10 sec: 404.8, 60 sec: 408.7, 300 sec: 360.9). Total num frames: 438272. Throughput: 0: 337.7. Samples: 437264. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:55:22,694][187912] Avg episode reward: [(0, '43.537')]
[36m[2025-06-29 17:55:27,595][187912] Fps is (10 sec: 409.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 438272. Throughput: 0: 356.0. Samples: 439264. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:55:27,595][187912] Avg episode reward: [(0, '37.314')]
[36m[2025-06-29 17:55:32,599][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 438272. Throughput: 0: 362.7. Samples: 441680. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:55:32,599][187912] Avg episode reward: [(0, '44.522')]
[36m[2025-06-29 17:55:37,592][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 442368. Throughput: 0: 354.9. Samples: 443616. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:55:37,592][187912] Avg episode reward: [(0, '41.177')]
[36m[2025-06-29 17:55:42,580][187912] Fps is (10 sec: 410.4, 60 sec: 341.3, 300 sec: 347.5). Total num frames: 442368. Throughput: 0: 353.2. Samples: 444672. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:55:42,580][187912] Avg episode reward: [(0, '44.763')]
[36m[2025-06-29 17:55:47,571][187912] Fps is (10 sec: 410.4, 60 sec: 344.5, 300 sec: 361.0). Total num frames: 446464. Throughput: 0: 350.3. Samples: 446640. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:55:47,572][187912] Avg episode reward: [(0, '51.756')]
[36m[2025-06-29 17:55:52,573][187912] Fps is (10 sec: 409.9, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 446464. Throughput: 0: 355.0. Samples: 448960. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:55:52,573][187912] Avg episode reward: [(0, '55.077')]
[36m[2025-06-29 17:55:57,564][187912] Fps is (10 sec: 409.9, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 450560. Throughput: 0: 356.7. Samples: 450160. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:55:57,565][187912] Avg episode reward: [(0, '46.786')]
[36m[2025-06-29 17:56:02,585][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 450560. Throughput: 0: 354.9. Samples: 452112. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:56:02,586][187912] Avg episode reward: [(0, '46.642')]
[36m[2025-06-29 17:56:07,594][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 450560. Throughput: 0: 382.7. Samples: 454448. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:56:07,594][187912] Avg episode reward: [(0, '52.473')]
[36m[2025-06-29 17:56:12,571][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 454656. Throughput: 0: 358.2. Samples: 455376. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 17:56:12,571][187912] Avg episode reward: [(0, '39.443')]
[37m[1m[2025-06-29 17:56:12,612][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000001776_454656.pth...
[36m[2025-06-29 17:56:12,683][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000001440_368640.pth
[36m[2025-06-29 17:56:17,564][187912] Fps is (10 sec: 410.8, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 454656. Throughput: 0: 354.8. Samples: 457632. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 17:56:17,564][187912] Avg episode reward: [(0, '44.067')]
[36m[2025-06-29 17:56:22,588][187912] Fps is (10 sec: 408.9, 60 sec: 341.9, 300 sec: 361.0). Total num frames: 458752. Throughput: 0: 356.7. Samples: 459664. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:56:22,589][187912] Avg episode reward: [(0, '45.728')]
[36m[2025-06-29 17:56:27,564][187912] Fps is (10 sec: 409.6, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 458752. Throughput: 0: 359.6. Samples: 460848. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:56:27,564][187912] Avg episode reward: [(0, '63.654')]
[36m[2025-06-29 17:56:32,564][187912] Fps is (10 sec: 410.6, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 462848. Throughput: 0: 361.0. Samples: 462880. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:56:32,564][187912] Avg episode reward: [(0, '64.399')]
[36m[2025-06-29 17:56:37,570][187912] Fps is (10 sec: 409.4, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 462848. Throughput: 0: 356.3. Samples: 464992. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:56:37,570][187912] Avg episode reward: [(0, '73.920')]
[36m[2025-06-29 17:56:42,580][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 462848. Throughput: 0: 352.2. Samples: 466016. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:56:42,580][187912] Avg episode reward: [(0, '81.709')]
[36m[2025-06-29 17:56:47,576][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 466944. Throughput: 0: 353.1. Samples: 468000. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:56:47,576][187912] Avg episode reward: [(0, '88.835')]
[36m[2025-06-29 17:56:52,580][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 466944. Throughput: 0: 350.3. Samples: 470208. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:56:52,581][187912] Avg episode reward: [(0, '84.951')]
[36m[2025-06-29 17:56:57,593][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 471040. Throughput: 0: 348.6. Samples: 471072. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 17:56:57,593][187912] Avg episode reward: [(0, '75.622')]
[36m[2025-06-29 17:57:02,581][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 471040. Throughput: 0: 348.3. Samples: 473312. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 17:57:02,581][187912] Avg episode reward: [(0, '75.590')]
[36m[2025-06-29 17:57:07,598][187912] Fps is (10 sec: 409.4, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 475136. Throughput: 0: 349.4. Samples: 475392. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 17:57:07,598][187912] Avg episode reward: [(0, '75.142')]
[36m[2025-06-29 17:57:12,596][187912] Fps is (10 sec: 409.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 475136. Throughput: 0: 348.6. Samples: 476544. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 17:57:12,596][187912] Avg episode reward: [(0, '61.928')]
[36m[2025-06-29 17:57:18,226][187912] Fps is (10 sec: 385.4, 60 sec: 405.1, 300 sec: 360.2). Total num frames: 479232. Throughput: 0: 348.3. Samples: 478784. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:57:18,226][187912] Avg episode reward: [(0, '64.572')]
[36m[2025-06-29 17:57:22,568][187912] Fps is (10 sec: 410.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 479232. Throughput: 0: 350.2. Samples: 480752. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:57:22,568][187912] Avg episode reward: [(0, '60.268')]
[36m[2025-06-29 17:57:27,573][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 479232. Throughput: 0: 352.8. Samples: 481888. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 17:57:27,573][187912] Avg episode reward: [(0, '59.139')]
[36m[2025-06-29 17:57:32,588][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 483328. Throughput: 0: 352.3. Samples: 483856. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:57:32,588][187912] Avg episode reward: [(0, '50.025')]
[36m[2025-06-29 17:57:37,566][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 483328. Throughput: 0: 354.6. Samples: 486160. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:57:37,566][187912] Avg episode reward: [(0, '43.280')]
[36m[2025-06-29 17:57:42,601][187912] Fps is (10 sec: 409.1, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 487424. Throughput: 0: 361.5. Samples: 487344. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 17:57:42,601][187912] Avg episode reward: [(0, '54.776')]
[36m[2025-06-29 17:57:47,566][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 487424. Throughput: 0: 356.4. Samples: 489344. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 17:57:47,567][187912] Avg episode reward: [(0, '53.283')]
[36m[2025-06-29 17:57:52,584][187912] Fps is (10 sec: 410.3, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 491520. Throughput: 0: 359.2. Samples: 491552. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 17:57:52,584][187912] Avg episode reward: [(0, '52.395')]
[33m[1428575 ms][navigation_task] - WARNING : Curriculum Level: 15, Curriculum progress fraction: 0.0 (navigation_task.py:262)
[33m[1428575 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.08203125
[33mCrash Rate: 0.23046875
[33mTimeout Rate: 0.6875 (navigation_task.py:265)
[33m[1428575 ms][navigation_task] - WARNING : 
[33mSuccesses: 168
[33mCrashes : 472
[33mTimeouts: 1408 (navigation_task.py:268)
[36m[2025-06-29 17:57:57,577][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 491520. Throughput: 0: 356.4. Samples: 492576. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 17:57:57,577][187912] Avg episode reward: [(0, '59.422')]
[36m[2025-06-29 17:58:02,579][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 491520. Throughput: 0: 359.7. Samples: 494736. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 17:58:02,579][187912] Avg episode reward: [(0, '76.875')]
[36m[2025-06-29 17:58:07,594][187912] Fps is (10 sec: 408.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 495616. Throughput: 0: 356.1. Samples: 496784. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:58:07,594][187912] Avg episode reward: [(0, '86.243')]
[36m[2025-06-29 17:58:12,578][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 495616. Throughput: 0: 356.2. Samples: 497920. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:58:12,578][187912] Avg episode reward: [(0, '77.987')]
[37m[1m[2025-06-29 17:58:12,633][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000001936_495616.pth...
[36m[2025-06-29 17:58:12,695][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000001600_409600.pth
[36m[2025-06-29 17:58:17,580][187912] Fps is (10 sec: 410.2, 60 sec: 345.0, 300 sec: 361.0). Total num frames: 499712. Throughput: 0: 356.7. Samples: 499904. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 17:58:17,580][187912] Avg episode reward: [(0, '91.690')]
[36m[2025-06-29 17:58:22,579][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 499712. Throughput: 0: 357.6. Samples: 502256. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 17:58:22,579][187912] Avg episode reward: [(0, '99.344')]
[36m[2025-06-29 17:58:27,562][187912] Fps is (10 sec: 410.3, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 503808. Throughput: 0: 359.4. Samples: 503504. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 17:58:27,563][187912] Avg episode reward: [(0, '75.729')]
[36m[2025-06-29 17:58:32,566][187912] Fps is (10 sec: 410.1, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 503808. Throughput: 0: 358.8. Samples: 505488. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 17:58:32,566][187912] Avg episode reward: [(0, '74.360')]
[31m[1469678 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1469678 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([12], device='cuda:0') (navigation_task.py:196)
[31m[1469678 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 17:58:38,148][187912] Fps is (10 sec: 387.0, 60 sec: 405.7, 300 sec: 360.3). Total num frames: 507904. Throughput: 0: 357.1. Samples: 507824. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 17:58:38,148][187912] Avg episode reward: [(0, '95.653')]
[36m[2025-06-29 17:58:42,579][187912] Fps is (10 sec: 409.1, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 507904. Throughput: 0: 358.4. Samples: 508704. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 17:58:42,579][187912] Avg episode reward: [(0, '95.317')]
[31m[1479912 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1479913 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[1479913 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 17:58:47,580][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 507904. Throughput: 0: 363.4. Samples: 511088. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 17:58:47,580][187912] Avg episode reward: [(0, '97.394')]
[36m[2025-06-29 17:58:52,560][187912] Fps is (10 sec: 410.4, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 512000. Throughput: 0: 364.0. Samples: 513152. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 17:58:52,561][187912] Avg episode reward: [(0, '105.787')]
[36m[2025-06-29 17:58:57,577][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 512000. Throughput: 0: 364.5. Samples: 514320. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 17:58:57,577][187912] Avg episode reward: [(0, '112.866')]
[36m[2025-06-29 17:59:02,580][187912] Fps is (10 sec: 408.8, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 516096. Throughput: 0: 365.2. Samples: 516336. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 17:59:02,580][187912] Avg episode reward: [(0, '99.785')]
[36m[2025-06-29 17:59:07,562][187912] Fps is (10 sec: 410.2, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 516096. Throughput: 0: 362.4. Samples: 518560. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 17:59:07,562][187912] Avg episode reward: [(0, '103.979')]
[31m[1504745 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1504745 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[1504746 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 17:59:12,591][187912] Fps is (10 sec: 409.1, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 520192. Throughput: 0: 358.5. Samples: 519648. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 17:59:12,591][187912] Avg episode reward: [(0, '104.565')]
[36m[2025-06-29 17:59:17,579][187912] Fps is (10 sec: 408.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 520192. Throughput: 0: 360.4. Samples: 521712. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 17:59:17,579][187912] Avg episode reward: [(0, '113.572')]
[36m[2025-06-29 17:59:22,565][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 520192. Throughput: 0: 363.8. Samples: 523984. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 17:59:22,565][187912] Avg episode reward: [(0, '91.397')]
[36m[2025-06-29 17:59:27,576][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 524288. Throughput: 0: 360.2. Samples: 524912. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:59:27,576][187912] Avg episode reward: [(0, '104.760')]
[36m[2025-06-29 17:59:32,583][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 524288. Throughput: 0: 359.4. Samples: 527264. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:59:32,583][187912] Avg episode reward: [(0, '92.387')]
[36m[2025-06-29 17:59:37,591][187912] Fps is (10 sec: 409.0, 60 sec: 344.5, 300 sec: 361.0). Total num frames: 528384. Throughput: 0: 356.7. Samples: 529216. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:59:37,591][187912] Avg episode reward: [(0, '110.525')]
[36m[2025-06-29 17:59:42,573][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 347.8). Total num frames: 528384. Throughput: 0: 357.0. Samples: 530384. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:59:42,573][187912] Avg episode reward: [(0, '98.159')]
[36m[2025-06-29 17:59:47,570][187912] Fps is (10 sec: 410.4, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 532480. Throughput: 0: 359.5. Samples: 532512. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:59:47,571][187912] Avg episode reward: [(0, '121.665')]
[36m[2025-06-29 17:59:52,586][187912] Fps is (10 sec: 409.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 532480. Throughput: 0: 362.1. Samples: 534864. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 17:59:52,586][187912] Avg episode reward: [(0, '120.577')]
[36m[2025-06-29 17:59:57,562][187912] Fps is (10 sec: 409.9, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 536576. Throughput: 0: 363.3. Samples: 535984. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 17:59:57,562][187912] Avg episode reward: [(0, '136.100')]
[36m[2025-06-29 18:00:02,596][187912] Fps is (10 sec: 409.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 536576. Throughput: 0: 361.5. Samples: 537984. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 18:00:02,596][187912] Avg episode reward: [(0, '126.589')]
[36m[2025-06-29 18:00:07,568][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 536576. Throughput: 0: 356.6. Samples: 540032. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 18:00:07,568][187912] Avg episode reward: [(0, '131.776')]
[36m[2025-06-29 18:00:12,565][187912] Fps is (10 sec: 410.9, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 540672. Throughput: 0: 354.2. Samples: 540848. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 18:00:12,565][187912] Avg episode reward: [(0, '122.504')]
[37m[1m[2025-06-29 18:00:12,607][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000002112_540672.pth...
[36m[2025-06-29 18:00:12,670][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000001776_454656.pth
[36m[2025-06-29 18:00:17,587][187912] Fps is (10 sec: 408.8, 60 sec: 341.3, 300 sec: 347.2). Total num frames: 540672. Throughput: 0: 350.9. Samples: 543056. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 18:00:17,587][187912] Avg episode reward: [(0, '118.615')]
[31m[1573600 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1573601 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([12], device='cuda:0') (navigation_task.py:196)
[31m[1573601 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:00:22,571][187912] Fps is (10 sec: 409.4, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 544768. Throughput: 0: 352.5. Samples: 545072. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 18:00:22,571][187912] Avg episode reward: [(0, '108.558')]
[36m[2025-06-29 18:00:27,563][187912] Fps is (10 sec: 410.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 544768. Throughput: 0: 353.5. Samples: 546288. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 18:00:27,563][187912] Avg episode reward: [(0, '111.909')]
[36m[2025-06-29 18:00:32,795][187912] Fps is (10 sec: 400.6, 60 sec: 408.2, 300 sec: 360.8). Total num frames: 548864. Throughput: 0: 354.5. Samples: 548544. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:00:32,795][187912] Avg episode reward: [(0, '117.380')]
[36m[2025-06-29 18:00:37,588][187912] Fps is (10 sec: 408.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 548864. Throughput: 0: 345.9. Samples: 550432. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:00:37,588][187912] Avg episode reward: [(0, '123.129')]
[36m[2025-06-29 18:00:42,583][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 548864. Throughput: 0: 344.4. Samples: 551488. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:00:42,583][187912] Avg episode reward: [(0, '128.717')]
[36m[2025-06-29 18:00:47,566][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 552960. Throughput: 0: 341.6. Samples: 553344. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 18:00:47,566][187912] Avg episode reward: [(0, '162.114')]
[36m[2025-06-29 18:00:52,578][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 552960. Throughput: 0: 344.1. Samples: 555520. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 18:00:52,578][187912] Avg episode reward: [(0, '151.971')]
[36m[2025-06-29 18:00:57,576][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 557056. Throughput: 0: 349.4. Samples: 556576. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:00:57,576][187912] Avg episode reward: [(0, '145.392')]
[36m[2025-06-29 18:01:02,580][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 557056. Throughput: 0: 343.2. Samples: 558496. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:01:02,580][187912] Avg episode reward: [(0, '151.665')]
[36m[2025-06-29 18:01:07,586][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 557056. Throughput: 0: 349.4. Samples: 560800. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:01:07,586][187912] Avg episode reward: [(0, '152.672')]
[36m[2025-06-29 18:01:12,559][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 561152. Throughput: 0: 341.7. Samples: 561664. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:01:12,560][187912] Avg episode reward: [(0, '119.679')]
[31m[1627262 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1627262 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[1627263 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:01:17,596][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 561152. Throughput: 0: 343.9. Samples: 563952. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:01:17,597][187912] Avg episode reward: [(0, '131.693')]
[36m[2025-06-29 18:01:22,561][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 565248. Throughput: 0: 343.0. Samples: 565856. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:01:22,561][187912] Avg episode reward: [(0, '109.022')]
[36m[2025-06-29 18:01:27,591][187912] Fps is (10 sec: 409.8, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 565248. Throughput: 0: 344.5. Samples: 566992. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:01:27,591][187912] Avg episode reward: [(0, '101.459')]
[31m[1644737 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1644737 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[1644737 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:01:32,563][187912] Fps is (10 sec: 409.5, 60 sec: 342.7, 300 sec: 361.0). Total num frames: 569344. Throughput: 0: 351.7. Samples: 569168. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:01:32,563][187912] Avg episode reward: [(0, '94.315')]
[36m[2025-06-29 18:01:37,568][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 569344. Throughput: 0: 351.4. Samples: 571328. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:01:37,568][187912] Avg episode reward: [(0, '99.408')]
[36m[2025-06-29 18:01:42,565][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 569344. Throughput: 0: 350.7. Samples: 572352. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:01:42,565][187912] Avg episode reward: [(0, '94.874')]
[36m[2025-06-29 18:01:47,561][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 573440. Throughput: 0: 351.4. Samples: 574304. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 18:01:47,561][187912] Avg episode reward: [(0, '118.483')]
[36m[2025-06-29 18:01:52,576][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 573440. Throughput: 0: 353.5. Samples: 576704. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 18:01:52,576][187912] Avg episode reward: [(0, '111.816')]
[36m[2025-06-29 18:01:57,559][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 577536. Throughput: 0: 353.4. Samples: 577568. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 18:01:57,559][187912] Avg episode reward: [(0, '122.258')]
[31m[1672080 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1672080 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[1672080 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:02:02,582][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 577536. Throughput: 0: 351.0. Samples: 579744. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 18:02:02,582][187912] Avg episode reward: [(0, '115.035')]
[31m[1676763 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1676763 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[1676764 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:02:07,565][187912] Fps is (10 sec: 409.3, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 581632. Throughput: 0: 353.7. Samples: 581776. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:02:07,566][187912] Avg episode reward: [(0, '99.312')]
[36m[2025-06-29 18:02:12,564][187912] Fps is (10 sec: 410.3, 60 sec: 341.3, 300 sec: 347.9). Total num frames: 581632. Throughput: 0: 355.4. Samples: 582976. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:02:12,564][187912] Avg episode reward: [(0, '97.537')]
[37m[1m[2025-06-29 18:02:12,606][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000002272_581632.pth...
[36m[2025-06-29 18:02:12,667][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000001936_495616.pth
[36m[2025-06-29 18:02:17,573][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 581632. Throughput: 0: 355.8. Samples: 585184. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:02:17,573][187912] Avg episode reward: [(0, '91.072')]
[36m[2025-06-29 18:02:22,564][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 585728. Throughput: 0: 352.7. Samples: 587200. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 18:02:22,564][187912] Avg episode reward: [(0, '101.924')]
[36m[2025-06-29 18:02:27,565][187912] Fps is (10 sec: 409.9, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 585728. Throughput: 0: 354.5. Samples: 588304. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 18:02:27,565][187912] Avg episode reward: [(0, '113.135')]
[36m[2025-06-29 18:02:32,580][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 589824. Throughput: 0: 357.5. Samples: 590400. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 18:02:32,580][187912] Avg episode reward: [(0, '129.745')]
[36m[2025-06-29 18:02:37,561][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 589824. Throughput: 0: 353.9. Samples: 592624. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 18:02:37,562][187912] Avg episode reward: [(0, '146.800')]
[36m[2025-06-29 18:02:42,589][187912] Fps is (10 sec: 409.2, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 593920. Throughput: 0: 359.6. Samples: 593760. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 18:02:42,589][187912] Avg episode reward: [(0, '159.679')]
[36m[2025-06-29 18:02:47,589][187912] Fps is (10 sec: 408.4, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 593920. Throughput: 0: 355.8. Samples: 595760. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 18:02:47,590][187912] Avg episode reward: [(0, '152.357')]
[36m[2025-06-29 18:02:52,781][187912] Fps is (10 sec: 401.9, 60 sec: 408.2, 300 sec: 360.8). Total num frames: 598016. Throughput: 0: 333.7. Samples: 596864. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 18:02:52,781][187912] Avg episode reward: [(0, '158.166')]
[36m[2025-06-29 18:02:57,567][187912] Fps is (10 sec: 410.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 598016. Throughput: 0: 355.5. Samples: 598976. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 18:02:57,567][187912] Avg episode reward: [(0, '174.537')]
[36m[2025-06-29 18:03:02,583][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 598016. Throughput: 0: 358.7. Samples: 601328. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 18:03:02,583][187912] Avg episode reward: [(0, '154.535')]
[36m[2025-06-29 18:03:07,591][187912] Fps is (10 sec: 408.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 602112. Throughput: 0: 359.6. Samples: 603392. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:03:07,591][187912] Avg episode reward: [(0, '156.616')]
[36m[2025-06-29 18:03:12,594][187912] Fps is (10 sec: 409.2, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 602112. Throughput: 0: 359.6. Samples: 604496. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:03:12,594][187912] Avg episode reward: [(0, '168.735')]
[36m[2025-06-29 18:03:17,586][187912] Fps is (10 sec: 409.8, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 606208. Throughput: 0: 355.2. Samples: 606384. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 18:03:17,586][187912] Avg episode reward: [(0, '169.780')]
[36m[2025-06-29 18:03:22,585][187912] Fps is (10 sec: 409.9, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 606208. Throughput: 0: 358.2. Samples: 608752. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 18:03:22,586][187912] Avg episode reward: [(0, '154.167')]
[36m[2025-06-29 18:03:27,561][187912] Fps is (10 sec: 410.6, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 610304. Throughput: 0: 358.3. Samples: 609872. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:03:27,562][187912] Avg episode reward: [(0, '173.398')]
[36m[2025-06-29 18:03:32,595][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 347.8). Total num frames: 610304. Throughput: 0: 360.8. Samples: 612000. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:03:32,595][187912] Avg episode reward: [(0, '168.650')]
[36m[2025-06-29 18:03:37,884][187912] Fps is (10 sec: 396.8, 60 sec: 407.4, 300 sec: 360.6). Total num frames: 614400. Throughput: 0: 389.2. Samples: 614416. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:03:37,884][187912] Avg episode reward: [(0, '167.514')]
[36m[2025-06-29 18:03:42,559][187912] Fps is (10 sec: 411.1, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 614400. Throughput: 0: 361.3. Samples: 615232. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:03:42,559][187912] Avg episode reward: [(0, '179.252')]
[36m[2025-06-29 18:03:47,570][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 614400. Throughput: 0: 360.3. Samples: 617536. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:03:47,570][187912] Avg episode reward: [(0, '188.667')]
[31m[1785444 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1785444 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[1785444 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:03:52,568][187912] Fps is (10 sec: 409.3, 60 sec: 342.5, 300 sec: 361.0). Total num frames: 618496. Throughput: 0: 358.6. Samples: 619520. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:03:52,568][187912] Avg episode reward: [(0, '178.160')]
[36m[2025-06-29 18:03:57,566][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 618496. Throughput: 0: 359.0. Samples: 620640. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:03:57,566][187912] Avg episode reward: [(0, '183.617')]
[36m[2025-06-29 18:04:02,576][187912] Fps is (10 sec: 409.3, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 622592. Throughput: 0: 361.3. Samples: 622640. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 18:04:02,576][187912] Avg episode reward: [(0, '187.492')]
[36m[2025-06-29 18:04:07,565][187912] Fps is (10 sec: 409.7, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 622592. Throughput: 0: 361.1. Samples: 624992. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 18:04:07,565][187912] Avg episode reward: [(0, '181.296')]
[36m[2025-06-29 18:04:12,585][187912] Fps is (10 sec: 409.2, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 626688. Throughput: 0: 360.3. Samples: 626096. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:04:12,585][187912] Avg episode reward: [(0, '184.475')]
[37m[1m[2025-06-29 18:04:12,644][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000002448_626688.pth...
[36m[2025-06-29 18:04:12,717][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000002112_540672.pth
[36m[2025-06-29 18:04:17,565][187912] Fps is (10 sec: 409.6, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 626688. Throughput: 0: 357.9. Samples: 628096. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:04:17,565][187912] Avg episode reward: [(0, '197.558')]
[36m[2025-06-29 18:04:22,572][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 626688. Throughput: 0: 356.6. Samples: 630352. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:04:22,572][187912] Avg episode reward: [(0, '193.170')]
[36m[2025-06-29 18:04:27,572][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 630784. Throughput: 0: 354.4. Samples: 631184. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 18:04:27,573][187912] Avg episode reward: [(0, '191.659')]
[36m[2025-06-29 18:04:32,595][187912] Fps is (10 sec: 408.6, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 630784. Throughput: 0: 353.9. Samples: 633472. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 18:04:32,595][187912] Avg episode reward: [(0, '186.609')]
[36m[2025-06-29 18:04:37,570][187912] Fps is (10 sec: 409.7, 60 sec: 343.1, 300 sec: 361.0). Total num frames: 634880. Throughput: 0: 353.4. Samples: 635424. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 18:04:37,570][187912] Avg episode reward: [(0, '185.308')]
[36m[2025-06-29 18:04:42,571][187912] Fps is (10 sec: 410.6, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 634880. Throughput: 0: 354.8. Samples: 636608. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 18:04:42,571][187912] Avg episode reward: [(0, '176.248')]
[36m[2025-06-29 18:04:47,585][187912] Fps is (10 sec: 409.0, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 638976. Throughput: 0: 360.8. Samples: 638880. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:04:47,585][187912] Avg episode reward: [(0, '179.482')]
[36m[2025-06-29 18:04:52,569][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 638976. Throughput: 0: 356.6. Samples: 641040. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:04:52,569][187912] Avg episode reward: [(0, '191.469')]
[36m[2025-06-29 18:04:58,142][187912] Fps is (10 sec: 388.0, 60 sec: 405.7, 300 sec: 360.3). Total num frames: 643072. Throughput: 0: 353.7. Samples: 642208. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 18:04:58,142][187912] Avg episode reward: [(0, '201.615')]
[36m[2025-06-29 18:05:02,579][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 643072. Throughput: 0: 360.4. Samples: 644320. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 18:05:02,579][187912] Avg episode reward: [(0, '203.044')]
[31m[1858137 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1858138 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[1858138 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:05:07,586][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 643072. Throughput: 0: 362.9. Samples: 646688. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 18:05:07,586][187912] Avg episode reward: [(0, '207.055')]
[36m[2025-06-29 18:05:12,577][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 647168. Throughput: 0: 364.0. Samples: 647568. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:05:12,578][187912] Avg episode reward: [(0, '220.738')]
[36m[2025-06-29 18:05:17,577][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 647168. Throughput: 0: 364.2. Samples: 649856. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:05:17,577][187912] Avg episode reward: [(0, '196.298')]
[31m[1872936 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1872937 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[1872937 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:05:22,592][187912] Fps is (10 sec: 409.0, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 651264. Throughput: 0: 366.0. Samples: 651904. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:05:22,592][187912] Avg episode reward: [(0, '178.636')]
[36m[2025-06-29 18:05:27,580][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 347.4). Total num frames: 651264. Throughput: 0: 367.2. Samples: 653136. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:05:27,580][187912] Avg episode reward: [(0, '202.264')]
[36m[2025-06-29 18:05:32,571][187912] Fps is (10 sec: 410.4, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 655360. Throughput: 0: 367.0. Samples: 655392. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:05:32,571][187912] Avg episode reward: [(0, '205.049')]
[36m[2025-06-29 18:05:37,586][187912] Fps is (10 sec: 409.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 655360. Throughput: 0: 364.3. Samples: 657440. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:05:37,586][187912] Avg episode reward: [(0, '216.922')]
[36m[2025-06-29 18:05:42,578][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 655360. Throughput: 0: 366.5. Samples: 658496. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:05:42,578][187912] Avg episode reward: [(0, '200.406')]
[36m[2025-06-29 18:05:47,564][187912] Fps is (10 sec: 410.5, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 659456. Throughput: 0: 359.2. Samples: 660480. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 18:05:47,564][187912] Avg episode reward: [(0, '204.204')]
[36m[2025-06-29 18:05:52,570][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 659456. Throughput: 0: 357.5. Samples: 662768. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 18:05:52,580][187912] Avg episode reward: [(0, '198.389')]
[36m[2025-06-29 18:05:57,589][187912] Fps is (10 sec: 408.6, 60 sec: 344.5, 300 sec: 361.0). Total num frames: 663552. Throughput: 0: 357.2. Samples: 663648. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 18:05:57,589][187912] Avg episode reward: [(0, '183.449')]
[31m[1913342 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[1913342 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[1913342 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:06:02,573][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 663552. Throughput: 0: 360.6. Samples: 666080. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 18:06:02,573][187912] Avg episode reward: [(0, '187.359')]
[36m[2025-06-29 18:06:07,565][187912] Fps is (10 sec: 410.6, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 667648. Throughput: 0: 361.1. Samples: 668144. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:06:07,565][187912] Avg episode reward: [(0, '209.809')]
[36m[2025-06-29 18:06:12,591][187912] Fps is (10 sec: 408.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 667648. Throughput: 0: 360.8. Samples: 669376. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:06:12,591][187912] Avg episode reward: [(0, '194.827')]
[37m[1m[2025-06-29 18:06:12,632][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000002608_667648.pth...
[36m[2025-06-29 18:06:12,699][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000002272_581632.pth
[36m[2025-06-29 18:06:17,559][187912] Fps is (10 sec: 409.8, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 671744. Throughput: 0: 361.0. Samples: 671632. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:06:17,559][187912] Avg episode reward: [(0, '212.535')]
[33m[1931984 ms][navigation_task] - WARNING : Curriculum Level: 15, Curriculum progress fraction: 0.0 (navigation_task.py:262)
[33m[1931985 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.5400390625
[33mCrash Rate: 0.27001953125
[33mTimeout Rate: 0.18994140625 (navigation_task.py:265)
[33m[1931985 ms][navigation_task] - WARNING : 
[33mSuccesses: 1106
[33mCrashes : 553
[33mTimeouts: 389 (navigation_task.py:268)
[36m[2025-06-29 18:06:22,588][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 671744. Throughput: 0: 359.8. Samples: 673632. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:06:22,589][187912] Avg episode reward: [(0, '210.327')]
[36m[2025-06-29 18:06:27,589][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 671744. Throughput: 0: 361.9. Samples: 674784. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:06:27,589][187912] Avg episode reward: [(0, '193.968')]
[36m[2025-06-29 18:06:32,583][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 675840. Throughput: 0: 361.4. Samples: 676752. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:06:32,584][187912] Avg episode reward: [(0, '188.686')]
[36m[2025-06-29 18:06:37,580][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 675840. Throughput: 0: 360.8. Samples: 679008. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:06:37,580][187912] Avg episode reward: [(0, '183.963')]
[36m[2025-06-29 18:06:42,561][187912] Fps is (10 sec: 410.5, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 679936. Throughput: 0: 362.9. Samples: 679968. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:06:42,561][187912] Avg episode reward: [(0, '169.087')]
[36m[2025-06-29 18:06:47,565][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 679936. Throughput: 0: 356.0. Samples: 682096. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:06:47,565][187912] Avg episode reward: [(0, '171.726')]
[36m[2025-06-29 18:06:52,577][187912] Fps is (10 sec: 409.0, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 684032. Throughput: 0: 354.4. Samples: 684096. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:06:52,577][187912] Avg episode reward: [(0, '181.400')]
[36m[2025-06-29 18:06:57,591][187912] Fps is (10 sec: 408.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 684032. Throughput: 0: 353.1. Samples: 685264. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:06:57,591][187912] Avg episode reward: [(0, '201.688')]
[36m[2025-06-29 18:07:02,564][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 684032. Throughput: 0: 351.2. Samples: 687440. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:07:02,565][187912] Avg episode reward: [(0, '195.077')]
[36m[2025-06-29 18:07:07,587][187912] Fps is (10 sec: 409.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 688128. Throughput: 0: 353.8. Samples: 689552. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 18:07:07,587][187912] Avg episode reward: [(0, '190.330')]
[36m[2025-06-29 18:07:12,590][187912] Fps is (10 sec: 408.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 688128. Throughput: 0: 352.3. Samples: 690640. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 18:07:12,590][187912] Avg episode reward: [(0, '185.013')]
[36m[2025-06-29 18:07:17,596][187912] Fps is (10 sec: 409.2, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 692224. Throughput: 0: 351.5. Samples: 692576. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 18:07:17,596][187912] Avg episode reward: [(0, '166.761')]
[36m[2025-06-29 18:07:22,596][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 692224. Throughput: 0: 352.6. Samples: 694880. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 18:07:22,596][187912] Avg episode reward: [(0, '157.739')]
[36m[2025-06-29 18:07:27,592][187912] Fps is (10 sec: 409.8, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 696320. Throughput: 0: 353.9. Samples: 695904. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 18:07:27,592][187912] Avg episode reward: [(0, '162.754')]
[36m[2025-06-29 18:07:32,598][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 696320. Throughput: 0: 354.6. Samples: 698064. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 18:07:32,598][187912] Avg episode reward: [(0, '141.857')]
[36m[2025-06-29 18:07:37,961][187912] Fps is (10 sec: 395.0, 60 sec: 407.0, 300 sec: 360.5). Total num frames: 700416. Throughput: 0: 359.2. Samples: 700400. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 18:07:37,961][187912] Avg episode reward: [(0, '159.669')]
[36m[2025-06-29 18:07:42,575][187912] Fps is (10 sec: 410.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 700416. Throughput: 0: 356.8. Samples: 701312. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 18:07:42,575][187912] Avg episode reward: [(0, '177.998')]
[36m[2025-06-29 18:07:47,567][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.4). Total num frames: 700416. Throughput: 0: 361.6. Samples: 703712. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 18:07:47,567][187912] Avg episode reward: [(0, '195.817')]
[36m[2025-06-29 18:07:52,570][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 704512. Throughput: 0: 361.4. Samples: 705808. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:07:52,570][187912] Avg episode reward: [(0, '204.266')]
[36m[2025-06-29 18:07:57,560][187912] Fps is (10 sec: 409.9, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 704512. Throughput: 0: 361.8. Samples: 706912. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:07:57,560][187912] Avg episode reward: [(0, '236.370')]
[36m[2025-06-29 18:08:02,576][187912] Fps is (10 sec: 409.4, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 708608. Throughput: 0: 365.3. Samples: 709008. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 18:08:02,576][187912] Avg episode reward: [(0, '231.401')]
[36m[2025-06-29 18:08:07,590][187912] Fps is (10 sec: 408.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 708608. Throughput: 0: 366.3. Samples: 711360. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 18:08:07,591][187912] Avg episode reward: [(0, '244.267')]
[36m[2025-06-29 18:08:12,579][187912] Fps is (10 sec: 409.5, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 712704. Throughput: 0: 369.2. Samples: 712512. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:08:12,580][187912] Avg episode reward: [(0, '241.272')]
[37m[1m[2025-06-29 18:08:12,619][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000002784_712704.pth...
[36m[2025-06-29 18:08:12,693][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000002448_626688.pth
[36m[2025-06-29 18:08:17,564][187912] Fps is (10 sec: 410.7, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 712704. Throughput: 0: 368.3. Samples: 714624. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:08:17,564][187912] Avg episode reward: [(0, '231.168')]
[36m[2025-06-29 18:08:22,857][187912] Fps is (10 sec: 398.5, 60 sec: 407.8, 300 sec: 360.6). Total num frames: 716800. Throughput: 0: 365.6. Samples: 716816. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 18:08:22,858][187912] Avg episode reward: [(0, '237.741')]
[36m[2025-06-29 18:08:27,560][187912] Fps is (10 sec: 409.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 716800. Throughput: 0: 364.9. Samples: 717728. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 18:08:27,560][187912] Avg episode reward: [(0, '208.924')]
[36m[2025-06-29 18:08:32,595][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.5). Total num frames: 716800. Throughput: 0: 362.4. Samples: 720032. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 18:08:32,596][187912] Avg episode reward: [(0, '212.072')]
[36m[2025-06-29 18:08:37,570][187912] Fps is (10 sec: 409.2, 60 sec: 343.6, 300 sec: 361.0). Total num frames: 720896. Throughput: 0: 360.5. Samples: 722032. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:08:37,570][187912] Avg episode reward: [(0, '214.767')]
[36m[2025-06-29 18:08:42,583][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 720896. Throughput: 0: 361.8. Samples: 723200. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:08:42,583][187912] Avg episode reward: [(0, '234.547')]
[36m[2025-06-29 18:08:47,578][187912] Fps is (10 sec: 409.3, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 724992. Throughput: 0: 359.1. Samples: 725168. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:08:47,578][187912] Avg episode reward: [(0, '232.995')]
[36m[2025-06-29 18:08:52,585][187912] Fps is (10 sec: 409.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 724992. Throughput: 0: 360.6. Samples: 727584. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:08:52,586][187912] Avg episode reward: [(0, '239.905')]
[36m[2025-06-29 18:08:57,589][187912] Fps is (10 sec: 409.1, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 729088. Throughput: 0: 359.7. Samples: 728704. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 18:08:57,589][187912] Avg episode reward: [(0, '222.269')]
[36m[2025-06-29 18:09:02,588][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 729088. Throughput: 0: 360.7. Samples: 730864. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 18:09:02,588][187912] Avg episode reward: [(0, '234.222')]
[36m[2025-06-29 18:09:07,682][187912] Fps is (10 sec: 405.8, 60 sec: 409.0, 300 sec: 360.9). Total num frames: 733184. Throughput: 0: 341.2. Samples: 732112. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 18:09:07,682][187912] Avg episode reward: [(0, '221.932')]
[36m[2025-06-29 18:09:12,580][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 733184. Throughput: 0: 366.4. Samples: 734224. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 18:09:12,580][187912] Avg episode reward: [(0, '233.912')]
[36m[2025-06-29 18:09:17,589][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 733184. Throughput: 0: 364.1. Samples: 736416. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 18:09:17,590][187912] Avg episode reward: [(0, '213.532')]
[36m[2025-06-29 18:09:22,582][187912] Fps is (10 sec: 409.5, 60 sec: 342.9, 300 sec: 361.0). Total num frames: 737280. Throughput: 0: 365.1. Samples: 738464. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 18:09:22,582][187912] Avg episode reward: [(0, '220.879')]
[36m[2025-06-29 18:09:27,572][187912] Fps is (10 sec: 410.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 737280. Throughput: 0: 366.0. Samples: 739664. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 18:09:27,572][187912] Avg episode reward: [(0, '205.383')]
[36m[2025-06-29 18:09:32,574][187912] Fps is (10 sec: 409.9, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 741376. Throughput: 0: 366.2. Samples: 741648. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:09:32,575][187912] Avg episode reward: [(0, '199.846')]
[36m[2025-06-29 18:09:37,572][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 741376. Throughput: 0: 364.2. Samples: 743968. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:09:37,572][187912] Avg episode reward: [(0, '203.171')]
[36m[2025-06-29 18:09:42,593][187912] Fps is (10 sec: 408.8, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 745472. Throughput: 0: 362.6. Samples: 745024. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:09:42,593][187912] Avg episode reward: [(0, '215.038')]
[36m[2025-06-29 18:09:47,593][187912] Fps is (10 sec: 408.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 745472. Throughput: 0: 359.4. Samples: 747040. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:09:47,593][187912] Avg episode reward: [(0, '240.484')]
[36m[2025-06-29 18:09:52,596][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.8). Total num frames: 745472. Throughput: 0: 383.0. Samples: 749312. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:09:52,596][187912] Avg episode reward: [(0, '259.869')]
[31m[2146041 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2146041 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[2146041 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:09:57,591][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 749568. Throughput: 0: 355.5. Samples: 750224. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 18:09:57,591][187912] Avg episode reward: [(0, '257.197')]
[36m[2025-06-29 18:10:02,597][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 749568. Throughput: 0: 360.8. Samples: 752656. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 18:10:02,598][187912] Avg episode reward: [(0, '260.525')]
[36m[2025-06-29 18:10:07,586][187912] Fps is (10 sec: 409.8, 60 sec: 341.9, 300 sec: 361.0). Total num frames: 753664. Throughput: 0: 359.4. Samples: 754640. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 18:10:07,586][187912] Avg episode reward: [(0, '264.747')]
[36m[2025-06-29 18:10:12,562][187912] Fps is (10 sec: 411.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 753664. Throughput: 0: 359.5. Samples: 755840. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 18:10:12,562][187912] Avg episode reward: [(0, '262.802')]
[37m[1m[2025-06-29 18:10:12,605][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000002944_753664.pth...
[36m[2025-06-29 18:10:12,666][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000002608_667648.pth
[36m[2025-06-29 18:10:17,595][187912] Fps is (10 sec: 409.3, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 757760. Throughput: 0: 360.0. Samples: 757856. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:10:17,595][187912] Avg episode reward: [(0, '266.976')]
[36m[2025-06-29 18:10:22,581][187912] Fps is (10 sec: 408.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 757760. Throughput: 0: 359.7. Samples: 760160. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:10:22,582][187912] Avg episode reward: [(0, '279.361')]
[36m[2025-06-29 18:10:27,593][187912] Fps is (10 sec: 409.7, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 761856. Throughput: 0: 361.6. Samples: 761296. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:10:27,593][187912] Avg episode reward: [(0, '267.816')]
[36m[2025-06-29 18:10:32,594][187912] Fps is (10 sec: 409.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 761856. Throughput: 0: 361.9. Samples: 763328. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:10:32,594][187912] Avg episode reward: [(0, '254.407')]
[31m[2186940 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2186940 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[2186940 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:10:37,605][187912] Fps is (10 sec: 0.0, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 761856. Throughput: 0: 364.0. Samples: 765696. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:10:37,605][187912] Avg episode reward: [(0, '213.020')]
[36m[2025-06-29 18:10:42,576][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 765952. Throughput: 0: 361.7. Samples: 766496. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 18:10:42,576][187912] Avg episode reward: [(0, '205.943')]
[31m[2197299 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2197299 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[2197299 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:10:47,593][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 765952. Throughput: 0: 358.1. Samples: 768768. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 18:10:47,593][187912] Avg episode reward: [(0, '210.559')]
[36m[2025-06-29 18:10:52,587][187912] Fps is (10 sec: 409.1, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 770048. Throughput: 0: 359.1. Samples: 770800. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 18:10:52,587][187912] Avg episode reward: [(0, '232.993')]
[36m[2025-06-29 18:10:57,590][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 770048. Throughput: 0: 358.9. Samples: 772000. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 18:10:57,590][187912] Avg episode reward: [(0, '236.226')]
[36m[2025-06-29 18:11:02,558][187912] Fps is (10 sec: 410.8, 60 sec: 409.9, 300 sec: 361.0). Total num frames: 774144. Throughput: 0: 363.0. Samples: 774176. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 18:11:02,559][187912] Avg episode reward: [(0, '233.718')]
[36m[2025-06-29 18:11:07,591][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 774144. Throughput: 0: 358.0. Samples: 776272. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 18:11:07,592][187912] Avg episode reward: [(0, '234.757')]
[36m[2025-06-29 18:11:12,574][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 774144. Throughput: 0: 355.3. Samples: 777280. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 18:11:12,575][187912] Avg episode reward: [(0, '233.537')]
[36m[2025-06-29 18:11:17,587][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 778240. Throughput: 0: 355.3. Samples: 779312. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:11:17,587][187912] Avg episode reward: [(0, '237.594')]
[36m[2025-06-29 18:11:22,576][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 778240. Throughput: 0: 354.0. Samples: 781616. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:11:22,577][187912] Avg episode reward: [(0, '251.456')]
[36m[2025-06-29 18:11:27,593][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 782336. Throughput: 0: 354.0. Samples: 782432. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 18:11:27,593][187912] Avg episode reward: [(0, '241.806')]
[36m[2025-06-29 18:11:32,588][187912] Fps is (10 sec: 409.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 782336. Throughput: 0: 353.5. Samples: 784672. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 18:11:32,588][187912] Avg episode reward: [(0, '235.155')]
[36m[2025-06-29 18:11:37,590][187912] Fps is (10 sec: 409.7, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 786432. Throughput: 0: 352.0. Samples: 786640. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:11:37,590][187912] Avg episode reward: [(0, '211.213')]
[36m[2025-06-29 18:11:42,572][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 786432. Throughput: 0: 351.4. Samples: 787808. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:11:42,573][187912] Avg episode reward: [(0, '204.866')]
[36m[2025-06-29 18:11:47,738][187912] Fps is (10 sec: 403.6, 60 sec: 408.6, 300 sec: 360.8). Total num frames: 790528. Throughput: 0: 354.9. Samples: 790208. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:11:47,738][187912] Avg episode reward: [(0, '180.554')]
[36m[2025-06-29 18:11:52,563][187912] Fps is (10 sec: 410.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 790528. Throughput: 0: 355.1. Samples: 792240. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:11:52,563][187912] Avg episode reward: [(0, '156.270')]
[36m[2025-06-29 18:11:57,569][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 790528. Throughput: 0: 359.5. Samples: 793456. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:11:57,569][187912] Avg episode reward: [(0, '166.869')]
[36m[2025-06-29 18:12:02,602][187912] Fps is (10 sec: 408.0, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 794624. Throughput: 0: 358.6. Samples: 795456. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:12:02,602][187912] Avg episode reward: [(0, '186.176')]
[36m[2025-06-29 18:12:07,594][187912] Fps is (10 sec: 408.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 794624. Throughput: 0: 357.5. Samples: 797712. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:12:07,594][187912] Avg episode reward: [(0, '219.560')]
[36m[2025-06-29 18:12:12,593][187912] Fps is (10 sec: 410.0, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 798720. Throughput: 0: 362.7. Samples: 798752. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:12:12,593][187912] Avg episode reward: [(0, '223.084')]
[37m[1m[2025-06-29 18:12:12,597][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000003120_798720.pth...
[36m[2025-06-29 18:12:12,666][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000002784_712704.pth
[36m[2025-06-29 18:12:17,593][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 798720. Throughput: 0: 358.4. Samples: 800800. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:12:17,593][187912] Avg episode reward: [(0, '230.600')]
[31m[2294165 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2294165 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[2294165 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:12:22,581][187912] Fps is (10 sec: 410.1, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 802816. Throughput: 0: 360.2. Samples: 802848. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:12:22,581][187912] Avg episode reward: [(0, '227.208')]
[36m[2025-06-29 18:12:27,558][187912] Fps is (10 sec: 411.0, 60 sec: 341.5, 300 sec: 361.1). Total num frames: 802816. Throughput: 0: 359.6. Samples: 803984. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:12:27,558][187912] Avg episode reward: [(0, '215.584')]
[36m[2025-06-29 18:12:32,599][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.5). Total num frames: 802816. Throughput: 0: 359.1. Samples: 806320. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:12:32,600][187912] Avg episode reward: [(0, '211.623')]
[36m[2025-06-29 18:12:37,585][187912] Fps is (10 sec: 408.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 806912. Throughput: 0: 357.5. Samples: 808336. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:12:37,585][187912] Avg episode reward: [(0, '209.861')]
[36m[2025-06-29 18:12:42,586][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 806912. Throughput: 0: 353.3. Samples: 809360. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:12:42,586][187912] Avg episode reward: [(0, '227.348')]
[36m[2025-06-29 18:12:47,577][187912] Fps is (10 sec: 409.9, 60 sec: 342.2, 300 sec: 361.0). Total num frames: 811008. Throughput: 0: 350.1. Samples: 811200. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 18:12:47,578][187912] Avg episode reward: [(0, '249.529')]
[31m[2324083 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2324083 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[2324083 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:12:52,590][187912] Fps is (10 sec: 409.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 811008. Throughput: 0: 352.0. Samples: 813552. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 18:12:52,590][187912] Avg episode reward: [(0, '242.949')]
[36m[2025-06-29 18:12:57,614][187912] Fps is (10 sec: 408.1, 60 sec: 409.3, 300 sec: 361.0). Total num frames: 815104. Throughput: 0: 354.3. Samples: 814704. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:12:57,614][187912] Avg episode reward: [(0, '222.873')]
[36m[2025-06-29 18:13:02,593][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 815104. Throughput: 0: 352.4. Samples: 816656. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:13:02,593][187912] Avg episode reward: [(0, '220.744')]
[36m[2025-06-29 18:13:08,291][187912] Fps is (10 sec: 383.6, 60 sec: 404.9, 300 sec: 360.1). Total num frames: 819200. Throughput: 0: 354.2. Samples: 819040. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:13:08,292][187912] Avg episode reward: [(0, '203.552')]
[36m[2025-06-29 18:13:12,571][187912] Fps is (10 sec: 410.5, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 819200. Throughput: 0: 354.0. Samples: 819920. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:13:12,571][187912] Avg episode reward: [(0, '209.480')]
[36m[2025-06-29 18:13:17,594][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.4). Total num frames: 819200. Throughput: 0: 353.5. Samples: 822224. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:13:17,594][187912] Avg episode reward: [(0, '220.567')]
[36m[2025-06-29 18:13:22,590][187912] Fps is (10 sec: 408.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 823296. Throughput: 0: 352.7. Samples: 824208. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 18:13:22,590][187912] Avg episode reward: [(0, '226.587')]
[36m[2025-06-29 18:13:27,569][187912] Fps is (10 sec: 410.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 823296. Throughput: 0: 356.8. Samples: 825408. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 18:13:27,569][187912] Avg episode reward: [(0, '223.823')]
[31m[2362642 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2362643 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[2362643 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:13:32,599][187912] Fps is (10 sec: 409.2, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 827392. Throughput: 0: 360.4. Samples: 827424. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:13:32,599][187912] Avg episode reward: [(0, '221.467')]
[36m[2025-06-29 18:13:37,603][187912] Fps is (10 sec: 408.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 827392. Throughput: 0: 357.9. Samples: 829664. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:13:37,603][187912] Avg episode reward: [(0, '190.356')]
[36m[2025-06-29 18:13:42,954][187912] Fps is (10 sec: 395.6, 60 sec: 407.1, 300 sec: 360.5). Total num frames: 831488. Throughput: 0: 353.6. Samples: 830736. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:13:42,954][187912] Avg episode reward: [(0, '195.658')]
[31m[2378108 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2378108 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([12], device='cuda:0') (navigation_task.py:196)
[31m[2378108 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:13:47,593][187912] Fps is (10 sec: 410.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 831488. Throughput: 0: 358.0. Samples: 832768. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:13:47,593][187912] Avg episode reward: [(0, '165.091')]
[36m[2025-06-29 18:13:52,559][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.2). Total num frames: 831488. Throughput: 0: 360.0. Samples: 834976. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:13:52,559][187912] Avg episode reward: [(0, '184.389')]
[36m[2025-06-29 18:13:57,591][187912] Fps is (10 sec: 409.7, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 835584. Throughput: 0: 354.0. Samples: 835856. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:13:57,591][187912] Avg episode reward: [(0, '185.658')]
[31m[2395747 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2395747 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[2395747 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:14:02,562][187912] Fps is (10 sec: 409.5, 60 sec: 341.5, 300 sec: 347.3). Total num frames: 835584. Throughput: 0: 352.3. Samples: 838064. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:14:02,562][187912] Avg episode reward: [(0, '199.619')]
[36m[2025-06-29 18:14:07,572][187912] Fps is (10 sec: 410.4, 60 sec: 345.5, 300 sec: 361.0). Total num frames: 839680. Throughput: 0: 353.6. Samples: 840112. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 18:14:07,572][187912] Avg episode reward: [(0, '208.544')]
[36m[2025-06-29 18:14:12,594][187912] Fps is (10 sec: 408.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 839680. Throughput: 0: 352.9. Samples: 841296. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 18:14:12,595][187912] Avg episode reward: [(0, '216.738')]
[37m[1m[2025-06-29 18:14:12,648][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000003280_839680.pth...
[36m[2025-06-29 18:14:12,710][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000002944_753664.pth
[36m[2025-06-29 18:14:17,578][187912] Fps is (10 sec: 409.4, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 843776. Throughput: 0: 360.0. Samples: 843616. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:14:17,578][187912] Avg episode reward: [(0, '217.529')]
[36m[2025-06-29 18:14:22,598][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 843776. Throughput: 0: 357.4. Samples: 845744. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:14:22,598][187912] Avg episode reward: [(0, '224.693')]
[36m[2025-06-29 18:14:27,571][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 843776. Throughput: 0: 361.1. Samples: 846848. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:14:27,572][187912] Avg episode reward: [(0, '249.127')]
[33m[2424807 ms][navigation_task] - WARNING : Curriculum Level: 15, Curriculum progress fraction: 0.0 (navigation_task.py:262)
[33m[2424807 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.654296875
[33mCrash Rate: 0.2744140625
[33mTimeout Rate: 0.0712890625 (navigation_task.py:265)
[33m[2424807 ms][navigation_task] - WARNING : 
[33mSuccesses: 1340
[33mCrashes : 562
[33mTimeouts: 146 (navigation_task.py:268)
[36m[2025-06-29 18:14:32,559][187912] Fps is (10 sec: 411.2, 60 sec: 341.6, 300 sec: 361.0). Total num frames: 847872. Throughput: 0: 359.4. Samples: 848928. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:14:32,559][187912] Avg episode reward: [(0, '234.874')]
[36m[2025-06-29 18:14:37,606][187912] Fps is (10 sec: 408.2, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 847872. Throughput: 0: 359.8. Samples: 851184. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:14:37,606][187912] Avg episode reward: [(0, '219.809')]
[36m[2025-06-29 18:14:42,577][187912] Fps is (10 sec: 408.9, 60 sec: 343.5, 300 sec: 361.0). Total num frames: 851968. Throughput: 0: 359.2. Samples: 852016. Policy #0 lag: (min: 15.0, avg: 15.2, max: 31.0)
[36m[2025-06-29 18:14:42,577][187912] Avg episode reward: [(0, '231.001')]
[36m[2025-06-29 18:14:47,597][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 851968. Throughput: 0: 360.3. Samples: 854288. Policy #0 lag: (min: 15.0, avg: 15.2, max: 31.0)
[36m[2025-06-29 18:14:47,597][187912] Avg episode reward: [(0, '232.747')]
[36m[2025-06-29 18:14:52,593][187912] Fps is (10 sec: 409.0, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 856064. Throughput: 0: 355.0. Samples: 856096. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:14:52,593][187912] Avg episode reward: [(0, '224.685')]
[36m[2025-06-29 18:14:57,580][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 856064. Throughput: 0: 356.0. Samples: 857312. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:14:57,580][187912] Avg episode reward: [(0, '243.563')]
[36m[2025-06-29 18:15:02,587][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 856064. Throughput: 0: 356.5. Samples: 859664. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:15:02,587][187912] Avg episode reward: [(0, '217.102')]
[36m[2025-06-29 18:15:07,588][187912] Fps is (10 sec: 409.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 860160. Throughput: 0: 355.3. Samples: 861728. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:15:07,588][187912] Avg episode reward: [(0, '222.104')]
[36m[2025-06-29 18:15:12,583][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 860160. Throughput: 0: 356.9. Samples: 862912. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:15:12,583][187912] Avg episode reward: [(0, '211.796')]
[36m[2025-06-29 18:15:17,560][187912] Fps is (10 sec: 410.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 864256. Throughput: 0: 358.0. Samples: 865040. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:15:17,561][187912] Avg episode reward: [(0, '180.904')]
[36m[2025-06-29 18:15:22,583][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 864256. Throughput: 0: 357.5. Samples: 867264. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:15:22,583][187912] Avg episode reward: [(0, '178.365')]
[36m[2025-06-29 18:15:27,589][187912] Fps is (10 sec: 408.4, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 868352. Throughput: 0: 362.9. Samples: 868352. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 18:15:27,589][187912] Avg episode reward: [(0, '210.374')]
[36m[2025-06-29 18:15:32,583][187912] Fps is (10 sec: 409.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 868352. Throughput: 0: 358.5. Samples: 870416. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 18:15:32,584][187912] Avg episode reward: [(0, '159.616')]
[36m[2025-06-29 18:15:37,571][187912] Fps is (10 sec: 410.3, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 872448. Throughput: 0: 364.6. Samples: 872496. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 18:15:37,572][187912] Avg episode reward: [(0, '176.499')]
[36m[2025-06-29 18:15:42,562][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 872448. Throughput: 0: 363.9. Samples: 873680. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 18:15:42,562][187912] Avg episode reward: [(0, '181.349')]
[36m[2025-06-29 18:15:48,066][187912] Fps is (10 sec: 390.3, 60 sec: 406.4, 300 sec: 360.4). Total num frames: 876544. Throughput: 0: 361.0. Samples: 876080. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 18:15:48,067][187912] Avg episode reward: [(0, '176.169')]
[36m[2025-06-29 18:15:52,594][187912] Fps is (10 sec: 408.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 876544. Throughput: 0: 364.8. Samples: 878144. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 18:15:52,594][187912] Avg episode reward: [(0, '195.677')]
[36m[2025-06-29 18:15:57,579][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 876544. Throughput: 0: 365.2. Samples: 879344. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 18:15:57,580][187912] Avg episode reward: [(0, '191.625')]
[36m[2025-06-29 18:16:02,581][187912] Fps is (10 sec: 410.1, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 880640. Throughput: 0: 363.2. Samples: 881392. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:16:02,582][187912] Avg episode reward: [(0, '189.342')]
[36m[2025-06-29 18:16:07,565][187912] Fps is (10 sec: 410.2, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 880640. Throughput: 0: 365.7. Samples: 883712. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:16:07,565][187912] Avg episode reward: [(0, '172.155')]
[36m[2025-06-29 18:16:12,558][187912] Fps is (10 sec: 410.6, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 884736. Throughput: 0: 365.1. Samples: 884768. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 18:16:12,558][187912] Avg episode reward: [(0, '191.424')]
[37m[1m[2025-06-29 18:16:12,617][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000003456_884736.pth...
[36m[2025-06-29 18:16:12,682][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000003120_798720.pth
[36m[2025-06-29 18:16:17,579][187912] Fps is (10 sec: 409.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 884736. Throughput: 0: 362.7. Samples: 886736. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 18:16:17,579][187912] Avg episode reward: [(0, '192.003')]
[36m[2025-06-29 18:16:22,672][187912] Fps is (10 sec: 405.0, 60 sec: 409.0, 300 sec: 360.9). Total num frames: 888832. Throughput: 0: 340.6. Samples: 887856. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:16:22,672][187912] Avg episode reward: [(0, '196.445')]
[36m[2025-06-29 18:16:27,581][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 888832. Throughput: 0: 357.5. Samples: 889776. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:16:27,582][187912] Avg episode reward: [(0, '199.412')]
[36m[2025-06-29 18:16:32,587][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 888832. Throughput: 0: 359.7. Samples: 892096. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:16:32,587][187912] Avg episode reward: [(0, '206.700')]
[36m[2025-06-29 18:16:37,562][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 892928. Throughput: 0: 355.8. Samples: 894144. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:16:37,562][187912] Avg episode reward: [(0, '200.442')]
[36m[2025-06-29 18:16:42,574][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 347.3). Total num frames: 892928. Throughput: 0: 353.1. Samples: 895232. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:16:42,574][187912] Avg episode reward: [(0, '199.179')]
[36m[2025-06-29 18:16:47,571][187912] Fps is (10 sec: 409.2, 60 sec: 344.2, 300 sec: 361.0). Total num frames: 897024. Throughput: 0: 351.4. Samples: 897200. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 18:16:47,571][187912] Avg episode reward: [(0, '186.109')]
[36m[2025-06-29 18:16:52,583][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 897024. Throughput: 0: 349.7. Samples: 899456. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 18:16:52,584][187912] Avg episode reward: [(0, '182.067')]
[36m[2025-06-29 18:16:57,590][187912] Fps is (10 sec: 408.8, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 901120. Throughput: 0: 351.7. Samples: 900608. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 18:16:57,591][187912] Avg episode reward: [(0, '191.237')]
[36m[2025-06-29 18:17:02,559][187912] Fps is (10 sec: 410.6, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 901120. Throughput: 0: 353.2. Samples: 902624. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 18:17:02,559][187912] Avg episode reward: [(0, '168.182')]
[36m[2025-06-29 18:17:07,568][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 901120. Throughput: 0: 381.3. Samples: 904976. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 18:17:07,568][187912] Avg episode reward: [(0, '166.527')]
[36m[2025-06-29 18:17:12,576][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 905216. Throughput: 0: 356.7. Samples: 905824. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 18:17:12,576][187912] Avg episode reward: [(0, '179.178')]
[36m[2025-06-29 18:17:17,559][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 905216. Throughput: 0: 359.3. Samples: 908256. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 18:17:17,559][187912] Avg episode reward: [(0, '183.050')]
[36m[2025-06-29 18:17:22,586][187912] Fps is (10 sec: 409.2, 60 sec: 341.8, 300 sec: 361.0). Total num frames: 909312. Throughput: 0: 359.6. Samples: 910336. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 18:17:22,586][187912] Avg episode reward: [(0, '182.032')]
[36m[2025-06-29 18:17:27,575][187912] Fps is (10 sec: 409.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 909312. Throughput: 0: 361.9. Samples: 911520. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 18:17:27,575][187912] Avg episode reward: [(0, '206.254')]
[36m[2025-06-29 18:17:32,560][187912] Fps is (10 sec: 410.7, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 913408. Throughput: 0: 364.5. Samples: 913600. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:17:32,560][187912] Avg episode reward: [(0, '214.647')]
[36m[2025-06-29 18:17:37,569][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 913408. Throughput: 0: 364.2. Samples: 915840. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:17:37,569][187912] Avg episode reward: [(0, '212.336')]
[36m[2025-06-29 18:17:42,591][187912] Fps is (10 sec: 408.3, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 917504. Throughput: 0: 363.0. Samples: 916944. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:17:42,591][187912] Avg episode reward: [(0, '210.971')]
[36m[2025-06-29 18:17:47,592][187912] Fps is (10 sec: 408.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 917504. Throughput: 0: 361.3. Samples: 918896. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:17:47,592][187912] Avg episode reward: [(0, '236.991')]
[36m[2025-06-29 18:17:52,602][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 917504. Throughput: 0: 358.5. Samples: 921120. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:17:52,602][187912] Avg episode reward: [(0, '223.826')]
[36m[2025-06-29 18:17:57,593][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 921600. Throughput: 0: 359.3. Samples: 922000. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 18:17:57,594][187912] Avg episode reward: [(0, '212.314')]
[36m[2025-06-29 18:18:02,586][187912] Fps is (10 sec: 410.2, 60 sec: 341.2, 300 sec: 348.0). Total num frames: 921600. Throughput: 0: 356.1. Samples: 924288. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 18:18:02,587][187912] Avg episode reward: [(0, '229.119')]
[36m[2025-06-29 18:18:07,590][187912] Fps is (10 sec: 409.7, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 925696. Throughput: 0: 353.4. Samples: 926240. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 18:18:07,590][187912] Avg episode reward: [(0, '192.657')]
[36m[2025-06-29 18:18:12,574][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 925696. Throughput: 0: 353.8. Samples: 927440. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 18:18:12,575][187912] Avg episode reward: [(0, '197.704')]
[37m[1m[2025-06-29 18:18:12,616][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000003616_925696.pth...
[36m[2025-06-29 18:18:12,686][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000003280_839680.pth
[36m[2025-06-29 18:18:17,581][187912] Fps is (10 sec: 410.0, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 929792. Throughput: 0: 360.4. Samples: 929824. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 18:18:17,581][187912] Avg episode reward: [(0, '206.678')]
[36m[2025-06-29 18:18:22,595][187912] Fps is (10 sec: 408.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 929792. Throughput: 0: 355.7. Samples: 931856. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 18:18:22,595][187912] Avg episode reward: [(0, '240.871')]
[36m[2025-06-29 18:18:28,039][187912] Fps is (10 sec: 391.6, 60 sec: 406.5, 300 sec: 360.5). Total num frames: 933888. Throughput: 0: 353.8. Samples: 933024. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:18:28,040][187912] Avg episode reward: [(0, '236.827')]
[36m[2025-06-29 18:18:32,570][187912] Fps is (10 sec: 410.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 933888. Throughput: 0: 361.1. Samples: 935136. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:18:32,571][187912] Avg episode reward: [(0, '279.876')]
[36m[2025-06-29 18:18:37,587][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.6). Total num frames: 933888. Throughput: 0: 363.1. Samples: 937456. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:18:37,587][187912] Avg episode reward: [(0, '281.393')]
[36m[2025-06-29 18:18:42,582][187912] Fps is (10 sec: 409.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 937984. Throughput: 0: 360.6. Samples: 938224. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:18:42,583][187912] Avg episode reward: [(0, '263.315')]
[36m[2025-06-29 18:18:47,595][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 937984. Throughput: 0: 359.8. Samples: 940480. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:18:47,595][187912] Avg episode reward: [(0, '247.503')]
[31m[2685437 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2685438 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[2685438 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:18:52,560][187912] Fps is (10 sec: 410.5, 60 sec: 409.9, 300 sec: 361.0). Total num frames: 942080. Throughput: 0: 362.6. Samples: 942544. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 18:18:52,560][187912] Avg episode reward: [(0, '252.584')]
[36m[2025-06-29 18:18:57,571][187912] Fps is (10 sec: 410.6, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 942080. Throughput: 0: 362.0. Samples: 943728. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 18:18:57,571][187912] Avg episode reward: [(0, '240.947')]
[36m[2025-06-29 18:19:02,564][187912] Fps is (10 sec: 409.4, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 946176. Throughput: 0: 361.0. Samples: 946064. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 18:19:02,564][187912] Avg episode reward: [(0, '231.157')]
[36m[2025-06-29 18:19:07,590][187912] Fps is (10 sec: 408.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 946176. Throughput: 0: 360.2. Samples: 948064. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 18:19:07,591][187912] Avg episode reward: [(0, '210.544')]
[36m[2025-06-29 18:19:12,598][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 946176. Throughput: 0: 362.0. Samples: 949152. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 18:19:12,598][187912] Avg episode reward: [(0, '192.373')]
[36m[2025-06-29 18:19:17,579][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 950272. Throughput: 0: 357.6. Samples: 951232. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:19:17,579][187912] Avg episode reward: [(0, '216.915')]
[36m[2025-06-29 18:19:22,589][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 950272. Throughput: 0: 355.5. Samples: 953456. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:19:22,589][187912] Avg episode reward: [(0, '198.612')]
[36m[2025-06-29 18:19:27,597][187912] Fps is (10 sec: 408.9, 60 sec: 343.9, 300 sec: 361.0). Total num frames: 954368. Throughput: 0: 359.4. Samples: 954400. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 18:19:27,597][187912] Avg episode reward: [(0, '198.007')]
[36m[2025-06-29 18:19:32,592][187912] Fps is (10 sec: 409.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 954368. Throughput: 0: 356.6. Samples: 956528. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 18:19:32,593][187912] Avg episode reward: [(0, '207.405')]
[36m[2025-06-29 18:19:37,565][187912] Fps is (10 sec: 410.9, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 958464. Throughput: 0: 355.9. Samples: 958560. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:19:37,565][187912] Avg episode reward: [(0, '211.118')]
[36m[2025-06-29 18:19:42,600][187912] Fps is (10 sec: 409.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 958464. Throughput: 0: 355.0. Samples: 959712. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:19:42,600][187912] Avg episode reward: [(0, '217.864')]
[31m[2737712 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2737712 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[2737713 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:19:47,558][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.2). Total num frames: 958464. Throughput: 0: 354.2. Samples: 962000. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:19:47,558][187912] Avg episode reward: [(0, '203.495')]
[31m[2745210 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2745211 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[2745211 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:19:52,591][187912] Fps is (10 sec: 410.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 962560. Throughput: 0: 353.4. Samples: 963968. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:19:52,591][187912] Avg episode reward: [(0, '219.793')]
[36m[2025-06-29 18:19:57,597][187912] Fps is (10 sec: 408.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 962560. Throughput: 0: 355.6. Samples: 965152. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:19:57,597][187912] Avg episode reward: [(0, '197.765')]
[36m[2025-06-29 18:20:02,586][187912] Fps is (10 sec: 409.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 966656. Throughput: 0: 356.2. Samples: 967264. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 18:20:02,587][187912] Avg episode reward: [(0, '195.422')]
[36m[2025-06-29 18:20:07,598][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 966656. Throughput: 0: 359.0. Samples: 969616. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 18:20:07,599][187912] Avg episode reward: [(0, '209.862')]
[36m[2025-06-29 18:20:12,572][187912] Fps is (10 sec: 410.2, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 970752. Throughput: 0: 364.3. Samples: 970784. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:20:12,572][187912] Avg episode reward: [(0, '238.164')]
[37m[1m[2025-06-29 18:20:12,615][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000003792_970752.pth...
[36m[2025-06-29 18:20:12,689][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000003456_884736.pth
[36m[2025-06-29 18:20:17,611][187912] Fps is (10 sec: 409.1, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 970752. Throughput: 0: 362.2. Samples: 972832. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:20:17,612][187912] Avg episode reward: [(0, '235.222')]
[36m[2025-06-29 18:20:22,587][187912] Fps is (10 sec: 409.0, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 974848. Throughput: 0: 362.5. Samples: 974880. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 18:20:22,587][187912] Avg episode reward: [(0, '252.931')]
[36m[2025-06-29 18:20:27,585][187912] Fps is (10 sec: 410.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 974848. Throughput: 0: 362.8. Samples: 976032. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 18:20:27,585][187912] Avg episode reward: [(0, '264.169')]
[36m[2025-06-29 18:20:33,089][187912] Fps is (10 sec: 390.0, 60 sec: 406.2, 300 sec: 360.4). Total num frames: 978944. Throughput: 0: 362.6. Samples: 978512. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:20:33,090][187912] Avg episode reward: [(0, '270.112')]
[36m[2025-06-29 18:20:37,596][187912] Fps is (10 sec: 409.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 978944. Throughput: 0: 368.3. Samples: 980544. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:20:37,596][187912] Avg episode reward: [(0, '249.902')]
[36m[2025-06-29 18:20:42,594][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.7). Total num frames: 978944. Throughput: 0: 367.3. Samples: 981680. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:20:42,595][187912] Avg episode reward: [(0, '242.187')]
[31m[2797087 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2797087 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[2797087 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:20:47,570][187912] Fps is (10 sec: 410.7, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 983040. Throughput: 0: 364.9. Samples: 983680. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 18:20:47,570][187912] Avg episode reward: [(0, '238.270')]
[36m[2025-06-29 18:20:52,575][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 983040. Throughput: 0: 362.9. Samples: 985936. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 18:20:52,575][187912] Avg episode reward: [(0, '220.671')]
[36m[2025-06-29 18:20:57,587][187912] Fps is (10 sec: 408.9, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 987136. Throughput: 0: 360.4. Samples: 987008. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:20:57,587][187912] Avg episode reward: [(0, '213.708')]
[36m[2025-06-29 18:21:02,569][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 987136. Throughput: 0: 362.3. Samples: 989120. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:21:02,569][187912] Avg episode reward: [(0, '209.351')]
[36m[2025-06-29 18:21:07,687][187912] Fps is (10 sec: 405.5, 60 sec: 409.0, 300 sec: 360.8). Total num frames: 991232. Throughput: 0: 339.5. Samples: 990192. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 18:21:07,688][187912] Avg episode reward: [(0, '231.269')]
[36m[2025-06-29 18:21:12,595][187912] Fps is (10 sec: 408.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 991232. Throughput: 0: 360.5. Samples: 992256. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 18:21:12,595][187912] Avg episode reward: [(0, '242.338')]
[36m[2025-06-29 18:21:17,580][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.2). Total num frames: 991232. Throughput: 0: 358.2. Samples: 994448. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 18:21:17,580][187912] Avg episode reward: [(0, '248.790')]
[36m[2025-06-29 18:21:22,577][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 995328. Throughput: 0: 352.9. Samples: 996416. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:21:22,577][187912] Avg episode reward: [(0, '227.503')]
[36m[2025-06-29 18:21:27,576][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 995328. Throughput: 0: 353.6. Samples: 997584. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:21:27,577][187912] Avg episode reward: [(0, '219.919')]
[36m[2025-06-29 18:21:32,596][187912] Fps is (10 sec: 408.8, 60 sec: 344.2, 300 sec: 361.0). Total num frames: 999424. Throughput: 0: 352.5. Samples: 999552. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:21:32,596][187912] Avg episode reward: [(0, '213.098')]
[36m[2025-06-29 18:21:37,565][187912] Fps is (10 sec: 410.1, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 999424. Throughput: 0: 353.9. Samples: 1001856. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:21:37,565][187912] Avg episode reward: [(0, '212.481')]
[36m[2025-06-29 18:21:42,560][187912] Fps is (10 sec: 411.1, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 1003520. Throughput: 0: 355.8. Samples: 1003008. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 18:21:42,560][187912] Avg episode reward: [(0, '219.287')]
[37m[1m[2025-06-29 18:21:42,604][187912] Saving new best policy, reward=219.287!
[36m[2025-06-29 18:21:47,590][187912] Fps is (10 sec: 408.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1003520. Throughput: 0: 355.0. Samples: 1005104. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 18:21:47,590][187912] Avg episode reward: [(0, '244.154')]
[37m[1m[2025-06-29 18:21:47,637][187912] Saving new best policy, reward=244.154!
[36m[2025-06-29 18:21:52,566][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 1003520. Throughput: 0: 381.5. Samples: 1007312. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 18:21:52,566][187912] Avg episode reward: [(0, '240.286')]
[36m[2025-06-29 18:21:57,572][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1007616. Throughput: 0: 354.7. Samples: 1008208. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:21:57,572][187912] Avg episode reward: [(0, '223.863')]
[36m[2025-06-29 18:22:02,588][187912] Fps is (10 sec: 408.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1007616. Throughput: 0: 353.7. Samples: 1010368. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:22:02,588][187912] Avg episode reward: [(0, '215.273')]
[36m[2025-06-29 18:22:07,578][187912] Fps is (10 sec: 409.3, 60 sec: 342.0, 300 sec: 361.0). Total num frames: 1011712. Throughput: 0: 351.6. Samples: 1012240. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 18:22:07,579][187912] Avg episode reward: [(0, '211.295')]
[36m[2025-06-29 18:22:12,559][187912] Fps is (10 sec: 410.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 1011712. Throughput: 0: 351.1. Samples: 1013376. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 18:22:12,559][187912] Avg episode reward: [(0, '266.634')]
[37m[1m[2025-06-29 18:22:12,602][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000003952_1011712.pth...
[36m[2025-06-29 18:22:12,673][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000003616_925696.pth
[37m[1m[2025-06-29 18:22:12,680][187912] Saving new best policy, reward=266.634!
[36m[2025-06-29 18:22:17,570][187912] Fps is (10 sec: 409.9, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 1015808. Throughput: 0: 357.9. Samples: 1015648. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:22:17,570][187912] Avg episode reward: [(0, '278.990')]
[37m[1m[2025-06-29 18:22:17,624][187912] Saving new best policy, reward=278.990!
[36m[2025-06-29 18:22:22,559][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1015808. Throughput: 0: 353.5. Samples: 1017760. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:22:22,559][187912] Avg episode reward: [(0, '312.182')]
[37m[1m[2025-06-29 18:22:22,600][187912] Saving new best policy, reward=312.182!
[36m[2025-06-29 18:22:27,595][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 1015808. Throughput: 0: 353.5. Samples: 1018928. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:22:27,595][187912] Avg episode reward: [(0, '314.162')]
[37m[1m[2025-06-29 18:22:27,636][187912] Saving new best policy, reward=314.162!
[36m[2025-06-29 18:22:32,574][187912] Fps is (10 sec: 409.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 1019904. Throughput: 0: 351.8. Samples: 1020928. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 18:22:32,574][187912] Avg episode reward: [(0, '292.951')]
[36m[2025-06-29 18:22:37,562][187912] Fps is (10 sec: 411.0, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 1019904. Throughput: 0: 354.2. Samples: 1023248. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 18:22:37,562][187912] Avg episode reward: [(0, '250.548')]
[31m[2912268 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2912268 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[2912268 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:22:42,586][187912] Fps is (10 sec: 409.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1024000. Throughput: 0: 351.9. Samples: 1024048. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:22:42,586][187912] Avg episode reward: [(0, '238.925')]
[33m[2920431 ms][navigation_task] - WARNING : Curriculum Level: 15, Curriculum progress fraction: 0.0 (navigation_task.py:262)
[33m[2920431 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.6591796875
[33mCrash Rate: 0.2646484375
[33mTimeout Rate: 0.076171875 (navigation_task.py:265)
[33m[2920431 ms][navigation_task] - WARNING : 
[33mSuccesses: 1350
[33mCrashes : 542
[33mTimeouts: 156 (navigation_task.py:268)
[36m[2025-06-29 18:22:47,558][187912] Fps is (10 sec: 409.8, 60 sec: 341.5, 300 sec: 361.1). Total num frames: 1024000. Throughput: 0: 355.8. Samples: 1026368. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:22:47,558][187912] Avg episode reward: [(0, '247.375')]
[36m[2025-06-29 18:22:52,581][187912] Fps is (10 sec: 409.8, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 1028096. Throughput: 0: 361.6. Samples: 1028512. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 18:22:52,581][187912] Avg episode reward: [(0, '238.752')]
[36m[2025-06-29 18:22:57,573][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1028096. Throughput: 0: 362.9. Samples: 1029712. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 18:22:57,573][187912] Avg episode reward: [(0, '266.578')]
[36m[2025-06-29 18:23:02,592][187912] Fps is (10 sec: 409.2, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 1032192. Throughput: 0: 363.2. Samples: 1032000. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 18:23:02,592][187912] Avg episode reward: [(0, '265.294')]
[36m[2025-06-29 18:23:07,591][187912] Fps is (10 sec: 408.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1032192. Throughput: 0: 362.0. Samples: 1034064. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 18:23:07,592][187912] Avg episode reward: [(0, '251.177')]
[36m[2025-06-29 18:23:12,561][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 1032192. Throughput: 0: 361.9. Samples: 1035200. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 18:23:12,561][187912] Avg episode reward: [(0, '231.877')]
[31m[2946127 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2946127 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[2946127 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:23:17,577][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1036288. Throughput: 0: 360.9. Samples: 1037168. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 18:23:17,577][187912] Avg episode reward: [(0, '248.720')]
[31m[2953941 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[2953942 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[2953942 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:23:22,562][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 347.7). Total num frames: 1036288. Throughput: 0: 359.1. Samples: 1039408. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 18:23:22,562][187912] Avg episode reward: [(0, '220.532')]
[36m[2025-06-29 18:23:27,583][187912] Fps is (10 sec: 409.4, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 1040384. Throughput: 0: 363.8. Samples: 1040416. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 18:23:27,583][187912] Avg episode reward: [(0, '226.452')]
[36m[2025-06-29 18:23:32,608][187912] Fps is (10 sec: 407.7, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 1040384. Throughput: 0: 358.7. Samples: 1042528. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 18:23:32,608][187912] Avg episode reward: [(0, '245.968')]
[36m[2025-06-29 18:23:37,584][187912] Fps is (10 sec: 409.6, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 1044480. Throughput: 0: 355.5. Samples: 1044512. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 18:23:37,584][187912] Avg episode reward: [(0, '255.563')]
[36m[2025-06-29 18:23:42,577][187912] Fps is (10 sec: 410.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1044480. Throughput: 0: 353.7. Samples: 1045632. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 18:23:42,578][187912] Avg episode reward: [(0, '242.484')]
[36m[2025-06-29 18:23:47,598][187912] Fps is (10 sec: 0.0, 60 sec: 341.1, 300 sec: 347.1). Total num frames: 1044480. Throughput: 0: 353.0. Samples: 1047888. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 18:23:47,599][187912] Avg episode reward: [(0, '241.901')]
[36m[2025-06-29 18:23:52,592][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1048576. Throughput: 0: 353.8. Samples: 1049984. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:23:52,592][187912] Avg episode reward: [(0, '234.597')]
[36m[2025-06-29 18:23:57,569][187912] Fps is (10 sec: 410.8, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 1048576. Throughput: 0: 353.4. Samples: 1051104. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:23:57,569][187912] Avg episode reward: [(0, '230.825')]
[36m[2025-06-29 18:24:02,569][187912] Fps is (10 sec: 410.5, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 1052672. Throughput: 0: 355.6. Samples: 1053168. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 18:24:02,569][187912] Avg episode reward: [(0, '242.092')]
[36m[2025-06-29 18:24:07,584][187912] Fps is (10 sec: 409.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1052672. Throughput: 0: 358.2. Samples: 1055536. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 18:24:07,584][187912] Avg episode reward: [(0, '225.642')]
[36m[2025-06-29 18:24:12,591][187912] Fps is (10 sec: 408.7, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 1056768. Throughput: 0: 360.1. Samples: 1056624. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:24:12,591][187912] Avg episode reward: [(0, '235.182')]
[37m[1m[2025-06-29 18:24:12,633][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000004128_1056768.pth...
[36m[2025-06-29 18:24:12,688][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000003792_970752.pth
[36m[2025-06-29 18:24:17,597][187912] Fps is (10 sec: 409.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1056768. Throughput: 0: 356.4. Samples: 1058560. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:24:17,597][187912] Avg episode reward: [(0, '250.748')]
[36m[2025-06-29 18:24:23,128][187912] Fps is (10 sec: 388.7, 60 sec: 405.8, 300 sec: 360.4). Total num frames: 1060864. Throughput: 0: 357.6. Samples: 1060800. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 18:24:23,129][187912] Avg episode reward: [(0, '258.464')]
[36m[2025-06-29 18:24:27,576][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1060864. Throughput: 0: 357.3. Samples: 1061712. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 18:24:27,576][187912] Avg episode reward: [(0, '250.986')]
[36m[2025-06-29 18:24:32,595][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 1060864. Throughput: 0: 358.8. Samples: 1064032. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 18:24:32,595][187912] Avg episode reward: [(0, '252.350')]
[36m[2025-06-29 18:24:37,560][187912] Fps is (10 sec: 410.3, 60 sec: 341.5, 300 sec: 361.1). Total num frames: 1064960. Throughput: 0: 357.9. Samples: 1066080. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:24:37,560][187912] Avg episode reward: [(0, '247.378')]
[36m[2025-06-29 18:24:42,592][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1064960. Throughput: 0: 357.5. Samples: 1067200. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:24:42,592][187912] Avg episode reward: [(0, '210.066')]
[36m[2025-06-29 18:24:47,580][187912] Fps is (10 sec: 408.8, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 1069056. Throughput: 0: 356.5. Samples: 1069216. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 18:24:47,580][187912] Avg episode reward: [(0, '197.477')]
[36m[2025-06-29 18:24:52,574][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1069056. Throughput: 0: 356.7. Samples: 1071584. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 18:24:52,574][187912] Avg episode reward: [(0, '193.263')]
[36m[2025-06-29 18:24:57,560][187912] Fps is (10 sec: 410.4, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 1073152. Throughput: 0: 356.5. Samples: 1072656. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 18:24:57,560][187912] Avg episode reward: [(0, '211.077')]
[36m[2025-06-29 18:25:02,593][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1073152. Throughput: 0: 358.1. Samples: 1074672. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 18:25:02,594][187912] Avg episode reward: [(0, '205.137')]
[36m[2025-06-29 18:25:07,562][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 1073152. Throughput: 0: 363.3. Samples: 1076944. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 18:25:07,562][187912] Avg episode reward: [(0, '225.924')]
[36m[2025-06-29 18:25:12,584][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1077248. Throughput: 0: 358.3. Samples: 1077840. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 18:25:12,584][187912] Avg episode reward: [(0, '236.474')]
[36m[2025-06-29 18:25:17,566][187912] Fps is (10 sec: 409.4, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 1077248. Throughput: 0: 359.3. Samples: 1080192. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 18:25:17,567][187912] Avg episode reward: [(0, '248.556')]
[36m[2025-06-29 18:25:22,600][187912] Fps is (10 sec: 408.9, 60 sec: 344.4, 300 sec: 361.0). Total num frames: 1081344. Throughput: 0: 356.7. Samples: 1082144. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 18:25:22,601][187912] Avg episode reward: [(0, '228.674')]
[36m[2025-06-29 18:25:27,577][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 347.7). Total num frames: 1081344. Throughput: 0: 358.5. Samples: 1083328. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 18:25:27,578][187912] Avg episode reward: [(0, '240.135')]
[36m[2025-06-29 18:25:32,618][187912] Fps is (10 sec: 408.9, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 1085440. Throughput: 0: 360.9. Samples: 1085472. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 18:25:32,618][187912] Avg episode reward: [(0, '264.624')]
[36m[2025-06-29 18:25:37,566][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1085440. Throughput: 0: 352.4. Samples: 1087440. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 18:25:37,566][187912] Avg episode reward: [(0, '256.891')]
[36m[2025-06-29 18:25:42,578][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 1085440. Throughput: 0: 352.2. Samples: 1088512. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 18:25:42,578][187912] Avg episode reward: [(0, '237.395')]
[36m[2025-06-29 18:25:47,559][187912] Fps is (10 sec: 409.9, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 1089536. Throughput: 0: 354.8. Samples: 1090624. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:25:47,559][187912] Avg episode reward: [(0, '266.912')]
[36m[2025-06-29 18:25:52,558][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 1089536. Throughput: 0: 356.7. Samples: 1092992. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:25:52,558][187912] Avg episode reward: [(0, '249.288')]
[36m[2025-06-29 18:25:57,568][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1093632. Throughput: 0: 356.0. Samples: 1093856. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:25:57,568][187912] Avg episode reward: [(0, '233.609')]
[36m[2025-06-29 18:26:02,577][187912] Fps is (10 sec: 408.8, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 1093632. Throughput: 0: 353.3. Samples: 1096096. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:26:02,578][187912] Avg episode reward: [(0, '229.058')]
[36m[2025-06-29 18:26:07,558][187912] Fps is (10 sec: 410.0, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 1097728. Throughput: 0: 352.7. Samples: 1098000. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:26:07,558][187912] Avg episode reward: [(0, '225.035')]
[31m[3121300 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[3121300 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[3121300 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:26:12,586][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1097728. Throughput: 0: 351.9. Samples: 1099168. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:26:12,586][187912] Avg episode reward: [(0, '226.948')]
[37m[1m[2025-06-29 18:26:12,629][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000004288_1097728.pth...
[36m[2025-06-29 18:26:12,690][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000003952_1011712.pth
[36m[2025-06-29 18:26:18,167][187912] Fps is (10 sec: 386.1, 60 sec: 405.5, 300 sec: 360.3). Total num frames: 1101824. Throughput: 0: 348.8. Samples: 1101360. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:26:18,167][187912] Avg episode reward: [(0, '244.068')]
[36m[2025-06-29 18:26:22,567][187912] Fps is (10 sec: 410.4, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 1101824. Throughput: 0: 353.8. Samples: 1103360. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:26:22,567][187912] Avg episode reward: [(0, '264.883')]
[36m[2025-06-29 18:26:27,580][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 1101824. Throughput: 0: 355.5. Samples: 1104512. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:26:27,580][187912] Avg episode reward: [(0, '271.111')]
[36m[2025-06-29 18:26:32,572][187912] Fps is (10 sec: 409.4, 60 sec: 341.6, 300 sec: 361.0). Total num frames: 1105920. Throughput: 0: 356.5. Samples: 1106672. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:26:32,573][187912] Avg episode reward: [(0, '289.804')]
[36m[2025-06-29 18:26:37,571][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 1105920. Throughput: 0: 355.5. Samples: 1108992. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:26:37,571][187912] Avg episode reward: [(0, '292.929')]
[36m[2025-06-29 18:26:42,562][187912] Fps is (10 sec: 410.0, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 1110016. Throughput: 0: 359.9. Samples: 1110048. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:26:42,563][187912] Avg episode reward: [(0, '239.599')]
[36m[2025-06-29 18:26:47,606][187912] Fps is (10 sec: 408.2, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 1110016. Throughput: 0: 360.3. Samples: 1112320. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:26:47,606][187912] Avg episode reward: [(0, '256.688')]
[36m[2025-06-29 18:26:52,595][187912] Fps is (10 sec: 408.3, 60 sec: 409.3, 300 sec: 361.0). Total num frames: 1114112. Throughput: 0: 363.8. Samples: 1114384. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 18:26:52,595][187912] Avg episode reward: [(0, '242.384')]
[36m[2025-06-29 18:26:57,595][187912] Fps is (10 sec: 410.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1114112. Throughput: 0: 362.2. Samples: 1115472. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 18:26:57,595][187912] Avg episode reward: [(0, '248.484')]
[36m[2025-06-29 18:27:02,587][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 1114112. Throughput: 0: 367.4. Samples: 1117680. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 18:27:02,587][187912] Avg episode reward: [(0, '255.817')]
[36m[2025-06-29 18:27:07,580][187912] Fps is (10 sec: 410.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1118208. Throughput: 0: 361.9. Samples: 1119648. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 18:27:07,580][187912] Avg episode reward: [(0, '298.062')]
[36m[2025-06-29 18:27:12,596][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 1118208. Throughput: 0: 362.5. Samples: 1120832. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 18:27:12,596][187912] Avg episode reward: [(0, '289.839')]
[36m[2025-06-29 18:27:17,560][187912] Fps is (10 sec: 410.4, 60 sec: 344.8, 300 sec: 361.0). Total num frames: 1122304. Throughput: 0: 359.6. Samples: 1122848. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 18:27:17,560][187912] Avg episode reward: [(0, '295.420')]
[31m[3194175 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[3194175 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[3194175 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:27:22,561][187912] Fps is (10 sec: 411.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1122304. Throughput: 0: 360.6. Samples: 1125216. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 18:27:22,562][187912] Avg episode reward: [(0, '301.504')]
[36m[2025-06-29 18:27:27,587][187912] Fps is (10 sec: 408.5, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 1126400. Throughput: 0: 360.7. Samples: 1126288. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 18:27:27,587][187912] Avg episode reward: [(0, '279.248')]
[36m[2025-06-29 18:27:32,599][187912] Fps is (10 sec: 408.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1126400. Throughput: 0: 356.0. Samples: 1128336. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 18:27:32,599][187912] Avg episode reward: [(0, '248.919')]
[31m[3208716 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[3208717 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[3208717 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:27:38,181][187912] Fps is (10 sec: 386.6, 60 sec: 405.5, 300 sec: 360.3). Total num frames: 1130496. Throughput: 0: 351.7. Samples: 1130416. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 18:27:38,181][187912] Avg episode reward: [(0, '227.890')]
[36m[2025-06-29 18:27:42,602][187912] Fps is (10 sec: 409.5, 60 sec: 341.1, 300 sec: 360.9). Total num frames: 1130496. Throughput: 0: 350.5. Samples: 1131248. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 18:27:42,602][187912] Avg episode reward: [(0, '199.561')]
[36m[2025-06-29 18:27:47,561][187912] Fps is (10 sec: 0.0, 60 sec: 341.6, 300 sec: 347.1). Total num frames: 1130496. Throughput: 0: 350.1. Samples: 1133424. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 18:27:47,561][187912] Avg episode reward: [(0, '194.162')]
[36m[2025-06-29 18:27:52,572][187912] Fps is (10 sec: 410.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 1134592. Throughput: 0: 351.7. Samples: 1135472. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:27:52,572][187912] Avg episode reward: [(0, '206.214')]
[36m[2025-06-29 18:27:57,577][187912] Fps is (10 sec: 408.9, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 1134592. Throughput: 0: 352.9. Samples: 1136704. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:27:57,577][187912] Avg episode reward: [(0, '224.616')]
[36m[2025-06-29 18:28:02,580][187912] Fps is (10 sec: 409.3, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 1138688. Throughput: 0: 352.6. Samples: 1138720. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 18:28:02,580][187912] Avg episode reward: [(0, '237.643')]
[36m[2025-06-29 18:28:07,566][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1138688. Throughput: 0: 349.8. Samples: 1140960. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 18:28:07,567][187912] Avg episode reward: [(0, '275.452')]
[36m[2025-06-29 18:28:13,158][187912] Fps is (10 sec: 387.2, 60 sec: 405.8, 300 sec: 360.3). Total num frames: 1142784. Throughput: 0: 344.1. Samples: 1141968. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 18:28:13,159][187912] Avg episode reward: [(0, '243.881')]
[37m[1m[2025-06-29 18:28:13,204][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000004464_1142784.pth...
[36m[2025-06-29 18:28:13,266][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000004128_1056768.pth
[36m[2025-06-29 18:28:17,589][187912] Fps is (10 sec: 408.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1142784. Throughput: 0: 345.0. Samples: 1143856. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 18:28:17,589][187912] Avg episode reward: [(0, '231.696')]
[36m[2025-06-29 18:28:22,581][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 1142784. Throughput: 0: 354.2. Samples: 1146144. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 18:28:22,581][187912] Avg episode reward: [(0, '228.773')]
[36m[2025-06-29 18:28:27,592][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1146880. Throughput: 0: 349.9. Samples: 1146992. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:28:27,592][187912] Avg episode reward: [(0, '210.197')]
[36m[2025-06-29 18:28:32,564][187912] Fps is (10 sec: 410.3, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 1146880. Throughput: 0: 356.2. Samples: 1149456. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:28:32,564][187912] Avg episode reward: [(0, '234.336')]
[36m[2025-06-29 18:28:37,581][187912] Fps is (10 sec: 410.0, 60 sec: 344.8, 300 sec: 361.0). Total num frames: 1150976. Throughput: 0: 356.9. Samples: 1151536. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 18:28:37,582][187912] Avg episode reward: [(0, '255.776')]
[36m[2025-06-29 18:28:42,579][187912] Fps is (10 sec: 409.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 1150976. Throughput: 0: 356.3. Samples: 1152736. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 18:28:42,579][187912] Avg episode reward: [(0, '263.642')]
[36m[2025-06-29 18:28:47,608][187912] Fps is (10 sec: 408.5, 60 sec: 409.3, 300 sec: 361.0). Total num frames: 1155072. Throughput: 0: 360.3. Samples: 1154944. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 18:28:47,608][187912] Avg episode reward: [(0, '296.121')]
[36m[2025-06-29 18:28:52,590][187912] Fps is (10 sec: 409.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1155072. Throughput: 0: 356.8. Samples: 1157024. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 18:28:52,590][187912] Avg episode reward: [(0, '283.752')]
[36m[2025-06-29 18:28:58,249][187912] Fps is (10 sec: 384.9, 60 sec: 405.1, 300 sec: 360.2). Total num frames: 1159168. Throughput: 0: 360.9. Samples: 1158240. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 18:28:58,250][187912] Avg episode reward: [(0, '259.972')]
[36m[2025-06-29 18:29:02,564][187912] Fps is (10 sec: 410.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1159168. Throughput: 0: 365.4. Samples: 1160288. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 18:29:02,564][187912] Avg episode reward: [(0, '281.745')]
[36m[2025-06-29 18:29:07,592][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 1159168. Throughput: 0: 366.8. Samples: 1162656. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 18:29:07,592][187912] Avg episode reward: [(0, '296.999')]
[36m[2025-06-29 18:29:12,559][187912] Fps is (10 sec: 409.8, 60 sec: 344.8, 300 sec: 361.0). Total num frames: 1163264. Throughput: 0: 367.2. Samples: 1163504. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 18:29:12,559][187912] Avg episode reward: [(0, '267.880')]
[31m[3308994 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[3308994 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[3308994 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:29:17,564][187912] Fps is (10 sec: 410.7, 60 sec: 341.5, 300 sec: 347.8). Total num frames: 1163264. Throughput: 0: 363.4. Samples: 1165808. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 18:29:17,565][187912] Avg episode reward: [(0, '280.271')]
[36m[2025-06-29 18:29:22,582][187912] Fps is (10 sec: 408.7, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 1167360. Throughput: 0: 363.0. Samples: 1167872. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 18:29:22,582][187912] Avg episode reward: [(0, '268.643')]
[31m[3319367 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[3319367 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[3319367 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:29:27,558][187912] Fps is (10 sec: 409.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 1167360. Throughput: 0: 361.4. Samples: 1168992. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 18:29:27,559][187912] Avg episode reward: [(0, '282.227')]
[31m[3324605 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[3324605 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[3324605 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:29:32,559][187912] Fps is (10 sec: 410.5, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 1171456. Throughput: 0: 362.3. Samples: 1171232. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:29:32,560][187912] Avg episode reward: [(0, '257.329')]
[36m[2025-06-29 18:29:37,581][187912] Fps is (10 sec: 408.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1171456. Throughput: 0: 362.7. Samples: 1173344. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:29:37,581][187912] Avg episode reward: [(0, '282.091')]
[36m[2025-06-29 18:29:42,588][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 1171456. Throughput: 0: 367.4. Samples: 1174528. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:29:42,588][187912] Avg episode reward: [(0, '270.943')]
[36m[2025-06-29 18:29:47,581][187912] Fps is (10 sec: 409.6, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 1175552. Throughput: 0: 361.1. Samples: 1176544. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:29:47,582][187912] Avg episode reward: [(0, '258.511')]
[36m[2025-06-29 18:29:52,576][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 1175552. Throughput: 0: 362.4. Samples: 1178960. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:29:52,577][187912] Avg episode reward: [(0, '250.542')]
[36m[2025-06-29 18:29:57,614][187912] Fps is (10 sec: 408.3, 60 sec: 345.0, 300 sec: 361.0). Total num frames: 1179648. Throughput: 0: 362.9. Samples: 1179856. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 18:29:57,614][187912] Avg episode reward: [(0, '237.029')]
[36m[2025-06-29 18:30:02,568][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1179648. Throughput: 0: 361.6. Samples: 1182080. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 18:30:02,568][187912] Avg episode reward: [(0, '256.107')]
[36m[2025-06-29 18:30:07,561][187912] Fps is (10 sec: 411.8, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 1183744. Throughput: 0: 358.2. Samples: 1183984. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:30:07,562][187912] Avg episode reward: [(0, '277.911')]
[36m[2025-06-29 18:30:12,579][187912] Fps is (10 sec: 409.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1183744. Throughput: 0: 357.5. Samples: 1185088. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:30:12,579][187912] Avg episode reward: [(0, '279.421')]
[37m[1m[2025-06-29 18:30:12,620][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000004624_1183744.pth...
[36m[2025-06-29 18:30:12,675][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000004288_1097728.pth
[36m[2025-06-29 18:30:17,565][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.2). Total num frames: 1183744. Throughput: 0: 356.2. Samples: 1187264. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:30:17,565][187912] Avg episode reward: [(0, '258.227')]
[36m[2025-06-29 18:30:22,573][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1187840. Throughput: 0: 354.2. Samples: 1189280. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 18:30:22,573][187912] Avg episode reward: [(0, '268.349')]
[31m[3380502 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[3380502 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[3380503 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:30:27,574][187912] Fps is (10 sec: 409.2, 60 sec: 341.2, 300 sec: 347.2). Total num frames: 1187840. Throughput: 0: 349.6. Samples: 1190256. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 18:30:27,574][187912] Avg episode reward: [(0, '236.270')]
[36m[2025-06-29 18:30:32,578][187912] Fps is (10 sec: 409.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1191936. Throughput: 0: 346.0. Samples: 1192112. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:30:32,578][187912] Avg episode reward: [(0, '236.756')]
[36m[2025-06-29 18:30:37,589][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1191936. Throughput: 0: 343.4. Samples: 1194416. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:30:37,590][187912] Avg episode reward: [(0, '227.409')]
[36m[2025-06-29 18:30:42,562][187912] Fps is (10 sec: 410.2, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 1196032. Throughput: 0: 348.5. Samples: 1195520. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 18:30:42,562][187912] Avg episode reward: [(0, '222.524')]
[36m[2025-06-29 18:30:47,566][187912] Fps is (10 sec: 410.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1196032. Throughput: 0: 341.7. Samples: 1197456. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 18:30:47,566][187912] Avg episode reward: [(0, '231.994')]
[36m[2025-06-29 18:30:52,565][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 1196032. Throughput: 0: 349.8. Samples: 1199728. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 18:30:52,565][187912] Avg episode reward: [(0, '255.830')]
[36m[2025-06-29 18:30:57,578][187912] Fps is (10 sec: 409.1, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 1200128. Throughput: 0: 344.9. Samples: 1200608. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:30:57,578][187912] Avg episode reward: [(0, '267.445')]
[36m[2025-06-29 18:31:02,597][187912] Fps is (10 sec: 408.3, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 1200128. Throughput: 0: 348.6. Samples: 1202960. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:31:02,597][187912] Avg episode reward: [(0, '265.186')]
[33m[3419408 ms][navigation_task] - WARNING : Curriculum Level: 15, Curriculum progress fraction: 0.0 (navigation_task.py:262)
[33m[3419408 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.65673828125
[33mCrash Rate: 0.26220703125
[33mTimeout Rate: 0.0810546875 (navigation_task.py:265)
[33m[3419409 ms][navigation_task] - WARNING : 
[33mSuccesses: 1345
[33mCrashes : 537
[33mTimeouts: 166 (navigation_task.py:268)
[36m[2025-06-29 18:31:07,585][187912] Fps is (10 sec: 409.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1204224. Throughput: 0: 349.4. Samples: 1205008. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 18:31:07,585][187912] Avg episode reward: [(0, '262.861')]
[36m[2025-06-29 18:31:12,563][187912] Fps is (10 sec: 411.0, 60 sec: 341.4, 300 sec: 347.8). Total num frames: 1204224. Throughput: 0: 353.5. Samples: 1206160. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 18:31:12,563][187912] Avg episode reward: [(0, '245.899')]
[36m[2025-06-29 18:31:17,564][187912] Fps is (10 sec: 410.5, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 1208320. Throughput: 0: 361.0. Samples: 1208352. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 18:31:17,564][187912] Avg episode reward: [(0, '252.135')]
[36m[2025-06-29 18:31:22,582][187912] Fps is (10 sec: 408.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1208320. Throughput: 0: 359.5. Samples: 1210592. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 18:31:22,582][187912] Avg episode reward: [(0, '272.567')]
[36m[2025-06-29 18:31:27,570][187912] Fps is (10 sec: 409.3, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 1212416. Throughput: 0: 362.2. Samples: 1211824. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:31:27,571][187912] Avg episode reward: [(0, '285.672')]
[36m[2025-06-29 18:31:32,573][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1212416. Throughput: 0: 363.0. Samples: 1213792. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:31:32,573][187912] Avg episode reward: [(0, '278.289')]
[36m[2025-06-29 18:31:37,569][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 1212416. Throughput: 0: 364.1. Samples: 1216112. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:31:37,569][187912] Avg episode reward: [(0, '305.790')]
[36m[2025-06-29 18:31:42,594][187912] Fps is (10 sec: 408.7, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 1216512. Throughput: 0: 364.7. Samples: 1217024. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 18:31:42,595][187912] Avg episode reward: [(0, '277.722')]
[36m[2025-06-29 18:31:47,577][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 1216512. Throughput: 0: 362.8. Samples: 1219280. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 18:31:47,577][187912] Avg episode reward: [(0, '255.491')]
[36m[2025-06-29 18:31:52,594][187912] Fps is (10 sec: 409.6, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 1220608. Throughput: 0: 362.6. Samples: 1221328. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:31:52,594][187912] Avg episode reward: [(0, '249.868')]
[36m[2025-06-29 18:31:57,561][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1220608. Throughput: 0: 361.6. Samples: 1222432. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:31:57,561][187912] Avg episode reward: [(0, '278.444')]
[36m[2025-06-29 18:32:02,589][187912] Fps is (10 sec: 409.8, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 1224704. Throughput: 0: 363.5. Samples: 1224720. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:32:02,589][187912] Avg episode reward: [(0, '276.664')]
[36m[2025-06-29 18:32:07,590][187912] Fps is (10 sec: 408.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1224704. Throughput: 0: 358.3. Samples: 1226720. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:32:07,590][187912] Avg episode reward: [(0, '305.499')]
[36m[2025-06-29 18:32:12,582][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 1224704. Throughput: 0: 354.0. Samples: 1227760. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:32:12,582][187912] Avg episode reward: [(0, '313.138')]
[37m[1m[2025-06-29 18:32:12,625][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000004784_1224704.pth...
[36m[2025-06-29 18:32:12,681][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000004464_1142784.pth
[36m[2025-06-29 18:32:17,568][187912] Fps is (10 sec: 410.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1228800. Throughput: 0: 354.9. Samples: 1229760. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 18:32:17,569][187912] Avg episode reward: [(0, '304.178')]
[31m[3493950 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[3493951 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[3493951 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:32:22,582][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 1228800. Throughput: 0: 355.1. Samples: 1232096. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 18:32:22,583][187912] Avg episode reward: [(0, '291.936')]
[36m[2025-06-29 18:32:27,587][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1232896. Throughput: 0: 353.5. Samples: 1232928. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:32:27,587][187912] Avg episode reward: [(0, '293.894')]
[36m[2025-06-29 18:32:32,569][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 347.8). Total num frames: 1232896. Throughput: 0: 356.7. Samples: 1235328. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:32:32,569][187912] Avg episode reward: [(0, '275.714')]
[36m[2025-06-29 18:32:37,595][187912] Fps is (10 sec: 409.3, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 1236992. Throughput: 0: 357.3. Samples: 1237408. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:32:37,595][187912] Avg episode reward: [(0, '257.586')]
[36m[2025-06-29 18:32:42,597][187912] Fps is (10 sec: 408.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1236992. Throughput: 0: 357.8. Samples: 1238544. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:32:42,597][187912] Avg episode reward: [(0, '287.819')]
[36m[2025-06-29 18:32:47,597][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 1236992. Throughput: 0: 353.4. Samples: 1240624. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:32:47,597][187912] Avg episode reward: [(0, '264.747')]
[36m[2025-06-29 18:32:52,570][187912] Fps is (10 sec: 410.7, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 1241088. Throughput: 0: 352.9. Samples: 1242592. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 18:32:52,570][187912] Avg episode reward: [(0, '242.974')]
[36m[2025-06-29 18:32:57,559][187912] Fps is (10 sec: 411.2, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 1241088. Throughput: 0: 356.1. Samples: 1243776. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 18:32:57,559][187912] Avg episode reward: [(0, '282.302')]
[36m[2025-06-29 18:33:02,588][187912] Fps is (10 sec: 408.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1245184. Throughput: 0: 358.2. Samples: 1245888. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:33:02,589][187912] Avg episode reward: [(0, '302.653')]
[36m[2025-06-29 18:33:07,582][187912] Fps is (10 sec: 408.7, 60 sec: 341.4, 300 sec: 347.8). Total num frames: 1245184. Throughput: 0: 358.4. Samples: 1248224. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:33:07,582][187912] Avg episode reward: [(0, '287.706')]
[36m[2025-06-29 18:33:12,566][187912] Fps is (10 sec: 410.5, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 1249280. Throughput: 0: 364.3. Samples: 1249312. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 18:33:12,566][187912] Avg episode reward: [(0, '280.300')]
[36m[2025-06-29 18:33:17,589][187912] Fps is (10 sec: 409.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1249280. Throughput: 0: 358.6. Samples: 1251472. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 18:33:17,589][187912] Avg episode reward: [(0, '297.307')]
[36m[2025-06-29 18:33:22,579][187912] Fps is (10 sec: 409.1, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 1253376. Throughput: 0: 359.2. Samples: 1253568. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 18:33:22,579][187912] Avg episode reward: [(0, '284.918')]
[36m[2025-06-29 18:33:27,578][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1253376. Throughput: 0: 360.3. Samples: 1254752. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 18:33:27,585][187912] Avg episode reward: [(0, '289.908')]
[36m[2025-06-29 18:33:32,916][187912] Fps is (10 sec: 396.2, 60 sec: 407.2, 300 sec: 360.6). Total num frames: 1257472. Throughput: 0: 363.3. Samples: 1257088. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:33:32,916][187912] Avg episode reward: [(0, '277.658')]
[36m[2025-06-29 18:33:37,580][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1257472. Throughput: 0: 366.5. Samples: 1259088. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:33:37,580][187912] Avg episode reward: [(0, '278.790')]
[36m[2025-06-29 18:33:42,594][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 1257472. Throughput: 0: 365.2. Samples: 1260224. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:33:42,594][187912] Avg episode reward: [(0, '266.213')]
[36m[2025-06-29 18:33:47,584][187912] Fps is (10 sec: 409.4, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 1261568. Throughput: 0: 364.8. Samples: 1262304. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:33:47,584][187912] Avg episode reward: [(0, '270.158')]
[36m[2025-06-29 18:33:52,594][187912] Fps is (10 sec: 409.6, 60 sec: 341.2, 300 sec: 347.9). Total num frames: 1261568. Throughput: 0: 362.2. Samples: 1264528. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:33:52,594][187912] Avg episode reward: [(0, '259.737')]
[36m[2025-06-29 18:33:57,569][187912] Fps is (10 sec: 410.2, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 1265664. Throughput: 0: 364.1. Samples: 1265696. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:33:57,570][187912] Avg episode reward: [(0, '272.076')]
[36m[2025-06-29 18:34:02,581][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1265664. Throughput: 0: 361.7. Samples: 1267744. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:34:02,581][187912] Avg episode reward: [(0, '285.405')]
[36m[2025-06-29 18:34:07,577][187912] Fps is (10 sec: 409.3, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 1269760. Throughput: 0: 360.5. Samples: 1269792. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 18:34:07,577][187912] Avg episode reward: [(0, '273.301')]
[36m[2025-06-29 18:34:12,601][187912] Fps is (10 sec: 408.8, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 1269760. Throughput: 0: 358.9. Samples: 1270912. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 18:34:12,602][187912] Avg episode reward: [(0, '259.618')]
[37m[1m[2025-06-29 18:34:12,645][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000004960_1269760.pth...
[36m[2025-06-29 18:34:12,698][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000004624_1183744.pth
[36m[2025-06-29 18:34:17,581][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 1269760. Throughput: 0: 357.9. Samples: 1273072. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 18:34:17,582][187912] Avg episode reward: [(0, '272.016')]
[36m[2025-06-29 18:34:22,561][187912] Fps is (10 sec: 411.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1273856. Throughput: 0: 357.5. Samples: 1275168. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 18:34:22,561][187912] Avg episode reward: [(0, '282.237')]
[36m[2025-06-29 18:34:27,572][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 1273856. Throughput: 0: 356.4. Samples: 1276256. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 18:34:27,572][187912] Avg episode reward: [(0, '264.802')]
[36m[2025-06-29 18:34:32,571][187912] Fps is (10 sec: 409.2, 60 sec: 343.3, 300 sec: 361.0). Total num frames: 1277952. Throughput: 0: 355.0. Samples: 1278272. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:34:32,571][187912] Avg episode reward: [(0, '304.876')]
[36m[2025-06-29 18:34:37,562][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1277952. Throughput: 0: 357.9. Samples: 1280624. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:34:37,562][187912] Avg episode reward: [(0, '330.715')]
[37m[1m[2025-06-29 18:34:37,618][187912] Saving new best policy, reward=330.715!
[36m[2025-06-29 18:34:42,559][187912] Fps is (10 sec: 410.1, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 1282048. Throughput: 0: 355.3. Samples: 1281680. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 18:34:42,559][187912] Avg episode reward: [(0, '313.295')]
[31m[3640098 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[3640098 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([12], device='cuda:0') (navigation_task.py:196)
[31m[3640098 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:34:47,601][187912] Fps is (10 sec: 408.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1282048. Throughput: 0: 354.3. Samples: 1283696. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 18:34:47,602][187912] Avg episode reward: [(0, '302.024')]
[36m[2025-06-29 18:34:53,212][187912] Fps is (10 sec: 384.5, 60 sec: 405.4, 300 sec: 360.3). Total num frames: 1286144. Throughput: 0: 355.5. Samples: 1286016. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:34:53,212][187912] Avg episode reward: [(0, '284.356')]
[36m[2025-06-29 18:34:57,560][187912] Fps is (10 sec: 411.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1286144. Throughput: 0: 355.2. Samples: 1286880. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:34:57,560][187912] Avg episode reward: [(0, '250.581')]
[36m[2025-06-29 18:35:02,591][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 1286144. Throughput: 0: 356.9. Samples: 1289136. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:35:02,591][187912] Avg episode reward: [(0, '227.888')]
[36m[2025-06-29 18:35:07,579][187912] Fps is (10 sec: 408.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1290240. Throughput: 0: 359.0. Samples: 1291328. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 18:35:07,579][187912] Avg episode reward: [(0, '247.311')]
[31m[3662335 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[3662335 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[3662335 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:35:12,568][187912] Fps is (10 sec: 410.5, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 1290240. Throughput: 0: 361.6. Samples: 1292528. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 18:35:12,569][187912] Avg episode reward: [(0, '227.368')]
[36m[2025-06-29 18:35:17,575][187912] Fps is (10 sec: 409.7, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 1294336. Throughput: 0: 365.1. Samples: 1294704. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 18:35:17,576][187912] Avg episode reward: [(0, '233.206')]
[36m[2025-06-29 18:35:22,594][187912] Fps is (10 sec: 408.5, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 1294336. Throughput: 0: 364.5. Samples: 1297040. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 18:35:22,594][187912] Avg episode reward: [(0, '271.125')]
[36m[2025-06-29 18:35:27,593][187912] Fps is (10 sec: 408.9, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 1298432. Throughput: 0: 367.7. Samples: 1298240. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 18:35:27,593][187912] Avg episode reward: [(0, '275.097')]
[36m[2025-06-29 18:35:32,576][187912] Fps is (10 sec: 410.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1298432. Throughput: 0: 370.3. Samples: 1300352. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 18:35:32,576][187912] Avg episode reward: [(0, '270.665')]
[36m[2025-06-29 18:35:37,572][187912] Fps is (10 sec: 410.4, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 1302528. Throughput: 0: 372.9. Samples: 1302560. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 18:35:37,573][187912] Avg episode reward: [(0, '273.159')]
[36m[2025-06-29 18:35:42,576][187912] Fps is (10 sec: 409.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1302528. Throughput: 0: 371.8. Samples: 1303616. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 18:35:42,576][187912] Avg episode reward: [(0, '255.591')]
[36m[2025-06-29 18:35:47,559][187912] Fps is (10 sec: 0.0, 60 sec: 341.6, 300 sec: 361.0). Total num frames: 1302528. Throughput: 0: 371.8. Samples: 1305856. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 18:35:47,559][187912] Avg episode reward: [(0, '254.380')]
[36m[2025-06-29 18:35:52,562][187912] Fps is (10 sec: 410.2, 60 sec: 345.1, 300 sec: 361.0). Total num frames: 1306624. Throughput: 0: 367.8. Samples: 1307872. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 18:35:52,562][187912] Avg episode reward: [(0, '258.772')]
[36m[2025-06-29 18:35:57,583][187912] Fps is (10 sec: 408.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1306624. Throughput: 0: 365.0. Samples: 1308960. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 18:35:57,583][187912] Avg episode reward: [(0, '264.302')]
[36m[2025-06-29 18:36:02,584][187912] Fps is (10 sec: 408.7, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 1310720. Throughput: 0: 358.3. Samples: 1310832. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:36:02,585][187912] Avg episode reward: [(0, '287.352')]
[36m[2025-06-29 18:36:07,568][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1310720. Throughput: 0: 358.6. Samples: 1313168. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:36:07,568][187912] Avg episode reward: [(0, '284.643')]
[36m[2025-06-29 18:36:12,572][187912] Fps is (10 sec: 410.1, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 1314816. Throughput: 0: 357.9. Samples: 1314336. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:36:12,572][187912] Avg episode reward: [(0, '295.677')]
[37m[1m[2025-06-29 18:36:12,612][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000005136_1314816.pth...
[36m[2025-06-29 18:36:12,681][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000004784_1224704.pth
[36m[2025-06-29 18:36:17,591][187912] Fps is (10 sec: 408.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1314816. Throughput: 0: 354.7. Samples: 1316320. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:36:17,591][187912] Avg episode reward: [(0, '296.435')]
[36m[2025-06-29 18:36:22,597][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 1314816. Throughput: 0: 357.1. Samples: 1318640. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:36:22,598][187912] Avg episode reward: [(0, '288.920')]
[36m[2025-06-29 18:36:27,580][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1318912. Throughput: 0: 353.4. Samples: 1319520. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 18:36:27,580][187912] Avg episode reward: [(0, '260.738')]
[36m[2025-06-29 18:36:32,582][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1318912. Throughput: 0: 353.6. Samples: 1321776. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 18:36:32,582][187912] Avg episode reward: [(0, '275.813')]
[36m[2025-06-29 18:36:37,572][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1323008. Throughput: 0: 351.9. Samples: 1323712. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 18:36:37,572][187912] Avg episode reward: [(0, '281.250')]
[36m[2025-06-29 18:36:42,580][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1323008. Throughput: 0: 354.2. Samples: 1324896. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 18:36:42,581][187912] Avg episode reward: [(0, '283.533')]
[36m[2025-06-29 18:36:47,561][187912] Fps is (10 sec: 410.1, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 1327104. Throughput: 0: 362.5. Samples: 1327136. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 18:36:47,561][187912] Avg episode reward: [(0, '290.803')]
[36m[2025-06-29 18:36:52,579][187912] Fps is (10 sec: 409.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1327104. Throughput: 0: 359.0. Samples: 1329328. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 18:36:52,579][187912] Avg episode reward: [(0, '302.343')]
[36m[2025-06-29 18:36:57,608][187912] Fps is (10 sec: 407.7, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 1331200. Throughput: 0: 359.9. Samples: 1330544. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 18:36:57,608][187912] Avg episode reward: [(0, '296.237')]
[36m[2025-06-29 18:37:02,588][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1331200. Throughput: 0: 361.3. Samples: 1332576. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 18:37:02,589][187912] Avg episode reward: [(0, '307.138')]
[36m[2025-06-29 18:37:07,596][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1331200. Throughput: 0: 359.5. Samples: 1334816. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 18:37:07,596][187912] Avg episode reward: [(0, '306.382')]
[36m[2025-06-29 18:37:12,573][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1335296. Throughput: 0: 358.1. Samples: 1335632. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 18:37:12,574][187912] Avg episode reward: [(0, '288.239')]
[36m[2025-06-29 18:37:17,574][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1335296. Throughput: 0: 359.5. Samples: 1337952. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 18:37:17,574][187912] Avg episode reward: [(0, '290.967')]
[36m[2025-06-29 18:37:22,565][187912] Fps is (10 sec: 409.9, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 1339392. Throughput: 0: 359.2. Samples: 1339872. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:37:22,566][187912] Avg episode reward: [(0, '296.293')]
[36m[2025-06-29 18:37:27,592][187912] Fps is (10 sec: 408.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1339392. Throughput: 0: 358.3. Samples: 1341024. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:37:27,592][187912] Avg episode reward: [(0, '300.369')]
[36m[2025-06-29 18:37:32,610][187912] Fps is (10 sec: 407.8, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 1343488. Throughput: 0: 358.4. Samples: 1343280. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 18:37:32,610][187912] Avg episode reward: [(0, '283.491')]
[36m[2025-06-29 18:37:37,573][187912] Fps is (10 sec: 410.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1343488. Throughput: 0: 351.3. Samples: 1345136. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 18:37:37,573][187912] Avg episode reward: [(0, '318.045')]
[36m[2025-06-29 18:37:42,583][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1343488. Throughput: 0: 349.7. Samples: 1346272. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 18:37:42,583][187912] Avg episode reward: [(0, '333.605')]
[37m[1m[2025-06-29 18:37:42,628][187912] Saving new best policy, reward=333.605!
[36m[2025-06-29 18:37:47,559][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1347584. Throughput: 0: 349.0. Samples: 1348272. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 18:37:47,560][187912] Avg episode reward: [(0, '309.991')]
[36m[2025-06-29 18:37:52,588][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1347584. Throughput: 0: 348.5. Samples: 1350496. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 18:37:52,589][187912] Avg episode reward: [(0, '285.666')]
[36m[2025-06-29 18:37:57,569][187912] Fps is (10 sec: 409.2, 60 sec: 341.6, 300 sec: 361.0). Total num frames: 1351680. Throughput: 0: 354.9. Samples: 1351600. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:37:57,570][187912] Avg episode reward: [(0, '306.175')]
[36m[2025-06-29 18:38:02,601][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1351680. Throughput: 0: 348.9. Samples: 1353664. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:38:02,601][187912] Avg episode reward: [(0, '285.914')]
[36m[2025-06-29 18:38:07,570][187912] Fps is (10 sec: 409.6, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 1355776. Throughput: 0: 354.1. Samples: 1355808. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:38:07,570][187912] Avg episode reward: [(0, '256.819')]
[31m[3844506 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[3844506 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[3844506 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:38:12,578][187912] Fps is (10 sec: 410.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1355776. Throughput: 0: 351.0. Samples: 1356816. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:38:12,578][187912] Avg episode reward: [(0, '264.163')]
[37m[1m[2025-06-29 18:38:12,651][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000005296_1355776.pth...
[36m[2025-06-29 18:38:12,718][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000004960_1269760.pth
[36m[2025-06-29 18:38:17,583][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 1355776. Throughput: 0: 345.1. Samples: 1358800. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:38:17,583][187912] Avg episode reward: [(0, '289.270')]
[36m[2025-06-29 18:38:22,584][187912] Fps is (10 sec: 409.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1359872. Throughput: 0: 346.9. Samples: 1360752. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 18:38:22,584][187912] Avg episode reward: [(0, '268.406')]
[36m[2025-06-29 18:38:27,580][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 347.5). Total num frames: 1359872. Throughput: 0: 348.5. Samples: 1361952. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 18:38:27,580][187912] Avg episode reward: [(0, '267.348')]
[36m[2025-06-29 18:38:32,593][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1363968. Throughput: 0: 349.3. Samples: 1364000. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 18:38:32,593][187912] Avg episode reward: [(0, '262.388')]
[31m[3870288 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[3870289 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[3870289 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[3870332 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[3870332 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[3870333 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:38:37,586][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1363968. Throughput: 0: 350.6. Samples: 1366272. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 18:38:37,586][187912] Avg episode reward: [(0, '267.182')]
[36m[2025-06-29 18:38:42,563][187912] Fps is (10 sec: 410.8, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 1368064. Throughput: 0: 353.5. Samples: 1367504. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 18:38:42,564][187912] Avg episode reward: [(0, '271.258')]
[36m[2025-06-29 18:38:47,575][187912] Fps is (10 sec: 410.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1368064. Throughput: 0: 353.6. Samples: 1369568. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 18:38:47,575][187912] Avg episode reward: [(0, '279.853')]
[31m[3884150 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[3884150 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[3884150 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:38:52,565][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 1368064. Throughput: 0: 358.4. Samples: 1371936. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 18:38:52,565][187912] Avg episode reward: [(0, '291.944')]
[36m[2025-06-29 18:38:57,585][187912] Fps is (10 sec: 409.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1372160. Throughput: 0: 356.2. Samples: 1372848. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 18:38:57,585][187912] Avg episode reward: [(0, '286.024')]
[36m[2025-06-29 18:39:02,582][187912] Fps is (10 sec: 408.9, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 1372160. Throughput: 0: 364.1. Samples: 1375184. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 18:39:02,582][187912] Avg episode reward: [(0, '304.932')]
[36m[2025-06-29 18:39:07,579][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1376256. Throughput: 0: 366.6. Samples: 1377248. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:39:07,580][187912] Avg episode reward: [(0, '275.514')]
[36m[2025-06-29 18:39:12,581][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1376256. Throughput: 0: 366.2. Samples: 1378432. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:39:12,581][187912] Avg episode reward: [(0, '286.935')]
[36m[2025-06-29 18:39:17,577][187912] Fps is (10 sec: 409.7, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 1380352. Throughput: 0: 365.6. Samples: 1380448. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:39:17,577][187912] Avg episode reward: [(0, '296.683')]
[36m[2025-06-29 18:39:22,590][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1380352. Throughput: 0: 365.8. Samples: 1382736. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:39:22,590][187912] Avg episode reward: [(0, '323.857')]
[36m[2025-06-29 18:39:27,564][187912] Fps is (10 sec: 410.1, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 1384448. Throughput: 0: 364.1. Samples: 1383888. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 18:39:27,564][187912] Avg episode reward: [(0, '286.847')]
[33m[3921111 ms][navigation_task] - WARNING : Curriculum Level: 15, Curriculum progress fraction: 0.0 (navigation_task.py:262)
[33m[3921111 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.685546875
[33mCrash Rate: 0.23876953125
[33mTimeout Rate: 0.07568359375 (navigation_task.py:265)
[33m[3921111 ms][navigation_task] - WARNING : 
[33mSuccesses: 1404
[33mCrashes : 489
[33mTimeouts: 155 (navigation_task.py:268)
[36m[2025-06-29 18:39:32,576][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1384448. Throughput: 0: 362.3. Samples: 1385872. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 18:39:32,576][187912] Avg episode reward: [(0, '315.435')]
[36m[2025-06-29 18:39:37,566][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 1384448. Throughput: 0: 360.9. Samples: 1388176. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 18:39:37,567][187912] Avg episode reward: [(0, '303.625')]
[36m[2025-06-29 18:39:42,589][187912] Fps is (10 sec: 409.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1388544. Throughput: 0: 360.5. Samples: 1389072. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:39:42,589][187912] Avg episode reward: [(0, '305.984')]
[36m[2025-06-29 18:39:47,592][187912] Fps is (10 sec: 408.6, 60 sec: 341.2, 300 sec: 347.8). Total num frames: 1388544. Throughput: 0: 359.4. Samples: 1391360. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:39:47,592][187912] Avg episode reward: [(0, '307.167')]
[36m[2025-06-29 18:39:52,562][187912] Fps is (10 sec: 410.7, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 1392640. Throughput: 0: 358.9. Samples: 1393392. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:39:52,562][187912] Avg episode reward: [(0, '315.307')]
[36m[2025-06-29 18:39:57,579][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1392640. Throughput: 0: 357.7. Samples: 1394528. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:39:57,579][187912] Avg episode reward: [(0, '295.084')]
[36m[2025-06-29 18:40:02,585][187912] Fps is (10 sec: 408.6, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 1396736. Throughput: 0: 362.6. Samples: 1396768. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:40:02,585][187912] Avg episode reward: [(0, '293.028')]
[36m[2025-06-29 18:40:07,606][187912] Fps is (10 sec: 408.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1396736. Throughput: 0: 358.3. Samples: 1398864. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:40:07,606][187912] Avg episode reward: [(0, '312.092')]
[36m[2025-06-29 18:40:12,814][187912] Fps is (10 sec: 400.4, 60 sec: 408.0, 300 sec: 360.7). Total num frames: 1400832. Throughput: 0: 357.1. Samples: 1400048. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:40:12,821][187912] Avg episode reward: [(0, '305.432')]
[37m[1m[2025-06-29 18:40:12,870][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000005472_1400832.pth...
[36m[2025-06-29 18:40:12,939][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000005136_1314816.pth
[36m[2025-06-29 18:40:17,581][187912] Fps is (10 sec: 410.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1400832. Throughput: 0: 361.9. Samples: 1402160. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:40:17,581][187912] Avg episode reward: [(0, '337.066')]
[37m[1m[2025-06-29 18:40:17,626][187912] Saving new best policy, reward=337.066!
[36m[2025-06-29 18:40:22,585][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 1400832. Throughput: 0: 358.3. Samples: 1404304. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:40:22,585][187912] Avg episode reward: [(0, '340.834')]
[37m[1m[2025-06-29 18:40:22,628][187912] Saving new best policy, reward=340.834!
[36m[2025-06-29 18:40:27,565][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1404928. Throughput: 0: 355.4. Samples: 1405056. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:40:27,566][187912] Avg episode reward: [(0, '345.147')]
[37m[1m[2025-06-29 18:40:27,604][187912] Saving new best policy, reward=345.147!
[36m[2025-06-29 18:40:32,561][187912] Fps is (10 sec: 410.6, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 1404928. Throughput: 0: 356.5. Samples: 1407392. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:40:32,561][187912] Avg episode reward: [(0, '311.971')]
[36m[2025-06-29 18:40:37,588][187912] Fps is (10 sec: 408.7, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 1409024. Throughput: 0: 355.4. Samples: 1409392. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:40:37,588][187912] Avg episode reward: [(0, '318.703')]
[36m[2025-06-29 18:40:42,572][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1409024. Throughput: 0: 357.7. Samples: 1410624. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:40:42,572][187912] Avg episode reward: [(0, '337.541')]
[31m[3998042 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[3998042 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[3998042 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:40:47,600][187912] Fps is (10 sec: 409.1, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 1413120. Throughput: 0: 357.9. Samples: 1412880. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:40:47,600][187912] Avg episode reward: [(0, '311.540')]
[36m[2025-06-29 18:40:52,574][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1413120. Throughput: 0: 357.9. Samples: 1414960. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:40:52,574][187912] Avg episode reward: [(0, '308.158')]
[36m[2025-06-29 18:40:57,570][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 1413120. Throughput: 0: 359.6. Samples: 1416144. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:40:57,570][187912] Avg episode reward: [(0, '306.894')]
[36m[2025-06-29 18:41:02,599][187912] Fps is (10 sec: 408.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1417216. Throughput: 0: 354.4. Samples: 1418112. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:41:02,599][187912] Avg episode reward: [(0, '303.250')]
[36m[2025-06-29 18:41:07,580][187912] Fps is (10 sec: 409.2, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 1417216. Throughput: 0: 355.6. Samples: 1420304. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:41:07,581][187912] Avg episode reward: [(0, '292.480')]
[36m[2025-06-29 18:41:12,586][187912] Fps is (10 sec: 410.1, 60 sec: 342.6, 300 sec: 361.0). Total num frames: 1421312. Throughput: 0: 361.8. Samples: 1421344. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:41:12,586][187912] Avg episode reward: [(0, '321.634')]
[36m[2025-06-29 18:41:17,570][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1421312. Throughput: 0: 359.0. Samples: 1423552. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:41:17,570][187912] Avg episode reward: [(0, '327.483')]
[36m[2025-06-29 18:41:22,592][187912] Fps is (10 sec: 409.4, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 1425408. Throughput: 0: 361.9. Samples: 1425680. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 18:41:22,592][187912] Avg episode reward: [(0, '354.404')]
[37m[1m[2025-06-29 18:41:22,631][187912] Saving new best policy, reward=354.404!
[36m[2025-06-29 18:41:27,596][187912] Fps is (10 sec: 408.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1425408. Throughput: 0: 360.3. Samples: 1426848. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 18:41:27,596][187912] Avg episode reward: [(0, '362.342')]
[37m[1m[2025-06-29 18:41:27,635][187912] Saving new best policy, reward=362.342!
[31m[4041053 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4041053 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[4041053 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:41:32,771][187912] Fps is (10 sec: 402.4, 60 sec: 408.2, 300 sec: 360.8). Total num frames: 1429504. Throughput: 0: 360.9. Samples: 1429184. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:41:32,771][187912] Avg episode reward: [(0, '354.704')]
[36m[2025-06-29 18:41:37,595][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1429504. Throughput: 0: 362.1. Samples: 1431264. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:41:37,595][187912] Avg episode reward: [(0, '349.996')]
[36m[2025-06-29 18:41:42,569][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 1429504. Throughput: 0: 362.0. Samples: 1432432. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:41:42,569][187912] Avg episode reward: [(0, '313.612')]
[36m[2025-06-29 18:41:47,594][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1433600. Throughput: 0: 362.3. Samples: 1434416. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:41:47,594][187912] Avg episode reward: [(0, '316.622')]
[36m[2025-06-29 18:41:52,577][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 347.2). Total num frames: 1433600. Throughput: 0: 364.5. Samples: 1436704. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:41:52,577][187912] Avg episode reward: [(0, '316.986')]
[36m[2025-06-29 18:41:57,564][187912] Fps is (10 sec: 410.9, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 1437696. Throughput: 0: 364.3. Samples: 1437728. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:41:57,564][187912] Avg episode reward: [(0, '305.969')]
[36m[2025-06-29 18:42:02,601][187912] Fps is (10 sec: 408.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1437696. Throughput: 0: 363.8. Samples: 1439936. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:42:02,601][187912] Avg episode reward: [(0, '301.338')]
[31m[4077161 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4077161 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[4077161 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:42:07,613][187912] Fps is (10 sec: 407.6, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 1441792. Throughput: 0: 358.9. Samples: 1441840. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 18:42:07,614][187912] Avg episode reward: [(0, '318.839')]
[36m[2025-06-29 18:42:12,587][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1441792. Throughput: 0: 358.5. Samples: 1442976. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 18:42:12,587][187912] Avg episode reward: [(0, '350.319')]
[37m[1m[2025-06-29 18:42:12,629][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000005632_1441792.pth...
[36m[2025-06-29 18:42:12,692][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000005296_1355776.pth
[36m[2025-06-29 18:42:17,573][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 1441792. Throughput: 0: 358.6. Samples: 1445248. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 18:42:17,573][187912] Avg episode reward: [(0, '345.665')]
[36m[2025-06-29 18:42:22,588][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1445888. Throughput: 0: 356.0. Samples: 1447280. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 18:42:22,588][187912] Avg episode reward: [(0, '336.304')]
[36m[2025-06-29 18:42:27,597][187912] Fps is (10 sec: 408.6, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 1445888. Throughput: 0: 355.3. Samples: 1448432. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 18:42:27,597][187912] Avg episode reward: [(0, '345.813')]
[36m[2025-06-29 18:42:32,595][187912] Fps is (10 sec: 409.3, 60 sec: 342.3, 300 sec: 361.0). Total num frames: 1449984. Throughput: 0: 356.6. Samples: 1450464. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 18:42:32,595][187912] Avg episode reward: [(0, '333.894')]
[36m[2025-06-29 18:42:37,585][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1449984. Throughput: 0: 358.0. Samples: 1452816. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 18:42:37,585][187912] Avg episode reward: [(0, '306.245')]
[36m[2025-06-29 18:42:42,583][187912] Fps is (10 sec: 410.1, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 1454080. Throughput: 0: 361.1. Samples: 1453984. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 18:42:42,583][187912] Avg episode reward: [(0, '307.708')]
[36m[2025-06-29 18:42:47,572][187912] Fps is (10 sec: 410.1, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 1454080. Throughput: 0: 358.3. Samples: 1456048. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 18:42:47,573][187912] Avg episode reward: [(0, '307.555')]
[36m[2025-06-29 18:42:52,602][187912] Fps is (10 sec: 408.8, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 1458176. Throughput: 0: 363.8. Samples: 1458208. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:42:52,602][187912] Avg episode reward: [(0, '304.186')]
[36m[2025-06-29 18:42:57,559][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 361.1). Total num frames: 1458176. Throughput: 0: 362.2. Samples: 1459264. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:42:57,559][187912] Avg episode reward: [(0, '302.518')]
[36m[2025-06-29 18:43:02,587][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 1458176. Throughput: 0: 361.5. Samples: 1461520. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:43:02,587][187912] Avg episode reward: [(0, '303.336')]
[36m[2025-06-29 18:43:07,585][187912] Fps is (10 sec: 408.5, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 1462272. Throughput: 0: 360.6. Samples: 1463504. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:43:07,585][187912] Avg episode reward: [(0, '293.671')]
[36m[2025-06-29 18:43:12,565][187912] Fps is (10 sec: 410.5, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 1462272. Throughput: 0: 360.1. Samples: 1464624. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:43:12,566][187912] Avg episode reward: [(0, '296.531')]
[36m[2025-06-29 18:43:17,560][187912] Fps is (10 sec: 410.6, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 1466368. Throughput: 0: 360.1. Samples: 1466656. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 18:43:17,560][187912] Avg episode reward: [(0, '268.348')]
[36m[2025-06-29 18:43:22,566][187912] Fps is (10 sec: 409.6, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 1466368. Throughput: 0: 362.5. Samples: 1469120. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 18:43:22,566][187912] Avg episode reward: [(0, '313.386')]
[36m[2025-06-29 18:43:27,585][187912] Fps is (10 sec: 408.6, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 1470464. Throughput: 0: 362.6. Samples: 1470304. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:43:27,585][187912] Avg episode reward: [(0, '314.842')]
[36m[2025-06-29 18:43:32,599][187912] Fps is (10 sec: 408.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1470464. Throughput: 0: 359.6. Samples: 1472240. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:43:32,599][187912] Avg episode reward: [(0, '315.358')]
[36m[2025-06-29 18:43:38,249][187912] Fps is (10 sec: 384.1, 60 sec: 405.1, 300 sec: 360.2). Total num frames: 1474560. Throughput: 0: 355.8. Samples: 1474448. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:43:38,249][187912] Avg episode reward: [(0, '336.255')]
[31m[4175640 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4175641 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([12], device='cuda:0') (navigation_task.py:196)
[31m[4175641 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[4175697 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4175698 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([12], device='cuda:0') (navigation_task.py:196)
[31m[4175698 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:43:42,574][187912] Fps is (10 sec: 410.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1474560. Throughput: 0: 357.2. Samples: 1475344. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:43:42,575][187912] Avg episode reward: [(0, '359.051')]
[36m[2025-06-29 18:43:47,579][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1474560. Throughput: 0: 355.3. Samples: 1477504. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:43:47,579][187912] Avg episode reward: [(0, '343.372')]
[36m[2025-06-29 18:43:52,602][187912] Fps is (10 sec: 408.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1478656. Throughput: 0: 356.1. Samples: 1479536. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 18:43:52,602][187912] Avg episode reward: [(0, '337.964')]
[36m[2025-06-29 18:43:57,591][187912] Fps is (10 sec: 409.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1478656. Throughput: 0: 356.1. Samples: 1480656. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 18:43:57,591][187912] Avg episode reward: [(0, '338.378')]
[36m[2025-06-29 18:44:02,573][187912] Fps is (10 sec: 410.8, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 1482752. Throughput: 0: 358.3. Samples: 1482784. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:44:02,573][187912] Avg episode reward: [(0, '330.854')]
[36m[2025-06-29 18:44:07,590][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1482752. Throughput: 0: 351.8. Samples: 1484960. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:44:07,590][187912] Avg episode reward: [(0, '331.611')]
[36m[2025-06-29 18:44:12,697][187912] Fps is (10 sec: 404.6, 60 sec: 408.7, 300 sec: 360.9). Total num frames: 1486848. Throughput: 0: 352.2. Samples: 1486192. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:44:12,697][187912] Avg episode reward: [(0, '322.707')]
[37m[1m[2025-06-29 18:44:12,742][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000005808_1486848.pth...
[36m[2025-06-29 18:44:12,809][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000005472_1400832.pth
[36m[2025-06-29 18:44:17,580][187912] Fps is (10 sec: 410.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1486848. Throughput: 0: 352.5. Samples: 1488096. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:44:17,580][187912] Avg episode reward: [(0, '312.988')]
[36m[2025-06-29 18:44:22,573][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 1486848. Throughput: 0: 356.3. Samples: 1490240. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:44:22,573][187912] Avg episode reward: [(0, '298.708')]
[36m[2025-06-29 18:44:27,563][187912] Fps is (10 sec: 410.3, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 1490944. Throughput: 0: 351.4. Samples: 1491152. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:44:27,563][187912] Avg episode reward: [(0, '297.589')]
[36m[2025-06-29 18:44:32,598][187912] Fps is (10 sec: 408.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1490944. Throughput: 0: 357.2. Samples: 1493584. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:44:32,598][187912] Avg episode reward: [(0, '298.212')]
[36m[2025-06-29 18:44:37,580][187912] Fps is (10 sec: 408.9, 60 sec: 345.2, 300 sec: 361.0). Total num frames: 1495040. Throughput: 0: 356.8. Samples: 1495584. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:44:37,580][187912] Avg episode reward: [(0, '322.990')]
[36m[2025-06-29 18:44:42,595][187912] Fps is (10 sec: 409.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1495040. Throughput: 0: 356.6. Samples: 1496704. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:44:42,595][187912] Avg episode reward: [(0, '332.110')]
[36m[2025-06-29 18:44:47,566][187912] Fps is (10 sec: 410.1, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 1499136. Throughput: 0: 358.1. Samples: 1498896. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:44:47,567][187912] Avg episode reward: [(0, '337.245')]
[36m[2025-06-29 18:44:52,567][187912] Fps is (10 sec: 410.7, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 1499136. Throughput: 0: 353.6. Samples: 1500864. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:44:52,568][187912] Avg episode reward: [(0, '353.801')]
[36m[2025-06-29 18:44:57,566][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 1499136. Throughput: 0: 349.5. Samples: 1501872. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:44:57,566][187912] Avg episode reward: [(0, '344.664')]
[36m[2025-06-29 18:45:02,569][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1503232. Throughput: 0: 347.8. Samples: 1503744. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:45:02,570][187912] Avg episode reward: [(0, '322.495')]
[36m[2025-06-29 18:45:07,578][187912] Fps is (10 sec: 409.1, 60 sec: 341.4, 300 sec: 347.4). Total num frames: 1503232. Throughput: 0: 353.0. Samples: 1506128. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:45:07,578][187912] Avg episode reward: [(0, '332.588')]
[36m[2025-06-29 18:45:12,561][187912] Fps is (10 sec: 409.9, 60 sec: 342.1, 300 sec: 361.0). Total num frames: 1507328. Throughput: 0: 358.4. Samples: 1507280. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:45:12,561][187912] Avg episode reward: [(0, '309.008')]
[36m[2025-06-29 18:45:17,568][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1507328. Throughput: 0: 349.4. Samples: 1509296. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:45:17,568][187912] Avg episode reward: [(0, '274.888')]
[31m[4274369 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4274369 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[4274369 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:45:22,596][187912] Fps is (10 sec: 408.2, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 1511424. Throughput: 0: 352.6. Samples: 1511456. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 18:45:22,596][187912] Avg episode reward: [(0, '277.201')]
[36m[2025-06-29 18:45:27,580][187912] Fps is (10 sec: 409.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1511424. Throughput: 0: 350.0. Samples: 1512448. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 18:45:27,581][187912] Avg episode reward: [(0, '245.273')]
[36m[2025-06-29 18:45:32,564][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 1511424. Throughput: 0: 353.1. Samples: 1514784. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 18:45:32,564][187912] Avg episode reward: [(0, '266.309')]
[36m[2025-06-29 18:45:37,564][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1515520. Throughput: 0: 355.9. Samples: 1516880. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 18:45:37,564][187912] Avg episode reward: [(0, '283.460')]
[36m[2025-06-29 18:45:42,569][187912] Fps is (10 sec: 409.4, 60 sec: 341.5, 300 sec: 347.2). Total num frames: 1515520. Throughput: 0: 360.2. Samples: 1518080. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 18:45:42,569][187912] Avg episode reward: [(0, '299.674')]
[36m[2025-06-29 18:45:47,567][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1519616. Throughput: 0: 364.5. Samples: 1520144. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:45:47,567][187912] Avg episode reward: [(0, '314.614')]
[36m[2025-06-29 18:45:52,565][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1519616. Throughput: 0: 365.3. Samples: 1522560. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:45:52,565][187912] Avg episode reward: [(0, '329.196')]
[36m[2025-06-29 18:45:57,583][187912] Fps is (10 sec: 409.0, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 1523712. Throughput: 0: 365.3. Samples: 1523728. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:45:57,583][187912] Avg episode reward: [(0, '320.730')]
[36m[2025-06-29 18:46:02,596][187912] Fps is (10 sec: 408.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1523712. Throughput: 0: 369.6. Samples: 1525936. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:46:02,596][187912] Avg episode reward: [(0, '334.530')]
[36m[2025-06-29 18:46:07,586][187912] Fps is (10 sec: 409.5, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 1527808. Throughput: 0: 366.7. Samples: 1527952. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 18:46:07,586][187912] Avg episode reward: [(0, '335.131')]
[36m[2025-06-29 18:46:12,590][187912] Fps is (10 sec: 409.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1527808. Throughput: 0: 370.8. Samples: 1529136. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 18:46:12,590][187912] Avg episode reward: [(0, '328.600')]
[37m[1m[2025-06-29 18:46:12,631][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000005968_1527808.pth...
[36m[2025-06-29 18:46:12,705][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000005632_1441792.pth
[36m[2025-06-29 18:46:18,056][187912] Fps is (10 sec: 391.2, 60 sec: 406.3, 300 sec: 360.4). Total num frames: 1531904. Throughput: 0: 366.1. Samples: 1531440. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:46:18,056][187912] Avg episode reward: [(0, '305.890')]
[36m[2025-06-29 18:46:22,581][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1531904. Throughput: 0: 372.1. Samples: 1533632. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:46:22,581][187912] Avg episode reward: [(0, '292.720')]
[36m[2025-06-29 18:46:27,593][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.3). Total num frames: 1531904. Throughput: 0: 371.0. Samples: 1534784. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:46:27,593][187912] Avg episode reward: [(0, '249.055')]
[36m[2025-06-29 18:46:32,605][187912] Fps is (10 sec: 408.6, 60 sec: 409.3, 300 sec: 361.0). Total num frames: 1536000. Throughput: 0: 373.0. Samples: 1536944. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:46:32,606][187912] Avg episode reward: [(0, '264.946')]
[31m[4350738 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4350738 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[4350738 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:46:37,602][187912] Fps is (10 sec: 409.2, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 1536000. Throughput: 0: 369.5. Samples: 1539200. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:46:37,602][187912] Avg episode reward: [(0, '247.833')]
[36m[2025-06-29 18:46:42,580][187912] Fps is (10 sec: 410.6, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 1540096. Throughput: 0: 364.5. Samples: 1540128. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 18:46:42,580][187912] Avg episode reward: [(0, '280.399')]
[36m[2025-06-29 18:46:47,559][187912] Fps is (10 sec: 411.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1540096. Throughput: 0: 367.9. Samples: 1542480. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 18:46:47,559][187912] Avg episode reward: [(0, '304.841')]
[36m[2025-06-29 18:46:52,559][187912] Fps is (10 sec: 410.5, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 1544192. Throughput: 0: 372.1. Samples: 1544688. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:46:52,559][187912] Avg episode reward: [(0, '324.062')]
[36m[2025-06-29 18:46:57,588][187912] Fps is (10 sec: 408.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1544192. Throughput: 0: 373.4. Samples: 1545936. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:46:57,588][187912] Avg episode reward: [(0, '314.336')]
[36m[2025-06-29 18:47:02,569][187912] Fps is (10 sec: 409.2, 60 sec: 409.8, 300 sec: 361.1). Total num frames: 1548288. Throughput: 0: 379.2. Samples: 1548320. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:47:02,569][187912] Avg episode reward: [(0, '307.453')]
[36m[2025-06-29 18:47:07,566][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1548288. Throughput: 0: 372.0. Samples: 1550368. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:47:07,566][187912] Avg episode reward: [(0, '289.713')]
[36m[2025-06-29 18:47:13,196][187912] Fps is (10 sec: 385.4, 60 sec: 405.5, 300 sec: 374.1). Total num frames: 1552384. Throughput: 0: 366.6. Samples: 1551504. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 18:47:13,197][187912] Avg episode reward: [(0, '284.930')]
[36m[2025-06-29 18:47:17,559][187912] Fps is (10 sec: 409.9, 60 sec: 344.2, 300 sec: 361.0). Total num frames: 1552384. Throughput: 0: 368.0. Samples: 1553488. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 18:47:17,559][187912] Avg episode reward: [(0, '290.572')]
[36m[2025-06-29 18:47:22,564][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1552384. Throughput: 0: 370.1. Samples: 1555840. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 18:47:22,564][187912] Avg episode reward: [(0, '322.516')]
[36m[2025-06-29 18:47:27,564][187912] Fps is (10 sec: 409.4, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 1556480. Throughput: 0: 366.7. Samples: 1556624. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:47:27,565][187912] Avg episode reward: [(0, '341.982')]
[36m[2025-06-29 18:47:32,580][187912] Fps is (10 sec: 408.9, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 1556480. Throughput: 0: 368.9. Samples: 1559088. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:47:32,581][187912] Avg episode reward: [(0, '354.531')]
[36m[2025-06-29 18:47:37,592][187912] Fps is (10 sec: 408.5, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 1560576. Throughput: 0: 366.7. Samples: 1561200. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:47:37,592][187912] Avg episode reward: [(0, '342.187')]
[36m[2025-06-29 18:47:42,574][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1560576. Throughput: 0: 364.6. Samples: 1562336. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:47:42,574][187912] Avg episode reward: [(0, '342.056')]
[36m[2025-06-29 18:47:47,565][187912] Fps is (10 sec: 410.7, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 1564672. Throughput: 0: 362.3. Samples: 1564624. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 18:47:47,565][187912] Avg episode reward: [(0, '334.657')]
[36m[2025-06-29 18:47:52,590][187912] Fps is (10 sec: 409.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1564672. Throughput: 0: 361.8. Samples: 1566656. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 18:47:52,590][187912] Avg episode reward: [(0, '301.616')]
[36m[2025-06-29 18:47:57,565][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 1564672. Throughput: 0: 365.3. Samples: 1567712. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 18:47:57,565][187912] Avg episode reward: [(0, '305.770')]
[33m[4435626 ms][navigation_task] - WARNING : Curriculum Level: 17, Curriculum progress fraction: 0.05714285714285714 (navigation_task.py:262)
[33m[4435626 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.759765625
[33mCrash Rate: 0.203125
[33mTimeout Rate: 0.037109375 (navigation_task.py:265)
[33m[4435626 ms][navigation_task] - WARNING : 
[33mSuccesses: 1556
[33mCrashes : 416
[33mTimeouts: 76 (navigation_task.py:268)
[36m[2025-06-29 18:48:02,560][187912] Fps is (10 sec: 410.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1568768. Throughput: 0: 361.6. Samples: 1569760. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 18:48:02,560][187912] Avg episode reward: [(0, '328.304')]
[36m[2025-06-29 18:48:07,561][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1568768. Throughput: 0: 363.8. Samples: 1572208. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 18:48:07,561][187912] Avg episode reward: [(0, '306.295')]
[36m[2025-06-29 18:48:12,560][187912] Fps is (10 sec: 409.6, 60 sec: 345.0, 300 sec: 361.0). Total num frames: 1572864. Throughput: 0: 366.6. Samples: 1573120. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 18:48:12,560][187912] Avg episode reward: [(0, '301.650')]
[37m[1m[2025-06-29 18:48:12,604][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000006144_1572864.pth...
[36m[2025-06-29 18:48:12,673][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000005808_1486848.pth
[36m[2025-06-29 18:48:17,559][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1572864. Throughput: 0: 360.0. Samples: 1575280. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 18:48:17,559][187912] Avg episode reward: [(0, '303.090')]
[36m[2025-06-29 18:48:22,593][187912] Fps is (10 sec: 408.3, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 1576960. Throughput: 0: 356.3. Samples: 1577232. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 18:48:22,593][187912] Avg episode reward: [(0, '303.065')]
[31m[4456262 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4456262 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[4456262 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:48:27,569][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1576960. Throughput: 0: 354.9. Samples: 1578304. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 18:48:27,569][187912] Avg episode reward: [(0, '293.717')]
[36m[2025-06-29 18:48:33,302][187912] Fps is (10 sec: 382.5, 60 sec: 404.7, 300 sec: 360.9). Total num frames: 1581056. Throughput: 0: 348.1. Samples: 1580544. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 18:48:33,303][187912] Avg episode reward: [(0, '328.296')]
[36m[2025-06-29 18:48:37,587][187912] Fps is (10 sec: 408.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1581056. Throughput: 0: 353.4. Samples: 1582560. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 18:48:37,587][187912] Avg episode reward: [(0, '339.923')]
[36m[2025-06-29 18:48:42,563][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1581056. Throughput: 0: 353.4. Samples: 1583616. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 18:48:42,563][187912] Avg episode reward: [(0, '348.817')]
[36m[2025-06-29 18:48:47,581][187912] Fps is (10 sec: 409.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1585152. Throughput: 0: 353.3. Samples: 1585664. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:48:47,581][187912] Avg episode reward: [(0, '345.447')]
[36m[2025-06-29 18:48:52,576][187912] Fps is (10 sec: 409.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1585152. Throughput: 0: 351.5. Samples: 1588032. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:48:52,576][187912] Avg episode reward: [(0, '353.068')]
[36m[2025-06-29 18:48:57,585][187912] Fps is (10 sec: 409.4, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 1589248. Throughput: 0: 355.0. Samples: 1589104. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 18:48:57,586][187912] Avg episode reward: [(0, '311.840')]
[36m[2025-06-29 18:49:02,596][187912] Fps is (10 sec: 408.8, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 1589248. Throughput: 0: 352.8. Samples: 1591168. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 18:49:02,596][187912] Avg episode reward: [(0, '307.116')]
[36m[2025-06-29 18:49:07,574][187912] Fps is (10 sec: 410.1, 60 sec: 409.5, 300 sec: 361.2). Total num frames: 1593344. Throughput: 0: 358.9. Samples: 1593376. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:49:07,574][187912] Avg episode reward: [(0, '308.055')]
[36m[2025-06-29 18:49:12,578][187912] Fps is (10 sec: 410.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1593344. Throughput: 0: 358.3. Samples: 1594432. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:49:12,579][187912] Avg episode reward: [(0, '333.639')]
[36m[2025-06-29 18:49:17,588][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1593344. Throughput: 0: 363.1. Samples: 1596624. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:49:17,589][187912] Avg episode reward: [(0, '325.525')]
[36m[2025-06-29 18:49:22,573][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1597440. Throughput: 0: 360.3. Samples: 1598768. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:49:22,574][187912] Avg episode reward: [(0, '318.746')]
[36m[2025-06-29 18:49:27,560][187912] Fps is (10 sec: 410.8, 60 sec: 341.4, 300 sec: 361.1). Total num frames: 1597440. Throughput: 0: 361.6. Samples: 1599888. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:49:27,560][187912] Avg episode reward: [(0, '308.679')]
[36m[2025-06-29 18:49:32,573][187912] Fps is (10 sec: 409.6, 60 sec: 345.5, 300 sec: 361.0). Total num frames: 1601536. Throughput: 0: 363.1. Samples: 1602000. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 18:49:32,574][187912] Avg episode reward: [(0, '263.782')]
[36m[2025-06-29 18:49:37,593][187912] Fps is (10 sec: 408.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1601536. Throughput: 0: 363.6. Samples: 1604400. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 18:49:37,594][187912] Avg episode reward: [(0, '258.598')]
[36m[2025-06-29 18:49:42,566][187912] Fps is (10 sec: 409.9, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 1605632. Throughput: 0: 365.7. Samples: 1605552. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:49:42,566][187912] Avg episode reward: [(0, '251.736')]
[36m[2025-06-29 18:49:47,587][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1605632. Throughput: 0: 366.3. Samples: 1607648. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:49:47,588][187912] Avg episode reward: [(0, '297.987')]
[36m[2025-06-29 18:49:52,588][187912] Fps is (10 sec: 408.7, 60 sec: 409.5, 300 sec: 374.9). Total num frames: 1609728. Throughput: 0: 364.0. Samples: 1609760. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:49:52,588][187912] Avg episode reward: [(0, '329.953')]
[36m[2025-06-29 18:49:57,592][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1609728. Throughput: 0: 364.7. Samples: 1610848. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:49:57,592][187912] Avg episode reward: [(0, '350.068')]
[36m[2025-06-29 18:50:03,298][187912] Fps is (10 sec: 382.5, 60 sec: 404.9, 300 sec: 374.0). Total num frames: 1613824. Throughput: 0: 363.7. Samples: 1613248. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 18:50:03,298][187912] Avg episode reward: [(0, '361.194')]
[36m[2025-06-29 18:50:07,559][187912] Fps is (10 sec: 411.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1613824. Throughput: 0: 369.2. Samples: 1615376. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 18:50:07,559][187912] Avg episode reward: [(0, '372.935')]
[37m[1m[2025-06-29 18:50:07,616][187912] Saving new best policy, reward=372.935!
[36m[2025-06-29 18:50:12,570][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1613824. Throughput: 0: 369.3. Samples: 1616512. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 18:50:12,570][187912] Avg episode reward: [(0, '362.976')]
[37m[1m[2025-06-29 18:50:12,612][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000006304_1613824.pth...
[36m[2025-06-29 18:50:12,676][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000005968_1527808.pth
[36m[2025-06-29 18:50:17,566][187912] Fps is (10 sec: 409.3, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 1617920. Throughput: 0: 364.5. Samples: 1618400. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 18:50:17,566][187912] Avg episode reward: [(0, '349.490')]
[36m[2025-06-29 18:50:22,596][187912] Fps is (10 sec: 408.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1617920. Throughput: 0: 361.6. Samples: 1620672. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 18:50:22,597][187912] Avg episode reward: [(0, '338.771')]
[36m[2025-06-29 18:50:27,586][187912] Fps is (10 sec: 408.8, 60 sec: 409.4, 300 sec: 374.9). Total num frames: 1622016. Throughput: 0: 360.7. Samples: 1621792. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:50:27,587][187912] Avg episode reward: [(0, '307.933')]
[36m[2025-06-29 18:50:32,593][187912] Fps is (10 sec: 409.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1622016. Throughput: 0: 361.9. Samples: 1623936. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:50:32,593][187912] Avg episode reward: [(0, '297.718')]
[36m[2025-06-29 18:50:37,833][187912] Fps is (10 sec: 399.7, 60 sec: 408.0, 300 sec: 374.6). Total num frames: 1626112. Throughput: 0: 338.1. Samples: 1625056. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 18:50:37,833][187912] Avg episode reward: [(0, '299.361')]
[36m[2025-06-29 18:50:42,582][187912] Fps is (10 sec: 410.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1626112. Throughput: 0: 359.9. Samples: 1627040. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 18:50:42,582][187912] Avg episode reward: [(0, '278.627')]
[36m[2025-06-29 18:50:47,597][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1626112. Throughput: 0: 360.5. Samples: 1629216. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 18:50:47,597][187912] Avg episode reward: [(0, '311.942')]
[36m[2025-06-29 18:50:52,575][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1630208. Throughput: 0: 352.9. Samples: 1631264. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:50:52,575][187912] Avg episode reward: [(0, '308.987')]
[36m[2025-06-29 18:50:57,583][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1630208. Throughput: 0: 350.8. Samples: 1632304. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:50:57,584][187912] Avg episode reward: [(0, '324.115')]
[36m[2025-06-29 18:51:02,595][187912] Fps is (10 sec: 408.8, 60 sec: 345.4, 300 sec: 361.0). Total num frames: 1634304. Throughput: 0: 354.3. Samples: 1634352. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:51:02,596][187912] Avg episode reward: [(0, '337.524')]
[36m[2025-06-29 18:51:07,587][187912] Fps is (10 sec: 409.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1634304. Throughput: 0: 357.1. Samples: 1636736. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:51:07,587][187912] Avg episode reward: [(0, '356.210')]
[36m[2025-06-29 18:51:12,593][187912] Fps is (10 sec: 409.7, 60 sec: 409.4, 300 sec: 361.6). Total num frames: 1638400. Throughput: 0: 356.6. Samples: 1637840. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 18:51:12,593][187912] Avg episode reward: [(0, '332.549')]
[36m[2025-06-29 18:51:17,595][187912] Fps is (10 sec: 409.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1638400. Throughput: 0: 354.1. Samples: 1639872. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 18:51:17,595][187912] Avg episode reward: [(0, '319.970')]
[36m[2025-06-29 18:51:22,558][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 1638400. Throughput: 0: 380.6. Samples: 1642080. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 18:51:22,559][187912] Avg episode reward: [(0, '347.331')]
[36m[2025-06-29 18:51:27,580][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1642496. Throughput: 0: 354.5. Samples: 1642992. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:51:27,580][187912] Avg episode reward: [(0, '323.597')]
[36m[2025-06-29 18:51:32,589][187912] Fps is (10 sec: 408.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1642496. Throughput: 0: 358.5. Samples: 1645344. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:51:32,589][187912] Avg episode reward: [(0, '320.291')]
[36m[2025-06-29 18:51:37,589][187912] Fps is (10 sec: 409.2, 60 sec: 342.7, 300 sec: 361.0). Total num frames: 1646592. Throughput: 0: 360.1. Samples: 1647472. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:51:37,590][187912] Avg episode reward: [(0, '310.616')]
[36m[2025-06-29 18:51:42,595][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1646592. Throughput: 0: 361.1. Samples: 1648560. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:51:42,595][187912] Avg episode reward: [(0, '340.459')]
[36m[2025-06-29 18:51:47,560][187912] Fps is (10 sec: 410.8, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 1650688. Throughput: 0: 362.2. Samples: 1650640. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 18:51:47,560][187912] Avg episode reward: [(0, '318.551')]
[36m[2025-06-29 18:51:52,560][187912] Fps is (10 sec: 411.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1650688. Throughput: 0: 356.8. Samples: 1652784. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 18:51:52,560][187912] Avg episode reward: [(0, '330.699')]
[36m[2025-06-29 18:51:58,107][187912] Fps is (10 sec: 388.4, 60 sec: 406.1, 300 sec: 360.3). Total num frames: 1654784. Throughput: 0: 355.1. Samples: 1654000. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:51:58,108][187912] Avg episode reward: [(0, '349.893')]
[36m[2025-06-29 18:52:02,590][187912] Fps is (10 sec: 408.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1654784. Throughput: 0: 356.7. Samples: 1655920. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:52:02,591][187912] Avg episode reward: [(0, '328.928')]
[36m[2025-06-29 18:52:07,583][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.8). Total num frames: 1654784. Throughput: 0: 353.9. Samples: 1658016. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:52:07,590][187912] Avg episode reward: [(0, '316.625')]
[36m[2025-06-29 18:52:12,593][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1658880. Throughput: 0: 354.0. Samples: 1658928. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:52:12,593][187912] Avg episode reward: [(0, '309.044')]
[37m[1m[2025-06-29 18:52:12,634][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000006480_1658880.pth...
[36m[2025-06-29 18:52:12,697][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000006144_1572864.pth
[36m[2025-06-29 18:52:17,577][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1658880. Throughput: 0: 353.9. Samples: 1661264. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:52:17,577][187912] Avg episode reward: [(0, '317.689')]
[36m[2025-06-29 18:52:22,596][187912] Fps is (10 sec: 409.5, 60 sec: 409.3, 300 sec: 361.0). Total num frames: 1662976. Throughput: 0: 352.3. Samples: 1663328. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:52:22,596][187912] Avg episode reward: [(0, '331.104')]
[36m[2025-06-29 18:52:27,603][187912] Fps is (10 sec: 408.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1662976. Throughput: 0: 352.3. Samples: 1664416. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:52:27,604][187912] Avg episode reward: [(0, '354.911')]
[31m[4705130 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4705130 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([12], device='cuda:0') (navigation_task.py:196)
[31m[4705131 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:52:32,564][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.2). Total num frames: 1662976. Throughput: 0: 352.3. Samples: 1666496. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:52:32,564][187912] Avg episode reward: [(0, '348.640')]
[36m[2025-06-29 18:52:37,611][187912] Fps is (10 sec: 409.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1667072. Throughput: 0: 348.8. Samples: 1668496. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 18:52:37,611][187912] Avg episode reward: [(0, '377.563')]
[37m[1m[2025-06-29 18:52:37,682][187912] Saving new best policy, reward=377.563!
[36m[2025-06-29 18:52:42,592][187912] Fps is (10 sec: 408.5, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 1667072. Throughput: 0: 350.3. Samples: 1669584. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 18:52:42,592][187912] Avg episode reward: [(0, '350.902')]
[36m[2025-06-29 18:52:47,579][187912] Fps is (10 sec: 410.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1671168. Throughput: 0: 348.5. Samples: 1671600. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:52:47,579][187912] Avg episode reward: [(0, '333.744')]
[36m[2025-06-29 18:52:52,583][187912] Fps is (10 sec: 410.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1671168. Throughput: 0: 351.6. Samples: 1673840. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:52:52,583][187912] Avg episode reward: [(0, '340.031')]
[36m[2025-06-29 18:52:57,568][187912] Fps is (10 sec: 410.0, 60 sec: 344.4, 300 sec: 361.0). Total num frames: 1675264. Throughput: 0: 355.8. Samples: 1674928. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:52:57,568][187912] Avg episode reward: [(0, '330.396')]
[36m[2025-06-29 18:53:02,568][187912] Fps is (10 sec: 410.2, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 1675264. Throughput: 0: 351.0. Samples: 1677056. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:53:02,568][187912] Avg episode reward: [(0, '331.825')]
[36m[2025-06-29 18:53:07,785][187912] Fps is (10 sec: 400.9, 60 sec: 408.2, 300 sec: 360.7). Total num frames: 1679360. Throughput: 0: 330.7. Samples: 1678272. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:53:07,785][187912] Avg episode reward: [(0, '334.485')]
[36m[2025-06-29 18:53:12,578][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1679360. Throughput: 0: 353.6. Samples: 1680320. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:53:12,578][187912] Avg episode reward: [(0, '312.043')]
[36m[2025-06-29 18:53:17,589][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 1679360. Throughput: 0: 357.8. Samples: 1682608. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:53:17,590][187912] Avg episode reward: [(0, '295.125')]
[36m[2025-06-29 18:53:22,577][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1683456. Throughput: 0: 360.8. Samples: 1684720. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:53:22,577][187912] Avg episode reward: [(0, '325.593')]
[36m[2025-06-29 18:53:27,592][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 348.0). Total num frames: 1683456. Throughput: 0: 361.6. Samples: 1685856. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:53:27,593][187912] Avg episode reward: [(0, '338.342')]
[36m[2025-06-29 18:53:32,571][187912] Fps is (10 sec: 409.8, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 1687552. Throughput: 0: 360.9. Samples: 1687840. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:53:32,571][187912] Avg episode reward: [(0, '347.775')]
[36m[2025-06-29 18:53:37,603][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1687552. Throughput: 0: 362.5. Samples: 1690160. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:53:37,603][187912] Avg episode reward: [(0, '362.626')]
[36m[2025-06-29 18:53:42,581][187912] Fps is (10 sec: 409.2, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 1691648. Throughput: 0: 364.7. Samples: 1691344. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 18:53:42,581][187912] Avg episode reward: [(0, '401.326')]
[37m[1m[2025-06-29 18:53:42,620][187912] Saving new best policy, reward=401.326!
[36m[2025-06-29 18:53:47,568][187912] Fps is (10 sec: 411.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1691648. Throughput: 0: 360.9. Samples: 1693296. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 18:53:47,569][187912] Avg episode reward: [(0, '391.069')]
[36m[2025-06-29 18:53:53,127][187912] Fps is (10 sec: 388.4, 60 sec: 405.9, 300 sec: 360.3). Total num frames: 1695744. Throughput: 0: 383.6. Samples: 1695664. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 18:53:53,128][187912] Avg episode reward: [(0, '378.164')]
[31m[4790677 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4790677 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([12], device='cuda:0') (navigation_task.py:196)
[31m[4790677 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:53:57,571][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1695744. Throughput: 0: 360.2. Samples: 1696528. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 18:53:57,572][187912] Avg episode reward: [(0, '358.014')]
[36m[2025-06-29 18:54:02,566][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 1695744. Throughput: 0: 362.9. Samples: 1698928. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 18:54:02,566][187912] Avg episode reward: [(0, '364.849')]
[36m[2025-06-29 18:54:07,565][187912] Fps is (10 sec: 409.9, 60 sec: 342.6, 300 sec: 361.0). Total num frames: 1699840. Throughput: 0: 362.4. Samples: 1701024. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 18:54:07,565][187912] Avg episode reward: [(0, '366.253')]
[36m[2025-06-29 18:54:12,567][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1699840. Throughput: 0: 361.1. Samples: 1702096. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 18:54:12,568][187912] Avg episode reward: [(0, '342.676')]
[37m[1m[2025-06-29 18:54:12,610][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000006640_1699840.pth...
[36m[2025-06-29 18:54:12,672][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000006304_1613824.pth
[36m[2025-06-29 18:54:17,591][187912] Fps is (10 sec: 408.6, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 1703936. Throughput: 0: 359.7. Samples: 1704032. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:54:17,591][187912] Avg episode reward: [(0, '354.103')]
[36m[2025-06-29 18:54:22,561][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1703936. Throughput: 0: 357.7. Samples: 1706240. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:54:22,561][187912] Avg episode reward: [(0, '348.618')]
[36m[2025-06-29 18:54:27,704][187912] Fps is (10 sec: 405.0, 60 sec: 408.8, 300 sec: 360.8). Total num frames: 1708032. Throughput: 0: 354.6. Samples: 1707344. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:54:27,704][187912] Avg episode reward: [(0, '340.962')]
[36m[2025-06-29 18:54:32,565][187912] Fps is (10 sec: 409.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1708032. Throughput: 0: 357.4. Samples: 1709376. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:54:32,565][187912] Avg episode reward: [(0, '284.529')]
[36m[2025-06-29 18:54:37,573][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 1708032. Throughput: 0: 356.4. Samples: 1711504. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:54:37,573][187912] Avg episode reward: [(0, '316.288')]
[31m[4835742 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4835743 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[4835743 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:54:42,595][187912] Fps is (10 sec: 408.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1712128. Throughput: 0: 351.1. Samples: 1712336. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 18:54:42,595][187912] Avg episode reward: [(0, '286.183')]
[36m[2025-06-29 18:54:47,558][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 1712128. Throughput: 0: 347.8. Samples: 1714576. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 18:54:47,558][187912] Avg episode reward: [(0, '267.276')]
[36m[2025-06-29 18:54:52,571][187912] Fps is (10 sec: 410.6, 60 sec: 344.5, 300 sec: 361.0). Total num frames: 1716224. Throughput: 0: 342.7. Samples: 1716448. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:54:52,572][187912] Avg episode reward: [(0, '281.240')]
[36m[2025-06-29 18:54:57,573][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 348.0). Total num frames: 1716224. Throughput: 0: 345.6. Samples: 1717648. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:54:57,573][187912] Avg episode reward: [(0, '288.811')]
[36m[2025-06-29 18:55:03,153][187912] Fps is (10 sec: 387.1, 60 sec: 405.6, 300 sec: 360.3). Total num frames: 1720320. Throughput: 0: 346.3. Samples: 1719808. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:55:03,153][187912] Avg episode reward: [(0, '241.624')]
[36m[2025-06-29 18:55:07,572][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1720320. Throughput: 0: 346.2. Samples: 1721824. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:55:07,572][187912] Avg episode reward: [(0, '259.984')]
[36m[2025-06-29 18:55:12,582][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 1720320. Throughput: 0: 346.2. Samples: 1722880. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:55:12,582][187912] Avg episode reward: [(0, '271.591')]
[36m[2025-06-29 18:55:17,587][187912] Fps is (10 sec: 409.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1724416. Throughput: 0: 344.7. Samples: 1724896. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:55:17,587][187912] Avg episode reward: [(0, '267.774')]
[36m[2025-06-29 18:55:22,566][187912] Fps is (10 sec: 410.3, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 1724416. Throughput: 0: 347.8. Samples: 1727152. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:55:22,566][187912] Avg episode reward: [(0, '267.558')]
[36m[2025-06-29 18:55:27,583][187912] Fps is (10 sec: 409.8, 60 sec: 342.0, 300 sec: 361.0). Total num frames: 1728512. Throughput: 0: 356.4. Samples: 1728368. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:55:27,583][187912] Avg episode reward: [(0, '287.248')]
[36m[2025-06-29 18:55:32,580][187912] Fps is (10 sec: 409.0, 60 sec: 341.2, 300 sec: 347.4). Total num frames: 1728512. Throughput: 0: 350.8. Samples: 1730368. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:55:32,580][187912] Avg episode reward: [(0, '299.177')]
[36m[2025-06-29 18:55:38,166][187912] Fps is (10 sec: 387.0, 60 sec: 405.6, 300 sec: 360.3). Total num frames: 1732608. Throughput: 0: 352.3. Samples: 1732512. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:55:38,166][187912] Avg episode reward: [(0, '312.485')]
[36m[2025-06-29 18:55:42,595][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1732608. Throughput: 0: 350.0. Samples: 1733408. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:55:42,595][187912] Avg episode reward: [(0, '317.713')]
[36m[2025-06-29 18:55:47,584][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 1732608. Throughput: 0: 356.1. Samples: 1735632. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:55:47,584][187912] Avg episode reward: [(0, '318.302')]
[36m[2025-06-29 18:55:52,558][187912] Fps is (10 sec: 411.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1736704. Throughput: 0: 352.1. Samples: 1737664. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:55:52,558][187912] Avg episode reward: [(0, '353.051')]
[31m[4910232 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4910232 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[4910232 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:55:57,597][187912] Fps is (10 sec: 409.1, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 1736704. Throughput: 0: 355.4. Samples: 1738880. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:55:57,597][187912] Avg episode reward: [(0, '368.277')]
[36m[2025-06-29 18:56:02,577][187912] Fps is (10 sec: 408.8, 60 sec: 344.6, 300 sec: 361.0). Total num frames: 1740800. Throughput: 0: 354.2. Samples: 1740832. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 18:56:02,577][187912] Avg episode reward: [(0, '355.663')]
[36m[2025-06-29 18:56:07,569][187912] Fps is (10 sec: 410.7, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 1740800. Throughput: 0: 356.2. Samples: 1743184. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 18:56:07,570][187912] Avg episode reward: [(0, '359.683')]
[36m[2025-06-29 18:56:12,574][187912] Fps is (10 sec: 409.7, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 1744896. Throughput: 0: 355.6. Samples: 1744368. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 18:56:12,574][187912] Avg episode reward: [(0, '363.825')]
[37m[1m[2025-06-29 18:56:12,615][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000006816_1744896.pth...
[36m[2025-06-29 18:56:12,678][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000006480_1658880.pth
[36m[2025-06-29 18:56:17,583][187912] Fps is (10 sec: 409.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1744896. Throughput: 0: 357.3. Samples: 1746448. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 18:56:17,584][187912] Avg episode reward: [(0, '357.265')]
[36m[2025-06-29 18:56:22,587][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 1744896. Throughput: 0: 361.6. Samples: 1748576. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 18:56:22,588][187912] Avg episode reward: [(0, '358.618')]
[36m[2025-06-29 18:56:27,574][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1748992. Throughput: 0: 356.8. Samples: 1749456. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:56:27,574][187912] Avg episode reward: [(0, '363.879')]
[36m[2025-06-29 18:56:32,570][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 1748992. Throughput: 0: 359.9. Samples: 1751824. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:56:32,570][187912] Avg episode reward: [(0, '362.088')]
[33m[4948867 ms][navigation_task] - WARNING : Curriculum Level: 19, Curriculum progress fraction: 0.11428571428571428 (navigation_task.py:262)
[33m[4948867 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.744140625
[33mCrash Rate: 0.21875
[33mTimeout Rate: 0.037109375 (navigation_task.py:265)
[33m[4948867 ms][navigation_task] - WARNING : 
[33mSuccesses: 1524
[33mCrashes : 448
[33mTimeouts: 76 (navigation_task.py:268)
[36m[2025-06-29 18:56:37,569][187912] Fps is (10 sec: 409.8, 60 sec: 344.8, 300 sec: 361.0). Total num frames: 1753088. Throughput: 0: 358.3. Samples: 1753792. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:56:37,569][187912] Avg episode reward: [(0, '373.747')]
[36m[2025-06-29 18:56:42,567][187912] Fps is (10 sec: 409.7, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 1753088. Throughput: 0: 355.1. Samples: 1754848. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:56:42,568][187912] Avg episode reward: [(0, '391.547')]
[31m[4958946 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4958947 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[4958947 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:56:47,595][187912] Fps is (10 sec: 408.6, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 1757184. Throughput: 0: 363.9. Samples: 1757216. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:56:47,595][187912] Avg episode reward: [(0, '362.060')]
[36m[2025-06-29 18:56:52,560][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 347.8). Total num frames: 1757184. Throughput: 0: 357.8. Samples: 1759280. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:56:52,561][187912] Avg episode reward: [(0, '367.428')]
[36m[2025-06-29 18:56:57,790][187912] Fps is (10 sec: 401.8, 60 sec: 408.3, 300 sec: 360.8). Total num frames: 1761280. Throughput: 0: 357.0. Samples: 1760512. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:56:57,790][187912] Avg episode reward: [(0, '364.274')]
[36m[2025-06-29 18:57:02,617][187912] Fps is (10 sec: 407.3, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 1761280. Throughput: 0: 361.0. Samples: 1762704. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:57:02,617][187912] Avg episode reward: [(0, '351.854')]
[36m[2025-06-29 18:57:07,565][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 1761280. Throughput: 0: 361.4. Samples: 1764832. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:57:07,566][187912] Avg episode reward: [(0, '333.511')]
[36m[2025-06-29 18:57:12,589][187912] Fps is (10 sec: 410.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1765376. Throughput: 0: 359.3. Samples: 1765632. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:57:12,589][187912] Avg episode reward: [(0, '362.829')]
[31m[4989202 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[4989202 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([12], device='cuda:0') (navigation_task.py:196)
[31m[4989202 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:57:17,591][187912] Fps is (10 sec: 408.5, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 1765376. Throughput: 0: 357.5. Samples: 1767920. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:57:17,592][187912] Avg episode reward: [(0, '340.529')]
[36m[2025-06-29 18:57:22,574][187912] Fps is (10 sec: 410.2, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 1769472. Throughput: 0: 358.4. Samples: 1769920. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 18:57:22,574][187912] Avg episode reward: [(0, '340.178')]
[36m[2025-06-29 18:57:27,580][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1769472. Throughput: 0: 362.9. Samples: 1771184. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 18:57:27,580][187912] Avg episode reward: [(0, '342.555')]
[36m[2025-06-29 18:57:32,594][187912] Fps is (10 sec: 408.8, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 1773568. Throughput: 0: 361.6. Samples: 1773488. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:57:32,594][187912] Avg episode reward: [(0, '359.272')]
[36m[2025-06-29 18:57:37,561][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1773568. Throughput: 0: 362.0. Samples: 1775568. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:57:37,561][187912] Avg episode reward: [(0, '357.393')]
[36m[2025-06-29 18:57:42,586][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 1773568. Throughput: 0: 361.5. Samples: 1776704. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:57:42,586][187912] Avg episode reward: [(0, '351.744')]
[36m[2025-06-29 18:57:47,581][187912] Fps is (10 sec: 408.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1777664. Throughput: 0: 354.4. Samples: 1778640. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 18:57:47,582][187912] Avg episode reward: [(0, '356.217')]
[36m[2025-06-29 18:57:52,603][187912] Fps is (10 sec: 408.9, 60 sec: 341.1, 300 sec: 347.1). Total num frames: 1777664. Throughput: 0: 355.6. Samples: 1780848. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 18:57:52,604][187912] Avg episode reward: [(0, '354.511')]
[36m[2025-06-29 18:57:57,574][187912] Fps is (10 sec: 409.9, 60 sec: 342.6, 300 sec: 361.0). Total num frames: 1781760. Throughput: 0: 359.2. Samples: 1781792. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:57:57,574][187912] Avg episode reward: [(0, '338.891')]
[36m[2025-06-29 18:58:02,596][187912] Fps is (10 sec: 409.9, 60 sec: 341.5, 300 sec: 347.3). Total num frames: 1781760. Throughput: 0: 357.7. Samples: 1784016. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:58:02,596][187912] Avg episode reward: [(0, '344.244')]
[36m[2025-06-29 18:58:07,579][187912] Fps is (10 sec: 409.4, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 1785856. Throughput: 0: 357.6. Samples: 1786016. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 18:58:07,580][187912] Avg episode reward: [(0, '346.131')]
[36m[2025-06-29 18:58:12,595][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1785856. Throughput: 0: 355.8. Samples: 1787200. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 18:58:12,595][187912] Avg episode reward: [(0, '336.697')]
[37m[1m[2025-06-29 18:58:12,644][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000006976_1785856.pth...
[36m[2025-06-29 18:58:12,702][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000006640_1699840.pth
[36m[2025-06-29 18:58:18,287][187912] Fps is (10 sec: 382.5, 60 sec: 404.9, 300 sec: 360.1). Total num frames: 1789952. Throughput: 0: 348.8. Samples: 1789424. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:58:18,288][187912] Avg episode reward: [(0, '345.054')]
[36m[2025-06-29 18:58:22,589][187912] Fps is (10 sec: 409.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1789952. Throughput: 0: 353.9. Samples: 1791504. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:58:22,589][187912] Avg episode reward: [(0, '349.031')]
[36m[2025-06-29 18:58:27,596][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 1789952. Throughput: 0: 355.8. Samples: 1792720. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:58:27,596][187912] Avg episode reward: [(0, '336.895')]
[36m[2025-06-29 18:58:32,561][187912] Fps is (10 sec: 410.7, 60 sec: 341.5, 300 sec: 361.1). Total num frames: 1794048. Throughput: 0: 359.6. Samples: 1794816. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:58:32,561][187912] Avg episode reward: [(0, '341.240')]
[31m[5070247 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5070247 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[5070247 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:58:37,561][187912] Fps is (10 sec: 411.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 1794048. Throughput: 0: 362.6. Samples: 1797152. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:58:37,562][187912] Avg episode reward: [(0, '362.319')]
[36m[2025-06-29 18:58:42,599][187912] Fps is (10 sec: 408.1, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 1798144. Throughput: 0: 363.9. Samples: 1798176. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:58:42,599][187912] Avg episode reward: [(0, '365.429')]
[36m[2025-06-29 18:58:47,569][187912] Fps is (10 sec: 409.3, 60 sec: 341.4, 300 sec: 347.8). Total num frames: 1798144. Throughput: 0: 365.4. Samples: 1800448. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:58:47,569][187912] Avg episode reward: [(0, '351.119')]
[36m[2025-06-29 18:58:52,593][187912] Fps is (10 sec: 409.8, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 1802240. Throughput: 0: 368.6. Samples: 1802608. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 18:58:52,593][187912] Avg episode reward: [(0, '367.401')]
[36m[2025-06-29 18:58:57,591][187912] Fps is (10 sec: 408.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1802240. Throughput: 0: 367.7. Samples: 1803744. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 18:58:57,591][187912] Avg episode reward: [(0, '344.982')]
[36m[2025-06-29 18:59:03,137][187912] Fps is (10 sec: 388.5, 60 sec: 405.9, 300 sec: 360.3). Total num frames: 1806336. Throughput: 0: 367.5. Samples: 1805904. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:59:03,137][187912] Avg episode reward: [(0, '359.012')]
[31m[5098941 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5098941 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[5098941 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:59:07,574][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1806336. Throughput: 0: 364.9. Samples: 1807920. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:59:07,574][187912] Avg episode reward: [(0, '341.842')]
[36m[2025-06-29 18:59:12,590][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 1806336. Throughput: 0: 362.4. Samples: 1809024. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:59:12,590][187912] Avg episode reward: [(0, '366.034')]
[36m[2025-06-29 18:59:17,578][187912] Fps is (10 sec: 409.4, 60 sec: 345.4, 300 sec: 361.0). Total num frames: 1810432. Throughput: 0: 363.2. Samples: 1811168. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:59:17,578][187912] Avg episode reward: [(0, '375.042')]
[36m[2025-06-29 18:59:22,584][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 347.3). Total num frames: 1810432. Throughput: 0: 358.6. Samples: 1813296. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:59:22,584][187912] Avg episode reward: [(0, '396.178')]
[31m[5118091 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5118091 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[5118091 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 18:59:27,574][187912] Fps is (10 sec: 409.8, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 1814528. Throughput: 0: 361.8. Samples: 1814448. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:59:27,574][187912] Avg episode reward: [(0, '384.040')]
[36m[2025-06-29 18:59:32,577][187912] Fps is (10 sec: 409.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1814528. Throughput: 0: 357.6. Samples: 1816544. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:59:32,577][187912] Avg episode reward: [(0, '400.637')]
[36m[2025-06-29 18:59:37,587][187912] Fps is (10 sec: 409.1, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 1818624. Throughput: 0: 356.7. Samples: 1818656. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:59:37,587][187912] Avg episode reward: [(0, '400.359')]
[36m[2025-06-29 18:59:42,569][187912] Fps is (10 sec: 409.9, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 1818624. Throughput: 0: 356.8. Samples: 1819792. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:59:42,569][187912] Avg episode reward: [(0, '410.760')]
[37m[1m[2025-06-29 18:59:42,610][187912] Saving new best policy, reward=410.760!
[36m[2025-06-29 18:59:47,591][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 1818624. Throughput: 0: 363.2. Samples: 1822048. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 18:59:47,591][187912] Avg episode reward: [(0, '404.005')]
[36m[2025-06-29 18:59:52,562][187912] Fps is (10 sec: 409.9, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 1822720. Throughput: 0: 357.1. Samples: 1823984. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:59:52,562][187912] Avg episode reward: [(0, '410.032')]
[36m[2025-06-29 18:59:57,583][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 347.8). Total num frames: 1822720. Throughput: 0: 357.4. Samples: 1825104. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 18:59:57,583][187912] Avg episode reward: [(0, '385.919')]
[36m[2025-06-29 19:00:02,580][187912] Fps is (10 sec: 408.9, 60 sec: 344.5, 300 sec: 361.0). Total num frames: 1826816. Throughput: 0: 355.5. Samples: 1827168. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 19:00:02,581][187912] Avg episode reward: [(0, '379.947')]
[36m[2025-06-29 19:00:07,568][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1826816. Throughput: 0: 358.9. Samples: 1829440. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 19:00:07,568][187912] Avg episode reward: [(0, '385.926')]
[36m[2025-06-29 19:00:12,577][187912] Fps is (10 sec: 409.7, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 1830912. Throughput: 0: 359.8. Samples: 1830640. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:00:12,577][187912] Avg episode reward: [(0, '397.744')]
[37m[1m[2025-06-29 19:00:12,621][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000007152_1830912.pth...
[36m[2025-06-29 19:00:12,698][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000006816_1744896.pth
[36m[2025-06-29 19:00:17,572][187912] Fps is (10 sec: 409.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1830912. Throughput: 0: 356.3. Samples: 1832576. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:00:17,572][187912] Avg episode reward: [(0, '388.138')]
[36m[2025-06-29 19:00:22,950][187912] Fps is (10 sec: 394.9, 60 sec: 407.1, 300 sec: 360.6). Total num frames: 1835008. Throughput: 0: 360.1. Samples: 1834992. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 19:00:22,950][187912] Avg episode reward: [(0, '402.059')]
[36m[2025-06-29 19:00:27,570][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1835008. Throughput: 0: 358.4. Samples: 1835920. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 19:00:27,570][187912] Avg episode reward: [(0, '411.939')]
[37m[1m[2025-06-29 19:00:27,622][187912] Saving new best policy, reward=411.939!
[36m[2025-06-29 19:00:32,558][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.8). Total num frames: 1835008. Throughput: 0: 361.9. Samples: 1838320. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 19:00:32,558][187912] Avg episode reward: [(0, '369.184')]
[36m[2025-06-29 19:00:37,562][187912] Fps is (10 sec: 409.9, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 1839104. Throughput: 0: 363.4. Samples: 1840336. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:00:37,562][187912] Avg episode reward: [(0, '389.147')]
[36m[2025-06-29 19:00:42,596][187912] Fps is (10 sec: 408.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1839104. Throughput: 0: 364.0. Samples: 1841488. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:00:42,596][187912] Avg episode reward: [(0, '396.221')]
[36m[2025-06-29 19:00:47,585][187912] Fps is (10 sec: 408.7, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 1843200. Throughput: 0: 363.0. Samples: 1843504. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:00:47,585][187912] Avg episode reward: [(0, '407.142')]
[36m[2025-06-29 19:00:52,575][187912] Fps is (10 sec: 410.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1843200. Throughput: 0: 364.0. Samples: 1845824. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:00:52,576][187912] Avg episode reward: [(0, '400.795')]
[36m[2025-06-29 19:00:57,564][187912] Fps is (10 sec: 410.5, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 1847296. Throughput: 0: 361.0. Samples: 1846880. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:00:57,564][187912] Avg episode reward: [(0, '411.094')]
[36m[2025-06-29 19:01:02,589][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1847296. Throughput: 0: 361.1. Samples: 1848832. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:01:02,589][187912] Avg episode reward: [(0, '396.422')]
[36m[2025-06-29 19:01:07,583][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 1847296. Throughput: 0: 361.7. Samples: 1851136. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:01:07,584][187912] Avg episode reward: [(0, '396.291')]
[36m[2025-06-29 19:01:12,571][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1851392. Throughput: 0: 356.6. Samples: 1851968. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 19:01:12,571][187912] Avg episode reward: [(0, '393.228')]
[36m[2025-06-29 19:01:17,590][187912] Fps is (10 sec: 409.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1851392. Throughput: 0: 353.2. Samples: 1854224. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 19:01:17,590][187912] Avg episode reward: [(0, '358.322')]
[36m[2025-06-29 19:01:22,586][187912] Fps is (10 sec: 409.0, 60 sec: 343.4, 300 sec: 361.0). Total num frames: 1855488. Throughput: 0: 352.5. Samples: 1856208. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 19:01:22,586][187912] Avg episode reward: [(0, '348.386')]
[36m[2025-06-29 19:01:27,558][187912] Fps is (10 sec: 410.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1855488. Throughput: 0: 354.4. Samples: 1857424. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 19:01:27,559][187912] Avg episode reward: [(0, '322.060')]
[36m[2025-06-29 19:01:32,570][187912] Fps is (10 sec: 410.2, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 1859584. Throughput: 0: 358.2. Samples: 1859616. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:01:32,570][187912] Avg episode reward: [(0, '356.400')]
[36m[2025-06-29 19:01:37,574][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1859584. Throughput: 0: 359.5. Samples: 1862000. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:01:37,574][187912] Avg episode reward: [(0, '379.427')]
[36m[2025-06-29 19:01:42,562][187912] Fps is (10 sec: 409.9, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 1863680. Throughput: 0: 361.6. Samples: 1863152. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:01:42,563][187912] Avg episode reward: [(0, '394.114')]
[36m[2025-06-29 19:01:47,591][187912] Fps is (10 sec: 408.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1863680. Throughput: 0: 361.2. Samples: 1865088. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:01:47,591][187912] Avg episode reward: [(0, '397.951')]
[31m[5263754 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5263754 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[5263754 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:01:52,585][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.4). Total num frames: 1863680. Throughput: 0: 361.9. Samples: 1867424. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:01:52,586][187912] Avg episode reward: [(0, '367.767')]
[36m[2025-06-29 19:01:57,558][187912] Fps is (10 sec: 410.9, 60 sec: 341.4, 300 sec: 361.1). Total num frames: 1867776. Throughput: 0: 364.9. Samples: 1868384. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:01:57,559][187912] Avg episode reward: [(0, '351.053')]
[36m[2025-06-29 19:02:02,594][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1867776. Throughput: 0: 366.9. Samples: 1870736. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:02:02,595][187912] Avg episode reward: [(0, '357.094')]
[36m[2025-06-29 19:02:07,592][187912] Fps is (10 sec: 408.2, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 1871872. Throughput: 0: 371.5. Samples: 1872928. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 19:02:07,592][187912] Avg episode reward: [(0, '363.444')]
[36m[2025-06-29 19:02:12,590][187912] Fps is (10 sec: 409.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1871872. Throughput: 0: 370.2. Samples: 1874096. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 19:02:12,590][187912] Avg episode reward: [(0, '379.777')]
[37m[1m[2025-06-29 19:02:12,658][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000007312_1871872.pth...
[36m[2025-06-29 19:02:12,725][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000006976_1785856.pth
[36m[2025-06-29 19:02:17,559][187912] Fps is (10 sec: 411.0, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 1875968. Throughput: 0: 364.9. Samples: 1876032. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 19:02:17,559][187912] Avg episode reward: [(0, '397.738')]
[31m[5294096 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5294096 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[5294096 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:02:22,591][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1875968. Throughput: 0: 362.2. Samples: 1878304. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 19:02:22,591][187912] Avg episode reward: [(0, '383.062')]
[36m[2025-06-29 19:02:27,827][187912] Fps is (10 sec: 398.9, 60 sec: 407.8, 300 sec: 360.7). Total num frames: 1880064. Throughput: 0: 358.4. Samples: 1879376. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 19:02:27,827][187912] Avg episode reward: [(0, '379.453')]
[36m[2025-06-29 19:02:32,561][187912] Fps is (10 sec: 410.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1880064. Throughput: 0: 363.3. Samples: 1881424. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 19:02:32,561][187912] Avg episode reward: [(0, '373.800')]
[36m[2025-06-29 19:02:37,598][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1880064. Throughput: 0: 360.4. Samples: 1883648. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 19:02:37,598][187912] Avg episode reward: [(0, '366.924')]
[36m[2025-06-29 19:02:42,583][187912] Fps is (10 sec: 408.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1884160. Throughput: 0: 358.6. Samples: 1884528. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:02:42,583][187912] Avg episode reward: [(0, '388.076')]
[36m[2025-06-29 19:02:47,579][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1884160. Throughput: 0: 357.8. Samples: 1886832. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:02:47,580][187912] Avg episode reward: [(0, '381.211')]
[36m[2025-06-29 19:02:52,583][187912] Fps is (10 sec: 409.6, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 1888256. Throughput: 0: 354.9. Samples: 1888896. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:02:52,583][187912] Avg episode reward: [(0, '383.731')]
[36m[2025-06-29 19:02:57,597][187912] Fps is (10 sec: 408.9, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 1888256. Throughput: 0: 355.1. Samples: 1890080. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:02:57,597][187912] Avg episode reward: [(0, '393.842')]
[36m[2025-06-29 19:03:02,580][187912] Fps is (10 sec: 409.7, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 1892352. Throughput: 0: 360.4. Samples: 1892256. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 19:03:02,580][187912] Avg episode reward: [(0, '413.108')]
[37m[1m[2025-06-29 19:03:02,622][187912] Saving new best policy, reward=413.108!
[36m[2025-06-29 19:03:07,592][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1892352. Throughput: 0: 353.4. Samples: 1894208. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 19:03:07,593][187912] Avg episode reward: [(0, '396.825')]
[36m[2025-06-29 19:03:12,574][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 348.0). Total num frames: 1892352. Throughput: 0: 354.0. Samples: 1895216. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 19:03:12,575][187912] Avg episode reward: [(0, '408.577')]
[36m[2025-06-29 19:03:17,570][187912] Fps is (10 sec: 410.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1896448. Throughput: 0: 353.0. Samples: 1897312. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 19:03:17,570][187912] Avg episode reward: [(0, '383.998')]
[36m[2025-06-29 19:03:22,572][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1896448. Throughput: 0: 356.1. Samples: 1899664. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 19:03:22,573][187912] Avg episode reward: [(0, '379.841')]
[36m[2025-06-29 19:03:27,574][187912] Fps is (10 sec: 409.4, 60 sec: 342.8, 300 sec: 361.0). Total num frames: 1900544. Throughput: 0: 356.7. Samples: 1900576. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 19:03:27,575][187912] Avg episode reward: [(0, '395.180')]
[36m[2025-06-29 19:03:32,573][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1900544. Throughput: 0: 358.1. Samples: 1902944. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 19:03:32,573][187912] Avg episode reward: [(0, '358.099')]
[36m[2025-06-29 19:03:37,589][187912] Fps is (10 sec: 409.0, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 1904640. Throughput: 0: 350.5. Samples: 1904672. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:03:37,590][187912] Avg episode reward: [(0, '365.262')]
[36m[2025-06-29 19:03:42,573][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1904640. Throughput: 0: 352.2. Samples: 1905920. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:03:42,573][187912] Avg episode reward: [(0, '404.739')]
[36m[2025-06-29 19:03:48,131][187912] Fps is (10 sec: 388.6, 60 sec: 405.9, 300 sec: 360.3). Total num frames: 1908736. Throughput: 0: 350.9. Samples: 1908240. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:03:48,131][187912] Avg episode reward: [(0, '381.239')]
[36m[2025-06-29 19:03:52,599][187912] Fps is (10 sec: 408.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1908736. Throughput: 0: 356.9. Samples: 1910272. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:03:52,599][187912] Avg episode reward: [(0, '357.889')]
[36m[2025-06-29 19:03:57,572][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.8). Total num frames: 1908736. Throughput: 0: 357.4. Samples: 1911296. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:03:57,572][187912] Avg episode reward: [(0, '365.349')]
[31m[5391997 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5391997 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[5391997 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:04:02,579][187912] Fps is (10 sec: 410.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1912832. Throughput: 0: 357.3. Samples: 1913392. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 19:04:02,579][187912] Avg episode reward: [(0, '371.574')]
[36m[2025-06-29 19:04:07,567][187912] Fps is (10 sec: 409.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 1912832. Throughput: 0: 354.2. Samples: 1915600. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 19:04:07,567][187912] Avg episode reward: [(0, '377.637')]
[36m[2025-06-29 19:04:12,575][187912] Fps is (10 sec: 409.8, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 1916928. Throughput: 0: 358.0. Samples: 1916688. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 19:04:12,575][187912] Avg episode reward: [(0, '388.850')]
[37m[1m[2025-06-29 19:04:12,624][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000007488_1916928.pth...
[36m[2025-06-29 19:04:12,691][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000007152_1830912.pth
[36m[2025-06-29 19:04:17,560][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1916928. Throughput: 0: 350.7. Samples: 1918720. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 19:04:17,560][187912] Avg episode reward: [(0, '401.418')]
[36m[2025-06-29 19:04:23,145][187912] Fps is (10 sec: 387.5, 60 sec: 405.7, 300 sec: 360.3). Total num frames: 1921024. Throughput: 0: 357.2. Samples: 1920944. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 19:04:23,146][187912] Avg episode reward: [(0, '419.772')]
[37m[1m[2025-06-29 19:04:23,203][187912] Saving new best policy, reward=419.772!
[36m[2025-06-29 19:04:27,564][187912] Fps is (10 sec: 409.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1921024. Throughput: 0: 352.1. Samples: 1921760. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 19:04:27,564][187912] Avg episode reward: [(0, '432.178')]
[37m[1m[2025-06-29 19:04:27,604][187912] Saving new best policy, reward=432.178!
[36m[2025-06-29 19:04:32,569][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 1921024. Throughput: 0: 354.7. Samples: 1924000. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 19:04:32,569][187912] Avg episode reward: [(0, '427.138')]
[36m[2025-06-29 19:04:37,587][187912] Fps is (10 sec: 408.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1925120. Throughput: 0: 348.9. Samples: 1925968. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:04:37,588][187912] Avg episode reward: [(0, '419.778')]
[36m[2025-06-29 19:04:42,572][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1925120. Throughput: 0: 353.1. Samples: 1927184. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:04:42,573][187912] Avg episode reward: [(0, '423.238')]
[36m[2025-06-29 19:04:47,573][187912] Fps is (10 sec: 410.2, 60 sec: 344.5, 300 sec: 361.0). Total num frames: 1929216. Throughput: 0: 352.4. Samples: 1929248. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:04:47,573][187912] Avg episode reward: [(0, '388.593')]
[36m[2025-06-29 19:04:52,565][187912] Fps is (10 sec: 409.9, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 1929216. Throughput: 0: 352.0. Samples: 1931440. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:04:52,565][187912] Avg episode reward: [(0, '372.992')]
[36m[2025-06-29 19:04:57,685][187912] Fps is (10 sec: 405.1, 60 sec: 408.8, 300 sec: 360.9). Total num frames: 1933312. Throughput: 0: 353.3. Samples: 1932624. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:04:57,685][187912] Avg episode reward: [(0, '353.207')]
[36m[2025-06-29 19:05:02,571][187912] Fps is (10 sec: 409.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1933312. Throughput: 0: 355.8. Samples: 1934736. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:05:02,571][187912] Avg episode reward: [(0, '325.798')]
[36m[2025-06-29 19:05:07,578][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 1933312. Throughput: 0: 364.8. Samples: 1937152. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:05:07,579][187912] Avg episode reward: [(0, '338.986')]
[33m[5462160 ms][navigation_task] - WARNING : Curriculum Level: 21, Curriculum progress fraction: 0.17142857142857143 (navigation_task.py:262)
[33m[5462161 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.7451171875
[33mCrash Rate: 0.20556640625
[33mTimeout Rate: 0.04931640625 (navigation_task.py:265)
[33m[5462161 ms][navigation_task] - WARNING : 
[33mSuccesses: 1526
[33mCrashes : 421
[33mTimeouts: 101 (navigation_task.py:268)
[36m[2025-06-29 19:05:12,558][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1937408. Throughput: 0: 361.6. Samples: 1938032. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 19:05:12,558][187912] Avg episode reward: [(0, '352.433')]
[36m[2025-06-29 19:05:17,560][187912] Fps is (10 sec: 410.3, 60 sec: 341.3, 300 sec: 347.6). Total num frames: 1937408. Throughput: 0: 364.2. Samples: 1940384. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 19:05:17,561][187912] Avg episode reward: [(0, '385.431')]
[36m[2025-06-29 19:05:22,568][187912] Fps is (10 sec: 409.2, 60 sec: 344.7, 300 sec: 361.0). Total num frames: 1941504. Throughput: 0: 366.0. Samples: 1942432. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:05:22,568][187912] Avg episode reward: [(0, '415.462')]
[36m[2025-06-29 19:05:27,581][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1941504. Throughput: 0: 363.0. Samples: 1943520. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:05:27,581][187912] Avg episode reward: [(0, '435.241')]
[37m[1m[2025-06-29 19:05:27,628][187912] Saving new best policy, reward=435.241!
[36m[2025-06-29 19:05:32,561][187912] Fps is (10 sec: 409.9, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 1945600. Throughput: 0: 364.2. Samples: 1945632. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 19:05:32,561][187912] Avg episode reward: [(0, '454.718')]
[37m[1m[2025-06-29 19:05:32,603][187912] Saving new best policy, reward=454.718!
[36m[2025-06-29 19:05:37,561][187912] Fps is (10 sec: 410.4, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 1945600. Throughput: 0: 359.9. Samples: 1947632. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 19:05:37,561][187912] Avg episode reward: [(0, '451.556')]
[36m[2025-06-29 19:05:43,214][187912] Fps is (10 sec: 384.5, 60 sec: 405.3, 300 sec: 360.2). Total num frames: 1949696. Throughput: 0: 354.6. Samples: 1948768. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 19:05:43,215][187912] Avg episode reward: [(0, '448.998')]
[36m[2025-06-29 19:05:47,575][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1949696. Throughput: 0: 357.3. Samples: 1950816. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 19:05:47,575][187912] Avg episode reward: [(0, '460.266')]
[37m[1m[2025-06-29 19:05:47,629][187912] Saving new best policy, reward=460.266!
[36m[2025-06-29 19:05:52,559][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 1949696. Throughput: 0: 351.8. Samples: 1952976. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 19:05:52,560][187912] Avg episode reward: [(0, '437.785')]
[36m[2025-06-29 19:05:57,581][187912] Fps is (10 sec: 409.4, 60 sec: 341.9, 300 sec: 361.0). Total num frames: 1953792. Throughput: 0: 351.1. Samples: 1953840. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:05:57,581][187912] Avg episode reward: [(0, '439.283')]
[36m[2025-06-29 19:06:02,569][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1953792. Throughput: 0: 353.0. Samples: 1956272. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:06:02,569][187912] Avg episode reward: [(0, '462.823')]
[37m[1m[2025-06-29 19:06:02,613][187912] Saving new best policy, reward=462.823!
[36m[2025-06-29 19:06:07,566][187912] Fps is (10 sec: 410.2, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 1957888. Throughput: 0: 352.0. Samples: 1958272. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-06-29 19:06:07,567][187912] Avg episode reward: [(0, '484.794')]
[37m[1m[2025-06-29 19:06:07,604][187912] Saving new best policy, reward=484.794!
[36m[2025-06-29 19:06:12,583][187912] Fps is (10 sec: 409.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 1957888. Throughput: 0: 354.5. Samples: 1959472. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-06-29 19:06:12,583][187912] Avg episode reward: [(0, '466.442')]
[37m[1m[2025-06-29 19:06:12,627][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000007648_1957888.pth...
[36m[2025-06-29 19:06:12,698][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000007312_1871872.pth
[36m[2025-06-29 19:06:17,814][187912] Fps is (10 sec: 399.7, 60 sec: 407.9, 300 sec: 360.7). Total num frames: 1961984. Throughput: 0: 354.3. Samples: 1961664. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:06:17,814][187912] Avg episode reward: [(0, '481.298')]
[36m[2025-06-29 19:06:22,559][187912] Fps is (10 sec: 410.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1961984. Throughput: 0: 355.6. Samples: 1963632. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:06:22,559][187912] Avg episode reward: [(0, '492.353')]
[37m[1m[2025-06-29 19:06:22,616][187912] Saving new best policy, reward=492.353!
[36m[2025-06-29 19:06:27,559][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 1961984. Throughput: 0: 359.0. Samples: 1964688. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:06:27,560][187912] Avg episode reward: [(0, '489.863')]
[36m[2025-06-29 19:06:32,598][187912] Fps is (10 sec: 408.0, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 1966080. Throughput: 0: 352.5. Samples: 1966688. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 19:06:32,599][187912] Avg episode reward: [(0, '496.884')]
[37m[1m[2025-06-29 19:06:32,639][187912] Saving new best policy, reward=496.884!
[36m[2025-06-29 19:06:37,575][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 1966080. Throughput: 0: 355.8. Samples: 1968992. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 19:06:37,575][187912] Avg episode reward: [(0, '470.934')]
[36m[2025-06-29 19:06:42,593][187912] Fps is (10 sec: 409.8, 60 sec: 344.9, 300 sec: 361.0). Total num frames: 1970176. Throughput: 0: 363.6. Samples: 1970208. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 19:06:42,593][187912] Avg episode reward: [(0, '438.946')]
[36m[2025-06-29 19:06:47,574][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1970176. Throughput: 0: 353.4. Samples: 1972176. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 19:06:47,574][187912] Avg episode reward: [(0, '419.383')]
[36m[2025-06-29 19:06:52,802][187912] Fps is (10 sec: 401.2, 60 sec: 407.9, 300 sec: 360.7). Total num frames: 1974272. Throughput: 0: 330.4. Samples: 1973216. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:06:52,803][187912] Avg episode reward: [(0, '421.074')]
[36m[2025-06-29 19:06:57,584][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1974272. Throughput: 0: 350.2. Samples: 1975232. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:06:57,585][187912] Avg episode reward: [(0, '423.762')]
[36m[2025-06-29 19:07:02,585][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 1974272. Throughput: 0: 351.3. Samples: 1977392. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:07:02,585][187912] Avg episode reward: [(0, '406.028')]
[36m[2025-06-29 19:07:07,558][187912] Fps is (10 sec: 410.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 1978368. Throughput: 0: 352.4. Samples: 1979488. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 19:07:07,558][187912] Avg episode reward: [(0, '398.424')]
[36m[2025-06-29 19:07:12,575][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 1978368. Throughput: 0: 355.8. Samples: 1980704. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 19:07:12,576][187912] Avg episode reward: [(0, '401.643')]
[36m[2025-06-29 19:07:17,574][187912] Fps is (10 sec: 409.0, 60 sec: 342.7, 300 sec: 361.0). Total num frames: 1982464. Throughput: 0: 355.8. Samples: 1982688. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:07:17,574][187912] Avg episode reward: [(0, '424.099')]
[36m[2025-06-29 19:07:22,559][187912] Fps is (10 sec: 410.3, 60 sec: 341.3, 300 sec: 347.4). Total num frames: 1982464. Throughput: 0: 355.0. Samples: 1984960. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:07:22,559][187912] Avg episode reward: [(0, '425.666')]
[36m[2025-06-29 19:07:27,559][187912] Fps is (10 sec: 410.2, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 1986560. Throughput: 0: 353.7. Samples: 1986112. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 19:07:27,559][187912] Avg episode reward: [(0, '410.566')]
[36m[2025-06-29 19:07:32,569][187912] Fps is (10 sec: 409.2, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 1986560. Throughput: 0: 355.2. Samples: 1988160. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 19:07:32,569][187912] Avg episode reward: [(0, '428.877')]
[36m[2025-06-29 19:07:38,176][187912] Fps is (10 sec: 385.8, 60 sec: 405.5, 300 sec: 360.3). Total num frames: 1990656. Throughput: 0: 382.2. Samples: 1990560. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 19:07:38,176][187912] Avg episode reward: [(0, '396.163')]
[36m[2025-06-29 19:07:42,595][187912] Fps is (10 sec: 408.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1990656. Throughput: 0: 360.8. Samples: 1991472. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 19:07:42,595][187912] Avg episode reward: [(0, '375.798')]
[36m[2025-06-29 19:07:47,593][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 1990656. Throughput: 0: 365.1. Samples: 1993824. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 19:07:47,594][187912] Avg episode reward: [(0, '385.352')]
[31m[5625076 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5625076 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[5625076 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:07:52,612][187912] Fps is (10 sec: 408.9, 60 sec: 342.4, 300 sec: 361.0). Total num frames: 1994752. Throughput: 0: 362.2. Samples: 1995808. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:07:52,613][187912] Avg episode reward: [(0, '365.893')]
[36m[2025-06-29 19:07:57,574][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 1994752. Throughput: 0: 360.2. Samples: 1996912. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:07:57,574][187912] Avg episode reward: [(0, '399.261')]
[36m[2025-06-29 19:08:02,590][187912] Fps is (10 sec: 410.5, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 1998848. Throughput: 0: 362.5. Samples: 1999008. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:08:02,590][187912] Avg episode reward: [(0, '419.434')]
[36m[2025-06-29 19:08:07,570][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 1998848. Throughput: 0: 364.0. Samples: 2001344. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:08:07,570][187912] Avg episode reward: [(0, '416.384')]
[36m[2025-06-29 19:08:12,571][187912] Fps is (10 sec: 410.3, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2002944. Throughput: 0: 364.3. Samples: 2002512. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 19:08:12,572][187912] Avg episode reward: [(0, '430.627')]
[37m[1m[2025-06-29 19:08:12,611][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000007824_2002944.pth...
[36m[2025-06-29 19:08:12,665][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000007488_1916928.pth
[36m[2025-06-29 19:08:17,608][187912] Fps is (10 sec: 408.0, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 2002944. Throughput: 0: 364.1. Samples: 2004560. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 19:08:17,609][187912] Avg episode reward: [(0, '446.809')]
[36m[2025-06-29 19:08:22,586][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 2002944. Throughput: 0: 365.7. Samples: 2006800. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 19:08:22,587][187912] Avg episode reward: [(0, '449.286')]
[36m[2025-06-29 19:08:27,569][187912] Fps is (10 sec: 411.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2007040. Throughput: 0: 359.7. Samples: 2007648. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 19:08:27,569][187912] Avg episode reward: [(0, '440.678')]
[31m[5664632 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5664632 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[5664632 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:08:32,572][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 2007040. Throughput: 0: 356.4. Samples: 2009856. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 19:08:32,572][187912] Avg episode reward: [(0, '427.412')]
[36m[2025-06-29 19:08:37,590][187912] Fps is (10 sec: 408.7, 60 sec: 344.7, 300 sec: 361.0). Total num frames: 2011136. Throughput: 0: 357.5. Samples: 2011888. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:08:37,590][187912] Avg episode reward: [(0, '436.122')]
[36m[2025-06-29 19:08:42,562][187912] Fps is (10 sec: 410.0, 60 sec: 341.5, 300 sec: 347.8). Total num frames: 2011136. Throughput: 0: 359.2. Samples: 2013072. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:08:42,562][187912] Avg episode reward: [(0, '468.479')]
[36m[2025-06-29 19:08:47,573][187912] Fps is (10 sec: 410.3, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 2015232. Throughput: 0: 361.4. Samples: 2015264. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:08:47,573][187912] Avg episode reward: [(0, '459.241')]
[36m[2025-06-29 19:08:52,594][187912] Fps is (10 sec: 408.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2015232. Throughput: 0: 357.9. Samples: 2017456. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:08:52,594][187912] Avg episode reward: [(0, '420.401')]
[36m[2025-06-29 19:08:57,870][187912] Fps is (10 sec: 397.8, 60 sec: 407.6, 300 sec: 360.6). Total num frames: 2019328. Throughput: 0: 356.4. Samples: 2018656. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:08:57,870][187912] Avg episode reward: [(0, '439.011')]
[36m[2025-06-29 19:09:02,593][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2019328. Throughput: 0: 356.0. Samples: 2020576. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:09:02,593][187912] Avg episode reward: [(0, '425.105')]
[36m[2025-06-29 19:09:07,594][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 2019328. Throughput: 0: 356.9. Samples: 2022864. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:09:07,594][187912] Avg episode reward: [(0, '423.098')]
[31m[5702851 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5702851 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[5702851 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:09:12,563][187912] Fps is (10 sec: 410.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2023424. Throughput: 0: 357.7. Samples: 2023744. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:09:12,563][187912] Avg episode reward: [(0, '424.025')]
[36m[2025-06-29 19:09:17,558][187912] Fps is (10 sec: 411.1, 60 sec: 341.6, 300 sec: 347.8). Total num frames: 2023424. Throughput: 0: 358.2. Samples: 2025968. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:09:17,558][187912] Avg episode reward: [(0, '450.700')]
[36m[2025-06-29 19:09:22,561][187912] Fps is (10 sec: 409.7, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 2027520. Throughput: 0: 361.8. Samples: 2028160. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:09:22,561][187912] Avg episode reward: [(0, '459.544')]
[36m[2025-06-29 19:09:27,612][187912] Fps is (10 sec: 407.4, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 2027520. Throughput: 0: 361.9. Samples: 2029376. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:09:27,613][187912] Avg episode reward: [(0, '478.627')]
[36m[2025-06-29 19:09:32,570][187912] Fps is (10 sec: 409.2, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2031616. Throughput: 0: 364.1. Samples: 2031648. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 19:09:32,570][187912] Avg episode reward: [(0, '482.074')]
[36m[2025-06-29 19:09:37,577][187912] Fps is (10 sec: 411.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2031616. Throughput: 0: 365.3. Samples: 2033888. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 19:09:37,577][187912] Avg episode reward: [(0, '454.616')]
[36m[2025-06-29 19:09:42,561][187912] Fps is (10 sec: 409.9, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2035712. Throughput: 0: 368.4. Samples: 2035120. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:09:42,561][187912] Avg episode reward: [(0, '445.625')]
[36m[2025-06-29 19:09:47,568][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2035712. Throughput: 0: 366.8. Samples: 2037072. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:09:47,569][187912] Avg episode reward: [(0, '429.217')]
[36m[2025-06-29 19:09:52,581][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 2035712. Throughput: 0: 367.0. Samples: 2039376. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:09:52,581][187912] Avg episode reward: [(0, '420.846')]
[36m[2025-06-29 19:09:57,573][187912] Fps is (10 sec: 409.4, 60 sec: 343.0, 300 sec: 361.0). Total num frames: 2039808. Throughput: 0: 367.9. Samples: 2040304. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 19:09:57,573][187912] Avg episode reward: [(0, '425.684')]
[36m[2025-06-29 19:10:02,577][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2039808. Throughput: 0: 371.8. Samples: 2042704. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 19:10:02,578][187912] Avg episode reward: [(0, '421.402')]
[36m[2025-06-29 19:10:07,593][187912] Fps is (10 sec: 408.8, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2043904. Throughput: 0: 369.9. Samples: 2044816. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:10:07,593][187912] Avg episode reward: [(0, '413.990')]
[36m[2025-06-29 19:10:12,564][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2043904. Throughput: 0: 369.1. Samples: 2045968. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:10:12,564][187912] Avg episode reward: [(0, '442.647')]
[37m[1m[2025-06-29 19:10:12,605][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000007984_2043904.pth...
[36m[2025-06-29 19:10:12,668][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000007648_1957888.pth
[36m[2025-06-29 19:10:17,585][187912] Fps is (10 sec: 409.9, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 2048000. Throughput: 0: 364.0. Samples: 2048032. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 19:10:17,585][187912] Avg episode reward: [(0, '437.766')]
[36m[2025-06-29 19:10:22,570][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2048000. Throughput: 0: 364.9. Samples: 2050304. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 19:10:22,570][187912] Avg episode reward: [(0, '421.030')]
[36m[2025-06-29 19:10:27,582][187912] Fps is (10 sec: 409.7, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 2052096. Throughput: 0: 363.6. Samples: 2051488. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 19:10:27,582][187912] Avg episode reward: [(0, '412.132')]
[36m[2025-06-29 19:10:32,562][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2052096. Throughput: 0: 367.7. Samples: 2053616. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 19:10:32,562][187912] Avg episode reward: [(0, '442.037')]
[36m[2025-06-29 19:10:37,565][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.9). Total num frames: 2052096. Throughput: 0: 367.1. Samples: 2055888. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 19:10:37,565][187912] Avg episode reward: [(0, '435.467')]
[36m[2025-06-29 19:10:42,567][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2056192. Throughput: 0: 365.6. Samples: 2056752. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 19:10:42,568][187912] Avg episode reward: [(0, '448.853')]
[36m[2025-06-29 19:10:47,582][187912] Fps is (10 sec: 408.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2056192. Throughput: 0: 363.7. Samples: 2059072. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 19:10:47,582][187912] Avg episode reward: [(0, '451.581')]
[36m[2025-06-29 19:10:52,568][187912] Fps is (10 sec: 409.6, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 2060288. Throughput: 0: 363.6. Samples: 2061168. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:10:52,569][187912] Avg episode reward: [(0, '472.090')]
[36m[2025-06-29 19:10:57,591][187912] Fps is (10 sec: 409.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2060288. Throughput: 0: 362.1. Samples: 2062272. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:10:57,591][187912] Avg episode reward: [(0, '476.085')]
[36m[2025-06-29 19:11:02,570][187912] Fps is (10 sec: 409.5, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 2064384. Throughput: 0: 364.2. Samples: 2064416. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:11:02,570][187912] Avg episode reward: [(0, '474.920')]
[36m[2025-06-29 19:11:07,582][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2064384. Throughput: 0: 364.3. Samples: 2066704. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:11:07,583][187912] Avg episode reward: [(0, '467.930')]
[36m[2025-06-29 19:11:12,586][187912] Fps is (10 sec: 409.0, 60 sec: 409.5, 300 sec: 361.3). Total num frames: 2068480. Throughput: 0: 365.5. Samples: 2067936. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 19:11:12,586][187912] Avg episode reward: [(0, '497.619')]
[37m[1m[2025-06-29 19:11:12,625][187912] Saving new best policy, reward=497.619!
[36m[2025-06-29 19:11:17,568][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2068480. Throughput: 0: 364.8. Samples: 2070032. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 19:11:17,569][187912] Avg episode reward: [(0, '476.629')]
[31m[5832107 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5832108 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[5832108 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:11:22,570][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2068480. Throughput: 0: 365.5. Samples: 2072336. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 19:11:22,570][187912] Avg episode reward: [(0, '438.040')]
[36m[2025-06-29 19:11:27,594][187912] Fps is (10 sec: 408.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2072576. Throughput: 0: 365.7. Samples: 2073216. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:11:27,594][187912] Avg episode reward: [(0, '414.290')]
[36m[2025-06-29 19:11:32,597][187912] Fps is (10 sec: 408.5, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 2072576. Throughput: 0: 366.1. Samples: 2075552. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:11:32,597][187912] Avg episode reward: [(0, '415.775')]
[36m[2025-06-29 19:11:37,570][187912] Fps is (10 sec: 410.6, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2076672. Throughput: 0: 364.8. Samples: 2077584. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 19:11:37,570][187912] Avg episode reward: [(0, '380.091')]
[31m[5850999 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5850999 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[5850999 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:11:42,561][187912] Fps is (10 sec: 411.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2076672. Throughput: 0: 365.0. Samples: 2078688. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 19:11:42,561][187912] Avg episode reward: [(0, '412.406')]
[36m[2025-06-29 19:11:47,595][187912] Fps is (10 sec: 408.6, 60 sec: 409.5, 300 sec: 361.3). Total num frames: 2080768. Throughput: 0: 363.9. Samples: 2080800. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 19:11:47,595][187912] Avg episode reward: [(0, '438.444')]
[36m[2025-06-29 19:11:52,577][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2080768. Throughput: 0: 364.1. Samples: 2083088. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 19:11:52,577][187912] Avg episode reward: [(0, '441.729')]
[36m[2025-06-29 19:11:58,188][187912] Fps is (10 sec: 386.7, 60 sec: 405.6, 300 sec: 374.1). Total num frames: 2084864. Throughput: 0: 354.4. Samples: 2084096. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 19:11:58,188][187912] Avg episode reward: [(0, '444.591')]
[36m[2025-06-29 19:12:02,580][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2084864. Throughput: 0: 356.2. Samples: 2086064. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 19:12:02,580][187912] Avg episode reward: [(0, '453.621')]
[36m[2025-06-29 19:12:07,595][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2084864. Throughput: 0: 354.7. Samples: 2088304. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 19:12:07,595][187912] Avg episode reward: [(0, '489.024')]
[36m[2025-06-29 19:12:12,576][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2088960. Throughput: 0: 353.2. Samples: 2089104. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:12:12,577][187912] Avg episode reward: [(0, '478.517')]
[37m[1m[2025-06-29 19:12:12,617][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000008160_2088960.pth...
[36m[2025-06-29 19:12:12,686][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000007824_2002944.pth
[36m[2025-06-29 19:12:17,601][187912] Fps is (10 sec: 409.3, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 2088960. Throughput: 0: 353.0. Samples: 2091440. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:12:17,601][187912] Avg episode reward: [(0, '496.228')]
[36m[2025-06-29 19:12:22,583][187912] Fps is (10 sec: 409.3, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 2093056. Throughput: 0: 350.8. Samples: 2093376. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:12:22,583][187912] Avg episode reward: [(0, '478.023')]
[36m[2025-06-29 19:12:27,594][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2093056. Throughput: 0: 352.1. Samples: 2094544. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:12:27,594][187912] Avg episode reward: [(0, '505.559')]
[37m[1m[2025-06-29 19:12:27,655][187912] Saving new best policy, reward=505.559!
[36m[2025-06-29 19:12:33,102][187912] Fps is (10 sec: 389.4, 60 sec: 406.2, 300 sec: 361.1). Total num frames: 2097152. Throughput: 0: 349.1. Samples: 2096688. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:12:33,103][187912] Avg episode reward: [(0, '489.278')]
[36m[2025-06-29 19:12:37,585][187912] Fps is (10 sec: 410.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2097152. Throughput: 0: 350.2. Samples: 2098848. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:12:37,585][187912] Avg episode reward: [(0, '485.640')]
[36m[2025-06-29 19:12:42,559][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2097152. Throughput: 0: 358.8. Samples: 2100016. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:12:42,559][187912] Avg episode reward: [(0, '472.653')]
[36m[2025-06-29 19:12:47,565][187912] Fps is (10 sec: 410.4, 60 sec: 341.5, 300 sec: 361.1). Total num frames: 2101248. Throughput: 0: 355.7. Samples: 2102064. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 19:12:47,565][187912] Avg episode reward: [(0, '483.645')]
[36m[2025-06-29 19:12:52,596][187912] Fps is (10 sec: 408.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2101248. Throughput: 0: 356.6. Samples: 2104352. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 19:12:52,596][187912] Avg episode reward: [(0, '442.409')]
[36m[2025-06-29 19:12:57,570][187912] Fps is (10 sec: 409.4, 60 sec: 344.9, 300 sec: 361.0). Total num frames: 2105344. Throughput: 0: 361.6. Samples: 2105376. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:12:57,571][187912] Avg episode reward: [(0, '415.357')]
[36m[2025-06-29 19:13:02,581][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2105344. Throughput: 0: 356.4. Samples: 2107472. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:13:02,581][187912] Avg episode reward: [(0, '379.403')]
[36m[2025-06-29 19:13:07,558][187912] Fps is (10 sec: 410.1, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 2109440. Throughput: 0: 359.0. Samples: 2109520. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 19:13:07,558][187912] Avg episode reward: [(0, '379.531')]
[36m[2025-06-29 19:13:12,581][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2109440. Throughput: 0: 359.9. Samples: 2110736. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 19:13:12,581][187912] Avg episode reward: [(0, '384.707')]
[36m[2025-06-29 19:13:18,115][187912] Fps is (10 sec: 388.0, 60 sec: 406.1, 300 sec: 374.2). Total num frames: 2113536. Throughput: 0: 364.0. Samples: 2113072. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 19:13:18,116][187912] Avg episode reward: [(0, '391.026')]
[36m[2025-06-29 19:13:22,582][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2113536. Throughput: 0: 360.9. Samples: 2115088. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 19:13:22,582][187912] Avg episode reward: [(0, '412.309')]
[36m[2025-06-29 19:13:27,587][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2113536. Throughput: 0: 358.9. Samples: 2116176. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 19:13:27,587][187912] Avg episode reward: [(0, '408.100')]
[31m[5965791 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[5965791 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[5965791 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:13:32,563][187912] Fps is (10 sec: 410.4, 60 sec: 344.4, 300 sec: 361.0). Total num frames: 2117632. Throughput: 0: 357.7. Samples: 2118160. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:13:32,563][187912] Avg episode reward: [(0, '410.853')]
[36m[2025-06-29 19:13:37,581][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2117632. Throughput: 0: 361.0. Samples: 2120592. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:13:37,581][187912] Avg episode reward: [(0, '402.343')]
[36m[2025-06-29 19:13:42,589][187912] Fps is (10 sec: 408.5, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 2121728. Throughput: 0: 363.9. Samples: 2121760. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 19:13:42,589][187912] Avg episode reward: [(0, '423.588')]
[36m[2025-06-29 19:13:47,564][187912] Fps is (10 sec: 410.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2121728. Throughput: 0: 365.6. Samples: 2123920. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 19:13:47,564][187912] Avg episode reward: [(0, '406.898')]
[33m[5984284 ms][navigation_task] - WARNING : Curriculum Level: 23, Curriculum progress fraction: 0.22857142857142856 (navigation_task.py:262)
[33m[5984284 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.77587890625
[33mCrash Rate: 0.1689453125
[33mTimeout Rate: 0.05517578125 (navigation_task.py:265)
[33m[5984284 ms][navigation_task] - WARNING : 
[33mSuccesses: 1589
[33mCrashes : 346
[33mTimeouts: 113 (navigation_task.py:268)
[36m[2025-06-29 19:13:52,585][187912] Fps is (10 sec: 409.8, 60 sec: 409.7, 300 sec: 361.4). Total num frames: 2125824. Throughput: 0: 362.8. Samples: 2125856. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:13:52,585][187912] Avg episode reward: [(0, '419.108')]
[36m[2025-06-29 19:13:57,562][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2125824. Throughput: 0: 358.9. Samples: 2126880. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:13:57,562][187912] Avg episode reward: [(0, '434.703')]
[36m[2025-06-29 19:14:02,583][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2125824. Throughput: 0: 363.1. Samples: 2129216. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:14:02,583][187912] Avg episode reward: [(0, '437.334')]
[36m[2025-06-29 19:14:07,575][187912] Fps is (10 sec: 409.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2129920. Throughput: 0: 360.6. Samples: 2131312. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 19:14:07,576][187912] Avg episode reward: [(0, '446.234')]
[36m[2025-06-29 19:14:12,562][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2129920. Throughput: 0: 360.0. Samples: 2132368. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 19:14:12,563][187912] Avg episode reward: [(0, '433.616')]
[37m[1m[2025-06-29 19:14:12,612][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000008320_2129920.pth...
[36m[2025-06-29 19:14:12,674][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000007984_2043904.pth
[36m[2025-06-29 19:14:17,563][187912] Fps is (10 sec: 410.1, 60 sec: 344.5, 300 sec: 361.0). Total num frames: 2134016. Throughput: 0: 360.5. Samples: 2134384. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 19:14:17,564][187912] Avg episode reward: [(0, '472.158')]
[36m[2025-06-29 19:14:22,579][187912] Fps is (10 sec: 408.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2134016. Throughput: 0: 357.7. Samples: 2136688. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 19:14:22,579][187912] Avg episode reward: [(0, '495.107')]
[36m[2025-06-29 19:14:27,582][187912] Fps is (10 sec: 408.9, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2138112. Throughput: 0: 355.3. Samples: 2137744. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 19:14:27,582][187912] Avg episode reward: [(0, '484.846')]
[36m[2025-06-29 19:14:32,560][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2138112. Throughput: 0: 353.1. Samples: 2139808. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 19:14:32,560][187912] Avg episode reward: [(0, '492.052')]
[36m[2025-06-29 19:14:37,565][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 2138112. Throughput: 0: 358.2. Samples: 2141968. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 19:14:37,565][187912] Avg episode reward: [(0, '528.998')]
[37m[1m[2025-06-29 19:14:37,617][187912] Saving new best policy, reward=528.998!
[36m[2025-06-29 19:14:42,578][187912] Fps is (10 sec: 408.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2142208. Throughput: 0: 353.6. Samples: 2142800. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:14:42,578][187912] Avg episode reward: [(0, '462.811')]
[36m[2025-06-29 19:14:47,559][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2142208. Throughput: 0: 354.0. Samples: 2145136. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:14:47,559][187912] Avg episode reward: [(0, '443.205')]
[36m[2025-06-29 19:14:52,560][187912] Fps is (10 sec: 410.3, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2146304. Throughput: 0: 350.7. Samples: 2147088. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 19:14:52,560][187912] Avg episode reward: [(0, '429.338')]
[36m[2025-06-29 19:14:57,563][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2146304. Throughput: 0: 352.0. Samples: 2148208. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 19:14:57,563][187912] Avg episode reward: [(0, '387.081')]
[36m[2025-06-29 19:15:02,576][187912] Fps is (10 sec: 408.9, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2150400. Throughput: 0: 356.2. Samples: 2150416. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:15:02,576][187912] Avg episode reward: [(0, '371.710')]
[36m[2025-06-29 19:15:07,575][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2150400. Throughput: 0: 353.5. Samples: 2152592. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:15:07,575][187912] Avg episode reward: [(0, '407.843')]
[36m[2025-06-29 19:15:12,715][187912] Fps is (10 sec: 404.0, 60 sec: 408.6, 300 sec: 360.8). Total num frames: 2154496. Throughput: 0: 355.9. Samples: 2153808. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 19:15:12,716][187912] Avg episode reward: [(0, '421.742')]
[36m[2025-06-29 19:15:17,565][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2154496. Throughput: 0: 355.5. Samples: 2155808. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 19:15:17,565][187912] Avg episode reward: [(0, '423.896')]
[31m[6073870 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6073871 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[6073871 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:15:22,566][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 2154496. Throughput: 0: 357.7. Samples: 2158064. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 19:15:22,566][187912] Avg episode reward: [(0, '436.043')]
[36m[2025-06-29 19:15:27,584][187912] Fps is (10 sec: 408.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2158592. Throughput: 0: 358.0. Samples: 2158912. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:15:27,584][187912] Avg episode reward: [(0, '469.208')]
[36m[2025-06-29 19:15:32,558][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2158592. Throughput: 0: 355.6. Samples: 2161136. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:15:32,559][187912] Avg episode reward: [(0, '464.741')]
[36m[2025-06-29 19:15:37,583][187912] Fps is (10 sec: 409.6, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 2162688. Throughput: 0: 355.0. Samples: 2163072. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 19:15:37,583][187912] Avg episode reward: [(0, '442.534')]
[36m[2025-06-29 19:15:42,582][187912] Fps is (10 sec: 408.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2162688. Throughput: 0: 355.4. Samples: 2164208. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 19:15:42,582][187912] Avg episode reward: [(0, '461.438')]
[36m[2025-06-29 19:15:47,795][187912] Fps is (10 sec: 401.1, 60 sec: 408.0, 300 sec: 360.7). Total num frames: 2166784. Throughput: 0: 355.6. Samples: 2166496. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 19:15:47,795][187912] Avg episode reward: [(0, '497.435')]
[36m[2025-06-29 19:15:52,568][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2166784. Throughput: 0: 353.8. Samples: 2168512. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 19:15:52,568][187912] Avg episode reward: [(0, '519.825')]
[36m[2025-06-29 19:15:57,560][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 2166784. Throughput: 0: 354.6. Samples: 2169712. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 19:15:57,561][187912] Avg episode reward: [(0, '478.973')]
[36m[2025-06-29 19:16:02,592][187912] Fps is (10 sec: 408.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2170880. Throughput: 0: 352.1. Samples: 2171664. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 19:16:02,593][187912] Avg episode reward: [(0, '500.429')]
[36m[2025-06-29 19:16:07,604][187912] Fps is (10 sec: 407.8, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 2170880. Throughput: 0: 352.1. Samples: 2173920. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 19:16:07,604][187912] Avg episode reward: [(0, '493.428')]
[36m[2025-06-29 19:16:12,580][187912] Fps is (10 sec: 410.1, 60 sec: 342.1, 300 sec: 361.0). Total num frames: 2174976. Throughput: 0: 357.7. Samples: 2175008. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:16:12,580][187912] Avg episode reward: [(0, '492.561')]
[37m[1m[2025-06-29 19:16:12,624][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000008496_2174976.pth...
[36m[2025-06-29 19:16:12,696][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000008160_2088960.pth
[36m[2025-06-29 19:16:17,581][187912] Fps is (10 sec: 410.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2174976. Throughput: 0: 353.6. Samples: 2177056. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:16:17,581][187912] Avg episode reward: [(0, '457.953')]
[36m[2025-06-29 19:16:22,589][187912] Fps is (10 sec: 409.2, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 2179072. Throughput: 0: 356.2. Samples: 2179104. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:16:22,589][187912] Avg episode reward: [(0, '520.795')]
[36m[2025-06-29 19:16:27,582][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2179072. Throughput: 0: 356.3. Samples: 2180240. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:16:27,582][187912] Avg episode reward: [(0, '501.268')]
[31m[6143648 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6143648 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[6143648 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[6145037 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6145037 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[6145037 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:16:33,305][187912] Fps is (10 sec: 382.2, 60 sec: 404.6, 300 sec: 360.1). Total num frames: 2183168. Throughput: 0: 354.4. Samples: 2182624. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:16:33,306][187912] Avg episode reward: [(0, '487.816')]
[36m[2025-06-29 19:16:37,559][187912] Fps is (10 sec: 410.6, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2183168. Throughput: 0: 359.2. Samples: 2184672. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:16:37,559][187912] Avg episode reward: [(0, '447.921')]
[36m[2025-06-29 19:16:42,586][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 2183168. Throughput: 0: 359.6. Samples: 2185904. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:16:42,586][187912] Avg episode reward: [(0, '447.174')]
[36m[2025-06-29 19:16:47,572][187912] Fps is (10 sec: 409.1, 60 sec: 342.6, 300 sec: 361.0). Total num frames: 2187264. Throughput: 0: 363.5. Samples: 2188016. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 19:16:47,572][187912] Avg episode reward: [(0, '446.353')]
[36m[2025-06-29 19:16:52,558][187912] Fps is (10 sec: 410.7, 60 sec: 341.4, 300 sec: 347.9). Total num frames: 2187264. Throughput: 0: 363.4. Samples: 2190256. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 19:16:52,558][187912] Avg episode reward: [(0, '449.084')]
[36m[2025-06-29 19:16:57,567][187912] Fps is (10 sec: 409.8, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2191360. Throughput: 0: 363.1. Samples: 2191344. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 19:16:57,567][187912] Avg episode reward: [(0, '469.332')]
[36m[2025-06-29 19:17:02,573][187912] Fps is (10 sec: 409.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2191360. Throughput: 0: 364.1. Samples: 2193440. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 19:17:02,574][187912] Avg episode reward: [(0, '501.565')]
[36m[2025-06-29 19:17:07,593][187912] Fps is (10 sec: 408.5, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 2195456. Throughput: 0: 364.1. Samples: 2195488. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:17:07,593][187912] Avg episode reward: [(0, '520.263')]
[36m[2025-06-29 19:17:12,564][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2195456. Throughput: 0: 362.5. Samples: 2196544. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:17:12,564][187912] Avg episode reward: [(0, '524.523')]
[36m[2025-06-29 19:17:17,597][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 2195456. Throughput: 0: 366.3. Samples: 2198848. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:17:17,597][187912] Avg episode reward: [(0, '490.878')]
[36m[2025-06-29 19:17:22,596][187912] Fps is (10 sec: 408.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2199552. Throughput: 0: 358.5. Samples: 2200816. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:17:22,596][187912] Avg episode reward: [(0, '512.726')]
[36m[2025-06-29 19:17:27,591][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 347.7). Total num frames: 2199552. Throughput: 0: 358.4. Samples: 2202032. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:17:27,591][187912] Avg episode reward: [(0, '488.193')]
[36m[2025-06-29 19:17:32,576][187912] Fps is (10 sec: 410.4, 60 sec: 345.5, 300 sec: 361.0). Total num frames: 2203648. Throughput: 0: 357.3. Samples: 2204096. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:17:32,576][187912] Avg episode reward: [(0, '449.986')]
[36m[2025-06-29 19:17:37,559][187912] Fps is (10 sec: 410.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2203648. Throughput: 0: 360.2. Samples: 2206464. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:17:37,560][187912] Avg episode reward: [(0, '443.891')]
[36m[2025-06-29 19:17:42,594][187912] Fps is (10 sec: 408.9, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 2207744. Throughput: 0: 359.6. Samples: 2207536. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:17:42,594][187912] Avg episode reward: [(0, '466.624')]
[36m[2025-06-29 19:17:47,593][187912] Fps is (10 sec: 408.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2207744. Throughput: 0: 359.0. Samples: 2209600. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:17:47,594][187912] Avg episode reward: [(0, '442.937')]
[36m[2025-06-29 19:17:52,742][187912] Fps is (10 sec: 403.6, 60 sec: 408.3, 300 sec: 360.8). Total num frames: 2211840. Throughput: 0: 338.1. Samples: 2210752. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:17:52,742][187912] Avg episode reward: [(0, '431.257')]
[36m[2025-06-29 19:17:57,576][187912] Fps is (10 sec: 410.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2211840. Throughput: 0: 362.2. Samples: 2212848. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:17:57,576][187912] Avg episode reward: [(0, '482.041')]
[36m[2025-06-29 19:18:02,575][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 2211840. Throughput: 0: 363.6. Samples: 2215200. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:18:02,576][187912] Avg episode reward: [(0, '486.033')]
[36m[2025-06-29 19:18:07,563][187912] Fps is (10 sec: 410.1, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2215936. Throughput: 0: 365.1. Samples: 2217232. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:18:07,563][187912] Avg episode reward: [(0, '490.525')]
[36m[2025-06-29 19:18:12,579][187912] Fps is (10 sec: 409.4, 60 sec: 341.2, 300 sec: 347.8). Total num frames: 2215936. Throughput: 0: 364.2. Samples: 2218416. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:18:12,579][187912] Avg episode reward: [(0, '511.542')]
[37m[1m[2025-06-29 19:18:12,621][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000008656_2215936.pth...
[36m[2025-06-29 19:18:12,682][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000008320_2129920.pth
[36m[2025-06-29 19:18:17,577][187912] Fps is (10 sec: 409.0, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 2220032. Throughput: 0: 363.7. Samples: 2220464. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 19:18:17,577][187912] Avg episode reward: [(0, '488.919')]
[36m[2025-06-29 19:18:22,590][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2220032. Throughput: 0: 360.3. Samples: 2222688. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 19:18:22,590][187912] Avg episode reward: [(0, '447.751')]
[36m[2025-06-29 19:18:27,562][187912] Fps is (10 sec: 410.2, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 2224128. Throughput: 0: 362.6. Samples: 2223840. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:18:27,563][187912] Avg episode reward: [(0, '440.199')]
[36m[2025-06-29 19:18:32,567][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2224128. Throughput: 0: 362.9. Samples: 2225920. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:18:32,567][187912] Avg episode reward: [(0, '426.396')]
[36m[2025-06-29 19:18:37,596][187912] Fps is (10 sec: 0.0, 60 sec: 341.1, 300 sec: 347.1). Total num frames: 2224128. Throughput: 0: 385.6. Samples: 2228048. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:18:37,596][187912] Avg episode reward: [(0, '432.823')]
[31m[6274720 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6274720 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[6274720 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:18:42,594][187912] Fps is (10 sec: 408.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2228224. Throughput: 0: 357.2. Samples: 2228928. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:18:42,594][187912] Avg episode reward: [(0, '458.715')]
[31m[6276501 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6276501 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[6276501 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:18:47,570][187912] Fps is (10 sec: 410.6, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 2228224. Throughput: 0: 357.7. Samples: 2231296. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:18:47,571][187912] Avg episode reward: [(0, '489.435')]
[36m[2025-06-29 19:18:52,562][187912] Fps is (10 sec: 410.9, 60 sec: 342.4, 300 sec: 361.0). Total num frames: 2232320. Throughput: 0: 358.0. Samples: 2233344. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:18:52,563][187912] Avg episode reward: [(0, '533.233')]
[37m[1m[2025-06-29 19:18:52,603][187912] Saving new best policy, reward=533.233!
[36m[2025-06-29 19:18:57,591][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2232320. Throughput: 0: 356.9. Samples: 2234480. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:18:57,591][187912] Avg episode reward: [(0, '558.808')]
[37m[1m[2025-06-29 19:18:57,639][187912] Saving new best policy, reward=558.808!
[36m[2025-06-29 19:19:02,590][187912] Fps is (10 sec: 408.5, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 2236416. Throughput: 0: 355.1. Samples: 2236448. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 19:19:02,590][187912] Avg episode reward: [(0, '516.990')]
[36m[2025-06-29 19:19:07,569][187912] Fps is (10 sec: 410.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2236416. Throughput: 0: 355.7. Samples: 2238688. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 19:19:07,569][187912] Avg episode reward: [(0, '504.913')]
[31m[6303147 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6303147 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[6303147 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:19:12,886][187912] Fps is (10 sec: 397.8, 60 sec: 407.5, 300 sec: 360.6). Total num frames: 2240512. Throughput: 0: 352.3. Samples: 2239808. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:19:12,887][187912] Avg episode reward: [(0, '503.907')]
[36m[2025-06-29 19:19:17,578][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2240512. Throughput: 0: 353.0. Samples: 2241808. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:19:17,579][187912] Avg episode reward: [(0, '468.513')]
[36m[2025-06-29 19:19:22,591][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 2240512. Throughput: 0: 356.7. Samples: 2244096. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:19:22,591][187912] Avg episode reward: [(0, '445.358')]
[36m[2025-06-29 19:19:27,588][187912] Fps is (10 sec: 409.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2244608. Throughput: 0: 357.4. Samples: 2245008. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:19:27,588][187912] Avg episode reward: [(0, '500.818')]
[36m[2025-06-29 19:19:32,563][187912] Fps is (10 sec: 410.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2244608. Throughput: 0: 354.9. Samples: 2247264. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:19:32,564][187912] Avg episode reward: [(0, '494.479')]
[36m[2025-06-29 19:19:37,601][187912] Fps is (10 sec: 409.0, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2248704. Throughput: 0: 352.8. Samples: 2249232. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 19:19:37,602][187912] Avg episode reward: [(0, '522.759')]
[36m[2025-06-29 19:19:42,563][187912] Fps is (10 sec: 409.6, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2248704. Throughput: 0: 353.6. Samples: 2250384. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 19:19:42,564][187912] Avg episode reward: [(0, '532.407')]
[31m[6336275 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6336275 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[6336275 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[6337006 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6337006 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[6337006 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:19:47,591][187912] Fps is (10 sec: 410.0, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 2252800. Throughput: 0: 359.1. Samples: 2252608. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 19:19:47,591][187912] Avg episode reward: [(0, '531.241')]
[36m[2025-06-29 19:19:52,562][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2252800. Throughput: 0: 352.1. Samples: 2254528. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 19:19:52,563][187912] Avg episode reward: [(0, '501.164')]
[31m[6349224 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6349224 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[6349224 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:19:57,560][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 2252800. Throughput: 0: 354.6. Samples: 2255648. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 19:19:57,561][187912] Avg episode reward: [(0, '491.541')]
[36m[2025-06-29 19:20:02,571][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2256896. Throughput: 0: 354.2. Samples: 2257744. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 19:20:02,571][187912] Avg episode reward: [(0, '476.720')]
[36m[2025-06-29 19:20:07,583][187912] Fps is (10 sec: 408.7, 60 sec: 341.3, 300 sec: 347.3). Total num frames: 2256896. Throughput: 0: 353.5. Samples: 2260000. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 19:20:07,583][187912] Avg episode reward: [(0, '452.602')]
[36m[2025-06-29 19:20:12,571][187912] Fps is (10 sec: 409.6, 60 sec: 343.1, 300 sec: 361.0). Total num frames: 2260992. Throughput: 0: 356.0. Samples: 2261024. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-06-29 19:20:12,572][187912] Avg episode reward: [(0, '488.339')]
[37m[1m[2025-06-29 19:20:12,618][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000008832_2260992.pth...
[36m[2025-06-29 19:20:12,690][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000008496_2174976.pth
[36m[2025-06-29 19:20:17,568][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2260992. Throughput: 0: 353.4. Samples: 2263168. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-06-29 19:20:17,568][187912] Avg episode reward: [(0, '458.502')]
[36m[2025-06-29 19:20:22,588][187912] Fps is (10 sec: 408.9, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2265088. Throughput: 0: 357.1. Samples: 2265296. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:20:22,589][187912] Avg episode reward: [(0, '484.533')]
[36m[2025-06-29 19:20:27,590][187912] Fps is (10 sec: 408.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2265088. Throughput: 0: 357.5. Samples: 2266480. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:20:27,590][187912] Avg episode reward: [(0, '456.235')]
[36m[2025-06-29 19:20:32,736][187912] Fps is (10 sec: 403.6, 60 sec: 408.4, 300 sec: 360.8). Total num frames: 2269184. Throughput: 0: 360.4. Samples: 2268880. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 19:20:32,737][187912] Avg episode reward: [(0, '470.087')]
[36m[2025-06-29 19:20:37,573][187912] Fps is (10 sec: 410.3, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2269184. Throughput: 0: 365.4. Samples: 2270976. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 19:20:37,574][187912] Avg episode reward: [(0, '453.536')]
[36m[2025-06-29 19:20:42,588][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.4). Total num frames: 2269184. Throughput: 0: 363.9. Samples: 2272032. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 19:20:42,589][187912] Avg episode reward: [(0, '475.985')]
[36m[2025-06-29 19:20:47,596][187912] Fps is (10 sec: 408.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2273280. Throughput: 0: 363.5. Samples: 2274112. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 19:20:47,597][187912] Avg episode reward: [(0, '490.465')]
[31m[6405319 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6405319 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[6405319 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:20:52,559][187912] Fps is (10 sec: 410.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2273280. Throughput: 0: 363.2. Samples: 2276336. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 19:20:52,559][187912] Avg episode reward: [(0, '524.282')]
[36m[2025-06-29 19:20:57,595][187912] Fps is (10 sec: 409.7, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 2277376. Throughput: 0: 363.9. Samples: 2277408. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:20:57,595][187912] Avg episode reward: [(0, '540.893')]
[36m[2025-06-29 19:21:02,586][187912] Fps is (10 sec: 408.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2277376. Throughput: 0: 359.0. Samples: 2279328. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:21:02,586][187912] Avg episode reward: [(0, '481.428')]
[36m[2025-06-29 19:21:07,585][187912] Fps is (10 sec: 410.0, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2281472. Throughput: 0: 336.7. Samples: 2280448. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 19:21:07,586][187912] Avg episode reward: [(0, '472.301')]
[36m[2025-06-29 19:21:12,569][187912] Fps is (10 sec: 410.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2281472. Throughput: 0: 357.9. Samples: 2282576. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 19:21:12,570][187912] Avg episode reward: [(0, '499.296')]
[36m[2025-06-29 19:21:17,559][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 2281472. Throughput: 0: 356.2. Samples: 2284848. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 19:21:17,560][187912] Avg episode reward: [(0, '483.878')]
[36m[2025-06-29 19:21:22,580][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2285568. Throughput: 0: 352.7. Samples: 2286848. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 19:21:22,580][187912] Avg episode reward: [(0, '466.390')]
[36m[2025-06-29 19:21:27,563][187912] Fps is (10 sec: 409.4, 60 sec: 341.5, 300 sec: 348.0). Total num frames: 2285568. Throughput: 0: 356.1. Samples: 2288048. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 19:21:27,563][187912] Avg episode reward: [(0, '496.990')]
[36m[2025-06-29 19:21:32,595][187912] Fps is (10 sec: 409.0, 60 sec: 342.1, 300 sec: 361.0). Total num frames: 2289664. Throughput: 0: 353.8. Samples: 2290032. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:21:32,596][187912] Avg episode reward: [(0, '531.628')]
[36m[2025-06-29 19:21:37,605][187912] Fps is (10 sec: 407.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2289664. Throughput: 0: 354.5. Samples: 2292304. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:21:37,605][187912] Avg episode reward: [(0, '519.059')]
[36m[2025-06-29 19:21:42,580][187912] Fps is (10 sec: 410.2, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 2293760. Throughput: 0: 356.4. Samples: 2293440. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:21:42,580][187912] Avg episode reward: [(0, '511.056')]
[36m[2025-06-29 19:21:47,580][187912] Fps is (10 sec: 410.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2293760. Throughput: 0: 357.7. Samples: 2295424. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:21:47,580][187912] Avg episode reward: [(0, '507.457')]
[36m[2025-06-29 19:21:52,580][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 2293760. Throughput: 0: 379.4. Samples: 2297520. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:21:52,581][187912] Avg episode reward: [(0, '499.164')]
[36m[2025-06-29 19:21:57,602][187912] Fps is (10 sec: 408.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2297856. Throughput: 0: 351.7. Samples: 2298416. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:21:57,603][187912] Avg episode reward: [(0, '504.739')]
[36m[2025-06-29 19:22:02,561][187912] Fps is (10 sec: 410.4, 60 sec: 341.5, 300 sec: 347.2). Total num frames: 2297856. Throughput: 0: 353.8. Samples: 2300768. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:22:02,561][187912] Avg episode reward: [(0, '511.271')]
[36m[2025-06-29 19:22:07,586][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2301952. Throughput: 0: 355.1. Samples: 2302832. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:22:07,587][187912] Avg episode reward: [(0, '493.679')]
[36m[2025-06-29 19:22:12,580][187912] Fps is (10 sec: 408.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2301952. Throughput: 0: 354.7. Samples: 2304016. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:22:12,580][187912] Avg episode reward: [(0, '502.752')]
[37m[1m[2025-06-29 19:22:12,631][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000008992_2301952.pth...
[36m[2025-06-29 19:22:12,703][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000008656_2215936.pth
[36m[2025-06-29 19:22:17,569][187912] Fps is (10 sec: 410.3, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 2306048. Throughput: 0: 356.8. Samples: 2306080. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 19:22:17,569][187912] Avg episode reward: [(0, '521.559')]
[36m[2025-06-29 19:22:22,597][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2306048. Throughput: 0: 353.5. Samples: 2308208. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 19:22:22,597][187912] Avg episode reward: [(0, '475.348')]
[36m[2025-06-29 19:22:28,219][187912] Fps is (10 sec: 384.6, 60 sec: 405.2, 300 sec: 360.2). Total num frames: 2310144. Throughput: 0: 347.4. Samples: 2309296. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 19:22:28,219][187912] Avg episode reward: [(0, '459.506')]
[36m[2025-06-29 19:22:32,579][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2310144. Throughput: 0: 352.4. Samples: 2311280. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 19:22:32,579][187912] Avg episode reward: [(0, '446.215')]
[33m[6506281 ms][navigation_task] - WARNING : Curriculum Level: 25, Curriculum progress fraction: 0.2857142857142857 (navigation_task.py:262)
[33m[6506281 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.767578125
[33mCrash Rate: 0.1826171875
[33mTimeout Rate: 0.0498046875 (navigation_task.py:265)
[33m[6506281 ms][navigation_task] - WARNING : 
[33mSuccesses: 1572
[33mCrashes : 374
[33mTimeouts: 102 (navigation_task.py:268)
[36m[2025-06-29 19:22:37,606][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 2310144. Throughput: 0: 356.8. Samples: 2313584. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 19:22:37,606][187912] Avg episode reward: [(0, '466.277')]
[36m[2025-06-29 19:22:42,576][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2314240. Throughput: 0: 355.8. Samples: 2314416. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:22:42,577][187912] Avg episode reward: [(0, '445.133')]
[36m[2025-06-29 19:22:47,586][187912] Fps is (10 sec: 410.4, 60 sec: 341.3, 300 sec: 347.3). Total num frames: 2314240. Throughput: 0: 355.4. Samples: 2316768. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:22:47,586][187912] Avg episode reward: [(0, '472.097')]
[36m[2025-06-29 19:22:52,590][187912] Fps is (10 sec: 409.0, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 2318336. Throughput: 0: 354.1. Samples: 2318768. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:22:52,590][187912] Avg episode reward: [(0, '486.186')]
[31m[6526056 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6526056 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[6526056 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:22:57,592][187912] Fps is (10 sec: 409.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2318336. Throughput: 0: 355.1. Samples: 2320000. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:22:57,592][187912] Avg episode reward: [(0, '456.861')]
[31m[6532392 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6532392 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[6532392 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:23:02,569][187912] Fps is (10 sec: 410.5, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 2322432. Throughput: 0: 361.2. Samples: 2322336. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:23:02,570][187912] Avg episode reward: [(0, '481.961')]
[36m[2025-06-29 19:23:07,595][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2322432. Throughput: 0: 359.1. Samples: 2324368. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:23:07,595][187912] Avg episode reward: [(0, '452.951')]
[36m[2025-06-29 19:23:12,561][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 2322432. Throughput: 0: 366.2. Samples: 2325536. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:23:12,561][187912] Avg episode reward: [(0, '460.652')]
[36m[2025-06-29 19:23:17,571][187912] Fps is (10 sec: 410.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2326528. Throughput: 0: 362.4. Samples: 2327584. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:23:17,571][187912] Avg episode reward: [(0, '474.523')]
[36m[2025-06-29 19:23:22,572][187912] Fps is (10 sec: 409.1, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 2326528. Throughput: 0: 360.4. Samples: 2329792. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:23:22,573][187912] Avg episode reward: [(0, '486.524')]
[36m[2025-06-29 19:23:27,574][187912] Fps is (10 sec: 409.5, 60 sec: 345.0, 300 sec: 361.0). Total num frames: 2330624. Throughput: 0: 360.9. Samples: 2330656. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:23:27,575][187912] Avg episode reward: [(0, '426.307')]
[36m[2025-06-29 19:23:32,587][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2330624. Throughput: 0: 362.3. Samples: 2333072. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:23:32,588][187912] Avg episode reward: [(0, '472.781')]
[31m[6566283 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6566284 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[6566284 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:23:37,595][187912] Fps is (10 sec: 408.8, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 2334720. Throughput: 0: 366.5. Samples: 2335264. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:23:37,595][187912] Avg episode reward: [(0, '488.311')]
[36m[2025-06-29 19:23:42,581][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2334720. Throughput: 0: 366.0. Samples: 2336464. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:23:42,582][187912] Avg episode reward: [(0, '486.405')]
[36m[2025-06-29 19:23:47,589][187912] Fps is (10 sec: 409.8, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2338816. Throughput: 0: 364.3. Samples: 2338736. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 19:23:47,589][187912] Avg episode reward: [(0, '495.071')]
[36m[2025-06-29 19:23:52,600][187912] Fps is (10 sec: 408.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2338816. Throughput: 0: 363.3. Samples: 2340720. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 19:23:52,600][187912] Avg episode reward: [(0, '555.336')]
[36m[2025-06-29 19:23:57,574][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 2338816. Throughput: 0: 360.4. Samples: 2341760. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 19:23:57,575][187912] Avg episode reward: [(0, '537.683')]
[36m[2025-06-29 19:24:02,596][187912] Fps is (10 sec: 409.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2342912. Throughput: 0: 358.9. Samples: 2343744. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 19:24:02,596][187912] Avg episode reward: [(0, '511.121')]
[36m[2025-06-29 19:24:07,570][187912] Fps is (10 sec: 409.8, 60 sec: 341.5, 300 sec: 347.5). Total num frames: 2342912. Throughput: 0: 360.5. Samples: 2346016. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 19:24:07,571][187912] Avg episode reward: [(0, '520.579')]
[36m[2025-06-29 19:24:12,566][187912] Fps is (10 sec: 410.8, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2347008. Throughput: 0: 364.2. Samples: 2347040. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:24:12,566][187912] Avg episode reward: [(0, '537.924')]
[37m[1m[2025-06-29 19:24:12,624][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000009168_2347008.pth...
[36m[2025-06-29 19:24:12,699][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000008832_2260992.pth
[36m[2025-06-29 19:24:17,578][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2347008. Throughput: 0: 357.1. Samples: 2349136. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:24:17,579][187912] Avg episode reward: [(0, '510.124')]
[36m[2025-06-29 19:24:22,574][187912] Fps is (10 sec: 409.3, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2351104. Throughput: 0: 353.2. Samples: 2351152. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:24:22,574][187912] Avg episode reward: [(0, '513.589')]
[36m[2025-06-29 19:24:27,574][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2351104. Throughput: 0: 354.2. Samples: 2352400. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:24:27,574][187912] Avg episode reward: [(0, '538.805')]
[36m[2025-06-29 19:24:32,582][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 2351104. Throughput: 0: 351.7. Samples: 2354560. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:24:32,582][187912] Avg episode reward: [(0, '492.862')]
[36m[2025-06-29 19:24:37,562][187912] Fps is (10 sec: 410.1, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2355200. Throughput: 0: 353.4. Samples: 2356608. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:24:37,562][187912] Avg episode reward: [(0, '511.766')]
[36m[2025-06-29 19:24:42,613][187912] Fps is (10 sec: 408.3, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 2355200. Throughput: 0: 354.9. Samples: 2357744. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:24:42,613][187912] Avg episode reward: [(0, '497.861')]
[36m[2025-06-29 19:24:47,584][187912] Fps is (10 sec: 408.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2359296. Throughput: 0: 354.9. Samples: 2359712. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 19:24:47,584][187912] Avg episode reward: [(0, '500.812')]
[31m[6645001 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6645001 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[6645002 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:24:52,576][187912] Fps is (10 sec: 411.1, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2359296. Throughput: 0: 354.4. Samples: 2361968. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 19:24:52,576][187912] Avg episode reward: [(0, '443.583')]
[36m[2025-06-29 19:24:57,572][187912] Fps is (10 sec: 410.1, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2363392. Throughput: 0: 356.9. Samples: 2363104. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:24:57,572][187912] Avg episode reward: [(0, '466.511')]
[36m[2025-06-29 19:25:02,574][187912] Fps is (10 sec: 409.7, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2363392. Throughput: 0: 353.1. Samples: 2365024. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:25:02,574][187912] Avg episode reward: [(0, '420.163')]
[36m[2025-06-29 19:25:07,562][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 2363392. Throughput: 0: 357.8. Samples: 2367248. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:25:07,563][187912] Avg episode reward: [(0, '444.779')]
[31m[6664119 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6664119 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[6664120 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:25:12,589][187912] Fps is (10 sec: 409.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2367488. Throughput: 0: 350.8. Samples: 2368192. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 19:25:12,589][187912] Avg episode reward: [(0, '466.970')]
[36m[2025-06-29 19:25:17,590][187912] Fps is (10 sec: 408.5, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 2367488. Throughput: 0: 355.5. Samples: 2370560. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 19:25:17,590][187912] Avg episode reward: [(0, '524.745')]
[36m[2025-06-29 19:25:22,584][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2371584. Throughput: 0: 356.4. Samples: 2372656. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:25:22,584][187912] Avg episode reward: [(0, '532.200')]
[36m[2025-06-29 19:25:27,611][187912] Fps is (10 sec: 408.7, 60 sec: 341.1, 300 sec: 347.3). Total num frames: 2371584. Throughput: 0: 357.7. Samples: 2373840. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:25:27,611][187912] Avg episode reward: [(0, '579.775')]
[37m[1m[2025-06-29 19:25:27,651][187912] Saving new best policy, reward=579.775!
[36m[2025-06-29 19:25:32,579][187912] Fps is (10 sec: 409.8, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2375680. Throughput: 0: 355.9. Samples: 2375728. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 19:25:32,579][187912] Avg episode reward: [(0, '572.688')]
[36m[2025-06-29 19:25:37,581][187912] Fps is (10 sec: 410.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2375680. Throughput: 0: 358.4. Samples: 2378096. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 19:25:37,581][187912] Avg episode reward: [(0, '559.249')]
[31m[6691041 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6691041 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[6691041 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:25:42,571][187912] Fps is (10 sec: 409.9, 60 sec: 409.9, 300 sec: 361.0). Total num frames: 2379776. Throughput: 0: 359.5. Samples: 2379280. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:25:42,572][187912] Avg episode reward: [(0, '558.036')]
[36m[2025-06-29 19:25:47,599][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2379776. Throughput: 0: 361.8. Samples: 2381312. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:25:47,599][187912] Avg episode reward: [(0, '556.539')]
[36m[2025-06-29 19:25:52,579][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 2379776. Throughput: 0: 364.0. Samples: 2383632. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:25:52,579][187912] Avg episode reward: [(0, '558.905')]
[36m[2025-06-29 19:25:57,579][187912] Fps is (10 sec: 410.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2383872. Throughput: 0: 364.2. Samples: 2384576. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 19:25:57,580][187912] Avg episode reward: [(0, '522.530')]
[36m[2025-06-29 19:26:02,559][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 2383872. Throughput: 0: 365.8. Samples: 2387008. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 19:26:02,560][187912] Avg episode reward: [(0, '552.760')]
[36m[2025-06-29 19:26:07,582][187912] Fps is (10 sec: 409.5, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 2387968. Throughput: 0: 364.8. Samples: 2389072. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:26:07,583][187912] Avg episode reward: [(0, '545.693')]
[36m[2025-06-29 19:26:12,566][187912] Fps is (10 sec: 409.3, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2387968. Throughput: 0: 363.7. Samples: 2390192. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:26:12,566][187912] Avg episode reward: [(0, '557.040')]
[37m[1m[2025-06-29 19:26:12,608][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000009328_2387968.pth...
[36m[2025-06-29 19:26:12,678][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000008992_2301952.pth
[36m[2025-06-29 19:26:17,571][187912] Fps is (10 sec: 410.0, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 2392064. Throughput: 0: 364.1. Samples: 2392112. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 19:26:17,572][187912] Avg episode reward: [(0, '489.077')]
[36m[2025-06-29 19:26:22,596][187912] Fps is (10 sec: 408.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2392064. Throughput: 0: 363.6. Samples: 2394464. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 19:26:22,596][187912] Avg episode reward: [(0, '481.007')]
[36m[2025-06-29 19:26:27,566][187912] Fps is (10 sec: 409.8, 60 sec: 409.9, 300 sec: 361.0). Total num frames: 2396160. Throughput: 0: 362.0. Samples: 2395568. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 19:26:27,567][187912] Avg episode reward: [(0, '428.516')]
[36m[2025-06-29 19:26:32,593][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2396160. Throughput: 0: 364.1. Samples: 2397696. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 19:26:32,594][187912] Avg episode reward: [(0, '405.598')]
[36m[2025-06-29 19:26:37,566][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 2396160. Throughput: 0: 361.3. Samples: 2399888. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 19:26:37,566][187912] Avg episode reward: [(0, '431.505')]
[36m[2025-06-29 19:26:42,559][187912] Fps is (10 sec: 411.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2400256. Throughput: 0: 360.0. Samples: 2400768. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 19:26:42,559][187912] Avg episode reward: [(0, '440.186')]
[36m[2025-06-29 19:26:47,563][187912] Fps is (10 sec: 409.7, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2400256. Throughput: 0: 354.8. Samples: 2402976. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 19:26:47,563][187912] Avg episode reward: [(0, '457.072')]
[36m[2025-06-29 19:26:52,601][187912] Fps is (10 sec: 407.9, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 2404352. Throughput: 0: 353.3. Samples: 2404976. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:26:52,601][187912] Avg episode reward: [(0, '499.693')]
[36m[2025-06-29 19:26:57,562][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2404352. Throughput: 0: 353.1. Samples: 2406080. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:26:57,562][187912] Avg episode reward: [(0, '544.264')]
[36m[2025-06-29 19:27:02,579][187912] Fps is (10 sec: 410.5, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 2408448. Throughput: 0: 362.3. Samples: 2408416. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 19:27:02,579][187912] Avg episode reward: [(0, '501.297')]
[36m[2025-06-29 19:27:07,561][187912] Fps is (10 sec: 409.6, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2408448. Throughput: 0: 355.1. Samples: 2410432. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 19:27:07,561][187912] Avg episode reward: [(0, '512.026')]
[36m[2025-06-29 19:27:12,583][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 2408448. Throughput: 0: 355.8. Samples: 2411584. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 19:27:12,583][187912] Avg episode reward: [(0, '547.011')]
[36m[2025-06-29 19:27:17,597][187912] Fps is (10 sec: 408.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2412544. Throughput: 0: 355.5. Samples: 2413696. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:27:17,597][187912] Avg episode reward: [(0, '501.223')]
[36m[2025-06-29 19:27:22,580][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 347.9). Total num frames: 2412544. Throughput: 0: 358.3. Samples: 2416016. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:27:22,581][187912] Avg episode reward: [(0, '481.806')]
[36m[2025-06-29 19:27:27,586][187912] Fps is (10 sec: 410.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2416640. Throughput: 0: 358.5. Samples: 2416912. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 19:27:27,587][187912] Avg episode reward: [(0, '464.560')]
[36m[2025-06-29 19:27:32,579][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2416640. Throughput: 0: 361.8. Samples: 2419264. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 19:27:32,579][187912] Avg episode reward: [(0, '475.898')]
[36m[2025-06-29 19:27:37,571][187912] Fps is (10 sec: 410.2, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2420736. Throughput: 0: 363.6. Samples: 2421328. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 19:27:37,572][187912] Avg episode reward: [(0, '494.277')]
[36m[2025-06-29 19:27:42,569][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2420736. Throughput: 0: 364.7. Samples: 2422496. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 19:27:42,569][187912] Avg episode reward: [(0, '520.522')]
[36m[2025-06-29 19:27:47,559][187912] Fps is (10 sec: 410.1, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2424832. Throughput: 0: 363.2. Samples: 2424752. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 19:27:47,559][187912] Avg episode reward: [(0, '522.734')]
[36m[2025-06-29 19:27:52,593][187912] Fps is (10 sec: 408.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2424832. Throughput: 0: 363.5. Samples: 2426800. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 19:27:52,593][187912] Avg episode reward: [(0, '526.889')]
[36m[2025-06-29 19:27:57,596][187912] Fps is (10 sec: 0.0, 60 sec: 341.1, 300 sec: 347.1). Total num frames: 2424832. Throughput: 0: 362.9. Samples: 2427920. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 19:27:57,596][187912] Avg episode reward: [(0, '531.310')]
[36m[2025-06-29 19:28:02,558][187912] Fps is (10 sec: 411.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2428928. Throughput: 0: 360.8. Samples: 2429920. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:28:02,558][187912] Avg episode reward: [(0, '492.458')]
[36m[2025-06-29 19:28:07,568][187912] Fps is (10 sec: 410.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2428928. Throughput: 0: 362.1. Samples: 2432304. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:28:07,568][187912] Avg episode reward: [(0, '476.829')]
[36m[2025-06-29 19:28:12,595][187912] Fps is (10 sec: 408.1, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 2433024. Throughput: 0: 363.3. Samples: 2433264. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 19:28:12,595][187912] Avg episode reward: [(0, '514.582')]
[37m[1m[2025-06-29 19:28:12,637][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000009504_2433024.pth...
[36m[2025-06-29 19:28:12,711][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000009168_2347008.pth
[36m[2025-06-29 19:28:17,562][187912] Fps is (10 sec: 409.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2433024. Throughput: 0: 361.0. Samples: 2435504. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 19:28:17,562][187912] Avg episode reward: [(0, '523.090')]
[36m[2025-06-29 19:28:22,574][187912] Fps is (10 sec: 410.5, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2437120. Throughput: 0: 359.1. Samples: 2437488. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:28:22,574][187912] Avg episode reward: [(0, '520.972')]
[36m[2025-06-29 19:28:27,563][187912] Fps is (10 sec: 409.6, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2437120. Throughput: 0: 359.9. Samples: 2438688. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:28:27,563][187912] Avg episode reward: [(0, '508.753')]
[36m[2025-06-29 19:28:32,904][187912] Fps is (10 sec: 396.5, 60 sec: 407.4, 300 sec: 360.6). Total num frames: 2441216. Throughput: 0: 354.3. Samples: 2440816. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:28:32,905][187912] Avg episode reward: [(0, '498.445')]
[36m[2025-06-29 19:28:37,569][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2441216. Throughput: 0: 358.6. Samples: 2442928. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:28:37,569][187912] Avg episode reward: [(0, '507.340')]
[36m[2025-06-29 19:28:42,589][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 2441216. Throughput: 0: 360.6. Samples: 2444144. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:28:42,590][187912] Avg episode reward: [(0, '535.221')]
[36m[2025-06-29 19:28:47,609][187912] Fps is (10 sec: 408.0, 60 sec: 341.0, 300 sec: 361.0). Total num frames: 2445312. Throughput: 0: 359.8. Samples: 2446128. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-06-29 19:28:47,609][187912] Avg episode reward: [(0, '504.445')]
[36m[2025-06-29 19:28:52,561][187912] Fps is (10 sec: 410.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2445312. Throughput: 0: 355.6. Samples: 2448304. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-06-29 19:28:52,561][187912] Avg episode reward: [(0, '529.934')]
[36m[2025-06-29 19:28:57,559][187912] Fps is (10 sec: 411.6, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 2449408. Throughput: 0: 359.7. Samples: 2449440. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:28:57,560][187912] Avg episode reward: [(0, '576.399')]
[36m[2025-06-29 19:29:02,573][187912] Fps is (10 sec: 409.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2449408. Throughput: 0: 357.2. Samples: 2451584. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:29:02,573][187912] Avg episode reward: [(0, '567.795')]
[36m[2025-06-29 19:29:07,576][187912] Fps is (10 sec: 408.9, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 2453504. Throughput: 0: 360.9. Samples: 2453728. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:29:07,577][187912] Avg episode reward: [(0, '505.720')]
[36m[2025-06-29 19:29:12,590][187912] Fps is (10 sec: 408.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2453504. Throughput: 0: 359.6. Samples: 2454880. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:29:12,590][187912] Avg episode reward: [(0, '558.801')]
[36m[2025-06-29 19:29:17,945][187912] Fps is (10 sec: 395.0, 60 sec: 407.0, 300 sec: 360.5). Total num frames: 2457600. Throughput: 0: 363.4. Samples: 2457184. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:29:17,946][187912] Avg episode reward: [(0, '531.727')]
[36m[2025-06-29 19:29:22,581][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2457600. Throughput: 0: 362.2. Samples: 2459232. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:29:22,582][187912] Avg episode reward: [(0, '477.539')]
[36m[2025-06-29 19:29:27,574][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2457600. Throughput: 0: 360.7. Samples: 2460368. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:29:27,574][187912] Avg episode reward: [(0, '506.227')]
[36m[2025-06-29 19:29:32,578][187912] Fps is (10 sec: 409.7, 60 sec: 343.2, 300 sec: 361.0). Total num frames: 2461696. Throughput: 0: 365.1. Samples: 2462544. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:29:32,578][187912] Avg episode reward: [(0, '544.716')]
[36m[2025-06-29 19:29:37,583][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2461696. Throughput: 0: 369.2. Samples: 2464928. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:29:37,583][187912] Avg episode reward: [(0, '542.115')]
[36m[2025-06-29 19:29:42,578][187912] Fps is (10 sec: 409.6, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 2465792. Throughput: 0: 363.9. Samples: 2465824. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 19:29:42,578][187912] Avg episode reward: [(0, '557.237')]
[36m[2025-06-29 19:29:47,568][187912] Fps is (10 sec: 410.2, 60 sec: 341.6, 300 sec: 361.0). Total num frames: 2465792. Throughput: 0: 369.8. Samples: 2468224. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 19:29:47,569][187912] Avg episode reward: [(0, '567.323')]
[31m[6943056 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[6943056 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[6943057 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:29:52,595][187912] Fps is (10 sec: 408.9, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 2469888. Throughput: 0: 367.1. Samples: 2470256. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:29:52,595][187912] Avg episode reward: [(0, '560.673')]
[36m[2025-06-29 19:29:57,579][187912] Fps is (10 sec: 409.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2469888. Throughput: 0: 367.7. Samples: 2471424. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:29:57,579][187912] Avg episode reward: [(0, '573.888')]
[36m[2025-06-29 19:30:02,605][187912] Fps is (10 sec: 409.2, 60 sec: 409.4, 300 sec: 374.8). Total num frames: 2473984. Throughput: 0: 370.1. Samples: 2473712. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 19:30:02,605][187912] Avg episode reward: [(0, '603.870')]
[37m[1m[2025-06-29 19:30:02,649][187912] Saving new best policy, reward=603.870!
[36m[2025-06-29 19:30:07,594][187912] Fps is (10 sec: 409.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2473984. Throughput: 0: 365.0. Samples: 2475664. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 19:30:07,595][187912] Avg episode reward: [(0, '584.381')]
[36m[2025-06-29 19:30:12,562][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2473984. Throughput: 0: 365.6. Samples: 2476816. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 19:30:12,562][187912] Avg episode reward: [(0, '571.770')]
[37m[1m[2025-06-29 19:30:12,627][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000009664_2473984.pth...
[36m[2025-06-29 19:30:12,700][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000009328_2387968.pth
[36m[2025-06-29 19:30:17,595][187912] Fps is (10 sec: 409.6, 60 sec: 343.3, 300 sec: 361.0). Total num frames: 2478080. Throughput: 0: 359.3. Samples: 2478720. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 19:30:17,595][187912] Avg episode reward: [(0, '573.225')]
[36m[2025-06-29 19:30:22,582][187912] Fps is (10 sec: 408.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2478080. Throughput: 0: 355.2. Samples: 2480912. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 19:30:22,582][187912] Avg episode reward: [(0, '564.555')]
[36m[2025-06-29 19:30:27,602][187912] Fps is (10 sec: 409.3, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 2482176. Throughput: 0: 360.7. Samples: 2482064. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:30:27,603][187912] Avg episode reward: [(0, '550.698')]
[36m[2025-06-29 19:30:32,571][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2482176. Throughput: 0: 354.5. Samples: 2484176. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:30:32,571][187912] Avg episode reward: [(0, '518.517')]
[36m[2025-06-29 19:30:37,580][187912] Fps is (10 sec: 410.5, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2486272. Throughput: 0: 356.7. Samples: 2486304. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 19:30:37,580][187912] Avg episode reward: [(0, '533.926')]
[36m[2025-06-29 19:30:42,594][187912] Fps is (10 sec: 408.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2486272. Throughput: 0: 354.7. Samples: 2487392. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 19:30:42,594][187912] Avg episode reward: [(0, '529.099')]
[36m[2025-06-29 19:30:47,581][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2486272. Throughput: 0: 351.1. Samples: 2489504. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 19:30:47,581][187912] Avg episode reward: [(0, '496.852')]
[36m[2025-06-29 19:30:52,564][187912] Fps is (10 sec: 410.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2490368. Throughput: 0: 352.6. Samples: 2491520. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:30:52,564][187912] Avg episode reward: [(0, '485.949')]
[33m[7009950 ms][navigation_task] - WARNING : Curriculum Level: 27, Curriculum progress fraction: 0.34285714285714286 (navigation_task.py:262)
[33m[7009950 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.7509765625
[33mCrash Rate: 0.21044921875
[33mTimeout Rate: 0.03857421875 (navigation_task.py:265)
[33m[7009950 ms][navigation_task] - WARNING : 
[33mSuccesses: 1538
[33mCrashes : 431
[33mTimeouts: 79 (navigation_task.py:268)
[36m[2025-06-29 19:30:57,558][187912] Fps is (10 sec: 410.6, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2490368. Throughput: 0: 351.7. Samples: 2492640. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:30:57,558][187912] Avg episode reward: [(0, '501.990')]
[36m[2025-06-29 19:31:02,596][187912] Fps is (10 sec: 408.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2494464. Throughput: 0: 356.3. Samples: 2494752. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 19:31:02,596][187912] Avg episode reward: [(0, '497.422')]
[36m[2025-06-29 19:31:07,558][187912] Fps is (10 sec: 409.6, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2494464. Throughput: 0: 361.1. Samples: 2497152. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 19:31:07,558][187912] Avg episode reward: [(0, '533.005')]
[36m[2025-06-29 19:31:12,579][187912] Fps is (10 sec: 410.3, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 2498560. Throughput: 0: 359.3. Samples: 2498224. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 19:31:12,579][187912] Avg episode reward: [(0, '539.648')]
[36m[2025-06-29 19:31:17,588][187912] Fps is (10 sec: 408.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2498560. Throughput: 0: 359.7. Samples: 2500368. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 19:31:17,588][187912] Avg episode reward: [(0, '555.230')]
[36m[2025-06-29 19:31:22,687][187912] Fps is (10 sec: 405.2, 60 sec: 408.9, 300 sec: 360.9). Total num frames: 2502656. Throughput: 0: 337.0. Samples: 2501504. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:31:22,687][187912] Avg episode reward: [(0, '581.586')]
[36m[2025-06-29 19:31:27,562][187912] Fps is (10 sec: 410.7, 60 sec: 341.6, 300 sec: 361.0). Total num frames: 2502656. Throughput: 0: 361.1. Samples: 2503632. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:31:27,562][187912] Avg episode reward: [(0, '585.038')]
[36m[2025-06-29 19:31:32,595][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2502656. Throughput: 0: 366.1. Samples: 2505984. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:31:32,595][187912] Avg episode reward: [(0, '609.196')]
[37m[1m[2025-06-29 19:31:32,599][187912] Saving new best policy, reward=609.196!
[36m[2025-06-29 19:31:37,604][187912] Fps is (10 sec: 407.9, 60 sec: 341.2, 300 sec: 360.9). Total num frames: 2506752. Throughput: 0: 366.3. Samples: 2508016. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 19:31:37,604][187912] Avg episode reward: [(0, '638.526')]
[37m[1m[2025-06-29 19:31:37,663][187912] Saving new best policy, reward=638.526!
[36m[2025-06-29 19:31:42,596][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2506752. Throughput: 0: 365.2. Samples: 2509088. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 19:31:42,596][187912] Avg episode reward: [(0, '625.473')]
[36m[2025-06-29 19:31:47,575][187912] Fps is (10 sec: 410.8, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2510848. Throughput: 0: 361.1. Samples: 2510992. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:31:47,575][187912] Avg episode reward: [(0, '643.599')]
[37m[1m[2025-06-29 19:31:47,613][187912] Saving new best policy, reward=643.599!
[36m[2025-06-29 19:31:52,581][187912] Fps is (10 sec: 410.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2510848. Throughput: 0: 354.3. Samples: 2513104. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:31:52,581][187912] Avg episode reward: [(0, '651.954')]
[37m[1m[2025-06-29 19:31:52,624][187912] Saving new best policy, reward=651.954!
[36m[2025-06-29 19:31:57,793][187912] Fps is (10 sec: 400.9, 60 sec: 408.0, 300 sec: 360.7). Total num frames: 2514944. Throughput: 0: 353.9. Samples: 2514224. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 19:31:57,794][187912] Avg episode reward: [(0, '642.850')]
[36m[2025-06-29 19:32:02,605][187912] Fps is (10 sec: 408.6, 60 sec: 341.3, 300 sec: 360.9). Total num frames: 2514944. Throughput: 0: 353.3. Samples: 2516272. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 19:32:02,605][187912] Avg episode reward: [(0, '628.213')]
[36m[2025-06-29 19:32:07,563][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2514944. Throughput: 0: 381.1. Samples: 2518608. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 19:32:07,563][187912] Avg episode reward: [(0, '600.738')]
[36m[2025-06-29 19:32:12,569][187912] Fps is (10 sec: 411.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2519040. Throughput: 0: 351.9. Samples: 2519472. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:32:12,569][187912] Avg episode reward: [(0, '599.682')]
[37m[1m[2025-06-29 19:32:12,610][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000009840_2519040.pth...
[36m[2025-06-29 19:32:12,676][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000009504_2433024.pth
[36m[2025-06-29 19:32:17,579][187912] Fps is (10 sec: 408.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2519040. Throughput: 0: 350.0. Samples: 2521728. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:32:17,580][187912] Avg episode reward: [(0, '578.949')]
[36m[2025-06-29 19:32:22,561][187912] Fps is (10 sec: 409.9, 60 sec: 342.1, 300 sec: 361.0). Total num frames: 2523136. Throughput: 0: 351.6. Samples: 2523824. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:32:22,561][187912] Avg episode reward: [(0, '573.642')]
[36m[2025-06-29 19:32:27,589][187912] Fps is (10 sec: 409.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2523136. Throughput: 0: 353.8. Samples: 2525008. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:32:27,589][187912] Avg episode reward: [(0, '552.463')]
[36m[2025-06-29 19:32:32,586][187912] Fps is (10 sec: 408.6, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 2527232. Throughput: 0: 361.5. Samples: 2527264. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:32:32,587][187912] Avg episode reward: [(0, '557.905')]
[36m[2025-06-29 19:32:37,559][187912] Fps is (10 sec: 410.8, 60 sec: 341.6, 300 sec: 361.0). Total num frames: 2527232. Throughput: 0: 363.9. Samples: 2529472. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:32:37,559][187912] Avg episode reward: [(0, '524.754')]
[36m[2025-06-29 19:32:42,898][187912] Fps is (10 sec: 397.2, 60 sec: 407.5, 300 sec: 360.6). Total num frames: 2531328. Throughput: 0: 362.9. Samples: 2530592. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:32:42,898][187912] Avg episode reward: [(0, '504.927')]
[36m[2025-06-29 19:32:47,579][187912] Fps is (10 sec: 408.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2531328. Throughput: 0: 363.9. Samples: 2532640. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:32:47,579][187912] Avg episode reward: [(0, '524.934')]
[36m[2025-06-29 19:32:52,609][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2531328. Throughput: 0: 363.4. Samples: 2534976. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:32:52,609][187912] Avg episode reward: [(0, '541.067')]
[36m[2025-06-29 19:32:57,566][187912] Fps is (10 sec: 410.1, 60 sec: 342.6, 300 sec: 361.0). Total num frames: 2535424. Throughput: 0: 363.4. Samples: 2535824. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:32:57,566][187912] Avg episode reward: [(0, '525.361')]
[36m[2025-06-29 19:33:02,568][187912] Fps is (10 sec: 411.3, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2535424. Throughput: 0: 363.1. Samples: 2538064. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:33:02,569][187912] Avg episode reward: [(0, '566.202')]
[36m[2025-06-29 19:33:07,558][187912] Fps is (10 sec: 409.9, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2539520. Throughput: 0: 362.0. Samples: 2540112. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:33:07,558][187912] Avg episode reward: [(0, '571.563')]
[36m[2025-06-29 19:33:12,591][187912] Fps is (10 sec: 408.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2539520. Throughput: 0: 361.6. Samples: 2541280. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:33:12,591][187912] Avg episode reward: [(0, '562.195')]
[36m[2025-06-29 19:33:17,584][187912] Fps is (10 sec: 408.6, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2543616. Throughput: 0: 362.0. Samples: 2543552. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 19:33:17,584][187912] Avg episode reward: [(0, '550.042')]
[36m[2025-06-29 19:33:22,560][187912] Fps is (10 sec: 410.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2543616. Throughput: 0: 361.9. Samples: 2545760. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 19:33:22,560][187912] Avg episode reward: [(0, '593.743')]
[36m[2025-06-29 19:33:27,584][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.5). Total num frames: 2543616. Throughput: 0: 363.8. Samples: 2546848. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 19:33:27,585][187912] Avg episode reward: [(0, '580.732')]
[36m[2025-06-29 19:33:32,614][187912] Fps is (10 sec: 407.4, 60 sec: 341.2, 300 sec: 360.9). Total num frames: 2547712. Throughput: 0: 357.4. Samples: 2548736. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:33:32,614][187912] Avg episode reward: [(0, '585.882')]
[36m[2025-06-29 19:33:37,591][187912] Fps is (10 sec: 409.3, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 2547712. Throughput: 0: 355.0. Samples: 2550944. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:33:37,591][187912] Avg episode reward: [(0, '620.430')]
[36m[2025-06-29 19:33:42,567][187912] Fps is (10 sec: 411.5, 60 sec: 343.2, 300 sec: 361.1). Total num frames: 2551808. Throughput: 0: 355.9. Samples: 2551840. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:33:42,567][187912] Avg episode reward: [(0, '612.334')]
[36m[2025-06-29 19:33:47,559][187912] Fps is (10 sec: 410.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2551808. Throughput: 0: 356.7. Samples: 2554112. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:33:47,559][187912] Avg episode reward: [(0, '603.595')]
[36m[2025-06-29 19:33:52,610][187912] Fps is (10 sec: 407.9, 60 sec: 409.6, 300 sec: 360.9). Total num frames: 2555904. Throughput: 0: 357.3. Samples: 2556208. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:33:52,610][187912] Avg episode reward: [(0, '614.047')]
[36m[2025-06-29 19:33:57,591][187912] Fps is (10 sec: 408.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2555904. Throughput: 0: 356.6. Samples: 2557328. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:33:57,592][187912] Avg episode reward: [(0, '577.187')]
[36m[2025-06-29 19:34:03,190][187912] Fps is (10 sec: 387.1, 60 sec: 405.4, 300 sec: 360.3). Total num frames: 2560000. Throughput: 0: 351.2. Samples: 2559568. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:34:03,191][187912] Avg episode reward: [(0, '546.525')]
[36m[2025-06-29 19:34:07,577][187912] Fps is (10 sec: 410.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2560000. Throughput: 0: 352.2. Samples: 2561616. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:34:07,577][187912] Avg episode reward: [(0, '546.871')]
[36m[2025-06-29 19:34:12,582][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.5). Total num frames: 2560000. Throughput: 0: 354.9. Samples: 2562816. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:34:12,582][187912] Avg episode reward: [(0, '553.930')]
[37m[1m[2025-06-29 19:34:12,624][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000010000_2560000.pth...
[36m[2025-06-29 19:34:12,688][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000009664_2473984.pth
[36m[2025-06-29 19:34:17,562][187912] Fps is (10 sec: 410.2, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2564096. Throughput: 0: 354.9. Samples: 2564688. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:34:17,562][187912] Avg episode reward: [(0, '529.996')]
[36m[2025-06-29 19:34:22,568][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2564096. Throughput: 0: 356.1. Samples: 2566960. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:34:22,568][187912] Avg episode reward: [(0, '548.583')]
[36m[2025-06-29 19:34:27,580][187912] Fps is (10 sec: 408.9, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2568192. Throughput: 0: 361.9. Samples: 2568128. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 19:34:27,580][187912] Avg episode reward: [(0, '572.342')]
[36m[2025-06-29 19:34:32,618][187912] Fps is (10 sec: 407.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2568192. Throughput: 0: 354.4. Samples: 2570080. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 19:34:32,618][187912] Avg episode reward: [(0, '578.418')]
[36m[2025-06-29 19:34:37,758][187912] Fps is (10 sec: 402.4, 60 sec: 408.5, 300 sec: 360.8). Total num frames: 2572288. Throughput: 0: 331.0. Samples: 2571152. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:34:37,758][187912] Avg episode reward: [(0, '567.127')]
[36m[2025-06-29 19:34:42,581][187912] Fps is (10 sec: 411.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2572288. Throughput: 0: 354.9. Samples: 2573296. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:34:42,581][187912] Avg episode reward: [(0, '597.538')]
[36m[2025-06-29 19:34:47,567][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.2). Total num frames: 2572288. Throughput: 0: 360.9. Samples: 2575584. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:34:47,568][187912] Avg episode reward: [(0, '619.939')]
[36m[2025-06-29 19:34:52,567][187912] Fps is (10 sec: 410.2, 60 sec: 341.6, 300 sec: 361.0). Total num frames: 2576384. Throughput: 0: 354.2. Samples: 2577552. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:34:52,567][187912] Avg episode reward: [(0, '629.611')]
[36m[2025-06-29 19:34:57,567][187912] Fps is (10 sec: 409.6, 60 sec: 341.5, 300 sec: 347.2). Total num frames: 2576384. Throughput: 0: 355.0. Samples: 2578784. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:34:57,567][187912] Avg episode reward: [(0, '638.440')]
[36m[2025-06-29 19:35:02,594][187912] Fps is (10 sec: 408.5, 60 sec: 344.8, 300 sec: 361.0). Total num frames: 2580480. Throughput: 0: 357.1. Samples: 2580768. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:35:02,594][187912] Avg episode reward: [(0, '659.479')]
[37m[1m[2025-06-29 19:35:02,643][187912] Saving new best policy, reward=659.479!
[36m[2025-06-29 19:35:07,596][187912] Fps is (10 sec: 408.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2580480. Throughput: 0: 360.3. Samples: 2583184. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:35:07,597][187912] Avg episode reward: [(0, '658.669')]
[36m[2025-06-29 19:35:12,572][187912] Fps is (10 sec: 410.5, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 2584576. Throughput: 0: 361.3. Samples: 2584384. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 19:35:12,573][187912] Avg episode reward: [(0, '602.873')]
[36m[2025-06-29 19:35:17,573][187912] Fps is (10 sec: 410.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2584576. Throughput: 0: 364.1. Samples: 2586448. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 19:35:17,573][187912] Avg episode reward: [(0, '616.538')]
[36m[2025-06-29 19:35:22,873][187912] Fps is (10 sec: 397.6, 60 sec: 407.5, 300 sec: 360.7). Total num frames: 2588672. Throughput: 0: 388.7. Samples: 2588688. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:35:22,873][187912] Avg episode reward: [(0, '592.108')]
[36m[2025-06-29 19:35:27,592][187912] Fps is (10 sec: 408.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2588672. Throughput: 0: 362.2. Samples: 2589600. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:35:27,593][187912] Avg episode reward: [(0, '563.792')]
[36m[2025-06-29 19:35:32,582][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 2588672. Throughput: 0: 362.9. Samples: 2591920. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:35:32,582][187912] Avg episode reward: [(0, '570.205')]
[36m[2025-06-29 19:35:37,579][187912] Fps is (10 sec: 410.1, 60 sec: 342.4, 300 sec: 361.0). Total num frames: 2592768. Throughput: 0: 366.8. Samples: 2594064. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 19:35:37,579][187912] Avg episode reward: [(0, '577.757')]
[31m[7294018 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7294018 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[7294018 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:35:42,579][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2592768. Throughput: 0: 365.1. Samples: 2595216. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 19:35:42,580][187912] Avg episode reward: [(0, '537.078')]
[36m[2025-06-29 19:35:47,569][187912] Fps is (10 sec: 410.0, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2596864. Throughput: 0: 364.6. Samples: 2597168. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:35:47,569][187912] Avg episode reward: [(0, '536.291')]
[36m[2025-06-29 19:35:52,606][187912] Fps is (10 sec: 408.5, 60 sec: 341.1, 300 sec: 360.9). Total num frames: 2596864. Throughput: 0: 364.4. Samples: 2599584. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:35:52,606][187912] Avg episode reward: [(0, '540.581')]
[36m[2025-06-29 19:35:57,577][187912] Fps is (10 sec: 409.3, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 2600960. Throughput: 0: 363.3. Samples: 2600736. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 19:35:57,578][187912] Avg episode reward: [(0, '559.677')]
[36m[2025-06-29 19:36:02,591][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2600960. Throughput: 0: 364.3. Samples: 2602848. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 19:36:02,591][187912] Avg episode reward: [(0, '570.405')]
[36m[2025-06-29 19:36:07,749][187912] Fps is (10 sec: 402.7, 60 sec: 408.6, 300 sec: 360.8). Total num frames: 2605056. Throughput: 0: 340.1. Samples: 2603952. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:36:07,750][187912] Avg episode reward: [(0, '574.111')]
[36m[2025-06-29 19:36:12,588][187912] Fps is (10 sec: 409.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2605056. Throughput: 0: 365.2. Samples: 2606032. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:36:12,589][187912] Avg episode reward: [(0, '593.856')]
[37m[1m[2025-06-29 19:36:12,631][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000010176_2605056.pth...
[36m[2025-06-29 19:36:12,702][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000009840_2519040.pth
[36m[2025-06-29 19:36:17,585][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.2). Total num frames: 2605056. Throughput: 0: 363.0. Samples: 2608256. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:36:17,585][187912] Avg episode reward: [(0, '577.155')]
[36m[2025-06-29 19:36:22,587][187912] Fps is (10 sec: 409.7, 60 sec: 343.0, 300 sec: 361.0). Total num frames: 2609152. Throughput: 0: 359.8. Samples: 2610256. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:36:22,587][187912] Avg episode reward: [(0, '558.880')]
[36m[2025-06-29 19:36:27,594][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2609152. Throughput: 0: 361.1. Samples: 2611472. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:36:27,594][187912] Avg episode reward: [(0, '569.599')]
[36m[2025-06-29 19:36:32,559][187912] Fps is (10 sec: 410.7, 60 sec: 409.8, 300 sec: 361.1). Total num frames: 2613248. Throughput: 0: 362.0. Samples: 2613456. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 19:36:32,560][187912] Avg episode reward: [(0, '578.539')]
[36m[2025-06-29 19:36:37,596][187912] Fps is (10 sec: 409.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2613248. Throughput: 0: 358.8. Samples: 2615728. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 19:36:37,596][187912] Avg episode reward: [(0, '523.849')]
[36m[2025-06-29 19:36:42,561][187912] Fps is (10 sec: 409.5, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 2617344. Throughput: 0: 357.1. Samples: 2616800. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 19:36:42,562][187912] Avg episode reward: [(0, '578.910')]
[36m[2025-06-29 19:36:47,568][187912] Fps is (10 sec: 410.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2617344. Throughput: 0: 358.2. Samples: 2618960. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 19:36:47,568][187912] Avg episode reward: [(0, '574.655')]
[36m[2025-06-29 19:36:53,355][187912] Fps is (10 sec: 379.5, 60 sec: 404.6, 300 sec: 360.3). Total num frames: 2621440. Throughput: 0: 380.0. Samples: 2621280. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:36:53,355][187912] Avg episode reward: [(0, '592.085')]
[36m[2025-06-29 19:36:57,594][187912] Fps is (10 sec: 408.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2621440. Throughput: 0: 358.7. Samples: 2622176. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:36:57,594][187912] Avg episode reward: [(0, '603.205')]
[36m[2025-06-29 19:37:02,564][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2621440. Throughput: 0: 359.6. Samples: 2624432. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:37:02,564][187912] Avg episode reward: [(0, '613.247')]
[36m[2025-06-29 19:37:07,558][187912] Fps is (10 sec: 411.1, 60 sec: 342.4, 300 sec: 361.0). Total num frames: 2625536. Throughput: 0: 362.9. Samples: 2626576. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:37:07,558][187912] Avg episode reward: [(0, '587.840')]
[36m[2025-06-29 19:37:12,569][187912] Fps is (10 sec: 409.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2625536. Throughput: 0: 360.4. Samples: 2627680. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:37:12,570][187912] Avg episode reward: [(0, '598.228')]
[36m[2025-06-29 19:37:17,572][187912] Fps is (10 sec: 409.0, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 2629632. Throughput: 0: 363.3. Samples: 2629808. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:37:17,572][187912] Avg episode reward: [(0, '582.085')]
[36m[2025-06-29 19:37:22,591][187912] Fps is (10 sec: 408.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2629632. Throughput: 0: 363.8. Samples: 2632096. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:37:22,591][187912] Avg episode reward: [(0, '592.500')]
[36m[2025-06-29 19:37:27,603][187912] Fps is (10 sec: 408.3, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 2633728. Throughput: 0: 364.5. Samples: 2633216. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 19:37:27,603][187912] Avg episode reward: [(0, '588.678')]
[36m[2025-06-29 19:37:32,564][187912] Fps is (10 sec: 410.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2633728. Throughput: 0: 361.3. Samples: 2635216. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 19:37:32,564][187912] Avg episode reward: [(0, '575.077')]
[36m[2025-06-29 19:37:37,584][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.5). Total num frames: 2633728. Throughput: 0: 366.8. Samples: 2637504. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 19:37:37,585][187912] Avg episode reward: [(0, '585.338')]
[36m[2025-06-29 19:37:42,573][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2637824. Throughput: 0: 360.0. Samples: 2638368. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:37:42,573][187912] Avg episode reward: [(0, '590.159')]
[36m[2025-06-29 19:37:47,586][187912] Fps is (10 sec: 409.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2637824. Throughput: 0: 362.5. Samples: 2640752. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:37:47,587][187912] Avg episode reward: [(0, '600.668')]
[36m[2025-06-29 19:37:52,577][187912] Fps is (10 sec: 409.4, 60 sec: 345.8, 300 sec: 361.0). Total num frames: 2641920. Throughput: 0: 359.7. Samples: 2642768. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 19:37:52,578][187912] Avg episode reward: [(0, '624.067')]
[36m[2025-06-29 19:37:57,592][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2641920. Throughput: 0: 359.3. Samples: 2643856. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 19:37:57,593][187912] Avg episode reward: [(0, '644.383')]
[36m[2025-06-29 19:38:02,607][187912] Fps is (10 sec: 408.4, 60 sec: 409.3, 300 sec: 360.9). Total num frames: 2646016. Throughput: 0: 360.6. Samples: 2646048. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:38:02,608][187912] Avg episode reward: [(0, '599.425')]
[36m[2025-06-29 19:38:07,563][187912] Fps is (10 sec: 410.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2646016. Throughput: 0: 355.1. Samples: 2648064. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:38:07,563][187912] Avg episode reward: [(0, '603.483')]
[36m[2025-06-29 19:38:13,028][187912] Fps is (10 sec: 393.1, 60 sec: 406.5, 300 sec: 360.5). Total num frames: 2650112. Throughput: 0: 354.3. Samples: 2649312. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:38:13,028][187912] Avg episode reward: [(0, '615.409')]
[37m[1m[2025-06-29 19:38:13,072][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000010352_2650112.pth...
[36m[2025-06-29 19:38:13,139][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000010000_2560000.pth
[36m[2025-06-29 19:38:17,594][187912] Fps is (10 sec: 408.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2650112. Throughput: 0: 359.2. Samples: 2651392. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:38:17,594][187912] Avg episode reward: [(0, '606.759')]
[31m[7452753 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7452753 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[7452753 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:38:22,589][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2650112. Throughput: 0: 360.1. Samples: 2653712. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:38:22,589][187912] Avg episode reward: [(0, '579.116')]
[36m[2025-06-29 19:38:27,595][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2654208. Throughput: 0: 360.0. Samples: 2654576. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 19:38:27,595][187912] Avg episode reward: [(0, '577.648')]
[36m[2025-06-29 19:38:32,579][187912] Fps is (10 sec: 410.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2654208. Throughput: 0: 359.5. Samples: 2656928. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 19:38:32,579][187912] Avg episode reward: [(0, '575.376')]
[36m[2025-06-29 19:38:37,562][187912] Fps is (10 sec: 411.0, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 2658304. Throughput: 0: 359.6. Samples: 2658944. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 19:38:37,562][187912] Avg episode reward: [(0, '572.432')]
[36m[2025-06-29 19:38:42,605][187912] Fps is (10 sec: 408.5, 60 sec: 341.1, 300 sec: 360.9). Total num frames: 2658304. Throughput: 0: 361.5. Samples: 2660128. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 19:38:42,606][187912] Avg episode reward: [(0, '589.713')]
[36m[2025-06-29 19:38:47,593][187912] Fps is (10 sec: 408.3, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2662400. Throughput: 0: 362.8. Samples: 2662368. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:38:47,593][187912] Avg episode reward: [(0, '577.216')]
[36m[2025-06-29 19:38:52,565][187912] Fps is (10 sec: 411.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2662400. Throughput: 0: 363.4. Samples: 2664416. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:38:52,565][187912] Avg episode reward: [(0, '576.014')]
[36m[2025-06-29 19:38:57,615][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.8). Total num frames: 2662400. Throughput: 0: 363.5. Samples: 2665520. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:38:57,615][187912] Avg episode reward: [(0, '583.849')]
[36m[2025-06-29 19:39:02,573][187912] Fps is (10 sec: 409.3, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2666496. Throughput: 0: 359.3. Samples: 2667552. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:39:02,573][187912] Avg episode reward: [(0, '573.703')]
[36m[2025-06-29 19:39:07,588][187912] Fps is (10 sec: 410.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2666496. Throughput: 0: 358.1. Samples: 2669824. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:39:07,588][187912] Avg episode reward: [(0, '558.252')]
[36m[2025-06-29 19:39:12,574][187912] Fps is (10 sec: 409.5, 60 sec: 343.9, 300 sec: 361.0). Total num frames: 2670592. Throughput: 0: 360.0. Samples: 2670768. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:39:12,574][187912] Avg episode reward: [(0, '563.368')]
[36m[2025-06-29 19:39:17,586][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2670592. Throughput: 0: 360.8. Samples: 2673168. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:39:17,587][187912] Avg episode reward: [(0, '574.160')]
[36m[2025-06-29 19:39:22,572][187912] Fps is (10 sec: 409.7, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 2674688. Throughput: 0: 362.9. Samples: 2675280. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 19:39:22,572][187912] Avg episode reward: [(0, '575.392')]
[36m[2025-06-29 19:39:27,584][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2674688. Throughput: 0: 361.8. Samples: 2676400. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 19:39:27,584][187912] Avg episode reward: [(0, '585.890')]
[36m[2025-06-29 19:39:32,579][187912] Fps is (10 sec: 409.3, 60 sec: 409.6, 300 sec: 361.2). Total num frames: 2678784. Throughput: 0: 363.5. Samples: 2678720. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:39:32,579][187912] Avg episode reward: [(0, '562.310')]
[33m[7530059 ms][navigation_task] - WARNING : Curriculum Level: 29, Curriculum progress fraction: 0.4 (navigation_task.py:262)
[33m[7530059 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.78271484375
[33mCrash Rate: 0.166015625
[33mTimeout Rate: 0.05126953125 (navigation_task.py:265)
[33m[7530059 ms][navigation_task] - WARNING : 
[33mSuccesses: 1603
[33mCrashes : 340
[33mTimeouts: 105 (navigation_task.py:268)
[36m[2025-06-29 19:39:37,566][187912] Fps is (10 sec: 410.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2678784. Throughput: 0: 359.8. Samples: 2680608. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:39:37,566][187912] Avg episode reward: [(0, '570.113')]
[36m[2025-06-29 19:39:42,590][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2678784. Throughput: 0: 359.7. Samples: 2681696. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:39:42,591][187912] Avg episode reward: [(0, '565.505')]
[36m[2025-06-29 19:39:47,594][187912] Fps is (10 sec: 408.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2682880. Throughput: 0: 358.2. Samples: 2683680. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:39:47,594][187912] Avg episode reward: [(0, '549.320')]
[36m[2025-06-29 19:39:52,585][187912] Fps is (10 sec: 409.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2682880. Throughput: 0: 359.5. Samples: 2686000. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:39:52,585][187912] Avg episode reward: [(0, '569.846')]
[36m[2025-06-29 19:39:57,564][187912] Fps is (10 sec: 410.8, 60 sec: 409.9, 300 sec: 361.0). Total num frames: 2686976. Throughput: 0: 361.0. Samples: 2687008. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 19:39:57,565][187912] Avg episode reward: [(0, '594.454')]
[36m[2025-06-29 19:40:02,593][187912] Fps is (10 sec: 409.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2686976. Throughput: 0: 358.0. Samples: 2689280. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 19:40:02,594][187912] Avg episode reward: [(0, '636.866')]
[36m[2025-06-29 19:40:07,590][187912] Fps is (10 sec: 408.5, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2691072. Throughput: 0: 359.3. Samples: 2691456. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:40:07,590][187912] Avg episode reward: [(0, '639.988')]
[36m[2025-06-29 19:40:12,582][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2691072. Throughput: 0: 362.7. Samples: 2692720. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:40:12,582][187912] Avg episode reward: [(0, '679.417')]
[37m[1m[2025-06-29 19:40:12,623][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000010512_2691072.pth...
[36m[2025-06-29 19:40:12,691][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000010176_2605056.pth
[37m[1m[2025-06-29 19:40:12,697][187912] Saving new best policy, reward=679.417!
[36m[2025-06-29 19:40:17,589][187912] Fps is (10 sec: 409.7, 60 sec: 409.6, 300 sec: 361.4). Total num frames: 2695168. Throughput: 0: 363.7. Samples: 2695088. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 19:40:17,589][187912] Avg episode reward: [(0, '650.253')]
[36m[2025-06-29 19:40:22,570][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2695168. Throughput: 0: 368.3. Samples: 2697184. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 19:40:22,571][187912] Avg episode reward: [(0, '693.720')]
[37m[1m[2025-06-29 19:40:22,612][187912] Saving new best policy, reward=693.720!
[36m[2025-06-29 19:40:27,561][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2695168. Throughput: 0: 368.6. Samples: 2698272. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 19:40:27,562][187912] Avg episode reward: [(0, '658.964')]
[36m[2025-06-29 19:40:32,581][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2699264. Throughput: 0: 370.2. Samples: 2700336. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:40:32,581][187912] Avg episode reward: [(0, '644.898')]
[36m[2025-06-29 19:40:37,559][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2699264. Throughput: 0: 369.3. Samples: 2702608. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:40:37,560][187912] Avg episode reward: [(0, '632.147')]
[36m[2025-06-29 19:40:42,595][187912] Fps is (10 sec: 409.0, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2703360. Throughput: 0: 366.0. Samples: 2703488. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:40:42,596][187912] Avg episode reward: [(0, '656.045')]
[36m[2025-06-29 19:40:47,594][187912] Fps is (10 sec: 408.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2703360. Throughput: 0: 369.8. Samples: 2705920. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:40:47,594][187912] Avg episode reward: [(0, '591.128')]
[36m[2025-06-29 19:40:52,571][187912] Fps is (10 sec: 410.6, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 2707456. Throughput: 0: 368.2. Samples: 2708016. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:40:52,571][187912] Avg episode reward: [(0, '643.713')]
[36m[2025-06-29 19:40:57,576][187912] Fps is (10 sec: 410.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2707456. Throughput: 0: 365.6. Samples: 2709168. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:40:57,577][187912] Avg episode reward: [(0, '635.782')]
[36m[2025-06-29 19:41:02,569][187912] Fps is (10 sec: 409.7, 60 sec: 409.8, 300 sec: 361.2). Total num frames: 2711552. Throughput: 0: 366.0. Samples: 2711552. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:41:02,569][187912] Avg episode reward: [(0, '627.612')]
[36m[2025-06-29 19:41:07,568][187912] Fps is (10 sec: 409.9, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2711552. Throughput: 0: 363.0. Samples: 2713520. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:41:07,568][187912] Avg episode reward: [(0, '589.957')]
[36m[2025-06-29 19:41:12,577][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2711552. Throughput: 0: 364.0. Samples: 2714656. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:41:12,577][187912] Avg episode reward: [(0, '598.540')]
[36m[2025-06-29 19:41:17,576][187912] Fps is (10 sec: 409.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2715648. Throughput: 0: 364.5. Samples: 2716736. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 19:41:17,576][187912] Avg episode reward: [(0, '565.000')]
[36m[2025-06-29 19:41:22,585][187912] Fps is (10 sec: 409.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2715648. Throughput: 0: 367.8. Samples: 2719168. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 19:41:22,585][187912] Avg episode reward: [(0, '584.650')]
[36m[2025-06-29 19:41:27,592][187912] Fps is (10 sec: 408.9, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 2719744. Throughput: 0: 367.7. Samples: 2720032. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:41:27,593][187912] Avg episode reward: [(0, '557.851')]
[36m[2025-06-29 19:41:32,562][187912] Fps is (10 sec: 410.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2719744. Throughput: 0: 365.8. Samples: 2722368. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:41:32,562][187912] Avg episode reward: [(0, '633.032')]
[36m[2025-06-29 19:41:37,572][187912] Fps is (10 sec: 410.4, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 2723840. Throughput: 0: 360.5. Samples: 2724240. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:41:37,573][187912] Avg episode reward: [(0, '632.250')]
[36m[2025-06-29 19:41:42,598][187912] Fps is (10 sec: 408.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2723840. Throughput: 0: 361.8. Samples: 2725456. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:41:42,599][187912] Avg episode reward: [(0, '629.433')]
[36m[2025-06-29 19:41:47,580][187912] Fps is (10 sec: 409.3, 60 sec: 409.7, 300 sec: 362.0). Total num frames: 2727936. Throughput: 0: 360.1. Samples: 2727760. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 19:41:47,580][187912] Avg episode reward: [(0, '601.940')]
[36m[2025-06-29 19:41:52,583][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2727936. Throughput: 0: 361.5. Samples: 2729792. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 19:41:52,584][187912] Avg episode reward: [(0, '627.596')]
[36m[2025-06-29 19:41:57,584][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2727936. Throughput: 0: 361.2. Samples: 2730912. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 19:41:57,584][187912] Avg episode reward: [(0, '550.270')]
[36m[2025-06-29 19:42:02,594][187912] Fps is (10 sec: 409.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2732032. Throughput: 0: 359.7. Samples: 2732928. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 19:42:02,594][187912] Avg episode reward: [(0, '544.608')]
[36m[2025-06-29 19:42:07,595][187912] Fps is (10 sec: 409.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2732032. Throughput: 0: 354.8. Samples: 2735136. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 19:42:07,595][187912] Avg episode reward: [(0, '552.880')]
[36m[2025-06-29 19:42:12,596][187912] Fps is (10 sec: 409.5, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 2736128. Throughput: 0: 358.4. Samples: 2736160. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:42:12,596][187912] Avg episode reward: [(0, '526.993')]
[37m[1m[2025-06-29 19:42:12,639][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000010688_2736128.pth...
[36m[2025-06-29 19:42:12,696][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000010352_2650112.pth
[36m[2025-06-29 19:42:17,581][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2736128. Throughput: 0: 353.3. Samples: 2738272. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:42:17,582][187912] Avg episode reward: [(0, '484.310')]
[36m[2025-06-29 19:42:22,588][187912] Fps is (10 sec: 409.9, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2740224. Throughput: 0: 358.6. Samples: 2740384. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:42:22,588][187912] Avg episode reward: [(0, '512.377')]
[36m[2025-06-29 19:42:27,595][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2740224. Throughput: 0: 356.3. Samples: 2741488. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:42:27,596][187912] Avg episode reward: [(0, '585.719')]
[36m[2025-06-29 19:42:33,301][187912] Fps is (10 sec: 382.3, 60 sec: 404.6, 300 sec: 374.0). Total num frames: 2744320. Throughput: 0: 349.9. Samples: 2743760. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:42:33,302][187912] Avg episode reward: [(0, '592.390')]
[31m[7708515 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7708515 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[7708515 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:42:37,563][187912] Fps is (10 sec: 410.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2744320. Throughput: 0: 357.1. Samples: 2745856. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:42:37,563][187912] Avg episode reward: [(0, '625.421')]
[36m[2025-06-29 19:42:42,601][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2744320. Throughput: 0: 358.6. Samples: 2747056. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:42:42,601][187912] Avg episode reward: [(0, '662.364')]
[36m[2025-06-29 19:42:47,577][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2748416. Throughput: 0: 360.3. Samples: 2749136. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:42:47,578][187912] Avg episode reward: [(0, '655.876')]
[36m[2025-06-29 19:42:52,579][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2748416. Throughput: 0: 362.1. Samples: 2751424. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:42:52,579][187912] Avg episode reward: [(0, '589.315')]
[36m[2025-06-29 19:42:57,594][187912] Fps is (10 sec: 408.9, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 2752512. Throughput: 0: 364.1. Samples: 2752544. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 19:42:57,595][187912] Avg episode reward: [(0, '576.647')]
[36m[2025-06-29 19:43:02,593][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2752512. Throughput: 0: 362.6. Samples: 2754592. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 19:43:02,593][187912] Avg episode reward: [(0, '560.376')]
[36m[2025-06-29 19:43:07,560][187912] Fps is (10 sec: 411.0, 60 sec: 409.8, 300 sec: 361.6). Total num frames: 2756608. Throughput: 0: 361.5. Samples: 2756640. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 19:43:07,560][187912] Avg episode reward: [(0, '540.069')]
[36m[2025-06-29 19:43:12,566][187912] Fps is (10 sec: 410.7, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2756608. Throughput: 0: 362.9. Samples: 2757808. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 19:43:12,566][187912] Avg episode reward: [(0, '574.972')]
[36m[2025-06-29 19:43:17,570][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2756608. Throughput: 0: 369.0. Samples: 2760096. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 19:43:17,570][187912] Avg episode reward: [(0, '567.996')]
[36m[2025-06-29 19:43:22,560][187912] Fps is (10 sec: 409.9, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2760704. Throughput: 0: 362.0. Samples: 2762144. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:43:22,560][187912] Avg episode reward: [(0, '578.156')]
[36m[2025-06-29 19:43:27,578][187912] Fps is (10 sec: 409.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2760704. Throughput: 0: 359.7. Samples: 2763232. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:43:27,578][187912] Avg episode reward: [(0, '590.737')]
[36m[2025-06-29 19:43:32,575][187912] Fps is (10 sec: 409.0, 60 sec: 345.5, 300 sec: 361.0). Total num frames: 2764800. Throughput: 0: 355.6. Samples: 2765136. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:43:32,575][187912] Avg episode reward: [(0, '604.445')]
[36m[2025-06-29 19:43:37,576][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2764800. Throughput: 0: 354.5. Samples: 2767376. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:43:37,576][187912] Avg episode reward: [(0, '571.682')]
[36m[2025-06-29 19:43:42,558][187912] Fps is (10 sec: 410.3, 60 sec: 409.9, 300 sec: 361.0). Total num frames: 2768896. Throughput: 0: 353.7. Samples: 2768448. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 19:43:42,558][187912] Avg episode reward: [(0, '599.504')]
[31m[7777631 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7777632 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[7777632 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:43:47,568][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2768896. Throughput: 0: 354.3. Samples: 2770528. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 19:43:47,568][187912] Avg episode reward: [(0, '632.511')]
[36m[2025-06-29 19:43:52,582][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2768896. Throughput: 0: 357.9. Samples: 2772752. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 19:43:52,582][187912] Avg episode reward: [(0, '627.772')]
[36m[2025-06-29 19:43:57,571][187912] Fps is (10 sec: 409.5, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2772992. Throughput: 0: 349.1. Samples: 2773520. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:43:57,571][187912] Avg episode reward: [(0, '570.802')]
[36m[2025-06-29 19:44:02,585][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2772992. Throughput: 0: 346.9. Samples: 2775712. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:44:02,585][187912] Avg episode reward: [(0, '635.555')]
[36m[2025-06-29 19:44:07,586][187912] Fps is (10 sec: 409.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2777088. Throughput: 0: 346.8. Samples: 2777760. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:44:07,586][187912] Avg episode reward: [(0, '652.921')]
[36m[2025-06-29 19:44:12,567][187912] Fps is (10 sec: 410.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2777088. Throughput: 0: 348.5. Samples: 2778912. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:44:12,567][187912] Avg episode reward: [(0, '661.215')]
[37m[1m[2025-06-29 19:44:12,616][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000010848_2777088.pth...
[36m[2025-06-29 19:44:12,679][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000010512_2691072.pth
[36m[2025-06-29 19:44:17,562][187912] Fps is (10 sec: 410.6, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 2781184. Throughput: 0: 354.9. Samples: 2781104. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:44:17,562][187912] Avg episode reward: [(0, '683.135')]
[36m[2025-06-29 19:44:22,584][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2781184. Throughput: 0: 351.2. Samples: 2783184. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:44:22,584][187912] Avg episode reward: [(0, '675.404')]
[36m[2025-06-29 19:44:27,587][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 2781184. Throughput: 0: 351.8. Samples: 2784288. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:44:27,588][187912] Avg episode reward: [(0, '675.593')]
[36m[2025-06-29 19:44:32,596][187912] Fps is (10 sec: 409.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2785280. Throughput: 0: 352.5. Samples: 2786400. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 19:44:32,596][187912] Avg episode reward: [(0, '651.718')]
[36m[2025-06-29 19:44:37,562][187912] Fps is (10 sec: 410.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2785280. Throughput: 0: 353.9. Samples: 2788672. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 19:44:37,562][187912] Avg episode reward: [(0, '636.207')]
[36m[2025-06-29 19:44:42,559][187912] Fps is (10 sec: 411.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2789376. Throughput: 0: 355.3. Samples: 2789504. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 19:44:42,559][187912] Avg episode reward: [(0, '625.275')]
[36m[2025-06-29 19:44:47,558][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2789376. Throughput: 0: 356.8. Samples: 2791760. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 19:44:47,558][187912] Avg episode reward: [(0, '629.835')]
[36m[2025-06-29 19:44:52,558][187912] Fps is (10 sec: 409.7, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 2793472. Throughput: 0: 355.1. Samples: 2793728. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 19:44:52,558][187912] Avg episode reward: [(0, '601.364')]
[36m[2025-06-29 19:44:57,572][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2793472. Throughput: 0: 355.2. Samples: 2794896. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 19:44:57,572][187912] Avg episode reward: [(0, '602.135')]
[36m[2025-06-29 19:45:02,595][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 2793472. Throughput: 0: 354.2. Samples: 2797056. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 19:45:02,595][187912] Avg episode reward: [(0, '599.681')]
[36m[2025-06-29 19:45:07,593][187912] Fps is (10 sec: 408.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2797568. Throughput: 0: 354.4. Samples: 2799136. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:45:07,593][187912] Avg episode reward: [(0, '622.645')]
[36m[2025-06-29 19:45:12,591][187912] Fps is (10 sec: 409.8, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 2797568. Throughput: 0: 354.1. Samples: 2800224. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:45:12,591][187912] Avg episode reward: [(0, '626.630')]
[36m[2025-06-29 19:45:17,586][187912] Fps is (10 sec: 409.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2801664. Throughput: 0: 353.5. Samples: 2802304. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:45:17,586][187912] Avg episode reward: [(0, '640.709')]
[36m[2025-06-29 19:45:22,578][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2801664. Throughput: 0: 357.9. Samples: 2804784. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:45:22,578][187912] Avg episode reward: [(0, '645.855')]
[31m[7878972 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7878972 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[7878972 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:45:27,609][187912] Fps is (10 sec: 408.7, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 2805760. Throughput: 0: 361.6. Samples: 2805792. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:45:27,609][187912] Avg episode reward: [(0, '643.902')]
[36m[2025-06-29 19:45:32,588][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2805760. Throughput: 0: 356.7. Samples: 2807824. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:45:32,589][187912] Avg episode reward: [(0, '576.188')]
[36m[2025-06-29 19:45:37,565][187912] Fps is (10 sec: 411.4, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2809856. Throughput: 0: 359.1. Samples: 2809888. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:45:37,565][187912] Avg episode reward: [(0, '577.498')]
[36m[2025-06-29 19:45:42,565][187912] Fps is (10 sec: 410.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2809856. Throughput: 0: 359.5. Samples: 2811072. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:45:42,566][187912] Avg episode reward: [(0, '536.543')]
[31m[7896955 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7896955 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[7896956 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:45:48,213][187912] Fps is (10 sec: 384.7, 60 sec: 405.2, 300 sec: 360.2). Total num frames: 2813952. Throughput: 0: 358.8. Samples: 2813424. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 19:45:48,213][187912] Avg episode reward: [(0, '523.929')]
[36m[2025-06-29 19:45:52,558][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2813952. Throughput: 0: 364.0. Samples: 2815504. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 19:45:52,558][187912] Avg episode reward: [(0, '539.724')]
[36m[2025-06-29 19:45:57,571][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 2813952. Throughput: 0: 363.9. Samples: 2816592. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 19:45:57,571][187912] Avg episode reward: [(0, '581.356')]
[36m[2025-06-29 19:46:02,593][187912] Fps is (10 sec: 408.1, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2818048. Throughput: 0: 365.1. Samples: 2818736. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:46:02,594][187912] Avg episode reward: [(0, '609.782')]
[31m[7920399 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7920399 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([12], device='cuda:0') (navigation_task.py:196)
[31m[7920399 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:46:07,576][187912] Fps is (10 sec: 409.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2818048. Throughput: 0: 362.3. Samples: 2821088. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:46:07,576][187912] Avg episode reward: [(0, '619.953')]
[36m[2025-06-29 19:46:12,561][187912] Fps is (10 sec: 410.9, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 2822144. Throughput: 0: 363.1. Samples: 2822112. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:46:12,561][187912] Avg episode reward: [(0, '631.536')]
[37m[1m[2025-06-29 19:46:12,602][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000011024_2822144.pth...
[36m[2025-06-29 19:46:12,672][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000010688_2736128.pth
[36m[2025-06-29 19:46:17,581][187912] Fps is (10 sec: 409.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2822144. Throughput: 0: 363.1. Samples: 2824160. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:46:17,582][187912] Avg episode reward: [(0, '630.676')]
[36m[2025-06-29 19:46:22,567][187912] Fps is (10 sec: 409.3, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 2826240. Throughput: 0: 364.1. Samples: 2826272. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 19:46:22,567][187912] Avg episode reward: [(0, '585.611')]
[36m[2025-06-29 19:46:27,607][187912] Fps is (10 sec: 408.6, 60 sec: 341.3, 300 sec: 360.9). Total num frames: 2826240. Throughput: 0: 362.3. Samples: 2827392. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 19:46:27,607][187912] Avg episode reward: [(0, '597.506')]
[36m[2025-06-29 19:46:32,574][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 2826240. Throughput: 0: 365.7. Samples: 2829648. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 19:46:32,575][187912] Avg episode reward: [(0, '597.322')]
[36m[2025-06-29 19:46:37,603][187912] Fps is (10 sec: 409.8, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 2830336. Throughput: 0: 358.7. Samples: 2831664. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:46:37,603][187912] Avg episode reward: [(0, '607.532')]
[36m[2025-06-29 19:46:42,578][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 2830336. Throughput: 0: 359.4. Samples: 2832768. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:46:42,578][187912] Avg episode reward: [(0, '576.340')]
[36m[2025-06-29 19:46:47,592][187912] Fps is (10 sec: 410.0, 60 sec: 344.9, 300 sec: 361.0). Total num frames: 2834432. Throughput: 0: 355.2. Samples: 2834720. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 19:46:47,592][187912] Avg episode reward: [(0, '648.158')]
[36m[2025-06-29 19:46:52,583][187912] Fps is (10 sec: 409.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2834432. Throughput: 0: 350.9. Samples: 2836880. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 19:46:52,583][187912] Avg episode reward: [(0, '655.703')]
[36m[2025-06-29 19:46:57,578][187912] Fps is (10 sec: 410.2, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2838528. Throughput: 0: 354.0. Samples: 2838048. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:46:57,578][187912] Avg episode reward: [(0, '655.461')]
[36m[2025-06-29 19:47:02,564][187912] Fps is (10 sec: 410.4, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2838528. Throughput: 0: 351.8. Samples: 2839984. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:47:02,564][187912] Avg episode reward: [(0, '663.690')]
[36m[2025-06-29 19:47:07,593][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 2838528. Throughput: 0: 354.3. Samples: 2842224. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:47:07,593][187912] Avg episode reward: [(0, '667.381')]
[31m[7982539 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7982539 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[7982539 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:47:12,576][187912] Fps is (10 sec: 409.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2842624. Throughput: 0: 348.3. Samples: 2843056. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 19:47:12,577][187912] Avg episode reward: [(0, '644.015')]
[36m[2025-06-29 19:47:17,572][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 2842624. Throughput: 0: 349.5. Samples: 2845376. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 19:47:17,572][187912] Avg episode reward: [(0, '617.952')]
[31m[7992578 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[7992578 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[7992578 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:47:22,580][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2846720. Throughput: 0: 350.0. Samples: 2847408. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 19:47:22,580][187912] Avg episode reward: [(0, '591.156')]
[36m[2025-06-29 19:47:27,604][187912] Fps is (10 sec: 408.3, 60 sec: 341.3, 300 sec: 347.9). Total num frames: 2846720. Throughput: 0: 350.4. Samples: 2848544. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 19:47:27,604][187912] Avg episode reward: [(0, '595.625')]
[36m[2025-06-29 19:47:32,575][187912] Fps is (10 sec: 409.8, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2850816. Throughput: 0: 356.4. Samples: 2850752. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 19:47:32,576][187912] Avg episode reward: [(0, '567.527')]
[36m[2025-06-29 19:47:37,608][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2850816. Throughput: 0: 355.4. Samples: 2852880. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 19:47:37,609][187912] Avg episode reward: [(0, '600.247')]
[36m[2025-06-29 19:47:42,571][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 2850816. Throughput: 0: 354.2. Samples: 2853984. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 19:47:42,571][187912] Avg episode reward: [(0, '604.001')]
[36m[2025-06-29 19:47:47,566][187912] Fps is (10 sec: 411.4, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2854912. Throughput: 0: 355.5. Samples: 2855984. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:47:47,566][187912] Avg episode reward: [(0, '621.881')]
[36m[2025-06-29 19:47:52,583][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 2854912. Throughput: 0: 354.9. Samples: 2858192. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:47:52,583][187912] Avg episode reward: [(0, '603.850')]
[36m[2025-06-29 19:47:57,576][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2859008. Throughput: 0: 356.6. Samples: 2859104. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:47:57,576][187912] Avg episode reward: [(0, '608.476')]
[31m[8035249 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8035249 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[8035249 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:48:02,587][187912] Fps is (10 sec: 409.4, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 2859008. Throughput: 0: 354.7. Samples: 2861344. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:48:02,587][187912] Avg episode reward: [(0, '604.504')]
[31m[8038481 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8038482 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[8038482 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:48:07,564][187912] Fps is (10 sec: 410.1, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 2863104. Throughput: 0: 354.3. Samples: 2863344. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:48:07,564][187912] Avg episode reward: [(0, '620.795')]
[36m[2025-06-29 19:48:12,582][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2863104. Throughput: 0: 354.7. Samples: 2864496. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:48:12,582][187912] Avg episode reward: [(0, '590.543')]
[37m[1m[2025-06-29 19:48:12,631][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000011184_2863104.pth...
[36m[2025-06-29 19:48:12,706][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000010848_2777088.pth
[36m[2025-06-29 19:48:18,236][187912] Fps is (10 sec: 383.8, 60 sec: 405.1, 300 sec: 360.2). Total num frames: 2867200. Throughput: 0: 348.7. Samples: 2866672. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:48:18,236][187912] Avg episode reward: [(0, '617.809')]
[33m[8052279 ms][navigation_task] - WARNING : Curriculum Level: 31, Curriculum progress fraction: 0.45714285714285713 (navigation_task.py:262)
[33m[8052280 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.783203125
[33mCrash Rate: 0.15673828125
[33mTimeout Rate: 0.06005859375 (navigation_task.py:265)
[33m[8052280 ms][navigation_task] - WARNING : 
[33mSuccesses: 1604
[33mCrashes : 321
[33mTimeouts: 123 (navigation_task.py:268)
[36m[2025-06-29 19:48:22,576][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2867200. Throughput: 0: 354.0. Samples: 2868800. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:48:22,576][187912] Avg episode reward: [(0, '612.519')]
[36m[2025-06-29 19:48:27,609][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 2867200. Throughput: 0: 352.1. Samples: 2869840. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:48:27,609][187912] Avg episode reward: [(0, '611.964')]
[36m[2025-06-29 19:48:32,594][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2871296. Throughput: 0: 352.8. Samples: 2871872. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:48:32,594][187912] Avg episode reward: [(0, '653.609')]
[36m[2025-06-29 19:48:37,559][187912] Fps is (10 sec: 411.6, 60 sec: 341.6, 300 sec: 347.1). Total num frames: 2871296. Throughput: 0: 355.7. Samples: 2874192. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:48:37,559][187912] Avg episode reward: [(0, '677.144')]
[36m[2025-06-29 19:48:42,560][187912] Fps is (10 sec: 411.0, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 2875392. Throughput: 0: 359.6. Samples: 2875280. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 19:48:42,561][187912] Avg episode reward: [(0, '709.069')]
[37m[1m[2025-06-29 19:48:42,621][187912] Saving new best policy, reward=709.069!
[36m[2025-06-29 19:48:47,577][187912] Fps is (10 sec: 408.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2875392. Throughput: 0: 354.9. Samples: 2877312. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 19:48:47,577][187912] Avg episode reward: [(0, '743.160')]
[37m[1m[2025-06-29 19:48:47,620][187912] Saving new best policy, reward=743.160!
[36m[2025-06-29 19:48:52,574][187912] Fps is (10 sec: 409.0, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 2879488. Throughput: 0: 359.4. Samples: 2879520. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 19:48:52,574][187912] Avg episode reward: [(0, '754.831')]
[37m[1m[2025-06-29 19:48:52,615][187912] Saving new best policy, reward=754.831!
[36m[2025-06-29 19:48:57,589][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2879488. Throughput: 0: 357.3. Samples: 2880576. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 19:48:57,589][187912] Avg episode reward: [(0, '676.065')]
[36m[2025-06-29 19:49:02,580][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 2879488. Throughput: 0: 362.3. Samples: 2882736. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 19:49:02,580][187912] Avg episode reward: [(0, '664.852')]
[31m[8100431 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8100431 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[8100431 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:49:07,563][187912] Fps is (10 sec: 410.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2883584. Throughput: 0: 356.4. Samples: 2884832. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 19:49:07,563][187912] Avg episode reward: [(0, '681.781')]
[36m[2025-06-29 19:49:12,595][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 2883584. Throughput: 0: 357.4. Samples: 2885920. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 19:49:12,595][187912] Avg episode reward: [(0, '660.998')]
[36m[2025-06-29 19:49:17,589][187912] Fps is (10 sec: 408.5, 60 sec: 345.1, 300 sec: 361.0). Total num frames: 2887680. Throughput: 0: 355.6. Samples: 2887872. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:49:17,589][187912] Avg episode reward: [(0, '652.198')]
[36m[2025-06-29 19:49:22,575][187912] Fps is (10 sec: 410.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2887680. Throughput: 0: 356.1. Samples: 2890224. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:49:22,575][187912] Avg episode reward: [(0, '682.838')]
[36m[2025-06-29 19:49:27,564][187912] Fps is (10 sec: 410.6, 60 sec: 409.9, 300 sec: 361.0). Total num frames: 2891776. Throughput: 0: 358.0. Samples: 2891392. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:49:27,565][187912] Avg episode reward: [(0, '704.875')]
[36m[2025-06-29 19:49:32,573][187912] Fps is (10 sec: 409.7, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2891776. Throughput: 0: 359.9. Samples: 2893504. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:49:32,573][187912] Avg episode reward: [(0, '597.778')]
[36m[2025-06-29 19:49:37,652][187912] Fps is (10 sec: 406.1, 60 sec: 409.0, 300 sec: 360.9). Total num frames: 2895872. Throughput: 0: 337.6. Samples: 2894736. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 19:49:37,652][187912] Avg episode reward: [(0, '601.024')]
[31m[8133382 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8133382 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[8133382 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:49:42,568][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2895872. Throughput: 0: 362.8. Samples: 2896896. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 19:49:42,568][187912] Avg episode reward: [(0, '605.862')]
[36m[2025-06-29 19:49:47,611][187912] Fps is (10 sec: 0.0, 60 sec: 341.1, 300 sec: 347.1). Total num frames: 2895872. Throughput: 0: 367.0. Samples: 2899264. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 19:49:47,611][187912] Avg episode reward: [(0, '592.737')]
[36m[2025-06-29 19:49:52,571][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2899968. Throughput: 0: 364.7. Samples: 2901248. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:49:52,571][187912] Avg episode reward: [(0, '601.863')]
[36m[2025-06-29 19:49:57,561][187912] Fps is (10 sec: 411.7, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2899968. Throughput: 0: 364.4. Samples: 2902304. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:49:57,561][187912] Avg episode reward: [(0, '673.119')]
[36m[2025-06-29 19:50:02,570][187912] Fps is (10 sec: 409.6, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 2904064. Throughput: 0: 368.9. Samples: 2904464. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 19:50:02,570][187912] Avg episode reward: [(0, '658.556')]
[36m[2025-06-29 19:50:07,586][187912] Fps is (10 sec: 408.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2904064. Throughput: 0: 366.8. Samples: 2906736. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 19:50:07,587][187912] Avg episode reward: [(0, '623.355')]
[36m[2025-06-29 19:50:12,580][187912] Fps is (10 sec: 409.2, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 2908160. Throughput: 0: 366.8. Samples: 2907904. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:50:12,580][187912] Avg episode reward: [(0, '646.461')]
[37m[1m[2025-06-29 19:50:12,622][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000011360_2908160.pth...
[36m[2025-06-29 19:50:12,684][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000011024_2822144.pth
[36m[2025-06-29 19:50:17,576][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2908160. Throughput: 0: 364.8. Samples: 2909920. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:50:17,577][187912] Avg episode reward: [(0, '667.441')]
[36m[2025-06-29 19:50:22,823][187912] Fps is (10 sec: 399.9, 60 sec: 407.9, 300 sec: 360.7). Total num frames: 2912256. Throughput: 0: 360.9. Samples: 2911040. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 19:50:22,824][187912] Avg episode reward: [(0, '672.952')]
[36m[2025-06-29 19:50:27,591][187912] Fps is (10 sec: 409.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2912256. Throughput: 0: 362.5. Samples: 2913216. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 19:50:27,591][187912] Avg episode reward: [(0, '657.768')]
[36m[2025-06-29 19:50:32,575][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 2912256. Throughput: 0: 363.7. Samples: 2915616. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 19:50:32,575][187912] Avg episode reward: [(0, '716.597')]
[36m[2025-06-29 19:50:37,567][187912] Fps is (10 sec: 410.6, 60 sec: 341.8, 300 sec: 361.0). Total num frames: 2916352. Throughput: 0: 363.8. Samples: 2917616. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:50:37,567][187912] Avg episode reward: [(0, '693.155')]
[36m[2025-06-29 19:50:42,597][187912] Fps is (10 sec: 408.7, 60 sec: 341.2, 300 sec: 347.8). Total num frames: 2916352. Throughput: 0: 363.8. Samples: 2918688. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:50:42,598][187912] Avg episode reward: [(0, '708.777')]
[36m[2025-06-29 19:50:47,569][187912] Fps is (10 sec: 409.5, 60 sec: 409.9, 300 sec: 361.0). Total num frames: 2920448. Throughput: 0: 360.9. Samples: 2920704. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:50:47,569][187912] Avg episode reward: [(0, '692.656')]
[36m[2025-06-29 19:50:52,565][187912] Fps is (10 sec: 410.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2920448. Throughput: 0: 357.5. Samples: 2922816. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:50:52,565][187912] Avg episode reward: [(0, '667.517')]
[36m[2025-06-29 19:50:57,615][187912] Fps is (10 sec: 407.7, 60 sec: 409.2, 300 sec: 361.0). Total num frames: 2924544. Throughput: 0: 356.7. Samples: 2923968. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 19:50:57,615][187912] Avg episode reward: [(0, '698.011')]
[36m[2025-06-29 19:51:02,582][187912] Fps is (10 sec: 408.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2924544. Throughput: 0: 354.8. Samples: 2925888. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 19:51:02,582][187912] Avg episode reward: [(0, '666.255')]
[36m[2025-06-29 19:51:07,576][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 2924544. Throughput: 0: 380.8. Samples: 2928080. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 19:51:07,577][187912] Avg episode reward: [(0, '694.683')]
[36m[2025-06-29 19:51:12,558][187912] Fps is (10 sec: 410.6, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2928640. Throughput: 0: 350.5. Samples: 2928976. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:51:12,558][187912] Avg episode reward: [(0, '718.749')]
[31m[8227799 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8227799 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[8227799 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:51:17,574][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 2928640. Throughput: 0: 348.8. Samples: 2931312. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:51:17,575][187912] Avg episode reward: [(0, '742.869')]
[36m[2025-06-29 19:51:22,590][187912] Fps is (10 sec: 408.3, 60 sec: 342.7, 300 sec: 361.0). Total num frames: 2932736. Throughput: 0: 346.5. Samples: 2933216. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 19:51:22,590][187912] Avg episode reward: [(0, '730.700')]
[36m[2025-06-29 19:51:27,592][187912] Fps is (10 sec: 408.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2932736. Throughput: 0: 348.8. Samples: 2934384. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 19:51:27,592][187912] Avg episode reward: [(0, '739.960')]
[36m[2025-06-29 19:51:32,571][187912] Fps is (10 sec: 410.4, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2936832. Throughput: 0: 355.5. Samples: 2936704. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 19:51:32,572][187912] Avg episode reward: [(0, '674.478')]
[36m[2025-06-29 19:51:37,571][187912] Fps is (10 sec: 410.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2936832. Throughput: 0: 355.9. Samples: 2938832. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 19:51:37,571][187912] Avg episode reward: [(0, '674.693')]
[36m[2025-06-29 19:51:43,021][187912] Fps is (10 sec: 392.0, 60 sec: 406.7, 300 sec: 360.5). Total num frames: 2940928. Throughput: 0: 354.1. Samples: 2940048. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 19:51:43,022][187912] Avg episode reward: [(0, '612.284')]
[36m[2025-06-29 19:51:47,608][187912] Fps is (10 sec: 408.1, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 2940928. Throughput: 0: 363.9. Samples: 2942272. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 19:51:47,609][187912] Avg episode reward: [(0, '651.793')]
[36m[2025-06-29 19:51:52,595][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 2940928. Throughput: 0: 362.9. Samples: 2944416. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 19:51:52,596][187912] Avg episode reward: [(0, '633.160')]
[36m[2025-06-29 19:51:57,588][187912] Fps is (10 sec: 410.4, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2945024. Throughput: 0: 362.8. Samples: 2945312. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 19:51:57,588][187912] Avg episode reward: [(0, '684.386')]
[31m[8274102 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8274102 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[8274102 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[8274966 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8274966 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[8274966 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:52:02,559][187912] Fps is (10 sec: 411.1, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2945024. Throughput: 0: 361.7. Samples: 2947584. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 19:52:02,559][187912] Avg episode reward: [(0, '688.065')]
[36m[2025-06-29 19:52:07,581][187912] Fps is (10 sec: 409.9, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2949120. Throughput: 0: 365.6. Samples: 2949664. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 19:52:07,582][187912] Avg episode reward: [(0, '700.309')]
[36m[2025-06-29 19:52:12,573][187912] Fps is (10 sec: 409.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2949120. Throughput: 0: 365.7. Samples: 2950832. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 19:52:12,573][187912] Avg episode reward: [(0, '645.073')]
[37m[1m[2025-06-29 19:52:12,622][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000011520_2949120.pth...
[36m[2025-06-29 19:52:12,692][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000011184_2863104.pth
[36m[2025-06-29 19:52:17,585][187912] Fps is (10 sec: 409.5, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 2953216. Throughput: 0: 363.6. Samples: 2953072. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 19:52:17,585][187912] Avg episode reward: [(0, '670.526')]
[36m[2025-06-29 19:52:22,592][187912] Fps is (10 sec: 408.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2953216. Throughput: 0: 361.4. Samples: 2955104. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 19:52:22,592][187912] Avg episode reward: [(0, '642.975')]
[36m[2025-06-29 19:52:27,622][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 2953216. Throughput: 0: 364.1. Samples: 2956288. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 19:52:27,622][187912] Avg episode reward: [(0, '650.401')]
[36m[2025-06-29 19:52:32,588][187912] Fps is (10 sec: 409.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2957312. Throughput: 0: 354.6. Samples: 2958224. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 19:52:32,588][187912] Avg episode reward: [(0, '655.115')]
[36m[2025-06-29 19:52:37,587][187912] Fps is (10 sec: 411.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2957312. Throughput: 0: 359.5. Samples: 2960592. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 19:52:37,587][187912] Avg episode reward: [(0, '713.259')]
[36m[2025-06-29 19:52:42,584][187912] Fps is (10 sec: 409.7, 60 sec: 343.8, 300 sec: 361.0). Total num frames: 2961408. Throughput: 0: 358.4. Samples: 2961440. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:52:42,585][187912] Avg episode reward: [(0, '732.413')]
[36m[2025-06-29 19:52:47,588][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2961408. Throughput: 0: 358.9. Samples: 2963744. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:52:47,589][187912] Avg episode reward: [(0, '703.104')]
[36m[2025-06-29 19:52:52,595][187912] Fps is (10 sec: 409.2, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2965504. Throughput: 0: 353.7. Samples: 2965584. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:52:52,596][187912] Avg episode reward: [(0, '690.005')]
[36m[2025-06-29 19:52:57,567][187912] Fps is (10 sec: 410.5, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2965504. Throughput: 0: 353.1. Samples: 2966720. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:52:57,567][187912] Avg episode reward: [(0, '647.933')]
[31m[8333215 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8333215 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[8333215 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:53:02,560][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 2965504. Throughput: 0: 351.8. Samples: 2968896. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:53:02,561][187912] Avg episode reward: [(0, '642.399')]
[36m[2025-06-29 19:53:07,597][187912] Fps is (10 sec: 408.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2969600. Throughput: 0: 349.1. Samples: 2970816. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 19:53:07,597][187912] Avg episode reward: [(0, '607.981')]
[36m[2025-06-29 19:53:12,570][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 347.9). Total num frames: 2969600. Throughput: 0: 348.1. Samples: 2971936. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 19:53:12,570][187912] Avg episode reward: [(0, '652.779')]
[36m[2025-06-29 19:53:17,588][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2973696. Throughput: 0: 349.9. Samples: 2973968. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 19:53:17,588][187912] Avg episode reward: [(0, '669.282')]
[31m[8353229 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8353229 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[8353229 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:53:22,589][187912] Fps is (10 sec: 408.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2973696. Throughput: 0: 347.4. Samples: 2976224. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 19:53:22,589][187912] Avg episode reward: [(0, '652.646')]
[36m[2025-06-29 19:53:27,562][187912] Fps is (10 sec: 410.6, 60 sec: 410.0, 300 sec: 361.0). Total num frames: 2977792. Throughput: 0: 355.4. Samples: 2977424. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:53:27,563][187912] Avg episode reward: [(0, '681.188')]
[36m[2025-06-29 19:53:32,588][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 2977792. Throughput: 0: 350.6. Samples: 2979520. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:53:32,588][187912] Avg episode reward: [(0, '675.100')]
[36m[2025-06-29 19:53:37,917][187912] Fps is (10 sec: 395.6, 60 sec: 407.4, 300 sec: 360.6). Total num frames: 2981888. Throughput: 0: 359.7. Samples: 2981888. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 19:53:37,917][187912] Avg episode reward: [(0, '656.458')]
[36m[2025-06-29 19:53:42,569][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2981888. Throughput: 0: 357.7. Samples: 2982816. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 19:53:42,569][187912] Avg episode reward: [(0, '672.111')]
[36m[2025-06-29 19:53:47,563][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 2981888. Throughput: 0: 359.1. Samples: 2985056. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 19:53:47,563][187912] Avg episode reward: [(0, '701.747')]
[36m[2025-06-29 19:53:52,568][187912] Fps is (10 sec: 409.6, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2985984. Throughput: 0: 362.9. Samples: 2987136. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:53:52,568][187912] Avg episode reward: [(0, '684.718')]
[36m[2025-06-29 19:53:57,584][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2985984. Throughput: 0: 364.0. Samples: 2988320. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:53:57,584][187912] Avg episode reward: [(0, '682.530')]
[36m[2025-06-29 19:54:02,560][187912] Fps is (10 sec: 409.9, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 2990080. Throughput: 0: 361.1. Samples: 2990208. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 19:54:02,560][187912] Avg episode reward: [(0, '669.322')]
[36m[2025-06-29 19:54:07,585][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 2990080. Throughput: 0: 361.3. Samples: 2992480. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 19:54:07,585][187912] Avg episode reward: [(0, '627.625')]
[36m[2025-06-29 19:54:12,584][187912] Fps is (10 sec: 408.6, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 2994176. Throughput: 0: 360.0. Samples: 2993632. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 19:54:12,584][187912] Avg episode reward: [(0, '627.528')]
[37m[1m[2025-06-29 19:54:12,623][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000011696_2994176.pth...
[36m[2025-06-29 19:54:12,692][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000011360_2908160.pth
[36m[2025-06-29 19:54:17,562][187912] Fps is (10 sec: 410.5, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 2994176. Throughput: 0: 359.3. Samples: 2995680. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 19:54:17,563][187912] Avg episode reward: [(0, '616.818')]
[36m[2025-06-29 19:54:22,595][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 2994176. Throughput: 0: 359.2. Samples: 2997936. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 19:54:22,596][187912] Avg episode reward: [(0, '604.160')]
[36m[2025-06-29 19:54:27,588][187912] Fps is (10 sec: 408.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 2998272. Throughput: 0: 356.1. Samples: 2998848. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 19:54:27,589][187912] Avg episode reward: [(0, '590.978')]
[36m[2025-06-29 19:54:32,571][187912] Fps is (10 sec: 410.6, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 2998272. Throughput: 0: 357.6. Samples: 3001152. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 19:54:32,571][187912] Avg episode reward: [(0, '622.814')]
[36m[2025-06-29 19:54:37,611][187912] Fps is (10 sec: 408.7, 60 sec: 343.1, 300 sec: 361.0). Total num frames: 3002368. Throughput: 0: 357.0. Samples: 3003216. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:54:37,611][187912] Avg episode reward: [(0, '625.270')]
[36m[2025-06-29 19:54:42,576][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3002368. Throughput: 0: 356.7. Samples: 3004368. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:54:42,576][187912] Avg episode reward: [(0, '622.463')]
[36m[2025-06-29 19:54:47,590][187912] Fps is (10 sec: 410.5, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 3006464. Throughput: 0: 361.7. Samples: 3006496. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 19:54:47,590][187912] Avg episode reward: [(0, '660.612')]
[36m[2025-06-29 19:54:52,577][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3006464. Throughput: 0: 361.7. Samples: 3008752. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 19:54:52,577][187912] Avg episode reward: [(0, '669.349')]
[36m[2025-06-29 19:54:57,606][187912] Fps is (10 sec: 409.0, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 3010560. Throughput: 0: 362.5. Samples: 3009952. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:54:57,606][187912] Avg episode reward: [(0, '695.842')]
[36m[2025-06-29 19:55:02,586][187912] Fps is (10 sec: 409.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3010560. Throughput: 0: 362.1. Samples: 3011984. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:55:02,586][187912] Avg episode reward: [(0, '682.996')]
[36m[2025-06-29 19:55:07,574][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 3010560. Throughput: 0: 362.5. Samples: 3014240. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:55:07,574][187912] Avg episode reward: [(0, '696.096')]
[36m[2025-06-29 19:55:12,566][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3014656. Throughput: 0: 362.8. Samples: 3015168. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:55:12,566][187912] Avg episode reward: [(0, '652.053')]
[36m[2025-06-29 19:55:17,578][187912] Fps is (10 sec: 409.4, 60 sec: 341.2, 300 sec: 347.4). Total num frames: 3014656. Throughput: 0: 363.7. Samples: 3017520. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:55:17,578][187912] Avg episode reward: [(0, '634.641')]
[36m[2025-06-29 19:55:22,583][187912] Fps is (10 sec: 408.9, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 3018752. Throughput: 0: 362.9. Samples: 3019536. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:55:22,583][187912] Avg episode reward: [(0, '628.057')]
[36m[2025-06-29 19:55:27,593][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3018752. Throughput: 0: 362.5. Samples: 3020688. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:55:27,593][187912] Avg episode reward: [(0, '628.598')]
[36m[2025-06-29 19:55:32,583][187912] Fps is (10 sec: 409.6, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 3022848. Throughput: 0: 364.2. Samples: 3022880. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:55:32,583][187912] Avg episode reward: [(0, '617.698')]
[36m[2025-06-29 19:55:37,590][187912] Fps is (10 sec: 409.7, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3022848. Throughput: 0: 360.4. Samples: 3024976. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:55:37,590][187912] Avg episode reward: [(0, '614.128')]
[36m[2025-06-29 19:55:42,923][187912] Fps is (10 sec: 396.1, 60 sec: 407.2, 300 sec: 360.6). Total num frames: 3026944. Throughput: 0: 356.9. Samples: 3026128. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:55:42,924][187912] Avg episode reward: [(0, '653.900')]
[31m[8500621 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8500622 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[8500622 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:55:47,565][187912] Fps is (10 sec: 410.6, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3026944. Throughput: 0: 361.8. Samples: 3028256. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:55:47,565][187912] Avg episode reward: [(0, '649.982')]
[36m[2025-06-29 19:55:52,581][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.2). Total num frames: 3026944. Throughput: 0: 363.0. Samples: 3030576. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:55:52,581][187912] Avg episode reward: [(0, '661.081')]
[36m[2025-06-29 19:55:57,595][187912] Fps is (10 sec: 408.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3031040. Throughput: 0: 360.3. Samples: 3031392. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 19:55:57,595][187912] Avg episode reward: [(0, '660.563')]
[36m[2025-06-29 19:56:02,576][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3031040. Throughput: 0: 359.1. Samples: 3033680. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 19:56:02,576][187912] Avg episode reward: [(0, '677.097')]
[36m[2025-06-29 19:56:07,568][187912] Fps is (10 sec: 410.7, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3035136. Throughput: 0: 362.4. Samples: 3035840. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:56:07,568][187912] Avg episode reward: [(0, '669.523')]
[36m[2025-06-29 19:56:12,582][187912] Fps is (10 sec: 409.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3035136. Throughput: 0: 364.2. Samples: 3037072. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:56:12,582][187912] Avg episode reward: [(0, '677.840')]
[37m[1m[2025-06-29 19:56:12,627][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000011856_3035136.pth...
[36m[2025-06-29 19:56:12,699][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000011520_2949120.pth
[36m[2025-06-29 19:56:17,589][187912] Fps is (10 sec: 408.7, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 3039232. Throughput: 0: 364.0. Samples: 3039264. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:56:17,589][187912] Avg episode reward: [(0, '672.839')]
[36m[2025-06-29 19:56:22,580][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3039232. Throughput: 0: 361.7. Samples: 3041248. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:56:22,580][187912] Avg episode reward: [(0, '662.475')]
[36m[2025-06-29 19:56:27,563][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 3039232. Throughput: 0: 363.1. Samples: 3042336. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:56:27,563][187912] Avg episode reward: [(0, '630.622')]
[36m[2025-06-29 19:56:32,588][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3043328. Throughput: 0: 358.6. Samples: 3044400. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 19:56:32,589][187912] Avg episode reward: [(0, '663.643')]
[36m[2025-06-29 19:56:37,560][187912] Fps is (10 sec: 409.7, 60 sec: 341.5, 300 sec: 347.7). Total num frames: 3043328. Throughput: 0: 361.1. Samples: 3046816. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 19:56:37,560][187912] Avg episode reward: [(0, '683.265')]
[36m[2025-06-29 19:56:42,595][187912] Fps is (10 sec: 409.3, 60 sec: 343.2, 300 sec: 361.0). Total num frames: 3047424. Throughput: 0: 362.3. Samples: 3047696. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:56:42,596][187912] Avg episode reward: [(0, '685.175')]
[36m[2025-06-29 19:56:47,576][187912] Fps is (10 sec: 408.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3047424. Throughput: 0: 364.4. Samples: 3050080. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 19:56:47,576][187912] Avg episode reward: [(0, '718.372')]
[36m[2025-06-29 19:56:52,564][187912] Fps is (10 sec: 410.9, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 3051520. Throughput: 0: 360.6. Samples: 3052064. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 19:56:52,564][187912] Avg episode reward: [(0, '699.760')]
[36m[2025-06-29 19:56:57,567][187912] Fps is (10 sec: 410.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3051520. Throughput: 0: 358.9. Samples: 3053216. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 19:56:57,567][187912] Avg episode reward: [(0, '685.331')]
[33m[8572682 ms][navigation_task] - WARNING : Curriculum Level: 33, Curriculum progress fraction: 0.5142857142857142 (navigation_task.py:262)
[33m[8572683 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.80419921875
[33mCrash Rate: 0.154296875
[33mTimeout Rate: 0.04150390625 (navigation_task.py:265)
[33m[8572683 ms][navigation_task] - WARNING : 
[33mSuccesses: 1647
[33mCrashes : 316
[33mTimeouts: 85 (navigation_task.py:268)
[36m[2025-06-29 19:57:02,584][187912] Fps is (10 sec: 408.8, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 3055616. Throughput: 0: 360.9. Samples: 3055504. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 19:57:02,585][187912] Avg episode reward: [(0, '674.457')]
[31m[8579761 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8579761 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[8579761 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:57:07,558][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3055616. Throughput: 0: 365.3. Samples: 3057680. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 19:57:07,558][187912] Avg episode reward: [(0, '671.190')]
[36m[2025-06-29 19:57:12,613][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 3055616. Throughput: 0: 365.5. Samples: 3058800. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 19:57:12,614][187912] Avg episode reward: [(0, '629.003')]
[36m[2025-06-29 19:57:17,588][187912] Fps is (10 sec: 408.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3059712. Throughput: 0: 363.4. Samples: 3060752. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:57:17,588][187912] Avg episode reward: [(0, '670.926')]
[36m[2025-06-29 19:57:22,580][187912] Fps is (10 sec: 411.0, 60 sec: 341.3, 300 sec: 361.1). Total num frames: 3059712. Throughput: 0: 362.1. Samples: 3063120. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:57:22,580][187912] Avg episode reward: [(0, '646.296')]
[36m[2025-06-29 19:57:27,579][187912] Fps is (10 sec: 410.0, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 3063808. Throughput: 0: 363.9. Samples: 3064064. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:57:27,579][187912] Avg episode reward: [(0, '645.938')]
[36m[2025-06-29 19:57:32,588][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3063808. Throughput: 0: 361.2. Samples: 3066336. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:57:32,588][187912] Avg episode reward: [(0, '656.624')]
[36m[2025-06-29 19:57:37,581][187912] Fps is (10 sec: 409.5, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 3067904. Throughput: 0: 359.7. Samples: 3068256. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:57:37,581][187912] Avg episode reward: [(0, '711.128')]
[36m[2025-06-29 19:57:42,591][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3067904. Throughput: 0: 362.5. Samples: 3069536. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:57:42,591][187912] Avg episode reward: [(0, '681.611')]
[36m[2025-06-29 19:57:47,593][187912] Fps is (10 sec: 409.1, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 3072000. Throughput: 0: 365.4. Samples: 3071952. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 19:57:47,593][187912] Avg episode reward: [(0, '734.523')]
[36m[2025-06-29 19:57:52,576][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3072000. Throughput: 0: 361.5. Samples: 3073952. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 19:57:52,577][187912] Avg episode reward: [(0, '721.409')]
[36m[2025-06-29 19:57:57,590][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3072000. Throughput: 0: 361.1. Samples: 3075040. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 19:57:57,590][187912] Avg episode reward: [(0, '736.350')]
[31m[8633351 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8633351 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([12], device='cuda:0') (navigation_task.py:196)
[31m[8633351 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:58:02,575][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3076096. Throughput: 0: 361.7. Samples: 3077024. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:58:02,575][187912] Avg episode reward: [(0, '727.079')]
[36m[2025-06-29 19:58:07,560][187912] Fps is (10 sec: 410.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3076096. Throughput: 0: 362.8. Samples: 3079440. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:58:07,560][187912] Avg episode reward: [(0, '768.890')]
[37m[1m[2025-06-29 19:58:07,609][187912] Saving new best policy, reward=768.890!
[36m[2025-06-29 19:58:12,592][187912] Fps is (10 sec: 408.9, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 3080192. Throughput: 0: 359.4. Samples: 3080240. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:58:12,592][187912] Avg episode reward: [(0, '751.173')]
[37m[1m[2025-06-29 19:58:12,632][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000012032_3080192.pth...
[36m[2025-06-29 19:58:12,696][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000011696_2994176.pth
[36m[2025-06-29 19:58:17,559][187912] Fps is (10 sec: 409.6, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3080192. Throughput: 0: 364.0. Samples: 3082704. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:58:17,559][187912] Avg episode reward: [(0, '775.068')]
[37m[1m[2025-06-29 19:58:17,602][187912] Saving new best policy, reward=775.068!
[36m[2025-06-29 19:58:22,596][187912] Fps is (10 sec: 409.4, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 3084288. Throughput: 0: 363.3. Samples: 3084608. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 19:58:22,596][187912] Avg episode reward: [(0, '774.309')]
[36m[2025-06-29 19:58:27,599][187912] Fps is (10 sec: 407.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3084288. Throughput: 0: 362.2. Samples: 3085840. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 19:58:27,600][187912] Avg episode reward: [(0, '773.281')]
[36m[2025-06-29 19:58:32,879][187912] Fps is (10 sec: 398.3, 60 sec: 407.6, 300 sec: 361.1). Total num frames: 3088384. Throughput: 0: 354.4. Samples: 3088000. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 19:58:32,879][187912] Avg episode reward: [(0, '742.969')]
[36m[2025-06-29 19:58:37,585][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3088384. Throughput: 0: 357.3. Samples: 3090032. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 19:58:37,585][187912] Avg episode reward: [(0, '763.059')]
[36m[2025-06-29 19:58:42,569][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3088384. Throughput: 0: 357.1. Samples: 3091104. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 19:58:42,570][187912] Avg episode reward: [(0, '712.148')]
[36m[2025-06-29 19:58:47,616][187912] Fps is (10 sec: 408.3, 60 sec: 341.2, 300 sec: 360.9). Total num frames: 3092480. Throughput: 0: 359.5. Samples: 3093216. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 19:58:47,616][187912] Avg episode reward: [(0, '706.567')]
[36m[2025-06-29 19:58:52,576][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3092480. Throughput: 0: 355.8. Samples: 3095456. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 19:58:52,576][187912] Avg episode reward: [(0, '671.268')]
[36m[2025-06-29 19:58:57,564][187912] Fps is (10 sec: 411.8, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 3096576. Throughput: 0: 362.2. Samples: 3096528. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:58:57,564][187912] Avg episode reward: [(0, '673.998')]
[36m[2025-06-29 19:59:02,569][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3096576. Throughput: 0: 351.9. Samples: 3098544. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 19:59:02,569][187912] Avg episode reward: [(0, '694.818')]
[36m[2025-06-29 19:59:07,707][187912] Fps is (10 sec: 403.8, 60 sec: 408.6, 300 sec: 360.9). Total num frames: 3100672. Throughput: 0: 334.5. Samples: 3099696. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 19:59:07,707][187912] Avg episode reward: [(0, '709.401')]
[36m[2025-06-29 19:59:12,578][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3100672. Throughput: 0: 351.1. Samples: 3101632. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 19:59:12,578][187912] Avg episode reward: [(0, '722.463')]
[36m[2025-06-29 19:59:17,583][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3100672. Throughput: 0: 355.0. Samples: 3103872. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 19:59:17,583][187912] Avg episode reward: [(0, '722.705')]
[36m[2025-06-29 19:59:22,576][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3104768. Throughput: 0: 356.3. Samples: 3106064. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:59:22,576][187912] Avg episode reward: [(0, '719.788')]
[36m[2025-06-29 19:59:27,566][187912] Fps is (10 sec: 410.3, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3104768. Throughput: 0: 359.5. Samples: 3107280. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:59:27,566][187912] Avg episode reward: [(0, '667.518')]
[31m[8722445 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8722445 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[8722445 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:59:32,593][187912] Fps is (10 sec: 408.9, 60 sec: 343.0, 300 sec: 361.0). Total num frames: 3108864. Throughput: 0: 358.2. Samples: 3109328. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 19:59:32,593][187912] Avg episode reward: [(0, '687.029')]
[36m[2025-06-29 19:59:37,595][187912] Fps is (10 sec: 408.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3108864. Throughput: 0: 360.7. Samples: 3111696. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 19:59:37,595][187912] Avg episode reward: [(0, '678.850')]
[31m[8734777 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8734777 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([12], device='cuda:0') (navigation_task.py:196)
[31m[8734777 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:59:42,566][187912] Fps is (10 sec: 410.7, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3112960. Throughput: 0: 363.4. Samples: 3112880. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:59:42,566][187912] Avg episode reward: [(0, '656.321')]
[31m[8737878 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8737879 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[8737879 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 19:59:47,575][187912] Fps is (10 sec: 410.4, 60 sec: 341.6, 300 sec: 361.0). Total num frames: 3112960. Throughput: 0: 365.5. Samples: 3114992. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:59:47,575][187912] Avg episode reward: [(0, '695.316')]
[36m[2025-06-29 19:59:52,575][187912] Fps is (10 sec: 409.2, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3117056. Throughput: 0: 390.1. Samples: 3117200. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:59:52,575][187912] Avg episode reward: [(0, '683.698')]
[36m[2025-06-29 19:59:57,569][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3117056. Throughput: 0: 370.9. Samples: 3118320. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 19:59:57,569][187912] Avg episode reward: [(0, '686.517')]
[36m[2025-06-29 20:00:02,595][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3117056. Throughput: 0: 371.8. Samples: 3120608. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:00:02,596][187912] Avg episode reward: [(0, '700.306')]
[36m[2025-06-29 20:00:07,615][187912] Fps is (10 sec: 407.7, 60 sec: 341.9, 300 sec: 360.9). Total num frames: 3121152. Throughput: 0: 367.7. Samples: 3122624. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 20:00:07,616][187912] Avg episode reward: [(0, '685.902')]
[36m[2025-06-29 20:00:12,594][187912] Fps is (10 sec: 409.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3121152. Throughput: 0: 366.4. Samples: 3123776. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 20:00:12,594][187912] Avg episode reward: [(0, '699.827')]
[37m[1m[2025-06-29 20:00:12,634][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000012192_3121152.pth...
[36m[2025-06-29 20:00:12,710][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000011856_3035136.pth
[36m[2025-06-29 20:00:17,567][187912] Fps is (10 sec: 411.6, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 3125248. Throughput: 0: 366.1. Samples: 3125792. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 20:00:17,567][187912] Avg episode reward: [(0, '713.763')]
[36m[2025-06-29 20:00:22,563][187912] Fps is (10 sec: 410.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3125248. Throughput: 0: 367.6. Samples: 3128224. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 20:00:22,563][187912] Avg episode reward: [(0, '794.030')]
[37m[1m[2025-06-29 20:00:22,605][187912] Saving new best policy, reward=794.030!
[36m[2025-06-29 20:00:27,595][187912] Fps is (10 sec: 408.5, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 3129344. Throughput: 0: 366.0. Samples: 3129360. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 20:00:27,595][187912] Avg episode reward: [(0, '724.658')]
[36m[2025-06-29 20:00:32,573][187912] Fps is (10 sec: 409.2, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3129344. Throughput: 0: 367.3. Samples: 3131520. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 20:00:32,573][187912] Avg episode reward: [(0, '770.731')]
[36m[2025-06-29 20:00:37,587][187912] Fps is (10 sec: 409.9, 60 sec: 409.7, 300 sec: 361.4). Total num frames: 3133440. Throughput: 0: 361.5. Samples: 3133472. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 20:00:37,588][187912] Avg episode reward: [(0, '806.377')]
[37m[1m[2025-06-29 20:00:37,634][187912] Saving new best policy, reward=806.377!
[36m[2025-06-29 20:00:42,573][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3133440. Throughput: 0: 358.4. Samples: 3134448. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 20:00:42,573][187912] Avg episode reward: [(0, '779.358')]
[36m[2025-06-29 20:00:47,590][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3133440. Throughput: 0: 355.2. Samples: 3136592. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 20:00:47,590][187912] Avg episode reward: [(0, '687.785')]
[36m[2025-06-29 20:00:52,591][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3137536. Throughput: 0: 356.1. Samples: 3138640. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:00:52,591][187912] Avg episode reward: [(0, '707.960')]
[36m[2025-06-29 20:00:57,588][187912] Fps is (10 sec: 409.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3137536. Throughput: 0: 354.5. Samples: 3139728. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:00:57,588][187912] Avg episode reward: [(0, '679.209')]
[36m[2025-06-29 20:01:02,576][187912] Fps is (10 sec: 410.2, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 3141632. Throughput: 0: 354.8. Samples: 3141760. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:01:02,576][187912] Avg episode reward: [(0, '629.770')]
[36m[2025-06-29 20:01:07,583][187912] Fps is (10 sec: 409.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3141632. Throughput: 0: 349.0. Samples: 3143936. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:01:07,583][187912] Avg episode reward: [(0, '638.141')]
[36m[2025-06-29 20:01:13,031][187912] Fps is (10 sec: 391.8, 60 sec: 406.6, 300 sec: 360.5). Total num frames: 3145728. Throughput: 0: 345.4. Samples: 3145056. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:01:13,032][187912] Avg episode reward: [(0, '708.699')]
[36m[2025-06-29 20:01:17,601][187912] Fps is (10 sec: 408.9, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 3145728. Throughput: 0: 342.5. Samples: 3146944. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:01:17,601][187912] Avg episode reward: [(0, '716.932')]
[36m[2025-06-29 20:01:22,582][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3145728. Throughput: 0: 350.6. Samples: 3149248. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:01:22,582][187912] Avg episode reward: [(0, '723.745')]
[36m[2025-06-29 20:01:27,562][187912] Fps is (10 sec: 411.2, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3149824. Throughput: 0: 348.2. Samples: 3150112. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:01:27,562][187912] Avg episode reward: [(0, '737.763')]
[36m[2025-06-29 20:01:32,572][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3149824. Throughput: 0: 354.3. Samples: 3152528. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:01:32,572][187912] Avg episode reward: [(0, '746.741')]
[36m[2025-06-29 20:01:37,581][187912] Fps is (10 sec: 408.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3153920. Throughput: 0: 349.6. Samples: 3154368. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 20:01:37,581][187912] Avg episode reward: [(0, '757.174')]
[31m[8855435 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8855436 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[8855436 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:01:42,593][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3153920. Throughput: 0: 351.6. Samples: 3155552. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 20:01:42,593][187912] Avg episode reward: [(0, '709.059')]
[36m[2025-06-29 20:01:47,574][187912] Fps is (10 sec: 409.8, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 3158016. Throughput: 0: 355.6. Samples: 3157760. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:01:47,575][187912] Avg episode reward: [(0, '720.029')]
[31m[8862225 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8862226 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[8862226 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:01:52,591][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3158016. Throughput: 0: 354.4. Samples: 3159888. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:01:52,591][187912] Avg episode reward: [(0, '711.760')]
[36m[2025-06-29 20:01:57,558][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 3158016. Throughput: 0: 355.4. Samples: 3160880. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:01:57,558][187912] Avg episode reward: [(0, '681.453')]
[36m[2025-06-29 20:02:02,581][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3162112. Throughput: 0: 356.4. Samples: 3162976. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 20:02:02,581][187912] Avg episode reward: [(0, '709.931')]
[36m[2025-06-29 20:02:07,607][187912] Fps is (10 sec: 407.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3162112. Throughput: 0: 356.8. Samples: 3165312. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 20:02:07,608][187912] Avg episode reward: [(0, '709.112')]
[36m[2025-06-29 20:02:12,568][187912] Fps is (10 sec: 410.1, 60 sec: 344.0, 300 sec: 361.0). Total num frames: 3166208. Throughput: 0: 358.4. Samples: 3166240. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:02:12,568][187912] Avg episode reward: [(0, '733.690')]
[37m[1m[2025-06-29 20:02:12,607][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000012368_3166208.pth...
[36m[2025-06-29 20:02:12,679][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000012032_3080192.pth
[36m[2025-06-29 20:02:17,565][187912] Fps is (10 sec: 411.4, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3166208. Throughput: 0: 358.1. Samples: 3168640. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:02:17,565][187912] Avg episode reward: [(0, '755.091')]
[31m[8894919 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8894919 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[8894919 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:02:22,578][187912] Fps is (10 sec: 409.2, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3170304. Throughput: 0: 363.8. Samples: 3170736. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:02:22,578][187912] Avg episode reward: [(0, '778.527')]
[36m[2025-06-29 20:02:27,590][187912] Fps is (10 sec: 408.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3170304. Throughput: 0: 363.0. Samples: 3171888. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:02:27,590][187912] Avg episode reward: [(0, '745.251')]
[31m[8905500 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[8905500 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[8905500 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:02:32,579][187912] Fps is (10 sec: 409.6, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3174400. Throughput: 0: 367.6. Samples: 3174304. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:02:32,579][187912] Avg episode reward: [(0, '706.070')]
[36m[2025-06-29 20:02:37,581][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3174400. Throughput: 0: 364.9. Samples: 3176304. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:02:37,581][187912] Avg episode reward: [(0, '725.462')]
[36m[2025-06-29 20:02:42,572][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 3174400. Throughput: 0: 366.1. Samples: 3177360. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:02:42,572][187912] Avg episode reward: [(0, '694.154')]
[36m[2025-06-29 20:02:47,596][187912] Fps is (10 sec: 409.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3178496. Throughput: 0: 364.7. Samples: 3179392. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:02:47,596][187912] Avg episode reward: [(0, '688.301')]
[36m[2025-06-29 20:02:52,577][187912] Fps is (10 sec: 409.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3178496. Throughput: 0: 360.8. Samples: 3181536. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:02:52,577][187912] Avg episode reward: [(0, '704.388')]
[36m[2025-06-29 20:02:57,571][187912] Fps is (10 sec: 410.6, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 3182592. Throughput: 0: 363.0. Samples: 3182576. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 20:02:57,572][187912] Avg episode reward: [(0, '766.860')]
[36m[2025-06-29 20:03:02,613][187912] Fps is (10 sec: 408.1, 60 sec: 341.1, 300 sec: 360.9). Total num frames: 3182592. Throughput: 0: 353.4. Samples: 3184560. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 20:03:02,614][187912] Avg episode reward: [(0, '724.096')]
[36m[2025-06-29 20:03:07,892][187912] Fps is (10 sec: 396.9, 60 sec: 407.7, 300 sec: 360.6). Total num frames: 3186688. Throughput: 0: 352.0. Samples: 3186688. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:03:07,892][187912] Avg episode reward: [(0, '723.712')]
[36m[2025-06-29 20:03:12,589][187912] Fps is (10 sec: 410.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3186688. Throughput: 0: 347.7. Samples: 3187536. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:03:12,589][187912] Avg episode reward: [(0, '751.377')]
[36m[2025-06-29 20:03:17,596][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 3186688. Throughput: 0: 342.6. Samples: 3189728. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:03:17,596][187912] Avg episode reward: [(0, '777.944')]
[36m[2025-06-29 20:03:22,587][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3190784. Throughput: 0: 342.4. Samples: 3191712. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 20:03:22,587][187912] Avg episode reward: [(0, '721.811')]
[36m[2025-06-29 20:03:27,575][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 347.5). Total num frames: 3190784. Throughput: 0: 345.6. Samples: 3192912. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 20:03:27,575][187912] Avg episode reward: [(0, '733.349')]
[36m[2025-06-29 20:03:32,601][187912] Fps is (10 sec: 409.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3194880. Throughput: 0: 344.9. Samples: 3194912. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:03:32,601][187912] Avg episode reward: [(0, '747.520')]
[36m[2025-06-29 20:03:37,575][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3194880. Throughput: 0: 348.8. Samples: 3197232. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:03:37,575][187912] Avg episode reward: [(0, '756.535')]
[36m[2025-06-29 20:03:42,584][187912] Fps is (10 sec: 410.3, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 3198976. Throughput: 0: 352.3. Samples: 3198432. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 20:03:42,584][187912] Avg episode reward: [(0, '756.029')]
[36m[2025-06-29 20:03:47,588][187912] Fps is (10 sec: 409.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3198976. Throughput: 0: 356.5. Samples: 3200592. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 20:03:47,588][187912] Avg episode reward: [(0, '741.001')]
[36m[2025-06-29 20:03:52,583][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 3198976. Throughput: 0: 362.7. Samples: 3202896. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 20:03:52,583][187912] Avg episode reward: [(0, '767.872')]
[36m[2025-06-29 20:03:57,583][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3203072. Throughput: 0: 360.9. Samples: 3203776. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 20:03:57,583][187912] Avg episode reward: [(0, '756.463')]
[36m[2025-06-29 20:04:02,570][187912] Fps is (10 sec: 410.1, 60 sec: 341.6, 300 sec: 347.3). Total num frames: 3203072. Throughput: 0: 366.4. Samples: 3206208. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 20:04:02,570][187912] Avg episode reward: [(0, '758.191')]
[36m[2025-06-29 20:04:07,561][187912] Fps is (10 sec: 410.5, 60 sec: 343.2, 300 sec: 361.0). Total num frames: 3207168. Throughput: 0: 368.6. Samples: 3208288. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:04:07,561][187912] Avg episode reward: [(0, '757.561')]
[36m[2025-06-29 20:04:12,575][187912] Fps is (10 sec: 409.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3207168. Throughput: 0: 368.4. Samples: 3209488. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:04:12,575][187912] Avg episode reward: [(0, '799.722')]
[37m[1m[2025-06-29 20:04:12,621][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000012528_3207168.pth...
[36m[2025-06-29 20:04:12,684][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000012192_3121152.pth
[36m[2025-06-29 20:04:17,576][187912] Fps is (10 sec: 409.0, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 3211264. Throughput: 0: 366.4. Samples: 3211392. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 20:04:17,576][187912] Avg episode reward: [(0, '796.601')]
[31m[9014320 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9014320 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([12], device='cuda:0') (navigation_task.py:196)
[31m[9014321 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:04:22,590][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3211264. Throughput: 0: 366.5. Samples: 3213728. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 20:04:22,590][187912] Avg episode reward: [(0, '730.920')]
[36m[2025-06-29 20:04:27,582][187912] Fps is (10 sec: 409.3, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 3215360. Throughput: 0: 364.8. Samples: 3214848. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:04:27,582][187912] Avg episode reward: [(0, '742.715')]
[36m[2025-06-29 20:04:32,580][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3215360. Throughput: 0: 362.7. Samples: 3216912. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:04:32,581][187912] Avg episode reward: [(0, '691.407')]
[36m[2025-06-29 20:04:37,590][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 3215360. Throughput: 0: 361.5. Samples: 3219168. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:04:37,590][187912] Avg episode reward: [(0, '647.977')]
[36m[2025-06-29 20:04:42,581][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3219456. Throughput: 0: 362.0. Samples: 3220064. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 20:04:42,581][187912] Avg episode reward: [(0, '654.273')]
[36m[2025-06-29 20:04:47,595][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 3219456. Throughput: 0: 359.6. Samples: 3222400. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 20:04:47,596][187912] Avg episode reward: [(0, '640.263')]
[36m[2025-06-29 20:04:52,613][187912] Fps is (10 sec: 408.3, 60 sec: 409.4, 300 sec: 360.9). Total num frames: 3223552. Throughput: 0: 356.9. Samples: 3224368. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:04:52,613][187912] Avg episode reward: [(0, '600.062')]
[31m[9047361 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9047362 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[9047362 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:04:57,595][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3223552. Throughput: 0: 354.0. Samples: 3225424. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:04:57,595][187912] Avg episode reward: [(0, '631.439')]
[36m[2025-06-29 20:05:02,568][187912] Fps is (10 sec: 411.4, 60 sec: 409.6, 300 sec: 361.1). Total num frames: 3227648. Throughput: 0: 362.0. Samples: 3227680. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:05:02,568][187912] Avg episode reward: [(0, '678.029')]
[36m[2025-06-29 20:05:07,577][187912] Fps is (10 sec: 410.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3227648. Throughput: 0: 360.6. Samples: 3229952. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:05:07,577][187912] Avg episode reward: [(0, '736.330')]
[31m[9062834 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9062834 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[9062834 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:05:12,573][187912] Fps is (10 sec: 409.4, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3231744. Throughput: 0: 362.7. Samples: 3231168. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 20:05:12,573][187912] Avg episode reward: [(0, '683.996')]
[36m[2025-06-29 20:05:17,582][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3231744. Throughput: 0: 363.4. Samples: 3233264. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 20:05:17,582][187912] Avg episode reward: [(0, '750.773')]
[36m[2025-06-29 20:05:22,606][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 3231744. Throughput: 0: 363.6. Samples: 3235536. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 20:05:22,606][187912] Avg episode reward: [(0, '766.973')]
[36m[2025-06-29 20:05:27,575][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3235840. Throughput: 0: 361.6. Samples: 3236336. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 20:05:27,576][187912] Avg episode reward: [(0, '748.811')]
[36m[2025-06-29 20:05:32,573][187912] Fps is (10 sec: 410.9, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 3235840. Throughput: 0: 361.1. Samples: 3238640. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 20:05:32,573][187912] Avg episode reward: [(0, '767.847')]
[31m[9090856 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9090856 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[9090856 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:05:37,590][187912] Fps is (10 sec: 409.0, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3239936. Throughput: 0: 361.8. Samples: 3240640. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:05:37,590][187912] Avg episode reward: [(0, '802.888')]
[36m[2025-06-29 20:05:42,584][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3239936. Throughput: 0: 364.9. Samples: 3241840. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:05:42,584][187912] Avg episode reward: [(0, '762.210')]
[33m[9095892 ms][navigation_task] - WARNING : Curriculum Level: 35, Curriculum progress fraction: 0.5714285714285714 (navigation_task.py:262)
[33m[9095892 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.80224609375
[33mCrash Rate: 0.14990234375
[33mTimeout Rate: 0.0478515625 (navigation_task.py:265)
[33m[9095892 ms][navigation_task] - WARNING : 
[33mSuccesses: 1643
[33mCrashes : 307
[33mTimeouts: 98 (navigation_task.py:268)
[36m[2025-06-29 20:05:47,574][187912] Fps is (10 sec: 410.3, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 3244032. Throughput: 0: 364.0. Samples: 3244064. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 20:05:47,574][187912] Avg episode reward: [(0, '735.950')]
[36m[2025-06-29 20:05:52,563][187912] Fps is (10 sec: 410.5, 60 sec: 341.6, 300 sec: 361.0). Total num frames: 3244032. Throughput: 0: 361.0. Samples: 3246192. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 20:05:52,563][187912] Avg episode reward: [(0, '738.710')]
[36m[2025-06-29 20:05:57,757][187912] Fps is (10 sec: 402.2, 60 sec: 408.5, 300 sec: 360.8). Total num frames: 3248128. Throughput: 0: 358.7. Samples: 3247376. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:05:57,757][187912] Avg episode reward: [(0, '722.606')]
[36m[2025-06-29 20:06:02,564][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3248128. Throughput: 0: 362.1. Samples: 3249552. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:06:02,564][187912] Avg episode reward: [(0, '743.119')]
[36m[2025-06-29 20:06:07,562][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.7). Total num frames: 3248128. Throughput: 0: 363.4. Samples: 3251872. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:06:07,563][187912] Avg episode reward: [(0, '795.493')]
[36m[2025-06-29 20:06:12,568][187912] Fps is (10 sec: 409.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3252224. Throughput: 0: 364.5. Samples: 3252736. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 20:06:12,569][187912] Avg episode reward: [(0, '782.874')]
[37m[1m[2025-06-29 20:06:12,611][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000012704_3252224.pth...
[36m[2025-06-29 20:06:12,680][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000012368_3166208.pth
[36m[2025-06-29 20:06:17,574][187912] Fps is (10 sec: 409.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3252224. Throughput: 0: 366.9. Samples: 3255152. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 20:06:17,574][187912] Avg episode reward: [(0, '767.542')]
[36m[2025-06-29 20:06:22,586][187912] Fps is (10 sec: 408.9, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 3256320. Throughput: 0: 367.7. Samples: 3257184. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 20:06:22,587][187912] Avg episode reward: [(0, '745.711')]
[36m[2025-06-29 20:06:27,574][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3256320. Throughput: 0: 367.0. Samples: 3258352. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 20:06:27,575][187912] Avg episode reward: [(0, '755.758')]
[36m[2025-06-29 20:06:32,559][187912] Fps is (10 sec: 410.7, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 3260416. Throughput: 0: 364.2. Samples: 3260448. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 20:06:32,559][187912] Avg episode reward: [(0, '754.114')]
[36m[2025-06-29 20:06:37,580][187912] Fps is (10 sec: 409.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3260416. Throughput: 0: 370.4. Samples: 3262864. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 20:06:37,580][187912] Avg episode reward: [(0, '780.929')]
[36m[2025-06-29 20:06:42,579][187912] Fps is (10 sec: 408.8, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3264512. Throughput: 0: 371.6. Samples: 3264032. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 20:06:42,579][187912] Avg episode reward: [(0, '816.109')]
[37m[1m[2025-06-29 20:06:42,620][187912] Saving new best policy, reward=816.109!
[36m[2025-06-29 20:06:47,581][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3264512. Throughput: 0: 364.7. Samples: 3265968. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 20:06:47,581][187912] Avg episode reward: [(0, '832.635')]
[37m[1m[2025-06-29 20:06:47,621][187912] Saving new best policy, reward=832.635!
[36m[2025-06-29 20:06:52,606][187912] Fps is (10 sec: 0.0, 60 sec: 341.1, 300 sec: 360.9). Total num frames: 3264512. Throughput: 0: 362.0. Samples: 3268176. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 20:06:52,606][187912] Avg episode reward: [(0, '812.635')]
[36m[2025-06-29 20:06:57,574][187912] Fps is (10 sec: 409.9, 60 sec: 342.4, 300 sec: 361.0). Total num frames: 3268608. Throughput: 0: 360.5. Samples: 3268960. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 20:06:57,574][187912] Avg episode reward: [(0, '776.960')]
[36m[2025-06-29 20:07:02,595][187912] Fps is (10 sec: 410.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3268608. Throughput: 0: 356.1. Samples: 3271184. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 20:07:02,596][187912] Avg episode reward: [(0, '777.867')]
[36m[2025-06-29 20:07:07,590][187912] Fps is (10 sec: 409.0, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 3272704. Throughput: 0: 352.7. Samples: 3273056. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 20:07:07,590][187912] Avg episode reward: [(0, '747.041')]
[36m[2025-06-29 20:07:12,578][187912] Fps is (10 sec: 410.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3272704. Throughput: 0: 355.2. Samples: 3274336. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 20:07:12,579][187912] Avg episode reward: [(0, '766.836')]
[36m[2025-06-29 20:07:17,603][187912] Fps is (10 sec: 409.1, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 3276800. Throughput: 0: 362.7. Samples: 3276784. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:07:17,603][187912] Avg episode reward: [(0, '785.114')]
[36m[2025-06-29 20:07:22,577][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3276800. Throughput: 0: 356.6. Samples: 3278912. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:07:22,577][187912] Avg episode reward: [(0, '842.730')]
[37m[1m[2025-06-29 20:07:22,620][187912] Saving new best policy, reward=842.730!
[36m[2025-06-29 20:07:28,248][187912] Fps is (10 sec: 384.8, 60 sec: 405.1, 300 sec: 360.2). Total num frames: 3280896. Throughput: 0: 349.6. Samples: 3280000. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:07:28,248][187912] Avg episode reward: [(0, '832.864')]
[36m[2025-06-29 20:07:32,583][187912] Fps is (10 sec: 409.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3280896. Throughput: 0: 358.0. Samples: 3282080. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:07:32,583][187912] Avg episode reward: [(0, '875.411')]
[37m[1m[2025-06-29 20:07:32,627][187912] Saving new best policy, reward=875.411!
[36m[2025-06-29 20:07:37,586][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3280896. Throughput: 0: 358.2. Samples: 3284288. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:07:37,586][187912] Avg episode reward: [(0, '839.860')]
[36m[2025-06-29 20:07:42,597][187912] Fps is (10 sec: 409.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3284992. Throughput: 0: 360.7. Samples: 3285200. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 20:07:42,598][187912] Avg episode reward: [(0, '845.796')]
[36m[2025-06-29 20:07:47,558][187912] Fps is (10 sec: 410.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3284992. Throughput: 0: 363.0. Samples: 3287504. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 20:07:47,558][187912] Avg episode reward: [(0, '797.819')]
[36m[2025-06-29 20:07:52,587][187912] Fps is (10 sec: 410.0, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 3289088. Throughput: 0: 368.7. Samples: 3289648. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:07:52,587][187912] Avg episode reward: [(0, '842.660')]
[36m[2025-06-29 20:07:57,567][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 361.1). Total num frames: 3289088. Throughput: 0: 364.9. Samples: 3290752. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:07:57,567][187912] Avg episode reward: [(0, '783.793')]
[36m[2025-06-29 20:08:02,591][187912] Fps is (10 sec: 409.4, 60 sec: 409.6, 300 sec: 361.4). Total num frames: 3293184. Throughput: 0: 360.3. Samples: 3292992. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 20:08:02,591][187912] Avg episode reward: [(0, '784.298')]
[36m[2025-06-29 20:08:07,560][187912] Fps is (10 sec: 409.9, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3293184. Throughput: 0: 359.2. Samples: 3295072. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 20:08:07,561][187912] Avg episode reward: [(0, '802.907')]
[36m[2025-06-29 20:08:12,599][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3293184. Throughput: 0: 364.7. Samples: 3296176. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 20:08:12,599][187912] Avg episode reward: [(0, '794.866')]
[37m[1m[2025-06-29 20:08:12,603][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000012864_3293184.pth...
[36m[2025-06-29 20:08:12,673][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000012528_3207168.pth
[36m[2025-06-29 20:08:17,562][187912] Fps is (10 sec: 409.5, 60 sec: 341.6, 300 sec: 361.0). Total num frames: 3297280. Throughput: 0: 359.3. Samples: 3298240. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 20:08:17,562][187912] Avg episode reward: [(0, '755.559')]
[36m[2025-06-29 20:08:22,579][187912] Fps is (10 sec: 410.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3297280. Throughput: 0: 360.9. Samples: 3300528. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 20:08:22,580][187912] Avg episode reward: [(0, '795.667')]
[36m[2025-06-29 20:08:27,590][187912] Fps is (10 sec: 408.5, 60 sec: 345.1, 300 sec: 361.0). Total num frames: 3301376. Throughput: 0: 360.2. Samples: 3301408. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:08:27,590][187912] Avg episode reward: [(0, '837.450')]
[31m[9264420 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9264421 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[9264421 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:08:32,588][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3301376. Throughput: 0: 361.4. Samples: 3303776. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:08:32,588][187912] Avg episode reward: [(0, '808.044')]
[36m[2025-06-29 20:08:37,592][187912] Fps is (10 sec: 409.5, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3305472. Throughput: 0: 354.8. Samples: 3305616. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:08:37,592][187912] Avg episode reward: [(0, '812.795')]
[36m[2025-06-29 20:08:42,578][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3305472. Throughput: 0: 357.6. Samples: 3306848. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:08:42,578][187912] Avg episode reward: [(0, '825.833')]
[36m[2025-06-29 20:08:48,020][187912] Fps is (10 sec: 392.8, 60 sec: 406.5, 300 sec: 374.3). Total num frames: 3309568. Throughput: 0: 354.7. Samples: 3309104. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:08:48,021][187912] Avg episode reward: [(0, '800.264')]
[36m[2025-06-29 20:08:52,560][187912] Fps is (10 sec: 410.3, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3309568. Throughput: 0: 357.3. Samples: 3311152. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:08:52,560][187912] Avg episode reward: [(0, '858.687')]
[36m[2025-06-29 20:08:57,579][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3309568. Throughput: 0: 358.2. Samples: 3312288. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:08:57,579][187912] Avg episode reward: [(0, '812.141')]
[36m[2025-06-29 20:09:02,578][187912] Fps is (10 sec: 408.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3313664. Throughput: 0: 355.4. Samples: 3314240. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:09:02,579][187912] Avg episode reward: [(0, '777.391')]
[36m[2025-06-29 20:09:07,586][187912] Fps is (10 sec: 409.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3313664. Throughput: 0: 356.2. Samples: 3316560. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:09:07,586][187912] Avg episode reward: [(0, '810.709')]
[36m[2025-06-29 20:09:12,582][187912] Fps is (10 sec: 409.4, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 3317760. Throughput: 0: 363.1. Samples: 3317744. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 20:09:12,583][187912] Avg episode reward: [(0, '801.678')]
[36m[2025-06-29 20:09:17,573][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3317760. Throughput: 0: 353.9. Samples: 3319696. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 20:09:17,573][187912] Avg episode reward: [(0, '737.002')]
[36m[2025-06-29 20:09:22,650][187912] Fps is (10 sec: 406.9, 60 sec: 409.1, 300 sec: 360.9). Total num frames: 3321856. Throughput: 0: 337.3. Samples: 3320816. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 20:09:22,650][187912] Avg episode reward: [(0, '736.173')]
[31m[9318130 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9318131 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[9318131 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:09:27,565][187912] Fps is (10 sec: 409.9, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3321856. Throughput: 0: 354.6. Samples: 3322800. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 20:09:27,565][187912] Avg episode reward: [(0, '774.532')]
[36m[2025-06-29 20:09:32,598][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3321856. Throughput: 0: 356.8. Samples: 3325008. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 20:09:32,598][187912] Avg episode reward: [(0, '759.851')]
[36m[2025-06-29 20:09:37,573][187912] Fps is (10 sec: 409.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3325952. Throughput: 0: 353.7. Samples: 3327072. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:09:37,574][187912] Avg episode reward: [(0, '766.717')]
[36m[2025-06-29 20:09:42,568][187912] Fps is (10 sec: 410.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3325952. Throughput: 0: 354.6. Samples: 3328240. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:09:42,568][187912] Avg episode reward: [(0, '815.350')]
[36m[2025-06-29 20:09:47,566][187912] Fps is (10 sec: 409.9, 60 sec: 343.9, 300 sec: 361.1). Total num frames: 3330048. Throughput: 0: 353.9. Samples: 3330160. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 20:09:47,566][187912] Avg episode reward: [(0, '803.765')]
[36m[2025-06-29 20:09:52,565][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3330048. Throughput: 0: 354.3. Samples: 3332496. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 20:09:52,565][187912] Avg episode reward: [(0, '799.263')]
[36m[2025-06-29 20:09:57,579][187912] Fps is (10 sec: 409.1, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3334144. Throughput: 0: 352.0. Samples: 3333584. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 20:09:57,579][187912] Avg episode reward: [(0, '797.273')]
[36m[2025-06-29 20:10:02,559][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3334144. Throughput: 0: 356.7. Samples: 3335744. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 20:10:02,560][187912] Avg episode reward: [(0, '788.845')]
[36m[2025-06-29 20:10:08,151][187912] Fps is (10 sec: 387.4, 60 sec: 405.8, 300 sec: 360.3). Total num frames: 3338240. Throughput: 0: 380.5. Samples: 3338128. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 20:10:08,152][187912] Avg episode reward: [(0, '799.967')]
[36m[2025-06-29 20:10:12,569][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3338240. Throughput: 0: 361.2. Samples: 3339056. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 20:10:12,569][187912] Avg episode reward: [(0, '840.292')]
[37m[1m[2025-06-29 20:10:12,611][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000013040_3338240.pth...
[36m[2025-06-29 20:10:12,678][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000012704_3252224.pth
[36m[2025-06-29 20:10:17,574][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3338240. Throughput: 0: 362.1. Samples: 3341296. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 20:10:17,574][187912] Avg episode reward: [(0, '826.246')]
[36m[2025-06-29 20:10:22,589][187912] Fps is (10 sec: 408.8, 60 sec: 341.7, 300 sec: 361.0). Total num frames: 3342336. Throughput: 0: 362.2. Samples: 3343376. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:10:22,589][187912] Avg episode reward: [(0, '808.834')]
[36m[2025-06-29 20:10:27,588][187912] Fps is (10 sec: 409.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3342336. Throughput: 0: 362.2. Samples: 3344544. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:10:27,588][187912] Avg episode reward: [(0, '832.199')]
[31m[9383130 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9383130 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[9383130 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:10:32,591][187912] Fps is (10 sec: 409.5, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3346432. Throughput: 0: 366.0. Samples: 3346640. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 20:10:32,592][187912] Avg episode reward: [(0, '810.342')]
[36m[2025-06-29 20:10:37,594][187912] Fps is (10 sec: 409.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3346432. Throughput: 0: 367.8. Samples: 3349056. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 20:10:37,594][187912] Avg episode reward: [(0, '815.035')]
[36m[2025-06-29 20:10:42,567][187912] Fps is (10 sec: 410.6, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3350528. Throughput: 0: 369.9. Samples: 3350224. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 20:10:42,568][187912] Avg episode reward: [(0, '832.614')]
[36m[2025-06-29 20:10:47,592][187912] Fps is (10 sec: 409.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3350528. Throughput: 0: 365.6. Samples: 3352208. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 20:10:47,592][187912] Avg episode reward: [(0, '894.528')]
[37m[1m[2025-06-29 20:10:47,632][187912] Saving new best policy, reward=894.528!
[36m[2025-06-29 20:10:52,835][187912] Fps is (10 sec: 398.9, 60 sec: 407.8, 300 sec: 360.9). Total num frames: 3354624. Throughput: 0: 341.6. Samples: 3353392. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 20:10:52,835][187912] Avg episode reward: [(0, '891.863')]
[36m[2025-06-29 20:10:57,592][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3354624. Throughput: 0: 367.1. Samples: 3355584. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 20:10:57,593][187912] Avg episode reward: [(0, '900.811')]
[37m[1m[2025-06-29 20:10:57,644][187912] Saving new best policy, reward=900.811!
[36m[2025-06-29 20:11:02,571][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3354624. Throughput: 0: 365.9. Samples: 3357760. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 20:11:02,572][187912] Avg episode reward: [(0, '855.299')]
[36m[2025-06-29 20:11:07,586][187912] Fps is (10 sec: 409.9, 60 sec: 344.6, 300 sec: 361.0). Total num frames: 3358720. Throughput: 0: 364.5. Samples: 3359776. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:11:07,586][187912] Avg episode reward: [(0, '795.314')]
[36m[2025-06-29 20:11:12,595][187912] Fps is (10 sec: 408.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3358720. Throughput: 0: 364.4. Samples: 3360944. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:11:12,595][187912] Avg episode reward: [(0, '777.129')]
[36m[2025-06-29 20:11:17,577][187912] Fps is (10 sec: 410.0, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3362816. Throughput: 0: 362.1. Samples: 3362928. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:11:17,577][187912] Avg episode reward: [(0, '755.762')]
[36m[2025-06-29 20:11:22,575][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3362816. Throughput: 0: 360.3. Samples: 3365264. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:11:22,575][187912] Avg episode reward: [(0, '773.859')]
[36m[2025-06-29 20:11:27,575][187912] Fps is (10 sec: 409.7, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 3366912. Throughput: 0: 358.3. Samples: 3366352. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:11:27,575][187912] Avg episode reward: [(0, '787.902')]
[36m[2025-06-29 20:11:32,578][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3366912. Throughput: 0: 361.4. Samples: 3368464. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:11:32,578][187912] Avg episode reward: [(0, '823.624')]
[36m[2025-06-29 20:11:38,272][187912] Fps is (10 sec: 382.9, 60 sec: 405.0, 300 sec: 360.2). Total num frames: 3371008. Throughput: 0: 384.5. Samples: 3370864. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 20:11:38,273][187912] Avg episode reward: [(0, '848.558')]
[36m[2025-06-29 20:11:42,591][187912] Fps is (10 sec: 409.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3371008. Throughput: 0: 359.8. Samples: 3371776. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 20:11:42,592][187912] Avg episode reward: [(0, '858.942')]
[36m[2025-06-29 20:11:47,577][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3371008. Throughput: 0: 361.6. Samples: 3374032. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 20:11:47,577][187912] Avg episode reward: [(0, '830.490')]
[36m[2025-06-29 20:11:52,589][187912] Fps is (10 sec: 409.7, 60 sec: 342.7, 300 sec: 361.0). Total num frames: 3375104. Throughput: 0: 361.6. Samples: 3376048. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 20:11:52,589][187912] Avg episode reward: [(0, '797.597')]
[36m[2025-06-29 20:11:57,568][187912] Fps is (10 sec: 410.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3375104. Throughput: 0: 361.5. Samples: 3377200. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 20:11:57,568][187912] Avg episode reward: [(0, '786.788')]
[31m[9470875 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9470875 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[9470876 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[9473485 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9473485 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[9473486 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:12:02,583][187912] Fps is (10 sec: 409.8, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 3379200. Throughput: 0: 362.3. Samples: 3379232. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 20:12:02,584][187912] Avg episode reward: [(0, '773.158')]
[36m[2025-06-29 20:12:07,584][187912] Fps is (10 sec: 408.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3379200. Throughput: 0: 361.2. Samples: 3381520. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 20:12:07,584][187912] Avg episode reward: [(0, '767.572')]
[36m[2025-06-29 20:12:12,604][187912] Fps is (10 sec: 408.7, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 3383296. Throughput: 0: 363.1. Samples: 3382704. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:12:12,605][187912] Avg episode reward: [(0, '797.815')]
[37m[1m[2025-06-29 20:12:12,609][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000013216_3383296.pth...
[36m[2025-06-29 20:12:12,689][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000012864_3293184.pth
[36m[2025-06-29 20:12:17,582][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3383296. Throughput: 0: 359.8. Samples: 3384656. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:12:17,583][187912] Avg episode reward: [(0, '778.942')]
[36m[2025-06-29 20:12:22,579][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.9). Total num frames: 3383296. Throughput: 0: 358.2. Samples: 3386736. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:12:22,580][187912] Avg episode reward: [(0, '781.925')]
[36m[2025-06-29 20:12:27,560][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3387392. Throughput: 0: 351.2. Samples: 3387568. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:12:27,560][187912] Avg episode reward: [(0, '793.020')]
[36m[2025-06-29 20:12:32,568][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3387392. Throughput: 0: 352.4. Samples: 3389888. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:12:32,568][187912] Avg episode reward: [(0, '758.851')]
[36m[2025-06-29 20:12:37,579][187912] Fps is (10 sec: 408.8, 60 sec: 345.3, 300 sec: 361.0). Total num frames: 3391488. Throughput: 0: 347.5. Samples: 3391680. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 20:12:37,579][187912] Avg episode reward: [(0, '756.503')]
[36m[2025-06-29 20:12:42,561][187912] Fps is (10 sec: 409.9, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3391488. Throughput: 0: 346.4. Samples: 3392784. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 20:12:42,561][187912] Avg episode reward: [(0, '723.424')]
[36m[2025-06-29 20:12:47,569][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 3391488. Throughput: 0: 349.6. Samples: 3394960. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 20:12:47,569][187912] Avg episode reward: [(0, '742.358')]
[31m[9520928 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9520929 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[9520929 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:12:52,585][187912] Fps is (10 sec: 408.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3395584. Throughput: 0: 344.5. Samples: 3397024. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:12:52,585][187912] Avg episode reward: [(0, '685.604')]
[36m[2025-06-29 20:12:57,586][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 3395584. Throughput: 0: 341.8. Samples: 3398080. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:12:57,586][187912] Avg episode reward: [(0, '681.164')]
[36m[2025-06-29 20:13:02,587][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3399680. Throughput: 0: 343.8. Samples: 3400128. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:13:02,587][187912] Avg episode reward: [(0, '683.409')]
[36m[2025-06-29 20:13:07,563][187912] Fps is (10 sec: 410.6, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3399680. Throughput: 0: 348.9. Samples: 3402432. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:13:07,563][187912] Avg episode reward: [(0, '739.417')]
[36m[2025-06-29 20:13:12,578][187912] Fps is (10 sec: 410.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3403776. Throughput: 0: 356.5. Samples: 3403616. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:13:12,579][187912] Avg episode reward: [(0, '783.197')]
[36m[2025-06-29 20:13:17,558][187912] Fps is (10 sec: 409.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3403776. Throughput: 0: 351.4. Samples: 3405696. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:13:17,558][187912] Avg episode reward: [(0, '787.465')]
[36m[2025-06-29 20:13:22,580][187912] Fps is (10 sec: 409.5, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3407872. Throughput: 0: 360.5. Samples: 3407904. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:13:22,581][187912] Avg episode reward: [(0, '795.255')]
[36m[2025-06-29 20:13:27,578][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3407872. Throughput: 0: 358.3. Samples: 3408912. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:13:27,579][187912] Avg episode reward: [(0, '745.008')]
[36m[2025-06-29 20:13:32,562][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 3407872. Throughput: 0: 360.2. Samples: 3411168. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:13:32,562][187912] Avg episode reward: [(0, '748.529')]
[36m[2025-06-29 20:13:37,597][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3411968. Throughput: 0: 358.3. Samples: 3413152. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:13:37,598][187912] Avg episode reward: [(0, '736.917')]
[36m[2025-06-29 20:13:42,565][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 347.7). Total num frames: 3411968. Throughput: 0: 358.6. Samples: 3414208. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:13:42,565][187912] Avg episode reward: [(0, '746.343')]
[36m[2025-06-29 20:13:47,572][187912] Fps is (10 sec: 410.6, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3416064. Throughput: 0: 356.7. Samples: 3416176. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 20:13:47,572][187912] Avg episode reward: [(0, '819.760')]
[36m[2025-06-29 20:13:52,584][187912] Fps is (10 sec: 408.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3416064. Throughput: 0: 356.1. Samples: 3418464. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 20:13:52,585][187912] Avg episode reward: [(0, '795.346')]
[36m[2025-06-29 20:13:57,656][187912] Fps is (10 sec: 406.2, 60 sec: 409.1, 300 sec: 360.9). Total num frames: 3420160. Throughput: 0: 353.2. Samples: 3419536. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 20:13:57,656][187912] Avg episode reward: [(0, '823.122')]
[36m[2025-06-29 20:14:02,579][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3420160. Throughput: 0: 352.5. Samples: 3421568. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 20:14:02,580][187912] Avg episode reward: [(0, '816.335')]
[36m[2025-06-29 20:14:07,569][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 3420160. Throughput: 0: 356.0. Samples: 3423920. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 20:14:07,569][187912] Avg episode reward: [(0, '793.979')]
[36m[2025-06-29 20:14:12,584][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3424256. Throughput: 0: 352.7. Samples: 3424784. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:14:12,584][187912] Avg episode reward: [(0, '744.100')]
[37m[1m[2025-06-29 20:14:12,624][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000013376_3424256.pth...
[36m[2025-06-29 20:14:12,686][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000013040_3338240.pth
[31m[9606950 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9606951 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[9606951 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:14:17,582][187912] Fps is (10 sec: 409.1, 60 sec: 341.2, 300 sec: 347.2). Total num frames: 3424256. Throughput: 0: 352.6. Samples: 3427040. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:14:17,582][187912] Avg episode reward: [(0, '749.014')]
[36m[2025-06-29 20:14:22,592][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3428352. Throughput: 0: 352.0. Samples: 3428992. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:14:22,593][187912] Avg episode reward: [(0, '758.074')]
[33m[9620472 ms][navigation_task] - WARNING : Curriculum Level: 37, Curriculum progress fraction: 0.6285714285714286 (navigation_task.py:262)
[33m[9620472 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.82568359375
[33mCrash Rate: 0.14892578125
[33mTimeout Rate: 0.025390625 (navigation_task.py:265)
[33m[9620472 ms][navigation_task] - WARNING : 
[33mSuccesses: 1691
[33mCrashes : 305
[33mTimeouts: 52 (navigation_task.py:268)
[36m[2025-06-29 20:14:27,568][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3428352. Throughput: 0: 356.2. Samples: 3430240. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:14:27,568][187912] Avg episode reward: [(0, '796.247')]
[36m[2025-06-29 20:14:32,590][187912] Fps is (10 sec: 409.7, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 3432448. Throughput: 0: 362.2. Samples: 3432480. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:14:32,590][187912] Avg episode reward: [(0, '824.904')]
[36m[2025-06-29 20:14:37,577][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3432448. Throughput: 0: 354.9. Samples: 3434432. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:14:37,577][187912] Avg episode reward: [(0, '837.399')]
[36m[2025-06-29 20:14:42,588][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 3432448. Throughput: 0: 356.1. Samples: 3435536. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:14:42,588][187912] Avg episode reward: [(0, '880.066')]
[36m[2025-06-29 20:14:47,594][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3436544. Throughput: 0: 356.1. Samples: 3437600. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 20:14:47,594][187912] Avg episode reward: [(0, '899.735')]
[36m[2025-06-29 20:14:52,583][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 3436544. Throughput: 0: 354.7. Samples: 3439888. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 20:14:52,583][187912] Avg episode reward: [(0, '889.376')]
[36m[2025-06-29 20:14:57,581][187912] Fps is (10 sec: 410.2, 60 sec: 341.8, 300 sec: 361.0). Total num frames: 3440640. Throughput: 0: 353.4. Samples: 3440688. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 20:14:57,581][187912] Avg episode reward: [(0, '937.529')]
[37m[1m[2025-06-29 20:14:57,620][187912] Saving new best policy, reward=937.529!
[36m[2025-06-29 20:15:02,561][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 347.8). Total num frames: 3440640. Throughput: 0: 354.7. Samples: 3442992. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 20:15:02,561][187912] Avg episode reward: [(0, '865.119')]
[36m[2025-06-29 20:15:07,572][187912] Fps is (10 sec: 410.0, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3444736. Throughput: 0: 354.3. Samples: 3444928. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:15:07,572][187912] Avg episode reward: [(0, '833.364')]
[36m[2025-06-29 20:15:12,595][187912] Fps is (10 sec: 408.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3444736. Throughput: 0: 354.3. Samples: 3446192. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:15:12,595][187912] Avg episode reward: [(0, '833.263')]
[36m[2025-06-29 20:15:17,928][187912] Fps is (10 sec: 395.5, 60 sec: 407.2, 300 sec: 360.6). Total num frames: 3448832. Throughput: 0: 351.1. Samples: 3448400. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 20:15:17,929][187912] Avg episode reward: [(0, '854.030')]
[36m[2025-06-29 20:15:22,565][187912] Fps is (10 sec: 410.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3448832. Throughput: 0: 356.0. Samples: 3450448. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 20:15:22,566][187912] Avg episode reward: [(0, '784.751')]
[36m[2025-06-29 20:15:27,596][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 3448832. Throughput: 0: 359.0. Samples: 3451696. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 20:15:27,596][187912] Avg episode reward: [(0, '855.061')]
[31m[9683105 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9683105 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[9683105 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:15:32,578][187912] Fps is (10 sec: 409.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3452928. Throughput: 0: 360.7. Samples: 3453824. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:15:32,579][187912] Avg episode reward: [(0, '815.967')]
[36m[2025-06-29 20:15:37,571][187912] Fps is (10 sec: 410.6, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 3452928. Throughput: 0: 360.3. Samples: 3456096. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:15:37,571][187912] Avg episode reward: [(0, '805.849')]
[36m[2025-06-29 20:15:42,594][187912] Fps is (10 sec: 409.0, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3457024. Throughput: 0: 363.6. Samples: 3457056. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:15:42,594][187912] Avg episode reward: [(0, '752.530')]
[36m[2025-06-29 20:15:47,590][187912] Fps is (10 sec: 408.8, 60 sec: 341.4, 300 sec: 347.4). Total num frames: 3457024. Throughput: 0: 361.0. Samples: 3459248. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:15:47,590][187912] Avg episode reward: [(0, '810.272')]
[36m[2025-06-29 20:15:52,583][187912] Fps is (10 sec: 410.0, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3461120. Throughput: 0: 363.3. Samples: 3461280. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:15:52,583][187912] Avg episode reward: [(0, '782.114')]
[36m[2025-06-29 20:15:57,568][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3461120. Throughput: 0: 362.2. Samples: 3462480. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:15:57,569][187912] Avg episode reward: [(0, '764.875')]
[36m[2025-06-29 20:16:03,276][187912] Fps is (10 sec: 383.1, 60 sec: 404.8, 300 sec: 360.2). Total num frames: 3465216. Throughput: 0: 358.8. Samples: 3464672. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 20:16:03,276][187912] Avg episode reward: [(0, '739.868')]
[36m[2025-06-29 20:16:07,581][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3465216. Throughput: 0: 363.2. Samples: 3466800. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 20:16:07,582][187912] Avg episode reward: [(0, '754.541')]
[36m[2025-06-29 20:16:12,618][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 3465216. Throughput: 0: 361.8. Samples: 3467984. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 20:16:12,618][187912] Avg episode reward: [(0, '738.476')]
[37m[1m[2025-06-29 20:16:12,625][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000013536_3465216.pth...
[36m[2025-06-29 20:16:12,696][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000013216_3383296.pth
[36m[2025-06-29 20:16:17,576][187912] Fps is (10 sec: 409.8, 60 sec: 343.4, 300 sec: 361.0). Total num frames: 3469312. Throughput: 0: 357.4. Samples: 3469904. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:16:17,576][187912] Avg episode reward: [(0, '697.181')]
[36m[2025-06-29 20:16:22,558][187912] Fps is (10 sec: 412.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 3469312. Throughput: 0: 356.0. Samples: 3472112. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:16:22,559][187912] Avg episode reward: [(0, '742.116')]
[36m[2025-06-29 20:16:27,575][187912] Fps is (10 sec: 409.6, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 3473408. Throughput: 0: 360.0. Samples: 3473248. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:16:27,575][187912] Avg episode reward: [(0, '786.221')]
[36m[2025-06-29 20:16:32,561][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 348.0). Total num frames: 3473408. Throughput: 0: 356.9. Samples: 3475296. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:16:32,561][187912] Avg episode reward: [(0, '763.306')]
[36m[2025-06-29 20:16:37,601][187912] Fps is (10 sec: 408.5, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 3477504. Throughput: 0: 336.2. Samples: 3476416. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 20:16:37,601][187912] Avg episode reward: [(0, '759.177')]
[36m[2025-06-29 20:16:42,576][187912] Fps is (10 sec: 409.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3477504. Throughput: 0: 356.2. Samples: 3478512. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 20:16:42,577][187912] Avg episode reward: [(0, '790.213')]
[36m[2025-06-29 20:16:47,573][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 3477504. Throughput: 0: 362.6. Samples: 3480736. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 20:16:47,573][187912] Avg episode reward: [(0, '826.950')]
[36m[2025-06-29 20:16:52,590][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3481600. Throughput: 0: 357.3. Samples: 3482880. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:16:52,591][187912] Avg episode reward: [(0, '844.355')]
[36m[2025-06-29 20:16:57,606][187912] Fps is (10 sec: 408.3, 60 sec: 341.1, 300 sec: 347.1). Total num frames: 3481600. Throughput: 0: 354.6. Samples: 3483936. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:16:57,606][187912] Avg episode reward: [(0, '829.991')]
[36m[2025-06-29 20:17:02,576][187912] Fps is (10 sec: 410.2, 60 sec: 345.4, 300 sec: 361.0). Total num frames: 3485696. Throughput: 0: 355.2. Samples: 3485888. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 20:17:02,576][187912] Avg episode reward: [(0, '866.904')]
[36m[2025-06-29 20:17:07,576][187912] Fps is (10 sec: 410.8, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 3485696. Throughput: 0: 352.9. Samples: 3488000. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 20:17:07,576][187912] Avg episode reward: [(0, '844.086')]
[36m[2025-06-29 20:17:12,630][187912] Fps is (10 sec: 407.4, 60 sec: 409.5, 300 sec: 360.9). Total num frames: 3489792. Throughput: 0: 352.3. Samples: 3489120. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 20:17:12,630][187912] Avg episode reward: [(0, '822.696')]
[36m[2025-06-29 20:17:17,573][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3489792. Throughput: 0: 353.3. Samples: 3491200. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 20:17:17,573][187912] Avg episode reward: [(0, '735.071')]
[36m[2025-06-29 20:17:22,562][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 3489792. Throughput: 0: 377.2. Samples: 3493376. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 20:17:22,562][187912] Avg episode reward: [(0, '725.460')]
[36m[2025-06-29 20:17:27,562][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3493888. Throughput: 0: 348.9. Samples: 3494208. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 20:17:27,562][187912] Avg episode reward: [(0, '720.532')]
[36m[2025-06-29 20:17:32,568][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 3493888. Throughput: 0: 349.5. Samples: 3496464. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 20:17:32,568][187912] Avg episode reward: [(0, '741.474')]
[36m[2025-06-29 20:17:37,565][187912] Fps is (10 sec: 409.5, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3497984. Throughput: 0: 345.8. Samples: 3498432. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:17:37,565][187912] Avg episode reward: [(0, '760.763')]
[36m[2025-06-29 20:17:42,584][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3497984. Throughput: 0: 349.3. Samples: 3499648. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:17:42,584][187912] Avg episode reward: [(0, '831.111')]
[36m[2025-06-29 20:17:47,570][187912] Fps is (10 sec: 409.4, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3502080. Throughput: 0: 358.1. Samples: 3502000. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:17:47,570][187912] Avg episode reward: [(0, '852.629')]
[31m[9822508 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9822508 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[9822508 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:17:52,599][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3502080. Throughput: 0: 357.5. Samples: 3504096. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:17:52,599][187912] Avg episode reward: [(0, '824.232')]
[36m[2025-06-29 20:17:57,590][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 3502080. Throughput: 0: 356.6. Samples: 3505152. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:17:57,590][187912] Avg episode reward: [(0, '836.753')]
[36m[2025-06-29 20:18:02,579][187912] Fps is (10 sec: 410.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3506176. Throughput: 0: 355.9. Samples: 3507216. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 20:18:02,579][187912] Avg episode reward: [(0, '817.996')]
[36m[2025-06-29 20:18:07,574][187912] Fps is (10 sec: 410.3, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 3506176. Throughput: 0: 359.4. Samples: 3509552. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 20:18:07,574][187912] Avg episode reward: [(0, '758.438')]
[36m[2025-06-29 20:18:12,590][187912] Fps is (10 sec: 409.1, 60 sec: 341.6, 300 sec: 361.0). Total num frames: 3510272. Throughput: 0: 360.0. Samples: 3510416. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:18:12,590][187912] Avg episode reward: [(0, '730.237')]
[37m[1m[2025-06-29 20:18:12,631][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000013712_3510272.pth...
[36m[2025-06-29 20:18:12,691][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000013376_3424256.pth
[36m[2025-06-29 20:18:17,561][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 3510272. Throughput: 0: 360.9. Samples: 3512704. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:18:17,562][187912] Avg episode reward: [(0, '778.401')]
[36m[2025-06-29 20:18:22,578][187912] Fps is (10 sec: 410.1, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 3514368. Throughput: 0: 364.0. Samples: 3514816. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:18:22,578][187912] Avg episode reward: [(0, '793.402')]
[36m[2025-06-29 20:18:27,564][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3514368. Throughput: 0: 364.3. Samples: 3516032. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:18:27,564][187912] Avg episode reward: [(0, '777.620')]
[36m[2025-06-29 20:18:32,595][187912] Fps is (10 sec: 408.9, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 3518464. Throughput: 0: 364.6. Samples: 3518416. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 20:18:32,596][187912] Avg episode reward: [(0, '850.953')]
[36m[2025-06-29 20:18:37,568][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3518464. Throughput: 0: 362.2. Samples: 3520384. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 20:18:37,568][187912] Avg episode reward: [(0, '845.335')]
[36m[2025-06-29 20:18:42,569][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 3518464. Throughput: 0: 365.3. Samples: 3521584. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 20:18:42,569][187912] Avg episode reward: [(0, '830.395')]
[36m[2025-06-29 20:18:47,575][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3522560. Throughput: 0: 365.2. Samples: 3523648. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 20:18:47,576][187912] Avg episode reward: [(0, '861.337')]
[36m[2025-06-29 20:18:52,575][187912] Fps is (10 sec: 409.3, 60 sec: 341.5, 300 sec: 347.2). Total num frames: 3522560. Throughput: 0: 364.1. Samples: 3525936. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 20:18:52,575][187912] Avg episode reward: [(0, '844.948')]
[36m[2025-06-29 20:18:57,575][187912] Fps is (10 sec: 409.6, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 3526656. Throughput: 0: 362.4. Samples: 3526720. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 20:18:57,575][187912] Avg episode reward: [(0, '868.638')]
[36m[2025-06-29 20:19:02,582][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3526656. Throughput: 0: 361.8. Samples: 3528992. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 20:19:02,582][187912] Avg episode reward: [(0, '868.642')]
[36m[2025-06-29 20:19:07,565][187912] Fps is (10 sec: 410.0, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 3530752. Throughput: 0: 355.3. Samples: 3530800. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 20:19:07,565][187912] Avg episode reward: [(0, '860.223')]
[36m[2025-06-29 20:19:12,582][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3530752. Throughput: 0: 353.3. Samples: 3531936. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 20:19:12,582][187912] Avg episode reward: [(0, '837.972')]
[36m[2025-06-29 20:19:17,571][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 3530752. Throughput: 0: 349.0. Samples: 3534112. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 20:19:17,571][187912] Avg episode reward: [(0, '866.398')]
[36m[2025-06-29 20:19:22,590][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3534848. Throughput: 0: 347.9. Samples: 3536048. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:19:22,590][187912] Avg episode reward: [(0, '803.342')]
[36m[2025-06-29 20:19:27,586][187912] Fps is (10 sec: 409.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 3534848. Throughput: 0: 346.9. Samples: 3537200. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:19:27,587][187912] Avg episode reward: [(0, '774.631')]
[36m[2025-06-29 20:19:32,579][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3538944. Throughput: 0: 341.3. Samples: 3539008. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 20:19:32,579][187912] Avg episode reward: [(0, '779.598')]
[31m[9930546 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9930546 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[9930546 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:19:37,589][187912] Fps is (10 sec: 409.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3538944. Throughput: 0: 343.0. Samples: 3541376. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 20:19:37,589][187912] Avg episode reward: [(0, '814.536')]
[36m[2025-06-29 20:19:42,563][187912] Fps is (10 sec: 410.2, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3543040. Throughput: 0: 353.2. Samples: 3542608. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:19:42,564][187912] Avg episode reward: [(0, '812.561')]
[36m[2025-06-29 20:19:47,560][187912] Fps is (10 sec: 410.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3543040. Throughput: 0: 348.6. Samples: 3544672. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:19:47,561][187912] Avg episode reward: [(0, '835.738')]
[31m[9944627 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9944627 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[9944627 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:19:52,577][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 3543040. Throughput: 0: 357.9. Samples: 3546912. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:19:52,577][187912] Avg episode reward: [(0, '841.370')]
[31m[9949598 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[9949598 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[9949598 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:19:57,562][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3547136. Throughput: 0: 352.5. Samples: 3547792. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 20:19:57,563][187912] Avg episode reward: [(0, '847.959')]
[36m[2025-06-29 20:20:02,562][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 3547136. Throughput: 0: 350.6. Samples: 3549888. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 20:20:02,562][187912] Avg episode reward: [(0, '806.873')]
[36m[2025-06-29 20:20:07,572][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3551232. Throughput: 0: 353.9. Samples: 3551968. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:20:07,572][187912] Avg episode reward: [(0, '789.915')]
[36m[2025-06-29 20:20:12,560][187912] Fps is (10 sec: 409.7, 60 sec: 341.5, 300 sec: 347.6). Total num frames: 3551232. Throughput: 0: 354.0. Samples: 3553120. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:20:12,560][187912] Avg episode reward: [(0, '767.488')]
[37m[1m[2025-06-29 20:20:12,604][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000013872_3551232.pth...
[36m[2025-06-29 20:20:12,660][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000013536_3465216.pth
[36m[2025-06-29 20:20:17,565][187912] Fps is (10 sec: 409.9, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3555328. Throughput: 0: 362.1. Samples: 3555296. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 20:20:17,565][187912] Avg episode reward: [(0, '781.413')]
[36m[2025-06-29 20:20:22,566][187912] Fps is (10 sec: 409.4, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3555328. Throughput: 0: 352.5. Samples: 3557232. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 20:20:22,566][187912] Avg episode reward: [(0, '778.362')]
[36m[2025-06-29 20:20:27,587][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 3555328. Throughput: 0: 350.4. Samples: 3558384. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 20:20:27,588][187912] Avg episode reward: [(0, '776.595')]
[36m[2025-06-29 20:20:32,564][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3559424. Throughput: 0: 352.3. Samples: 3560528. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 20:20:32,564][187912] Avg episode reward: [(0, '734.867')]
[36m[2025-06-29 20:20:37,575][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 3559424. Throughput: 0: 350.2. Samples: 3562672. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 20:20:37,575][187912] Avg episode reward: [(0, '734.756')]
[36m[2025-06-29 20:20:42,570][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3563520. Throughput: 0: 350.2. Samples: 3563552. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 20:20:42,571][187912] Avg episode reward: [(0, '718.846')]
[36m[2025-06-29 20:20:47,578][187912] Fps is (10 sec: 409.5, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 3563520. Throughput: 0: 353.7. Samples: 3565808. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 20:20:47,578][187912] Avg episode reward: [(0, '722.153')]
[36m[2025-06-29 20:20:52,579][187912] Fps is (10 sec: 409.2, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3567616. Throughput: 0: 353.7. Samples: 3567888. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 20:20:52,579][187912] Avg episode reward: [(0, '753.992')]
[36m[2025-06-29 20:20:57,594][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 347.9). Total num frames: 3567616. Throughput: 0: 353.2. Samples: 3569024. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 20:20:57,594][187912] Avg episode reward: [(0, '780.951')]
[36m[2025-06-29 20:21:03,170][187912] Fps is (10 sec: 386.7, 60 sec: 405.5, 300 sec: 360.3). Total num frames: 3571712. Throughput: 0: 351.2. Samples: 3571312. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:21:03,171][187912] Avg episode reward: [(0, '771.247')]
[36m[2025-06-29 20:21:07,590][187912] Fps is (10 sec: 409.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3571712. Throughput: 0: 357.5. Samples: 3573328. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:21:07,590][187912] Avg episode reward: [(0, '788.296')]
[36m[2025-06-29 20:21:12,601][187912] Fps is (10 sec: 0.0, 60 sec: 341.1, 300 sec: 347.1). Total num frames: 3571712. Throughput: 0: 355.8. Samples: 3574400. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:21:12,601][187912] Avg episode reward: [(0, '753.381')]
[36m[2025-06-29 20:21:17,585][187912] Fps is (10 sec: 409.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3575808. Throughput: 0: 350.1. Samples: 3576288. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 20:21:17,585][187912] Avg episode reward: [(0, '797.084')]
[36m[2025-06-29 20:21:22,581][187912] Fps is (10 sec: 410.4, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 3575808. Throughput: 0: 355.9. Samples: 3578688. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 20:21:22,581][187912] Avg episode reward: [(0, '756.317')]
[36m[2025-06-29 20:21:27,570][187912] Fps is (10 sec: 410.2, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 3579904. Throughput: 0: 359.8. Samples: 3579744. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:21:27,570][187912] Avg episode reward: [(0, '801.536')]
[36m[2025-06-29 20:21:32,570][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 347.2). Total num frames: 3579904. Throughput: 0: 356.7. Samples: 3581856. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:21:32,571][187912] Avg episode reward: [(0, '745.088')]
[36m[2025-06-29 20:21:37,669][187912] Fps is (10 sec: 405.6, 60 sec: 409.0, 300 sec: 360.9). Total num frames: 3584000. Throughput: 0: 334.3. Samples: 3582960. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:21:37,669][187912] Avg episode reward: [(0, '748.666')]
[36m[2025-06-29 20:21:42,586][187912] Fps is (10 sec: 409.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3584000. Throughput: 0: 354.2. Samples: 3584960. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:21:42,586][187912] Avg episode reward: [(0, '734.341')]
[36m[2025-06-29 20:21:47,608][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 3584000. Throughput: 0: 356.1. Samples: 3587136. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:21:47,609][187912] Avg episode reward: [(0, '752.295')]
[31m[10063271 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[10063271 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[10063272 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:21:52,577][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3588096. Throughput: 0: 353.5. Samples: 3589232. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:21:52,577][187912] Avg episode reward: [(0, '724.336')]
[36m[2025-06-29 20:21:57,567][187912] Fps is (10 sec: 411.3, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 3588096. Throughput: 0: 356.2. Samples: 3590416. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:21:57,567][187912] Avg episode reward: [(0, '767.198')]
[36m[2025-06-29 20:22:02,559][187912] Fps is (10 sec: 410.3, 60 sec: 344.8, 300 sec: 361.0). Total num frames: 3592192. Throughput: 0: 354.7. Samples: 3592240. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:22:02,560][187912] Avg episode reward: [(0, '801.256')]
[31m[10079244 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[10079244 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[10079244 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:22:07,580][187912] Fps is (10 sec: 409.0, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 3592192. Throughput: 0: 352.4. Samples: 3594544. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:22:07,580][187912] Avg episode reward: [(0, '792.458')]
[36m[2025-06-29 20:22:12,566][187912] Fps is (10 sec: 409.3, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 3596288. Throughput: 0: 354.9. Samples: 3595712. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:22:12,566][187912] Avg episode reward: [(0, '782.086')]
[37m[1m[2025-06-29 20:22:12,607][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000014048_3596288.pth...
[36m[2025-06-29 20:22:12,671][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000013712_3510272.pth
[36m[2025-06-29 20:22:17,578][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3596288. Throughput: 0: 351.6. Samples: 3597680. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:22:17,578][187912] Avg episode reward: [(0, '761.469')]
[36m[2025-06-29 20:22:22,596][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 3596288. Throughput: 0: 377.9. Samples: 3599936. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:22:22,596][187912] Avg episode reward: [(0, '767.906')]
[36m[2025-06-29 20:22:27,569][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3600384. Throughput: 0: 351.8. Samples: 3600784. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 20:22:27,569][187912] Avg episode reward: [(0, '786.237')]
[36m[2025-06-29 20:22:32,583][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 3600384. Throughput: 0: 352.2. Samples: 3602976. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 20:22:32,584][187912] Avg episode reward: [(0, '797.679')]
[36m[2025-06-29 20:22:37,567][187912] Fps is (10 sec: 409.7, 60 sec: 341.9, 300 sec: 361.0). Total num frames: 3604480. Throughput: 0: 346.0. Samples: 3604800. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:22:37,567][187912] Avg episode reward: [(0, '838.638')]
[36m[2025-06-29 20:22:42,586][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 3604480. Throughput: 0: 345.1. Samples: 3605952. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:22:42,586][187912] Avg episode reward: [(0, '841.251')]
[36m[2025-06-29 20:22:47,717][187912] Fps is (10 sec: 403.5, 60 sec: 408.9, 300 sec: 360.9). Total num frames: 3608576. Throughput: 0: 354.0. Samples: 3608224. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 20:22:47,718][187912] Avg episode reward: [(0, '886.470')]
[36m[2025-06-29 20:22:52,561][187912] Fps is (10 sec: 410.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3608576. Throughput: 0: 352.5. Samples: 3610400. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 20:22:52,561][187912] Avg episode reward: [(0, '826.300')]
[36m[2025-06-29 20:22:57,583][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 3608576. Throughput: 0: 350.8. Samples: 3611504. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 20:22:57,583][187912] Avg episode reward: [(0, '816.373')]
[36m[2025-06-29 20:23:02,581][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3612672. Throughput: 0: 350.9. Samples: 3613472. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 20:23:02,581][187912] Avg episode reward: [(0, '806.797')]
[36m[2025-06-29 20:23:07,570][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 3612672. Throughput: 0: 350.8. Samples: 3615712. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 20:23:07,570][187912] Avg episode reward: [(0, '818.665')]
[36m[2025-06-29 20:23:12,579][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3616768. Throughput: 0: 355.1. Samples: 3616768. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 20:23:12,579][187912] Avg episode reward: [(0, '789.116')]
[33m[10146091 ms][navigation_task] - WARNING : Curriculum Level: 39, Curriculum progress fraction: 0.6857142857142857 (navigation_task.py:262)
[33m[10146092 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.810546875
[33mCrash Rate: 0.16015625
[33mTimeout Rate: 0.029296875 (navigation_task.py:265)
[33m[10146092 ms][navigation_task] - WARNING : 
[33mSuccesses: 1660
[33mCrashes : 328
[33mTimeouts: 60 (navigation_task.py:268)
[36m[2025-06-29 20:23:17,598][187912] Fps is (10 sec: 408.4, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 3616768. Throughput: 0: 351.9. Samples: 3618816. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 20:23:17,598][187912] Avg episode reward: [(0, '874.729')]
[36m[2025-06-29 20:23:22,594][187912] Fps is (10 sec: 409.0, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3620864. Throughput: 0: 357.5. Samples: 3620896. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:23:22,594][187912] Avg episode reward: [(0, '906.055')]
[36m[2025-06-29 20:23:27,609][187912] Fps is (10 sec: 409.1, 60 sec: 341.1, 300 sec: 347.1). Total num frames: 3620864. Throughput: 0: 357.5. Samples: 3622048. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:23:27,610][187912] Avg episode reward: [(0, '910.220')]
[36m[2025-06-29 20:23:32,575][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 3620864. Throughput: 0: 358.8. Samples: 3624320. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:23:32,575][187912] Avg episode reward: [(0, '882.620')]
[36m[2025-06-29 20:23:37,594][187912] Fps is (10 sec: 410.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3624960. Throughput: 0: 352.8. Samples: 3626288. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:23:37,594][187912] Avg episode reward: [(0, '907.004')]
[36m[2025-06-29 20:23:42,587][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 3624960. Throughput: 0: 353.0. Samples: 3627392. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:23:42,588][187912] Avg episode reward: [(0, '840.670')]
[36m[2025-06-29 20:23:47,597][187912] Fps is (10 sec: 409.5, 60 sec: 342.0, 300 sec: 361.0). Total num frames: 3629056. Throughput: 0: 352.9. Samples: 3629360. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:23:47,597][187912] Avg episode reward: [(0, '795.733')]
[36m[2025-06-29 20:23:52,608][187912] Fps is (10 sec: 408.7, 60 sec: 341.1, 300 sec: 347.1). Total num frames: 3629056. Throughput: 0: 355.6. Samples: 3631728. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:23:52,609][187912] Avg episode reward: [(0, '864.785')]
[36m[2025-06-29 20:23:57,573][187912] Fps is (10 sec: 410.6, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 3633152. Throughput: 0: 355.6. Samples: 3632768. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:23:57,574][187912] Avg episode reward: [(0, '829.778')]
[36m[2025-06-29 20:24:02,561][187912] Fps is (10 sec: 411.5, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 3633152. Throughput: 0: 356.2. Samples: 3634832. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:24:02,561][187912] Avg episode reward: [(0, '862.609')]
[36m[2025-06-29 20:24:08,215][187912] Fps is (10 sec: 384.9, 60 sec: 405.2, 300 sec: 360.2). Total num frames: 3637248. Throughput: 0: 355.6. Samples: 3637120. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:24:08,216][187912] Avg episode reward: [(0, '819.213')]
[36m[2025-06-29 20:24:12,590][187912] Fps is (10 sec: 408.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3637248. Throughput: 0: 354.3. Samples: 3637984. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:24:12,591][187912] Avg episode reward: [(0, '912.349')]
[37m[1m[2025-06-29 20:24:12,642][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000014208_3637248.pth...
[36m[2025-06-29 20:24:12,707][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000013872_3551232.pth
[36m[2025-06-29 20:24:17,593][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 3637248. Throughput: 0: 351.9. Samples: 3640160. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:24:17,593][187912] Avg episode reward: [(0, '875.130')]
[36m[2025-06-29 20:24:22,575][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3641344. Throughput: 0: 353.6. Samples: 3642192. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:24:22,575][187912] Avg episode reward: [(0, '890.735')]
[36m[2025-06-29 20:24:27,561][187912] Fps is (10 sec: 410.9, 60 sec: 341.6, 300 sec: 347.1). Total num frames: 3641344. Throughput: 0: 354.7. Samples: 3643344. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:24:27,561][187912] Avg episode reward: [(0, '865.306')]
[36m[2025-06-29 20:24:32,600][187912] Fps is (10 sec: 408.6, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 3645440. Throughput: 0: 358.0. Samples: 3645472. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:24:32,601][187912] Avg episode reward: [(0, '896.550')]
[36m[2025-06-29 20:24:37,594][187912] Fps is (10 sec: 408.2, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 3645440. Throughput: 0: 354.6. Samples: 3647680. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:24:37,594][187912] Avg episode reward: [(0, '843.580')]
[36m[2025-06-29 20:24:42,919][187912] Fps is (10 sec: 397.0, 60 sec: 407.4, 300 sec: 360.6). Total num frames: 3649536. Throughput: 0: 353.2. Samples: 3648784. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:24:42,919][187912] Avg episode reward: [(0, '851.044')]
[36m[2025-06-29 20:24:47,560][187912] Fps is (10 sec: 411.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3649536. Throughput: 0: 355.6. Samples: 3650832. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:24:47,560][187912] Avg episode reward: [(0, '846.005')]
[31m[10243054 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[10243054 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([12], device='cuda:0') (navigation_task.py:196)
[31m[10243055 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:24:52,575][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 3649536. Throughput: 0: 361.8. Samples: 3653168. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:24:52,576][187912] Avg episode reward: [(0, '853.171')]
[36m[2025-06-29 20:24:57,564][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3653632. Throughput: 0: 357.2. Samples: 3654048. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:24:57,564][187912] Avg episode reward: [(0, '832.765')]
[36m[2025-06-29 20:25:02,587][187912] Fps is (10 sec: 409.1, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 3653632. Throughput: 0: 358.8. Samples: 3656304. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:25:02,588][187912] Avg episode reward: [(0, '812.429')]
[36m[2025-06-29 20:25:07,589][187912] Fps is (10 sec: 408.5, 60 sec: 344.9, 300 sec: 361.0). Total num frames: 3657728. Throughput: 0: 360.1. Samples: 3658400. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:25:07,590][187912] Avg episode reward: [(0, '841.005')]
[36m[2025-06-29 20:25:12,572][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 3657728. Throughput: 0: 360.1. Samples: 3659552. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:25:12,573][187912] Avg episode reward: [(0, '845.776')]
[36m[2025-06-29 20:25:17,592][187912] Fps is (10 sec: 409.5, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3661824. Throughput: 0: 364.2. Samples: 3661856. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 20:25:17,592][187912] Avg episode reward: [(0, '861.741')]
[36m[2025-06-29 20:25:22,565][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3661824. Throughput: 0: 361.1. Samples: 3663920. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 20:25:22,565][187912] Avg episode reward: [(0, '846.936')]
[36m[2025-06-29 20:25:28,025][187912] Fps is (10 sec: 392.6, 60 sec: 406.5, 300 sec: 360.4). Total num frames: 3665920. Throughput: 0: 362.5. Samples: 3665136. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:25:28,026][187912] Avg episode reward: [(0, '812.943')]
[36m[2025-06-29 20:25:32,585][187912] Fps is (10 sec: 408.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3665920. Throughput: 0: 359.6. Samples: 3667024. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:25:32,586][187912] Avg episode reward: [(0, '746.613')]
[36m[2025-06-29 20:25:37,594][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 3665920. Throughput: 0: 359.0. Samples: 3669328. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:25:37,594][187912] Avg episode reward: [(0, '756.080')]
[36m[2025-06-29 20:25:42,582][187912] Fps is (10 sec: 409.7, 60 sec: 343.3, 300 sec: 361.0). Total num frames: 3670016. Throughput: 0: 360.0. Samples: 3670256. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:25:42,582][187912] Avg episode reward: [(0, '729.893')]
[36m[2025-06-29 20:25:47,569][187912] Fps is (10 sec: 410.6, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 3670016. Throughput: 0: 362.1. Samples: 3672592. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:25:47,570][187912] Avg episode reward: [(0, '796.733')]
[36m[2025-06-29 20:25:52,591][187912] Fps is (10 sec: 409.2, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 3674112. Throughput: 0: 362.3. Samples: 3674704. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 20:25:52,591][187912] Avg episode reward: [(0, '809.969')]
[36m[2025-06-29 20:25:57,588][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 347.8). Total num frames: 3674112. Throughput: 0: 362.9. Samples: 3675888. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 20:25:57,588][187912] Avg episode reward: [(0, '855.510')]
[36m[2025-06-29 20:26:02,580][187912] Fps is (10 sec: 410.1, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 3678208. Throughput: 0: 363.1. Samples: 3678192. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 20:26:02,580][187912] Avg episode reward: [(0, '874.029')]
[36m[2025-06-29 20:26:07,576][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3678208. Throughput: 0: 361.2. Samples: 3680176. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 20:26:07,576][187912] Avg episode reward: [(0, '888.688')]
[31m[10322401 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[10322401 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[10322402 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:26:12,562][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 3678208. Throughput: 0: 362.1. Samples: 3681264. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 20:26:12,562][187912] Avg episode reward: [(0, '887.994')]
[37m[1m[2025-06-29 20:26:12,604][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000014368_3678208.pth...
[36m[2025-06-29 20:26:12,666][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000014048_3596288.pth
[36m[2025-06-29 20:26:17,594][187912] Fps is (10 sec: 408.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3682304. Throughput: 0: 362.6. Samples: 3683344. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:26:17,594][187912] Avg episode reward: [(0, '871.213')]
[36m[2025-06-29 20:26:22,566][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 3682304. Throughput: 0: 359.7. Samples: 3685504. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:26:22,566][187912] Avg episode reward: [(0, '886.032')]
[36m[2025-06-29 20:26:27,565][187912] Fps is (10 sec: 410.8, 60 sec: 344.0, 300 sec: 361.0). Total num frames: 3686400. Throughput: 0: 359.6. Samples: 3686432. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:26:27,565][187912] Avg episode reward: [(0, '852.053')]
[36m[2025-06-29 20:26:32,581][187912] Fps is (10 sec: 409.0, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 3686400. Throughput: 0: 355.8. Samples: 3688608. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:26:32,581][187912] Avg episode reward: [(0, '818.579')]
[36m[2025-06-29 20:26:37,569][187912] Fps is (10 sec: 409.5, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 3690496. Throughput: 0: 352.2. Samples: 3690544. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 20:26:37,569][187912] Avg episode reward: [(0, '784.837')]
[36m[2025-06-29 20:26:42,569][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 361.1). Total num frames: 3690496. Throughput: 0: 351.4. Samples: 3691696. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 20:26:42,569][187912] Avg episode reward: [(0, '788.720')]
[36m[2025-06-29 20:26:47,590][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 3690496. Throughput: 0: 350.9. Samples: 3693984. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 20:26:47,590][187912] Avg episode reward: [(0, '754.846')]
[31m[10364161 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[10364161 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[10364161 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:26:52,566][187912] Fps is (10 sec: 409.7, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3694592. Throughput: 0: 351.4. Samples: 3695984. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:26:52,566][187912] Avg episode reward: [(0, '753.306')]
[36m[2025-06-29 20:26:57,592][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 3694592. Throughput: 0: 352.5. Samples: 3697136. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:26:57,592][187912] Avg episode reward: [(0, '779.797')]
[36m[2025-06-29 20:27:02,580][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3698688. Throughput: 0: 350.0. Samples: 3699088. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 20:27:02,580][187912] Avg episode reward: [(0, '791.580')]
[36m[2025-06-29 20:27:07,592][187912] Fps is (10 sec: 409.6, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 3698688. Throughput: 0: 352.5. Samples: 3701376. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 20:27:07,592][187912] Avg episode reward: [(0, '835.319')]
[36m[2025-06-29 20:27:12,591][187912] Fps is (10 sec: 409.1, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 3702784. Throughput: 0: 357.5. Samples: 3702528. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 20:27:12,591][187912] Avg episode reward: [(0, '880.682')]
[36m[2025-06-29 20:27:17,567][187912] Fps is (10 sec: 410.6, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3702784. Throughput: 0: 356.4. Samples: 3704640. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 20:27:17,567][187912] Avg episode reward: [(0, '823.949')]
[36m[2025-06-29 20:27:22,575][187912] Fps is (10 sec: 410.2, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 3706880. Throughput: 0: 363.7. Samples: 3706912. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:27:22,576][187912] Avg episode reward: [(0, '841.996')]
[36m[2025-06-29 20:27:27,568][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3706880. Throughput: 0: 360.9. Samples: 3707936. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:27:27,568][187912] Avg episode reward: [(0, '835.143')]
[36m[2025-06-29 20:27:32,564][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 3706880. Throughput: 0: 362.2. Samples: 3710272. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:27:32,565][187912] Avg episode reward: [(0, '807.677')]
[36m[2025-06-29 20:27:37,579][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3710976. Throughput: 0: 359.4. Samples: 3712160. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 20:27:37,580][187912] Avg episode reward: [(0, '825.004')]
[36m[2025-06-29 20:27:42,563][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 347.3). Total num frames: 3710976. Throughput: 0: 356.1. Samples: 3713152. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 20:27:42,563][187912] Avg episode reward: [(0, '831.168')]
[36m[2025-06-29 20:27:47,584][187912] Fps is (10 sec: 409.4, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3715072. Throughput: 0: 355.9. Samples: 3715104. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 20:27:47,584][187912] Avg episode reward: [(0, '863.240')]
[31m[10424197 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[10424198 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[10424198 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:27:52,563][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3715072. Throughput: 0: 357.2. Samples: 3717440. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 20:27:52,563][187912] Avg episode reward: [(0, '839.215')]
[31m[10428943 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[10428943 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[10428943 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:27:57,575][187912] Fps is (10 sec: 409.9, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 3719168. Throughput: 0: 357.8. Samples: 3718624. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 20:27:57,576][187912] Avg episode reward: [(0, '860.319')]
[36m[2025-06-29 20:28:02,599][187912] Fps is (10 sec: 408.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3719168. Throughput: 0: 355.3. Samples: 3720640. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 20:28:02,599][187912] Avg episode reward: [(0, '831.873')]
[31m[10439781 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[10439781 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[10439781 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:28:07,562][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 3719168. Throughput: 0: 358.2. Samples: 3723024. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 20:28:07,562][187912] Avg episode reward: [(0, '848.613')]
[36m[2025-06-29 20:28:12,564][187912] Fps is (10 sec: 411.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3723264. Throughput: 0: 355.2. Samples: 3723920. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:28:12,564][187912] Avg episode reward: [(0, '861.983')]
[37m[1m[2025-06-29 20:28:12,604][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000014544_3723264.pth...
[36m[2025-06-29 20:28:12,674][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000014208_3637248.pth
[36m[2025-06-29 20:28:17,588][187912] Fps is (10 sec: 408.5, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 3723264. Throughput: 0: 354.3. Samples: 3726224. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:28:17,588][187912] Avg episode reward: [(0, '841.837')]
[36m[2025-06-29 20:28:22,567][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.1). Total num frames: 3727360. Throughput: 0: 360.3. Samples: 3728368. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:28:22,567][187912] Avg episode reward: [(0, '855.997')]
[36m[2025-06-29 20:28:27,564][187912] Fps is (10 sec: 410.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3727360. Throughput: 0: 364.1. Samples: 3729536. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:28:27,564][187912] Avg episode reward: [(0, '870.912')]
[36m[2025-06-29 20:28:32,577][187912] Fps is (10 sec: 409.2, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 3731456. Throughput: 0: 367.0. Samples: 3731616. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:28:32,577][187912] Avg episode reward: [(0, '842.629')]
[36m[2025-06-29 20:28:37,579][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3731456. Throughput: 0: 367.2. Samples: 3733968. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:28:37,579][187912] Avg episode reward: [(0, '795.629')]
[36m[2025-06-29 20:28:42,558][187912] Fps is (10 sec: 410.4, 60 sec: 409.6, 300 sec: 361.1). Total num frames: 3735552. Throughput: 0: 365.3. Samples: 3735056. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 20:28:42,559][187912] Avg episode reward: [(0, '830.239')]
[36m[2025-06-29 20:28:47,563][187912] Fps is (10 sec: 410.2, 60 sec: 341.5, 300 sec: 361.1). Total num frames: 3735552. Throughput: 0: 367.6. Samples: 3737168. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 20:28:47,563][187912] Avg episode reward: [(0, '807.570')]
[36m[2025-06-29 20:28:52,562][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 3735552. Throughput: 0: 364.8. Samples: 3739440. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 20:28:52,562][187912] Avg episode reward: [(0, '751.236')]
[36m[2025-06-29 20:28:57,565][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3739648. Throughput: 0: 366.2. Samples: 3740400. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 20:28:57,565][187912] Avg episode reward: [(0, '779.410')]
[36m[2025-06-29 20:29:02,583][187912] Fps is (10 sec: 408.8, 60 sec: 341.4, 300 sec: 347.9). Total num frames: 3739648. Throughput: 0: 367.3. Samples: 3742752. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 20:29:02,583][187912] Avg episode reward: [(0, '798.132')]
[36m[2025-06-29 20:29:07,569][187912] Fps is (10 sec: 409.4, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3743744. Throughput: 0: 365.1. Samples: 3744800. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:29:07,569][187912] Avg episode reward: [(0, '750.438')]
[36m[2025-06-29 20:29:12,564][187912] Fps is (10 sec: 410.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3743744. Throughput: 0: 363.7. Samples: 3745904. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:29:12,564][187912] Avg episode reward: [(0, '778.952')]
[36m[2025-06-29 20:29:17,586][187912] Fps is (10 sec: 408.9, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3747840. Throughput: 0: 362.2. Samples: 3747920. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:29:17,587][187912] Avg episode reward: [(0, '764.309')]
[36m[2025-06-29 20:29:22,597][187912] Fps is (10 sec: 408.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3747840. Throughput: 0: 359.3. Samples: 3750144. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:29:22,598][187912] Avg episode reward: [(0, '748.491')]
[36m[2025-06-29 20:29:27,610][187912] Fps is (10 sec: 408.6, 60 sec: 409.3, 300 sec: 361.0). Total num frames: 3751936. Throughput: 0: 359.4. Samples: 3751248. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 20:29:27,610][187912] Avg episode reward: [(0, '747.941')]
[36m[2025-06-29 20:29:32,561][187912] Fps is (10 sec: 411.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3751936. Throughput: 0: 359.8. Samples: 3753360. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 20:29:32,562][187912] Avg episode reward: [(0, '785.409')]
[36m[2025-06-29 20:29:37,574][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.5). Total num frames: 3751936. Throughput: 0: 360.8. Samples: 3755680. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 20:29:37,575][187912] Avg episode reward: [(0, '821.965')]
[36m[2025-06-29 20:29:42,568][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3756032. Throughput: 0: 359.8. Samples: 3756592. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 20:29:42,568][187912] Avg episode reward: [(0, '849.298')]
[36m[2025-06-29 20:29:47,590][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3756032. Throughput: 0: 358.3. Samples: 3758880. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 20:29:47,591][187912] Avg episode reward: [(0, '898.034')]
[36m[2025-06-29 20:29:52,588][187912] Fps is (10 sec: 408.7, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 3760128. Throughput: 0: 358.6. Samples: 3760944. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 20:29:52,589][187912] Avg episode reward: [(0, '930.496')]
[36m[2025-06-29 20:29:57,581][187912] Fps is (10 sec: 410.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3760128. Throughput: 0: 360.4. Samples: 3762128. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 20:29:57,582][187912] Avg episode reward: [(0, '966.899')]
[37m[1m[2025-06-29 20:29:57,621][187912] Saving new best policy, reward=966.899!
[36m[2025-06-29 20:30:02,573][187912] Fps is (10 sec: 410.2, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 3764224. Throughput: 0: 363.1. Samples: 3764256. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 20:30:02,573][187912] Avg episode reward: [(0, '954.630')]
[36m[2025-06-29 20:30:07,563][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3764224. Throughput: 0: 359.7. Samples: 3766320. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 20:30:07,564][187912] Avg episode reward: [(0, '963.210')]
[36m[2025-06-29 20:30:13,137][187912] Fps is (10 sec: 387.7, 60 sec: 405.7, 300 sec: 360.3). Total num frames: 3768320. Throughput: 0: 356.7. Samples: 3767488. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 20:30:13,138][187912] Avg episode reward: [(0, '1001.648')]
[37m[1m[2025-06-29 20:30:13,181][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000014720_3768320.pth...
[36m[2025-06-29 20:30:13,252][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000014368_3678208.pth
[37m[1m[2025-06-29 20:30:13,258][187912] Saving new best policy, reward=1001.648!
[36m[2025-06-29 20:30:17,582][187912] Fps is (10 sec: 408.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3768320. Throughput: 0: 358.6. Samples: 3769504. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 20:30:17,582][187912] Avg episode reward: [(0, '979.959')]
[36m[2025-06-29 20:30:22,564][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.7). Total num frames: 3768320. Throughput: 0: 359.9. Samples: 3771872. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 20:30:22,564][187912] Avg episode reward: [(0, '983.047')]
[36m[2025-06-29 20:30:27,592][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3772416. Throughput: 0: 358.9. Samples: 3772752. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 20:30:27,593][187912] Avg episode reward: [(0, '982.087')]
[36m[2025-06-29 20:30:32,592][187912] Fps is (10 sec: 408.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3772416. Throughput: 0: 362.7. Samples: 3775200. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 20:30:32,592][187912] Avg episode reward: [(0, '874.401')]
[36m[2025-06-29 20:30:37,583][187912] Fps is (10 sec: 410.0, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 3776512. Throughput: 0: 361.6. Samples: 3777216. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 20:30:37,583][187912] Avg episode reward: [(0, '860.104')]
[36m[2025-06-29 20:30:42,586][187912] Fps is (10 sec: 409.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3776512. Throughput: 0: 362.3. Samples: 3778432. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 20:30:42,586][187912] Avg episode reward: [(0, '844.445')]
[36m[2025-06-29 20:30:47,601][187912] Fps is (10 sec: 408.8, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 3780608. Throughput: 0: 363.9. Samples: 3780640. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 20:30:47,602][187912] Avg episode reward: [(0, '864.940')]
[36m[2025-06-29 20:30:52,579][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3780608. Throughput: 0: 366.1. Samples: 3782800. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 20:30:52,579][187912] Avg episode reward: [(0, '857.276')]
[36m[2025-06-29 20:30:57,933][187912] Fps is (10 sec: 396.5, 60 sec: 407.2, 300 sec: 360.6). Total num frames: 3784704. Throughput: 0: 367.9. Samples: 3783968. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 20:30:57,933][187912] Avg episode reward: [(0, '892.480')]
[36m[2025-06-29 20:31:02,579][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3784704. Throughput: 0: 364.1. Samples: 3785888. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 20:31:02,580][187912] Avg episode reward: [(0, '826.997')]
[36m[2025-06-29 20:31:07,575][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3784704. Throughput: 0: 362.9. Samples: 3788208. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 20:31:07,575][187912] Avg episode reward: [(0, '803.671')]
[36m[2025-06-29 20:31:12,584][187912] Fps is (10 sec: 409.4, 60 sec: 344.5, 300 sec: 361.0). Total num frames: 3788800. Throughput: 0: 363.1. Samples: 3789088. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 20:31:12,584][187912] Avg episode reward: [(0, '807.518')]
[36m[2025-06-29 20:31:17,587][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3788800. Throughput: 0: 364.5. Samples: 3791600. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 20:31:17,587][187912] Avg episode reward: [(0, '763.261')]
[36m[2025-06-29 20:31:22,580][187912] Fps is (10 sec: 409.8, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 3792896. Throughput: 0: 369.1. Samples: 3793824. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 20:31:22,580][187912] Avg episode reward: [(0, '757.519')]
[36m[2025-06-29 20:31:27,571][187912] Fps is (10 sec: 410.2, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3792896. Throughput: 0: 368.5. Samples: 3795008. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 20:31:27,571][187912] Avg episode reward: [(0, '774.884')]
[36m[2025-06-29 20:31:32,588][187912] Fps is (10 sec: 409.3, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3796992. Throughput: 0: 365.6. Samples: 3797088. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:31:32,589][187912] Avg episode reward: [(0, '794.542')]
[36m[2025-06-29 20:31:37,576][187912] Fps is (10 sec: 409.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3796992. Throughput: 0: 369.8. Samples: 3799440. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:31:37,576][187912] Avg episode reward: [(0, '787.865')]
[36m[2025-06-29 20:31:42,566][187912] Fps is (10 sec: 410.5, 60 sec: 409.7, 300 sec: 374.9). Total num frames: 3801088. Throughput: 0: 370.7. Samples: 3800512. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:31:42,566][187912] Avg episode reward: [(0, '787.078')]
[36m[2025-06-29 20:31:47,558][187912] Fps is (10 sec: 410.3, 60 sec: 341.6, 300 sec: 361.0). Total num frames: 3801088. Throughput: 0: 370.7. Samples: 3802560. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:31:47,559][187912] Avg episode reward: [(0, '823.089')]
[33m[10665530 ms][navigation_task] - WARNING : Curriculum Level: 41, Curriculum progress fraction: 0.7428571428571429 (navigation_task.py:262)
[33m[10665531 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.8052708506584167
[33mCrash Rate: 0.1527574360370636
[33mTimeout Rate: 0.04197169467806816 (navigation_task.py:265)
[33m[10665531 ms][navigation_task] - WARNING : 
[33mSuccesses: 1650
[33mCrashes : 313
[33mTimeouts: 86 (navigation_task.py:268)
[36m[2025-06-29 20:31:52,580][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3801088. Throughput: 0: 367.2. Samples: 3804736. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:31:52,580][187912] Avg episode reward: [(0, '827.022')]
[36m[2025-06-29 20:31:57,561][187912] Fps is (10 sec: 409.5, 60 sec: 343.5, 300 sec: 361.0). Total num frames: 3805184. Throughput: 0: 368.5. Samples: 3805664. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 20:31:57,561][187912] Avg episode reward: [(0, '864.388')]
[36m[2025-06-29 20:32:02,578][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3805184. Throughput: 0: 362.0. Samples: 3807888. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 20:32:02,578][187912] Avg episode reward: [(0, '840.404')]
[36m[2025-06-29 20:32:07,586][187912] Fps is (10 sec: 408.6, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 3809280. Throughput: 0: 357.3. Samples: 3809904. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 20:32:07,587][187912] Avg episode reward: [(0, '856.265')]
[36m[2025-06-29 20:32:12,576][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3809280. Throughput: 0: 357.6. Samples: 3811104. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 20:32:12,577][187912] Avg episode reward: [(0, '810.341')]
[37m[1m[2025-06-29 20:32:12,627][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000014880_3809280.pth...
[36m[2025-06-29 20:32:12,689][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000014544_3723264.pth
[36m[2025-06-29 20:32:17,576][187912] Fps is (10 sec: 410.0, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 3813376. Throughput: 0: 357.4. Samples: 3813168. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 20:32:17,576][187912] Avg episode reward: [(0, '798.298')]
[36m[2025-06-29 20:32:22,571][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3813376. Throughput: 0: 352.0. Samples: 3815280. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 20:32:22,572][187912] Avg episode reward: [(0, '755.936')]
[36m[2025-06-29 20:32:27,568][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3813376. Throughput: 0: 353.4. Samples: 3816416. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 20:32:27,568][187912] Avg episode reward: [(0, '766.948')]
[36m[2025-06-29 20:32:32,564][187912] Fps is (10 sec: 409.9, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3817472. Throughput: 0: 355.2. Samples: 3818544. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:32:32,564][187912] Avg episode reward: [(0, '721.910')]
[36m[2025-06-29 20:32:37,571][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3817472. Throughput: 0: 358.5. Samples: 3820864. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:32:37,571][187912] Avg episode reward: [(0, '743.184')]
[36m[2025-06-29 20:32:42,575][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3821568. Throughput: 0: 358.3. Samples: 3821792. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:32:42,575][187912] Avg episode reward: [(0, '756.684')]
[36m[2025-06-29 20:32:47,577][187912] Fps is (10 sec: 409.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3821568. Throughput: 0: 360.2. Samples: 3824096. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:32:47,577][187912] Avg episode reward: [(0, '813.229')]
[36m[2025-06-29 20:32:52,562][187912] Fps is (10 sec: 410.1, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 3825664. Throughput: 0: 362.5. Samples: 3826208. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:32:52,563][187912] Avg episode reward: [(0, '768.324')]
[36m[2025-06-29 20:32:57,563][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3825664. Throughput: 0: 361.4. Samples: 3827360. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:32:57,563][187912] Avg episode reward: [(0, '826.528')]
[36m[2025-06-29 20:33:02,591][187912] Fps is (10 sec: 408.4, 60 sec: 409.5, 300 sec: 374.9). Total num frames: 3829760. Throughput: 0: 367.9. Samples: 3829728. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:33:02,592][187912] Avg episode reward: [(0, '859.934')]
[36m[2025-06-29 20:33:07,561][187912] Fps is (10 sec: 409.7, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3829760. Throughput: 0: 366.0. Samples: 3831744. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:33:07,561][187912] Avg episode reward: [(0, '847.811')]
[36m[2025-06-29 20:33:12,568][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3829760. Throughput: 0: 366.6. Samples: 3832912. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:33:12,568][187912] Avg episode reward: [(0, '788.778')]
[36m[2025-06-29 20:33:17,576][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3833856. Throughput: 0: 365.1. Samples: 3834976. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:33:17,577][187912] Avg episode reward: [(0, '843.696')]
[36m[2025-06-29 20:33:22,563][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3833856. Throughput: 0: 365.9. Samples: 3837328. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:33:22,564][187912] Avg episode reward: [(0, '861.926')]
[36m[2025-06-29 20:33:27,594][187912] Fps is (10 sec: 408.9, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 3837952. Throughput: 0: 362.9. Samples: 3838128. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:33:27,595][187912] Avg episode reward: [(0, '835.310')]
[31m[10764960 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[10764960 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[10764960 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:33:32,593][187912] Fps is (10 sec: 408.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3837952. Throughput: 0: 361.8. Samples: 3840384. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:33:32,593][187912] Avg episode reward: [(0, '809.464')]
[36m[2025-06-29 20:33:37,580][187912] Fps is (10 sec: 410.2, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 3842048. Throughput: 0: 358.6. Samples: 3842352. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:33:37,581][187912] Avg episode reward: [(0, '826.059')]
[36m[2025-06-29 20:33:42,569][187912] Fps is (10 sec: 410.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3842048. Throughput: 0: 360.8. Samples: 3843600. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:33:42,569][187912] Avg episode reward: [(0, '834.681')]
[36m[2025-06-29 20:33:47,595][187912] Fps is (10 sec: 409.0, 60 sec: 409.5, 300 sec: 374.8). Total num frames: 3846144. Throughput: 0: 361.6. Samples: 3846000. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:33:47,595][187912] Avg episode reward: [(0, '814.514')]
[36m[2025-06-29 20:33:52,595][187912] Fps is (10 sec: 408.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3846144. Throughput: 0: 359.6. Samples: 3847936. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:33:52,595][187912] Avg episode reward: [(0, '840.919')]
[36m[2025-06-29 20:33:57,566][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3846144. Throughput: 0: 360.2. Samples: 3849120. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:33:57,566][187912] Avg episode reward: [(0, '910.591')]
[36m[2025-06-29 20:34:02,584][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3850240. Throughput: 0: 356.6. Samples: 3851024. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:34:02,585][187912] Avg episode reward: [(0, '947.639')]
[36m[2025-06-29 20:34:07,564][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3850240. Throughput: 0: 354.8. Samples: 3853296. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:34:07,564][187912] Avg episode reward: [(0, '910.530')]
[31m[10802695 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[10802696 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[10802696 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:34:12,570][187912] Fps is (10 sec: 410.2, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3854336. Throughput: 0: 361.1. Samples: 3854368. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 20:34:12,570][187912] Avg episode reward: [(0, '878.495')]
[37m[1m[2025-06-29 20:34:12,620][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000015056_3854336.pth...
[36m[2025-06-29 20:34:12,677][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000014720_3768320.pth
[36m[2025-06-29 20:34:17,570][187912] Fps is (10 sec: 409.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3854336. Throughput: 0: 354.7. Samples: 3856336. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 20:34:17,571][187912] Avg episode reward: [(0, '827.103')]
[36m[2025-06-29 20:34:22,563][187912] Fps is (10 sec: 409.9, 60 sec: 409.6, 300 sec: 361.1). Total num frames: 3858432. Throughput: 0: 358.2. Samples: 3858464. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:34:22,563][187912] Avg episode reward: [(0, '850.518')]
[36m[2025-06-29 20:34:27,575][187912] Fps is (10 sec: 409.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3858432. Throughput: 0: 357.6. Samples: 3859696. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:34:27,575][187912] Avg episode reward: [(0, '825.678')]
[36m[2025-06-29 20:34:33,220][187912] Fps is (10 sec: 384.3, 60 sec: 405.4, 300 sec: 374.1). Total num frames: 3862528. Throughput: 0: 350.7. Samples: 3862000. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:34:33,221][187912] Avg episode reward: [(0, '854.044')]
[36m[2025-06-29 20:34:37,564][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3862528. Throughput: 0: 359.7. Samples: 3864112. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:34:37,564][187912] Avg episode reward: [(0, '869.471')]
[36m[2025-06-29 20:34:42,587][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3862528. Throughput: 0: 359.3. Samples: 3865296. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:34:42,587][187912] Avg episode reward: [(0, '859.094')]
[36m[2025-06-29 20:34:47,581][187912] Fps is (10 sec: 408.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3866624. Throughput: 0: 362.0. Samples: 3867312. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 20:34:47,581][187912] Avg episode reward: [(0, '768.474')]
[36m[2025-06-29 20:34:52,560][187912] Fps is (10 sec: 410.7, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3866624. Throughput: 0: 364.5. Samples: 3869696. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 20:34:52,560][187912] Avg episode reward: [(0, '822.811')]
[36m[2025-06-29 20:34:57,581][187912] Fps is (10 sec: 409.6, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 3870720. Throughput: 0: 364.0. Samples: 3870752. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:34:57,581][187912] Avg episode reward: [(0, '786.720')]
[36m[2025-06-29 20:35:02,575][187912] Fps is (10 sec: 409.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3870720. Throughput: 0: 364.0. Samples: 3872720. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:35:02,576][187912] Avg episode reward: [(0, '802.376')]
[36m[2025-06-29 20:35:07,574][187912] Fps is (10 sec: 409.9, 60 sec: 409.5, 300 sec: 361.7). Total num frames: 3874816. Throughput: 0: 364.4. Samples: 3874864. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:35:07,574][187912] Avg episode reward: [(0, '808.401')]
[36m[2025-06-29 20:35:12,585][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3874816. Throughput: 0: 365.1. Samples: 3876128. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:35:12,585][187912] Avg episode reward: [(0, '873.877')]
[36m[2025-06-29 20:35:18,031][187912] Fps is (10 sec: 391.7, 60 sec: 406.5, 300 sec: 374.3). Total num frames: 3878912. Throughput: 0: 367.4. Samples: 3878464. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:35:18,031][187912] Avg episode reward: [(0, '887.728')]
[36m[2025-06-29 20:35:22,592][187912] Fps is (10 sec: 409.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3878912. Throughput: 0: 365.3. Samples: 3880560. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:35:22,593][187912] Avg episode reward: [(0, '847.917')]
[36m[2025-06-29 20:35:27,590][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3878912. Throughput: 0: 363.7. Samples: 3881664. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:35:27,590][187912] Avg episode reward: [(0, '795.594')]
[31m[10883714 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[10883715 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([12], device='cuda:0') (navigation_task.py:196)
[31m[10883715 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:35:32,586][187912] Fps is (10 sec: 409.9, 60 sec: 345.0, 300 sec: 361.0). Total num frames: 3883008. Throughput: 0: 362.3. Samples: 3883616. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:35:32,586][187912] Avg episode reward: [(0, '828.132')]
[36m[2025-06-29 20:35:37,571][187912] Fps is (10 sec: 410.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3883008. Throughput: 0: 359.4. Samples: 3885872. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:35:37,571][187912] Avg episode reward: [(0, '752.887')]
[36m[2025-06-29 20:35:42,566][187912] Fps is (10 sec: 410.4, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 3887104. Throughput: 0: 362.8. Samples: 3887072. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:35:42,566][187912] Avg episode reward: [(0, '711.441')]
[36m[2025-06-29 20:35:47,572][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3887104. Throughput: 0: 364.8. Samples: 3889136. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:35:47,572][187912] Avg episode reward: [(0, '693.892')]
[36m[2025-06-29 20:35:52,578][187912] Fps is (10 sec: 409.1, 60 sec: 409.5, 300 sec: 361.4). Total num frames: 3891200. Throughput: 0: 363.7. Samples: 3891232. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-06-29 20:35:52,578][187912] Avg episode reward: [(0, '684.901')]
[36m[2025-06-29 20:35:57,590][187912] Fps is (10 sec: 408.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3891200. Throughput: 0: 361.6. Samples: 3892400. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-06-29 20:35:57,590][187912] Avg episode reward: [(0, '691.376')]
[36m[2025-06-29 20:36:02,594][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3891200. Throughput: 0: 362.6. Samples: 3894624. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-06-29 20:36:02,594][187912] Avg episode reward: [(0, '764.335')]
[36m[2025-06-29 20:36:07,584][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3895296. Throughput: 0: 358.8. Samples: 3896704. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-06-29 20:36:07,584][187912] Avg episode reward: [(0, '819.646')]
[36m[2025-06-29 20:36:12,572][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3895296. Throughput: 0: 359.6. Samples: 3897840. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-06-29 20:36:12,572][187912] Avg episode reward: [(0, '909.187')]
[37m[1m[2025-06-29 20:36:12,614][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000015216_3895296.pth...
[36m[2025-06-29 20:36:12,679][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000014880_3809280.pth
[36m[2025-06-29 20:36:17,580][187912] Fps is (10 sec: 409.8, 60 sec: 343.9, 300 sec: 361.0). Total num frames: 3899392. Throughput: 0: 361.3. Samples: 3899872. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:36:17,580][187912] Avg episode reward: [(0, '898.990')]
[36m[2025-06-29 20:36:22,596][187912] Fps is (10 sec: 408.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3899392. Throughput: 0: 361.8. Samples: 3902160. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:36:22,596][187912] Avg episode reward: [(0, '919.609')]
[36m[2025-06-29 20:36:27,594][187912] Fps is (10 sec: 409.0, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3903488. Throughput: 0: 361.4. Samples: 3903344. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:36:27,595][187912] Avg episode reward: [(0, '831.100')]
[36m[2025-06-29 20:36:32,558][187912] Fps is (10 sec: 411.2, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3903488. Throughput: 0: 359.6. Samples: 3905312. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:36:32,558][187912] Avg episode reward: [(0, '794.061')]
[36m[2025-06-29 20:36:38,339][187912] Fps is (10 sec: 381.2, 60 sec: 404.4, 300 sec: 360.1). Total num frames: 3907584. Throughput: 0: 354.2. Samples: 3907440. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 20:36:38,348][187912] Avg episode reward: [(0, '812.209')]
[36m[2025-06-29 20:36:42,568][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3907584. Throughput: 0: 352.9. Samples: 3908272. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 20:36:42,569][187912] Avg episode reward: [(0, '800.280')]
[36m[2025-06-29 20:36:47,571][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3907584. Throughput: 0: 353.2. Samples: 3910512. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 20:36:47,572][187912] Avg episode reward: [(0, '756.200')]
[36m[2025-06-29 20:36:52,567][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3911680. Throughput: 0: 353.6. Samples: 3912608. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 20:36:52,567][187912] Avg episode reward: [(0, '773.657')]
[36m[2025-06-29 20:36:57,591][187912] Fps is (10 sec: 408.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3911680. Throughput: 0: 351.5. Samples: 3913664. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 20:36:57,591][187912] Avg episode reward: [(0, '798.302')]
[36m[2025-06-29 20:37:02,602][187912] Fps is (10 sec: 408.2, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 3915776. Throughput: 0: 354.0. Samples: 3915808. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:37:02,602][187912] Avg episode reward: [(0, '777.349')]
[36m[2025-06-29 20:37:07,576][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3915776. Throughput: 0: 349.7. Samples: 3917888. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:37:07,576][187912] Avg episode reward: [(0, '668.977')]
[36m[2025-06-29 20:37:12,598][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 3915776. Throughput: 0: 346.6. Samples: 3918944. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:37:12,598][187912] Avg episode reward: [(0, '671.267')]
[36m[2025-06-29 20:37:17,569][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3919872. Throughput: 0: 347.6. Samples: 3920960. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 20:37:17,570][187912] Avg episode reward: [(0, '658.914')]
[36m[2025-06-29 20:37:22,560][187912] Fps is (10 sec: 411.2, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3919872. Throughput: 0: 357.5. Samples: 3923248. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 20:37:22,561][187912] Avg episode reward: [(0, '684.258')]
[36m[2025-06-29 20:37:27,581][187912] Fps is (10 sec: 409.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3923968. Throughput: 0: 352.6. Samples: 3924144. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:37:27,581][187912] Avg episode reward: [(0, '670.528')]
[36m[2025-06-29 20:37:32,578][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3923968. Throughput: 0: 355.2. Samples: 3926496. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:37:32,578][187912] Avg episode reward: [(0, '760.452')]
[36m[2025-06-29 20:37:37,566][187912] Fps is (10 sec: 410.2, 60 sec: 345.8, 300 sec: 361.0). Total num frames: 3928064. Throughput: 0: 354.5. Samples: 3928560. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:37:37,567][187912] Avg episode reward: [(0, '800.184')]
[36m[2025-06-29 20:37:42,594][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3928064. Throughput: 0: 357.3. Samples: 3929744. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:37:42,595][187912] Avg episode reward: [(0, '836.419')]
[31m[11018737 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11018737 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[11018738 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:37:47,583][187912] Fps is (10 sec: 408.9, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 3932160. Throughput: 0: 361.7. Samples: 3932080. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 20:37:47,584][187912] Avg episode reward: [(0, '750.026')]
[36m[2025-06-29 20:37:52,603][187912] Fps is (10 sec: 409.2, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 3932160. Throughput: 0: 359.2. Samples: 3934064. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 20:37:52,603][187912] Avg episode reward: [(0, '765.793')]
[36m[2025-06-29 20:37:57,566][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 3932160. Throughput: 0: 360.1. Samples: 3935136. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 20:37:57,566][187912] Avg episode reward: [(0, '818.650')]
[36m[2025-06-29 20:38:02,570][187912] Fps is (10 sec: 411.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3936256. Throughput: 0: 359.5. Samples: 3937136. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:38:02,570][187912] Avg episode reward: [(0, '792.812')]
[36m[2025-06-29 20:38:07,595][187912] Fps is (10 sec: 408.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3936256. Throughput: 0: 356.4. Samples: 3939296. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:38:07,595][187912] Avg episode reward: [(0, '729.631')]
[36m[2025-06-29 20:38:12,593][187912] Fps is (10 sec: 408.6, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3940352. Throughput: 0: 360.8. Samples: 3940384. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:38:12,593][187912] Avg episode reward: [(0, '816.075')]
[37m[1m[2025-06-29 20:38:12,635][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000015392_3940352.pth...
[36m[2025-06-29 20:38:12,699][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000015056_3854336.pth
[31m[11046570 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11046571 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[11046571 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:38:17,578][187912] Fps is (10 sec: 410.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3940352. Throughput: 0: 356.6. Samples: 3942544. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:38:17,579][187912] Avg episode reward: [(0, '807.676')]
[36m[2025-06-29 20:38:22,591][187912] Fps is (10 sec: 409.7, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 3944448. Throughput: 0: 356.4. Samples: 3944608. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:38:22,591][187912] Avg episode reward: [(0, '796.596')]
[36m[2025-06-29 20:38:27,586][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3944448. Throughput: 0: 357.4. Samples: 3945824. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:38:27,586][187912] Avg episode reward: [(0, '808.005')]
[36m[2025-06-29 20:38:32,878][187912] Fps is (10 sec: 398.2, 60 sec: 407.6, 300 sec: 360.6). Total num frames: 3948544. Throughput: 0: 356.1. Samples: 3948208. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:38:32,878][187912] Avg episode reward: [(0, '807.423')]
[31m[11068153 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11068153 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[11068153 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:38:37,588][187912] Fps is (10 sec: 409.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3948544. Throughput: 0: 361.4. Samples: 3950320. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:38:37,588][187912] Avg episode reward: [(0, '751.318')]
[36m[2025-06-29 20:38:42,582][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 3948544. Throughput: 0: 364.0. Samples: 3951520. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:38:42,582][187912] Avg episode reward: [(0, '796.808')]
[36m[2025-06-29 20:38:47,564][187912] Fps is (10 sec: 410.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3952640. Throughput: 0: 366.6. Samples: 3953632. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:38:47,564][187912] Avg episode reward: [(0, '780.715')]
[36m[2025-06-29 20:38:52,587][187912] Fps is (10 sec: 409.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3952640. Throughput: 0: 368.4. Samples: 3955872. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:38:52,587][187912] Avg episode reward: [(0, '848.647')]
[36m[2025-06-29 20:38:57,569][187912] Fps is (10 sec: 409.4, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3956736. Throughput: 0: 364.3. Samples: 3956768. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:38:57,569][187912] Avg episode reward: [(0, '852.956')]
[36m[2025-06-29 20:39:02,582][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3956736. Throughput: 0: 365.1. Samples: 3958976. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:39:02,582][187912] Avg episode reward: [(0, '898.520')]
[36m[2025-06-29 20:39:07,558][187912] Fps is (10 sec: 410.0, 60 sec: 409.9, 300 sec: 361.0). Total num frames: 3960832. Throughput: 0: 364.0. Samples: 3960976. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:39:07,558][187912] Avg episode reward: [(0, '897.001')]
[36m[2025-06-29 20:39:12,582][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3960832. Throughput: 0: 365.5. Samples: 3962272. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:39:12,582][187912] Avg episode reward: [(0, '934.004')]
[36m[2025-06-29 20:39:17,608][187912] Fps is (10 sec: 407.6, 60 sec: 409.4, 300 sec: 360.9). Total num frames: 3964928. Throughput: 0: 367.7. Samples: 3964656. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 20:39:17,609][187912] Avg episode reward: [(0, '896.491')]
[36m[2025-06-29 20:39:22,578][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3964928. Throughput: 0: 363.5. Samples: 3966672. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 20:39:22,578][187912] Avg episode reward: [(0, '914.300')]
[36m[2025-06-29 20:39:27,568][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.9). Total num frames: 3964928. Throughput: 0: 362.1. Samples: 3967808. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 20:39:27,568][187912] Avg episode reward: [(0, '854.717')]
[36m[2025-06-29 20:39:32,563][187912] Fps is (10 sec: 410.2, 60 sec: 343.1, 300 sec: 361.0). Total num frames: 3969024. Throughput: 0: 362.0. Samples: 3969920. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:39:32,563][187912] Avg episode reward: [(0, '842.828')]
[36m[2025-06-29 20:39:37,562][187912] Fps is (10 sec: 409.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 3969024. Throughput: 0: 365.4. Samples: 3972304. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:39:37,562][187912] Avg episode reward: [(0, '846.283')]
[36m[2025-06-29 20:39:42,592][187912] Fps is (10 sec: 408.4, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 3973120. Throughput: 0: 364.6. Samples: 3973184. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:39:42,592][187912] Avg episode reward: [(0, '860.563')]
[36m[2025-06-29 20:39:47,563][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3973120. Throughput: 0: 366.4. Samples: 3975456. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:39:47,563][187912] Avg episode reward: [(0, '798.813')]
[36m[2025-06-29 20:39:52,583][187912] Fps is (10 sec: 409.9, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 3977216. Throughput: 0: 367.1. Samples: 3977504. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:39:52,583][187912] Avg episode reward: [(0, '860.062')]
[36m[2025-06-29 20:39:57,572][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3977216. Throughput: 0: 363.8. Samples: 3978640. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:39:57,572][187912] Avg episode reward: [(0, '856.001')]
[36m[2025-06-29 20:40:02,571][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 3977216. Throughput: 0: 357.6. Samples: 3980736. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:40:02,571][187912] Avg episode reward: [(0, '882.469')]
[36m[2025-06-29 20:40:07,586][187912] Fps is (10 sec: 409.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 3981312. Throughput: 0: 358.7. Samples: 3982816. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 20:40:07,587][187912] Avg episode reward: [(0, '814.783')]
[31m[11163569 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11163569 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[11163569 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:40:12,576][187912] Fps is (10 sec: 409.4, 60 sec: 341.4, 300 sec: 347.7). Total num frames: 3981312. Throughput: 0: 356.9. Samples: 3983872. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 20:40:12,577][187912] Avg episode reward: [(0, '782.443')]
[37m[1m[2025-06-29 20:40:12,617][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000015552_3981312.pth...
[36m[2025-06-29 20:40:12,681][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000015216_3895296.pth
[36m[2025-06-29 20:40:17,565][187912] Fps is (10 sec: 410.5, 60 sec: 341.6, 300 sec: 361.0). Total num frames: 3985408. Throughput: 0: 355.9. Samples: 3985936. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 20:40:17,565][187912] Avg episode reward: [(0, '768.463')]
[31m[11171728 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11171728 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[11171728 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:40:22,575][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3985408. Throughput: 0: 353.3. Samples: 3988208. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 20:40:22,575][187912] Avg episode reward: [(0, '725.278')]
[33m[11176689 ms][navigation_task] - WARNING : Curriculum Level: 43, Curriculum progress fraction: 0.8 (navigation_task.py:262)
[33m[11176690 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.76953125
[33mCrash Rate: 0.17822265625
[33mTimeout Rate: 0.05224609375 (navigation_task.py:265)
[33m[11176690 ms][navigation_task] - WARNING : 
[33mSuccesses: 1576
[33mCrashes : 365
[33mTimeouts: 107 (navigation_task.py:268)
[36m[2025-06-29 20:40:27,584][187912] Fps is (10 sec: 408.8, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 3989504. Throughput: 0: 358.1. Samples: 3989296. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:40:27,585][187912] Avg episode reward: [(0, '693.724')]
[36m[2025-06-29 20:40:32,562][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 3989504. Throughput: 0: 355.2. Samples: 3991440. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:40:32,562][187912] Avg episode reward: [(0, '744.007')]
[36m[2025-06-29 20:40:37,922][187912] Fps is (10 sec: 396.2, 60 sec: 407.2, 300 sec: 360.6). Total num frames: 3993600. Throughput: 0: 354.7. Samples: 3993584. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:40:37,922][187912] Avg episode reward: [(0, '752.835')]
[36m[2025-06-29 20:40:42,587][187912] Fps is (10 sec: 408.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3993600. Throughput: 0: 349.4. Samples: 3994368. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:40:42,587][187912] Avg episode reward: [(0, '788.726')]
[36m[2025-06-29 20:40:47,577][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 3993600. Throughput: 0: 353.0. Samples: 3996624. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:40:47,577][187912] Avg episode reward: [(0, '840.520')]
[36m[2025-06-29 20:40:52,576][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3997696. Throughput: 0: 353.9. Samples: 3998736. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:40:52,576][187912] Avg episode reward: [(0, '853.659')]
[36m[2025-06-29 20:40:57,565][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 3997696. Throughput: 0: 357.4. Samples: 3999952. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:40:57,565][187912] Avg episode reward: [(0, '862.100')]
[36m[2025-06-29 20:41:02,566][187912] Fps is (10 sec: 410.0, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 4001792. Throughput: 0: 356.3. Samples: 4001968. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:41:02,566][187912] Avg episode reward: [(0, '860.059')]
[36m[2025-06-29 20:41:07,560][187912] Fps is (10 sec: 409.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4001792. Throughput: 0: 357.1. Samples: 4004272. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:41:07,560][187912] Avg episode reward: [(0, '850.553')]
[36m[2025-06-29 20:41:12,592][187912] Fps is (10 sec: 408.5, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 4005888. Throughput: 0: 359.4. Samples: 4005472. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 20:41:12,593][187912] Avg episode reward: [(0, '835.320')]
[36m[2025-06-29 20:41:17,597][187912] Fps is (10 sec: 408.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4005888. Throughput: 0: 359.9. Samples: 4007648. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 20:41:17,597][187912] Avg episode reward: [(0, '817.907')]
[36m[2025-06-29 20:41:22,636][187912] Fps is (10 sec: 407.8, 60 sec: 409.2, 300 sec: 361.0). Total num frames: 4009984. Throughput: 0: 341.7. Samples: 4008864. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 20:41:22,636][187912] Avg episode reward: [(0, '812.977')]
[36m[2025-06-29 20:41:27,598][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4009984. Throughput: 0: 369.7. Samples: 4011008. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 20:41:27,598][187912] Avg episode reward: [(0, '864.017')]
[36m[2025-06-29 20:41:32,573][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 348.0). Total num frames: 4009984. Throughput: 0: 368.7. Samples: 4013216. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 20:41:32,574][187912] Avg episode reward: [(0, '869.105')]
[36m[2025-06-29 20:41:37,586][187912] Fps is (10 sec: 410.1, 60 sec: 343.3, 300 sec: 361.0). Total num frames: 4014080. Throughput: 0: 368.3. Samples: 4015312. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:41:37,586][187912] Avg episode reward: [(0, '867.797')]
[36m[2025-06-29 20:41:42,568][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4014080. Throughput: 0: 368.3. Samples: 4016528. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:41:42,568][187912] Avg episode reward: [(0, '828.700')]
[36m[2025-06-29 20:41:47,570][187912] Fps is (10 sec: 410.3, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 4018176. Throughput: 0: 368.0. Samples: 4018528. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:41:47,570][187912] Avg episode reward: [(0, '865.150')]
[36m[2025-06-29 20:41:52,573][187912] Fps is (10 sec: 409.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4018176. Throughput: 0: 371.1. Samples: 4020976. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:41:52,573][187912] Avg episode reward: [(0, '817.213')]
[36m[2025-06-29 20:41:57,584][187912] Fps is (10 sec: 409.0, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 4022272. Throughput: 0: 371.3. Samples: 4022176. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:41:57,584][187912] Avg episode reward: [(0, '801.820')]
[36m[2025-06-29 20:42:02,561][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4022272. Throughput: 0: 366.2. Samples: 4024112. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:42:02,562][187912] Avg episode reward: [(0, '833.143')]
[36m[2025-06-29 20:42:08,057][187912] Fps is (10 sec: 391.1, 60 sec: 406.2, 300 sec: 374.3). Total num frames: 4026368. Throughput: 0: 384.3. Samples: 4026320. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 20:42:08,058][187912] Avg episode reward: [(0, '823.189')]
[36m[2025-06-29 20:42:12,564][187912] Fps is (10 sec: 409.5, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4026368. Throughput: 0: 359.7. Samples: 4027184. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 20:42:12,564][187912] Avg episode reward: [(0, '834.592')]
[37m[1m[2025-06-29 20:42:12,627][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000015728_4026368.pth...
[36m[2025-06-29 20:42:12,695][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000015392_3940352.pth
[36m[2025-06-29 20:42:17,562][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4026368. Throughput: 0: 357.8. Samples: 4029312. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 20:42:17,562][187912] Avg episode reward: [(0, '848.469')]
[36m[2025-06-29 20:42:22,560][187912] Fps is (10 sec: 409.7, 60 sec: 341.8, 300 sec: 361.0). Total num frames: 4030464. Throughput: 0: 357.9. Samples: 4031408. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:42:22,560][187912] Avg episode reward: [(0, '849.406')]
[36m[2025-06-29 20:42:27,595][187912] Fps is (10 sec: 408.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4030464. Throughput: 0: 355.3. Samples: 4032528. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:42:27,596][187912] Avg episode reward: [(0, '811.671')]
[36m[2025-06-29 20:42:32,584][187912] Fps is (10 sec: 408.6, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 4034560. Throughput: 0: 356.9. Samples: 4034592. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:42:32,584][187912] Avg episode reward: [(0, '827.729')]
[36m[2025-06-29 20:42:37,561][187912] Fps is (10 sec: 411.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4034560. Throughput: 0: 353.2. Samples: 4036864. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:42:37,561][187912] Avg episode reward: [(0, '822.037')]
[36m[2025-06-29 20:42:42,786][187912] Fps is (10 sec: 401.5, 60 sec: 408.1, 300 sec: 360.8). Total num frames: 4038656. Throughput: 0: 348.0. Samples: 4037904. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:42:42,786][187912] Avg episode reward: [(0, '821.783')]
[36m[2025-06-29 20:42:47,601][187912] Fps is (10 sec: 408.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4038656. Throughput: 0: 353.1. Samples: 4040016. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:42:47,601][187912] Avg episode reward: [(0, '881.982')]
[36m[2025-06-29 20:42:52,564][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4038656. Throughput: 0: 362.7. Samples: 4042464. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:42:52,565][187912] Avg episode reward: [(0, '926.103')]
[36m[2025-06-29 20:42:57,562][187912] Fps is (10 sec: 411.2, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4042752. Throughput: 0: 359.8. Samples: 4043376. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:42:57,562][187912] Avg episode reward: [(0, '947.161')]
[36m[2025-06-29 20:43:02,614][187912] Fps is (10 sec: 407.6, 60 sec: 341.0, 300 sec: 361.0). Total num frames: 4042752. Throughput: 0: 364.4. Samples: 4045728. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:43:02,615][187912] Avg episode reward: [(0, '929.905')]
[36m[2025-06-29 20:43:07,571][187912] Fps is (10 sec: 409.2, 60 sec: 344.1, 300 sec: 361.0). Total num frames: 4046848. Throughput: 0: 360.8. Samples: 4047648. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:43:07,571][187912] Avg episode reward: [(0, '929.627')]
[36m[2025-06-29 20:43:12,575][187912] Fps is (10 sec: 411.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4046848. Throughput: 0: 361.8. Samples: 4048800. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:43:12,576][187912] Avg episode reward: [(0, '875.069')]
[31m[11346757 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11346757 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[11346758 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:43:17,586][187912] Fps is (10 sec: 409.0, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 4050944. Throughput: 0: 364.1. Samples: 4050976. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:43:17,586][187912] Avg episode reward: [(0, '849.448')]
[36m[2025-06-29 20:43:22,562][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4050944. Throughput: 0: 363.4. Samples: 4053216. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:43:22,562][187912] Avg episode reward: [(0, '887.997')]
[36m[2025-06-29 20:43:27,722][187912] Fps is (10 sec: 404.1, 60 sec: 408.7, 300 sec: 361.2). Total num frames: 4055040. Throughput: 0: 365.7. Samples: 4054336. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 20:43:27,723][187912] Avg episode reward: [(0, '862.263')]
[36m[2025-06-29 20:43:32,561][187912] Fps is (10 sec: 409.7, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4055040. Throughput: 0: 364.1. Samples: 4056384. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 20:43:32,561][187912] Avg episode reward: [(0, '806.936')]
[36m[2025-06-29 20:43:37,576][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4055040. Throughput: 0: 360.4. Samples: 4058688. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 20:43:37,576][187912] Avg episode reward: [(0, '845.875')]
[36m[2025-06-29 20:43:42,571][187912] Fps is (10 sec: 409.2, 60 sec: 342.6, 300 sec: 361.0). Total num frames: 4059136. Throughput: 0: 359.4. Samples: 4059552. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 20:43:42,571][187912] Avg episode reward: [(0, '848.512')]
[36m[2025-06-29 20:43:47,586][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4059136. Throughput: 0: 358.6. Samples: 4061856. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 20:43:47,587][187912] Avg episode reward: [(0, '869.856')]
[36m[2025-06-29 20:43:52,576][187912] Fps is (10 sec: 409.4, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 4063232. Throughput: 0: 359.1. Samples: 4063808. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 20:43:52,576][187912] Avg episode reward: [(0, '891.185')]
[36m[2025-06-29 20:43:57,586][187912] Fps is (10 sec: 409.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4063232. Throughput: 0: 358.7. Samples: 4064944. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 20:43:57,586][187912] Avg episode reward: [(0, '920.007')]
[36m[2025-06-29 20:44:02,589][187912] Fps is (10 sec: 409.1, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 4067328. Throughput: 0: 360.5. Samples: 4067200. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 20:44:02,589][187912] Avg episode reward: [(0, '946.804')]
[36m[2025-06-29 20:44:07,589][187912] Fps is (10 sec: 409.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4067328. Throughput: 0: 355.7. Samples: 4069232. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 20:44:07,589][187912] Avg episode reward: [(0, '912.080')]
[36m[2025-06-29 20:44:12,562][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 4067328. Throughput: 0: 357.5. Samples: 4070368. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 20:44:12,562][187912] Avg episode reward: [(0, '917.574')]
[37m[1m[2025-06-29 20:44:12,604][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000015888_4067328.pth...
[36m[2025-06-29 20:44:12,660][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000015552_3981312.pth
[36m[2025-06-29 20:44:17,570][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4071424. Throughput: 0: 355.5. Samples: 4072384. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:44:17,571][187912] Avg episode reward: [(0, '922.655')]
[36m[2025-06-29 20:44:22,594][187912] Fps is (10 sec: 408.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4071424. Throughput: 0: 355.8. Samples: 4074704. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:44:22,594][187912] Avg episode reward: [(0, '921.958')]
[36m[2025-06-29 20:44:27,586][187912] Fps is (10 sec: 409.0, 60 sec: 342.1, 300 sec: 361.0). Total num frames: 4075520. Throughput: 0: 356.1. Samples: 4075584. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:44:27,586][187912] Avg episode reward: [(0, '893.489')]
[36m[2025-06-29 20:44:32,567][187912] Fps is (10 sec: 410.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4075520. Throughput: 0: 357.1. Samples: 4077920. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:44:32,567][187912] Avg episode reward: [(0, '950.180')]
[36m[2025-06-29 20:44:37,594][187912] Fps is (10 sec: 409.3, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 4079616. Throughput: 0: 358.6. Samples: 4079952. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:44:37,594][187912] Avg episode reward: [(0, '908.227')]
[36m[2025-06-29 20:44:42,576][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4079616. Throughput: 0: 359.5. Samples: 4081120. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:44:42,577][187912] Avg episode reward: [(0, '925.709')]
[36m[2025-06-29 20:44:47,655][187912] Fps is (10 sec: 407.1, 60 sec: 409.1, 300 sec: 360.9). Total num frames: 4083712. Throughput: 0: 361.1. Samples: 4083472. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:44:47,655][187912] Avg episode reward: [(0, '878.972')]
[36m[2025-06-29 20:44:52,619][187912] Fps is (10 sec: 407.8, 60 sec: 341.1, 300 sec: 360.9). Total num frames: 4083712. Throughput: 0: 360.6. Samples: 4085472. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:44:52,620][187912] Avg episode reward: [(0, '869.867')]
[36m[2025-06-29 20:44:57,564][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4083712. Throughput: 0: 360.2. Samples: 4086576. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:44:57,564][187912] Avg episode reward: [(0, '886.482')]
[36m[2025-06-29 20:45:02,564][187912] Fps is (10 sec: 411.9, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4087808. Throughput: 0: 360.6. Samples: 4088608. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 20:45:02,564][187912] Avg episode reward: [(0, '861.923')]
[36m[2025-06-29 20:45:07,581][187912] Fps is (10 sec: 408.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4087808. Throughput: 0: 363.1. Samples: 4091040. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 20:45:07,581][187912] Avg episode reward: [(0, '892.897')]
[36m[2025-06-29 20:45:12,586][187912] Fps is (10 sec: 408.7, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 4091904. Throughput: 0: 363.7. Samples: 4091952. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 20:45:12,586][187912] Avg episode reward: [(0, '907.849')]
[36m[2025-06-29 20:45:17,564][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4091904. Throughput: 0: 365.9. Samples: 4094384. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 20:45:17,564][187912] Avg episode reward: [(0, '943.714')]
[36m[2025-06-29 20:45:22,591][187912] Fps is (10 sec: 409.4, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 4096000. Throughput: 0: 363.4. Samples: 4096304. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 20:45:22,591][187912] Avg episode reward: [(0, '942.574')]
[36m[2025-06-29 20:45:27,581][187912] Fps is (10 sec: 408.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4096000. Throughput: 0: 364.4. Samples: 4097520. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 20:45:27,581][187912] Avg episode reward: [(0, '1000.622')]
[36m[2025-06-29 20:45:32,928][187912] Fps is (10 sec: 396.3, 60 sec: 407.1, 300 sec: 361.0). Total num frames: 4100096. Throughput: 0: 359.4. Samples: 4099744. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 20:45:32,928][187912] Avg episode reward: [(0, '956.108')]
[36m[2025-06-29 20:45:37,591][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4100096. Throughput: 0: 360.8. Samples: 4101696. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 20:45:37,591][187912] Avg episode reward: [(0, '951.136')]
[36m[2025-06-29 20:45:42,591][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4100096. Throughput: 0: 361.4. Samples: 4102848. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 20:45:42,591][187912] Avg episode reward: [(0, '913.378')]
[36m[2025-06-29 20:45:47,582][187912] Fps is (10 sec: 410.0, 60 sec: 341.7, 300 sec: 361.0). Total num frames: 4104192. Throughput: 0: 360.4. Samples: 4104832. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:45:47,582][187912] Avg episode reward: [(0, '910.418')]
[36m[2025-06-29 20:45:52,570][187912] Fps is (10 sec: 410.4, 60 sec: 341.6, 300 sec: 361.0). Total num frames: 4104192. Throughput: 0: 358.5. Samples: 4107168. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:45:52,571][187912] Avg episode reward: [(0, '868.474')]
[36m[2025-06-29 20:45:57,579][187912] Fps is (10 sec: 409.7, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 4108288. Throughput: 0: 363.8. Samples: 4108320. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:45:57,580][187912] Avg episode reward: [(0, '869.259')]
[36m[2025-06-29 20:46:02,572][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4108288. Throughput: 0: 358.0. Samples: 4110496. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:46:02,572][187912] Avg episode reward: [(0, '862.334')]
[36m[2025-06-29 20:46:07,590][187912] Fps is (10 sec: 409.2, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 4112384. Throughput: 0: 359.8. Samples: 4112496. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:46:07,590][187912] Avg episode reward: [(0, '851.107')]
[36m[2025-06-29 20:46:12,568][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4112384. Throughput: 0: 359.9. Samples: 4113712. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:46:12,568][187912] Avg episode reward: [(0, '856.334')]
[37m[1m[2025-06-29 20:46:12,625][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000016064_4112384.pth...
[36m[2025-06-29 20:46:12,690][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000015728_4026368.pth
[36m[2025-06-29 20:46:18,194][187912] Fps is (10 sec: 386.3, 60 sec: 405.3, 300 sec: 360.3). Total num frames: 4116480. Throughput: 0: 358.4. Samples: 4115968. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:46:18,195][187912] Avg episode reward: [(0, '908.418')]
[36m[2025-06-29 20:46:22,567][187912] Fps is (10 sec: 409.7, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4116480. Throughput: 0: 363.2. Samples: 4118032. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:46:22,567][187912] Avg episode reward: [(0, '890.502')]
[36m[2025-06-29 20:46:27,570][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4116480. Throughput: 0: 363.5. Samples: 4119200. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:46:27,571][187912] Avg episode reward: [(0, '889.646')]
[36m[2025-06-29 20:46:32,570][187912] Fps is (10 sec: 409.5, 60 sec: 343.4, 300 sec: 361.0). Total num frames: 4120576. Throughput: 0: 366.0. Samples: 4121296. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 20:46:32,570][187912] Avg episode reward: [(0, '939.289')]
[36m[2025-06-29 20:46:37,577][187912] Fps is (10 sec: 409.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4120576. Throughput: 0: 365.1. Samples: 4123600. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 20:46:37,578][187912] Avg episode reward: [(0, '943.558')]
[36m[2025-06-29 20:46:42,591][187912] Fps is (10 sec: 408.7, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 4124672. Throughput: 0: 364.0. Samples: 4124704. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 20:46:42,591][187912] Avg episode reward: [(0, '877.786')]
[31m[11556686 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11556686 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[11556686 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:46:47,577][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4124672. Throughput: 0: 361.6. Samples: 4126768. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 20:46:47,578][187912] Avg episode reward: [(0, '944.061')]
[36m[2025-06-29 20:46:52,575][187912] Fps is (10 sec: 410.2, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 4128768. Throughput: 0: 362.8. Samples: 4128816. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:46:52,576][187912] Avg episode reward: [(0, '909.807')]
[31m[11570471 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11570472 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[11570472 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:46:57,565][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4128768. Throughput: 0: 360.9. Samples: 4129952. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:46:57,565][187912] Avg episode reward: [(0, '862.852')]
[36m[2025-06-29 20:47:02,581][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.7). Total num frames: 4128768. Throughput: 0: 367.7. Samples: 4132288. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:47:02,582][187912] Avg episode reward: [(0, '894.232')]
[36m[2025-06-29 20:47:07,592][187912] Fps is (10 sec: 408.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4132864. Throughput: 0: 360.7. Samples: 4134272. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:47:07,592][187912] Avg episode reward: [(0, '928.370')]
[36m[2025-06-29 20:47:12,566][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4132864. Throughput: 0: 358.8. Samples: 4135344. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:47:12,566][187912] Avg episode reward: [(0, '936.022')]
[36m[2025-06-29 20:47:17,578][187912] Fps is (10 sec: 410.2, 60 sec: 344.9, 300 sec: 361.0). Total num frames: 4136960. Throughput: 0: 355.5. Samples: 4137296. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:47:17,578][187912] Avg episode reward: [(0, '960.013')]
[36m[2025-06-29 20:47:22,563][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4136960. Throughput: 0: 357.1. Samples: 4139664. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:47:22,564][187912] Avg episode reward: [(0, '978.774')]
[36m[2025-06-29 20:47:27,592][187912] Fps is (10 sec: 409.0, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 4141056. Throughput: 0: 356.3. Samples: 4140736. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:47:27,593][187912] Avg episode reward: [(0, '1028.256')]
[37m[1m[2025-06-29 20:47:27,632][187912] Saving new best policy, reward=1028.256!
[36m[2025-06-29 20:47:32,598][187912] Fps is (10 sec: 408.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4141056. Throughput: 0: 351.8. Samples: 4142608. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:47:32,598][187912] Avg episode reward: [(0, '976.360')]
[36m[2025-06-29 20:47:37,597][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.3). Total num frames: 4141056. Throughput: 0: 352.2. Samples: 4144672. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:47:37,597][187912] Avg episode reward: [(0, '1008.593')]
[36m[2025-06-29 20:47:42,563][187912] Fps is (10 sec: 411.0, 60 sec: 341.5, 300 sec: 361.1). Total num frames: 4145152. Throughput: 0: 344.6. Samples: 4145456. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 20:47:42,563][187912] Avg episode reward: [(0, '1027.148')]
[36m[2025-06-29 20:47:47,612][187912] Fps is (10 sec: 409.0, 60 sec: 341.1, 300 sec: 360.9). Total num frames: 4145152. Throughput: 0: 343.9. Samples: 4147776. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 20:47:47,612][187912] Avg episode reward: [(0, '952.025')]
[36m[2025-06-29 20:47:52,566][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4149248. Throughput: 0: 344.4. Samples: 4149760. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:47:52,566][187912] Avg episode reward: [(0, '932.033')]
[36m[2025-06-29 20:47:57,589][187912] Fps is (10 sec: 410.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4149248. Throughput: 0: 347.6. Samples: 4150992. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:47:57,589][187912] Avg episode reward: [(0, '945.653')]
[36m[2025-06-29 20:48:02,618][187912] Fps is (10 sec: 407.5, 60 sec: 409.3, 300 sec: 360.9). Total num frames: 4153344. Throughput: 0: 351.7. Samples: 4153136. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:48:02,619][187912] Avg episode reward: [(0, '969.841')]
[36m[2025-06-29 20:48:07,588][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4153344. Throughput: 0: 341.1. Samples: 4155024. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:48:07,588][187912] Avg episode reward: [(0, '893.149')]
[36m[2025-06-29 20:48:12,593][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 4153344. Throughput: 0: 341.3. Samples: 4156096. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:48:12,594][187912] Avg episode reward: [(0, '917.529')]
[37m[1m[2025-06-29 20:48:12,635][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000016224_4153344.pth...
[36m[2025-06-29 20:48:12,698][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000015888_4067328.pth
[36m[2025-06-29 20:48:17,570][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4157440. Throughput: 0: 345.1. Samples: 4158128. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 20:48:17,571][187912] Avg episode reward: [(0, '897.683')]
[36m[2025-06-29 20:48:22,577][187912] Fps is (10 sec: 410.3, 60 sec: 341.3, 300 sec: 347.3). Total num frames: 4157440. Throughput: 0: 351.1. Samples: 4160464. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 20:48:22,578][187912] Avg episode reward: [(0, '932.694')]
[36m[2025-06-29 20:48:27,602][187912] Fps is (10 sec: 408.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4161536. Throughput: 0: 356.3. Samples: 4161504. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 20:48:27,602][187912] Avg episode reward: [(0, '944.098')]
[36m[2025-06-29 20:48:32,573][187912] Fps is (10 sec: 409.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4161536. Throughput: 0: 348.0. Samples: 4163424. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 20:48:32,574][187912] Avg episode reward: [(0, '901.441')]
[36m[2025-06-29 20:48:37,565][187912] Fps is (10 sec: 411.1, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 4165632. Throughput: 0: 353.4. Samples: 4165664. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:48:37,565][187912] Avg episode reward: [(0, '869.940')]
[36m[2025-06-29 20:48:42,568][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4165632. Throughput: 0: 347.5. Samples: 4166624. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:48:42,568][187912] Avg episode reward: [(0, '914.549')]
[36m[2025-06-29 20:48:47,574][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 4165632. Throughput: 0: 352.0. Samples: 4168960. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:48:47,575][187912] Avg episode reward: [(0, '912.730')]
[36m[2025-06-29 20:48:52,569][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4169728. Throughput: 0: 355.0. Samples: 4170992. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:48:52,569][187912] Avg episode reward: [(0, '805.536')]
[31m[11688558 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11688558 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[11688558 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:48:57,569][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 4169728. Throughput: 0: 356.8. Samples: 4172144. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:48:57,569][187912] Avg episode reward: [(0, '889.196')]
[31m[11694492 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11694492 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[11694492 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:49:02,558][187912] Fps is (10 sec: 410.1, 60 sec: 341.7, 300 sec: 361.0). Total num frames: 4173824. Throughput: 0: 353.9. Samples: 4174048. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:49:02,558][187912] Avg episode reward: [(0, '890.708')]
[33m[11696899 ms][navigation_task] - WARNING : Curriculum Level: 45, Curriculum progress fraction: 0.8571428571428571 (navigation_task.py:262)
[33m[11696899 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.79248046875
[33mCrash Rate: 0.1640625
[33mTimeout Rate: 0.04345703125 (navigation_task.py:265)
[33m[11696899 ms][navigation_task] - WARNING : 
[33mSuccesses: 1623
[33mCrashes : 336
[33mTimeouts: 89 (navigation_task.py:268)
[36m[2025-06-29 20:49:07,565][187912] Fps is (10 sec: 409.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4173824. Throughput: 0: 353.9. Samples: 4176384. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:49:07,565][187912] Avg episode reward: [(0, '928.782')]
[36m[2025-06-29 20:49:12,569][187912] Fps is (10 sec: 409.2, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 4177920. Throughput: 0: 356.5. Samples: 4177536. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:49:12,569][187912] Avg episode reward: [(0, '958.437')]
[36m[2025-06-29 20:49:17,595][187912] Fps is (10 sec: 408.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4177920. Throughput: 0: 361.8. Samples: 4179712. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:49:17,595][187912] Avg episode reward: [(0, '1005.626')]
[36m[2025-06-29 20:49:22,569][187912] Fps is (10 sec: 409.6, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 4182016. Throughput: 0: 364.1. Samples: 4182048. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 20:49:22,569][187912] Avg episode reward: [(0, '1047.966')]
[37m[1m[2025-06-29 20:49:22,607][187912] Saving new best policy, reward=1047.966!
[36m[2025-06-29 20:49:27,575][187912] Fps is (10 sec: 410.4, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4182016. Throughput: 0: 364.7. Samples: 4183040. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 20:49:27,575][187912] Avg episode reward: [(0, '1073.746')]
[37m[1m[2025-06-29 20:49:27,617][187912] Saving new best policy, reward=1073.746!
[36m[2025-06-29 20:49:32,574][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 4182016. Throughput: 0: 362.7. Samples: 4185280. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 20:49:32,574][187912] Avg episode reward: [(0, '1017.000')]
[36m[2025-06-29 20:49:37,570][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4186112. Throughput: 0: 365.5. Samples: 4187440. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 20:49:37,571][187912] Avg episode reward: [(0, '967.007')]
[36m[2025-06-29 20:49:42,591][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 347.2). Total num frames: 4186112. Throughput: 0: 365.3. Samples: 4188592. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 20:49:42,592][187912] Avg episode reward: [(0, '994.172')]
[36m[2025-06-29 20:49:47,583][187912] Fps is (10 sec: 409.1, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 4190208. Throughput: 0: 363.9. Samples: 4190432. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:49:47,583][187912] Avg episode reward: [(0, '938.670')]
[36m[2025-06-29 20:49:52,562][187912] Fps is (10 sec: 410.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4190208. Throughput: 0: 362.7. Samples: 4192704. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:49:52,562][187912] Avg episode reward: [(0, '977.198')]
[36m[2025-06-29 20:49:57,560][187912] Fps is (10 sec: 410.5, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 4194304. Throughput: 0: 363.8. Samples: 4193904. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:49:57,560][187912] Avg episode reward: [(0, '941.750')]
[31m[11752920 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11752920 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([12], device='cuda:0') (navigation_task.py:196)
[31m[11752920 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:50:02,591][187912] Fps is (10 sec: 408.4, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 4194304. Throughput: 0: 358.4. Samples: 4195840. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:50:02,592][187912] Avg episode reward: [(0, '971.861')]
[36m[2025-06-29 20:50:07,568][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 4194304. Throughput: 0: 358.1. Samples: 4198160. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:50:07,568][187912] Avg episode reward: [(0, '983.843')]
[36m[2025-06-29 20:50:12,595][187912] Fps is (10 sec: 409.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4198400. Throughput: 0: 356.5. Samples: 4199088. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:50:12,595][187912] Avg episode reward: [(0, '975.587')]
[37m[1m[2025-06-29 20:50:12,650][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000016400_4198400.pth...
[36m[2025-06-29 20:50:12,715][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000016064_4112384.pth
[36m[2025-06-29 20:50:17,583][187912] Fps is (10 sec: 409.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 4198400. Throughput: 0: 356.5. Samples: 4201328. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:50:17,584][187912] Avg episode reward: [(0, '974.054')]
[36m[2025-06-29 20:50:22,559][187912] Fps is (10 sec: 411.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4202496. Throughput: 0: 356.0. Samples: 4203456. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:50:22,559][187912] Avg episode reward: [(0, '990.006')]
[36m[2025-06-29 20:50:27,591][187912] Fps is (10 sec: 409.3, 60 sec: 341.2, 300 sec: 347.5). Total num frames: 4202496. Throughput: 0: 355.9. Samples: 4204608. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:50:27,591][187912] Avg episode reward: [(0, '995.947')]
[36m[2025-06-29 20:50:32,589][187912] Fps is (10 sec: 408.4, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 4206592. Throughput: 0: 359.8. Samples: 4206624. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:50:32,589][187912] Avg episode reward: [(0, '995.175')]
[31m[11790058 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11790058 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[11790058 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:50:37,593][187912] Fps is (10 sec: 409.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4206592. Throughput: 0: 359.2. Samples: 4208880. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:50:37,594][187912] Avg episode reward: [(0, '954.555')]
[36m[2025-06-29 20:50:42,589][187912] Fps is (10 sec: 409.6, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 4210688. Throughput: 0: 359.2. Samples: 4210080. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:50:42,589][187912] Avg episode reward: [(0, '988.667')]
[36m[2025-06-29 20:50:47,564][187912] Fps is (10 sec: 410.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4210688. Throughput: 0: 364.7. Samples: 4212240. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:50:47,564][187912] Avg episode reward: [(0, '990.399')]
[36m[2025-06-29 20:50:53,262][187912] Fps is (10 sec: 383.7, 60 sec: 404.9, 300 sec: 360.2). Total num frames: 4214784. Throughput: 0: 361.4. Samples: 4214672. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 20:50:53,263][187912] Avg episode reward: [(0, '1004.298')]
[36m[2025-06-29 20:50:57,606][187912] Fps is (10 sec: 407.9, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 4214784. Throughput: 0: 366.8. Samples: 4215600. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 20:50:57,606][187912] Avg episode reward: [(0, '973.505')]
[36m[2025-06-29 20:51:02,586][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 4214784. Throughput: 0: 366.6. Samples: 4217824. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 20:51:02,587][187912] Avg episode reward: [(0, '1015.181')]
[36m[2025-06-29 20:51:07,559][187912] Fps is (10 sec: 411.5, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 4218880. Throughput: 0: 364.1. Samples: 4219840. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 20:51:07,559][187912] Avg episode reward: [(0, '980.557')]
[36m[2025-06-29 20:51:12,581][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 347.8). Total num frames: 4218880. Throughput: 0: 365.2. Samples: 4221040. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 20:51:12,581][187912] Avg episode reward: [(0, '945.351')]
[36m[2025-06-29 20:51:17,569][187912] Fps is (10 sec: 409.2, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 4222976. Throughput: 0: 367.8. Samples: 4223168. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:51:17,569][187912] Avg episode reward: [(0, '942.456')]
[36m[2025-06-29 20:51:22,603][187912] Fps is (10 sec: 408.7, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 4222976. Throughput: 0: 368.3. Samples: 4225456. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:51:22,603][187912] Avg episode reward: [(0, '910.092')]
[36m[2025-06-29 20:51:27,594][187912] Fps is (10 sec: 408.6, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 4227072. Throughput: 0: 365.1. Samples: 4226512. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:51:27,594][187912] Avg episode reward: [(0, '943.693')]
[36m[2025-06-29 20:51:32,575][187912] Fps is (10 sec: 410.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4227072. Throughput: 0: 363.3. Samples: 4228592. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:51:32,575][187912] Avg episode reward: [(0, '919.666')]
[36m[2025-06-29 20:51:37,571][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 4227072. Throughput: 0: 364.4. Samples: 4230816. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:51:37,571][187912] Avg episode reward: [(0, '936.415')]
[36m[2025-06-29 20:51:42,601][187912] Fps is (10 sec: 408.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4231168. Throughput: 0: 357.4. Samples: 4231680. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:51:42,601][187912] Avg episode reward: [(0, '978.872')]
[36m[2025-06-29 20:51:47,578][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 4231168. Throughput: 0: 357.0. Samples: 4233888. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:51:47,578][187912] Avg episode reward: [(0, '1003.243')]
[36m[2025-06-29 20:51:52,574][187912] Fps is (10 sec: 410.7, 60 sec: 345.3, 300 sec: 361.0). Total num frames: 4235264. Throughput: 0: 354.4. Samples: 4235792. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:51:52,574][187912] Avg episode reward: [(0, '1050.259')]
[36m[2025-06-29 20:51:57,565][187912] Fps is (10 sec: 410.1, 60 sec: 341.6, 300 sec: 361.0). Total num frames: 4235264. Throughput: 0: 355.0. Samples: 4237008. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:51:57,566][187912] Avg episode reward: [(0, '1029.902')]
[36m[2025-06-29 20:52:02,583][187912] Fps is (10 sec: 409.2, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 4239360. Throughput: 0: 358.3. Samples: 4239296. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 20:52:02,583][187912] Avg episode reward: [(0, '1001.288')]
[36m[2025-06-29 20:52:07,581][187912] Fps is (10 sec: 409.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4239360. Throughput: 0: 354.3. Samples: 4241392. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 20:52:07,581][187912] Avg episode reward: [(0, '1006.512')]
[36m[2025-06-29 20:52:13,216][187912] Fps is (10 sec: 385.2, 60 sec: 405.3, 300 sec: 360.2). Total num frames: 4243456. Throughput: 0: 351.1. Samples: 4242528. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 20:52:13,216][187912] Avg episode reward: [(0, '964.957')]
[37m[1m[2025-06-29 20:52:13,262][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000016576_4243456.pth...
[36m[2025-06-29 20:52:13,327][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000016224_4153344.pth
[36m[2025-06-29 20:52:17,564][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4243456. Throughput: 0: 354.9. Samples: 4244560. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 20:52:17,564][187912] Avg episode reward: [(0, '895.196')]
[36m[2025-06-29 20:52:22,590][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 4243456. Throughput: 0: 357.2. Samples: 4246896. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 20:52:22,590][187912] Avg episode reward: [(0, '947.375')]
[36m[2025-06-29 20:52:27,564][187912] Fps is (10 sec: 409.6, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4247552. Throughput: 0: 356.6. Samples: 4247712. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 20:52:27,564][187912] Avg episode reward: [(0, '994.721')]
[36m[2025-06-29 20:52:32,587][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4247552. Throughput: 0: 356.2. Samples: 4249920. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 20:52:32,587][187912] Avg episode reward: [(0, '1018.775')]
[36m[2025-06-29 20:52:37,585][187912] Fps is (10 sec: 408.7, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 4251648. Throughput: 0: 358.0. Samples: 4251904. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 20:52:37,585][187912] Avg episode reward: [(0, '1088.324')]
[37m[1m[2025-06-29 20:52:37,623][187912] Saving new best policy, reward=1088.324!
[36m[2025-06-29 20:52:42,587][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4251648. Throughput: 0: 357.5. Samples: 4253104. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 20:52:42,587][187912] Avg episode reward: [(0, '1147.167')]
[37m[1m[2025-06-29 20:52:42,632][187912] Saving new best policy, reward=1147.167!
[36m[2025-06-29 20:52:47,825][187912] Fps is (10 sec: 400.0, 60 sec: 407.9, 300 sec: 360.7). Total num frames: 4255744. Throughput: 0: 355.8. Samples: 4255392. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:52:47,825][187912] Avg episode reward: [(0, '1103.050')]
[36m[2025-06-29 20:52:52,583][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4255744. Throughput: 0: 354.8. Samples: 4257360. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:52:52,584][187912] Avg episode reward: [(0, '1041.210')]
[36m[2025-06-29 20:52:57,584][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.2). Total num frames: 4255744. Throughput: 0: 358.5. Samples: 4258432. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:52:57,584][187912] Avg episode reward: [(0, '1036.363')]
[36m[2025-06-29 20:53:02,569][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4259840. Throughput: 0: 352.7. Samples: 4260432. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:53:02,569][187912] Avg episode reward: [(0, '990.931')]
[36m[2025-06-29 20:53:07,582][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4259840. Throughput: 0: 354.9. Samples: 4262864. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:53:07,582][187912] Avg episode reward: [(0, '1007.304')]
[36m[2025-06-29 20:53:12,570][187912] Fps is (10 sec: 409.6, 60 sec: 345.0, 300 sec: 361.0). Total num frames: 4263936. Throughput: 0: 361.2. Samples: 4263968. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 20:53:12,570][187912] Avg episode reward: [(0, '1033.115')]
[36m[2025-06-29 20:53:17,558][187912] Fps is (10 sec: 410.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4263936. Throughput: 0: 358.6. Samples: 4266048. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 20:53:17,558][187912] Avg episode reward: [(0, '1045.691')]
[31m[11953861 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11953861 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[11953861 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:53:22,558][187912] Fps is (10 sec: 410.1, 60 sec: 409.8, 300 sec: 361.1). Total num frames: 4268032. Throughput: 0: 360.4. Samples: 4268112. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 20:53:22,558][187912] Avg episode reward: [(0, '1085.780')]
[36m[2025-06-29 20:53:27,558][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4268032. Throughput: 0: 360.0. Samples: 4269296. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 20:53:27,558][187912] Avg episode reward: [(0, '1072.696')]
[36m[2025-06-29 20:53:32,586][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 4268032. Throughput: 0: 359.2. Samples: 4271472. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 20:53:32,586][187912] Avg episode reward: [(0, '1074.363')]
[36m[2025-06-29 20:53:37,592][187912] Fps is (10 sec: 408.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4272128. Throughput: 0: 359.8. Samples: 4273552. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 20:53:37,592][187912] Avg episode reward: [(0, '1059.795')]
[36m[2025-06-29 20:53:42,583][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4272128. Throughput: 0: 360.9. Samples: 4274672. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 20:53:42,583][187912] Avg episode reward: [(0, '983.102')]
[31m[11976716 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[11976716 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[11976716 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:53:47,577][187912] Fps is (10 sec: 410.2, 60 sec: 342.8, 300 sec: 361.0). Total num frames: 4276224. Throughput: 0: 359.4. Samples: 4276608. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 20:53:47,577][187912] Avg episode reward: [(0, '963.417')]
[36m[2025-06-29 20:53:52,581][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4276224. Throughput: 0: 357.0. Samples: 4278928. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 20:53:52,581][187912] Avg episode reward: [(0, '955.620')]
[36m[2025-06-29 20:53:57,573][187912] Fps is (10 sec: 409.8, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 4280320. Throughput: 0: 357.3. Samples: 4280048. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 20:53:57,573][187912] Avg episode reward: [(0, '936.560')]
[36m[2025-06-29 20:54:02,601][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4280320. Throughput: 0: 356.6. Samples: 4282112. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 20:54:02,601][187912] Avg episode reward: [(0, '920.177')]
[36m[2025-06-29 20:54:07,932][187912] Fps is (10 sec: 395.4, 60 sec: 407.2, 300 sec: 360.6). Total num frames: 4284416. Throughput: 0: 359.0. Samples: 4284400. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 20:54:07,932][187912] Avg episode reward: [(0, '951.488')]
[36m[2025-06-29 20:54:12,570][187912] Fps is (10 sec: 410.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4284416. Throughput: 0: 356.9. Samples: 4285360. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 20:54:12,570][187912] Avg episode reward: [(0, '985.375')]
[37m[1m[2025-06-29 20:54:12,613][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000016736_4284416.pth...
[36m[2025-06-29 20:54:12,731][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000016400_4198400.pth
[36m[2025-06-29 20:54:17,611][187912] Fps is (10 sec: 0.0, 60 sec: 341.0, 300 sec: 347.1). Total num frames: 4284416. Throughput: 0: 357.5. Samples: 4287568. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 20:54:17,612][187912] Avg episode reward: [(0, '949.794')]
[31m[12011193 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12011193 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[12011193 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:54:22,565][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4288512. Throughput: 0: 355.8. Samples: 4289552. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 20:54:22,566][187912] Avg episode reward: [(0, '1009.367')]
[36m[2025-06-29 20:54:27,590][187912] Fps is (10 sec: 410.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4288512. Throughput: 0: 355.9. Samples: 4290688. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 20:54:27,590][187912] Avg episode reward: [(0, '1014.430')]
[31m[12025617 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12025617 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[12025618 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:54:32,575][187912] Fps is (10 sec: 409.2, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 4292608. Throughput: 0: 356.6. Samples: 4292656. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 20:54:32,576][187912] Avg episode reward: [(0, '971.321')]
[36m[2025-06-29 20:54:37,573][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4292608. Throughput: 0: 355.3. Samples: 4294912. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 20:54:37,573][187912] Avg episode reward: [(0, '983.182')]
[31m[12035294 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12035294 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([12], device='cuda:0') (navigation_task.py:196)
[31m[12035295 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:54:42,901][187912] Fps is (10 sec: 396.7, 60 sec: 407.4, 300 sec: 360.6). Total num frames: 4296704. Throughput: 0: 350.9. Samples: 4295952. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:54:42,901][187912] Avg episode reward: [(0, '964.926')]
[36m[2025-06-29 20:54:47,572][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4296704. Throughput: 0: 354.0. Samples: 4298032. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:54:47,572][187912] Avg episode reward: [(0, '911.078')]
[36m[2025-06-29 20:54:52,560][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 4296704. Throughput: 0: 359.6. Samples: 4300448. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:54:52,560][187912] Avg episode reward: [(0, '876.294')]
[36m[2025-06-29 20:54:57,595][187912] Fps is (10 sec: 408.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4300800. Throughput: 0: 356.1. Samples: 4301392. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:54:57,595][187912] Avg episode reward: [(0, '868.986')]
[36m[2025-06-29 20:55:02,586][187912] Fps is (10 sec: 408.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4300800. Throughput: 0: 357.9. Samples: 4303664. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:55:02,587][187912] Avg episode reward: [(0, '867.215')]
[36m[2025-06-29 20:55:07,592][187912] Fps is (10 sec: 409.7, 60 sec: 343.3, 300 sec: 361.0). Total num frames: 4304896. Throughput: 0: 357.8. Samples: 4305664. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 20:55:07,592][187912] Avg episode reward: [(0, '893.882')]
[36m[2025-06-29 20:55:12,612][187912] Fps is (10 sec: 408.6, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 4304896. Throughput: 0: 356.8. Samples: 4306752. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 20:55:12,612][187912] Avg episode reward: [(0, '933.810')]
[36m[2025-06-29 20:55:17,563][187912] Fps is (10 sec: 410.8, 60 sec: 409.9, 300 sec: 361.0). Total num frames: 4308992. Throughput: 0: 360.3. Samples: 4308864. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 20:55:17,563][187912] Avg episode reward: [(0, '952.520')]
[36m[2025-06-29 20:55:22,595][187912] Fps is (10 sec: 410.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4308992. Throughput: 0: 352.2. Samples: 4310768. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 20:55:22,595][187912] Avg episode reward: [(0, '998.521')]
[36m[2025-06-29 20:55:27,562][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.2). Total num frames: 4308992. Throughput: 0: 358.3. Samples: 4311952. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 20:55:27,562][187912] Avg episode reward: [(0, '966.499')]
[36m[2025-06-29 20:55:32,586][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4313088. Throughput: 0: 355.1. Samples: 4314016. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 20:55:32,586][187912] Avg episode reward: [(0, '997.460')]
[36m[2025-06-29 20:55:37,575][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 4313088. Throughput: 0: 354.0. Samples: 4316384. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 20:55:37,576][187912] Avg episode reward: [(0, '995.012')]
[36m[2025-06-29 20:55:42,590][187912] Fps is (10 sec: 409.4, 60 sec: 343.1, 300 sec: 361.0). Total num frames: 4317184. Throughput: 0: 351.7. Samples: 4317216. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:55:42,590][187912] Avg episode reward: [(0, '1039.649')]
[36m[2025-06-29 20:55:47,583][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 347.9). Total num frames: 4317184. Throughput: 0: 352.4. Samples: 4319520. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:55:47,583][187912] Avg episode reward: [(0, '1021.773')]
[36m[2025-06-29 20:55:52,578][187912] Fps is (10 sec: 410.1, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 4321280. Throughput: 0: 352.5. Samples: 4321520. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:55:52,578][187912] Avg episode reward: [(0, '1059.041')]
[36m[2025-06-29 20:55:57,599][187912] Fps is (10 sec: 408.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4321280. Throughput: 0: 354.6. Samples: 4322704. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:55:57,599][187912] Avg episode reward: [(0, '1019.547')]
[36m[2025-06-29 20:56:02,900][187912] Fps is (10 sec: 396.8, 60 sec: 407.5, 300 sec: 360.6). Total num frames: 4325376. Throughput: 0: 355.7. Samples: 4324992. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:56:02,901][187912] Avg episode reward: [(0, '1016.652')]
[36m[2025-06-29 20:56:07,577][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4325376. Throughput: 0: 361.4. Samples: 4327024. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:56:07,577][187912] Avg episode reward: [(0, '957.608')]
[36m[2025-06-29 20:56:12,594][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 4325376. Throughput: 0: 361.7. Samples: 4328240. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:56:12,594][187912] Avg episode reward: [(0, '1001.577')]
[37m[1m[2025-06-29 20:56:12,647][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000016896_4325376.pth...
[36m[2025-06-29 20:56:12,718][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000016576_4243456.pth
[36m[2025-06-29 20:56:17,590][187912] Fps is (10 sec: 409.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4329472. Throughput: 0: 360.5. Samples: 4330240. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:56:17,590][187912] Avg episode reward: [(0, '960.993')]
[36m[2025-06-29 20:56:22,595][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 4329472. Throughput: 0: 362.9. Samples: 4332720. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:56:22,595][187912] Avg episode reward: [(0, '1002.461')]
[36m[2025-06-29 20:56:27,559][187912] Fps is (10 sec: 410.9, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 4333568. Throughput: 0: 364.7. Samples: 4333616. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:56:27,559][187912] Avg episode reward: [(0, '1010.002')]
[36m[2025-06-29 20:56:32,583][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4333568. Throughput: 0: 365.5. Samples: 4335968. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:56:32,583][187912] Avg episode reward: [(0, '1001.575')]
[36m[2025-06-29 20:56:37,594][187912] Fps is (10 sec: 408.2, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 4337664. Throughput: 0: 365.4. Samples: 4337968. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:56:37,595][187912] Avg episode reward: [(0, '1000.374')]
[36m[2025-06-29 20:56:42,573][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4337664. Throughput: 0: 362.9. Samples: 4339024. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:56:42,574][187912] Avg episode reward: [(0, '971.511')]
[36m[2025-06-29 20:56:48,143][187912] Fps is (10 sec: 388.3, 60 sec: 405.8, 300 sec: 360.3). Total num frames: 4341760. Throughput: 0: 361.8. Samples: 4341360. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 20:56:48,143][187912] Avg episode reward: [(0, '899.694')]
[36m[2025-06-29 20:56:52,559][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4341760. Throughput: 0: 362.5. Samples: 4343328. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 20:56:52,559][187912] Avg episode reward: [(0, '956.853')]
[36m[2025-06-29 20:56:57,596][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 4341760. Throughput: 0: 359.8. Samples: 4344432. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 20:56:57,596][187912] Avg episode reward: [(0, '968.505')]
[36m[2025-06-29 20:57:02,571][187912] Fps is (10 sec: 409.1, 60 sec: 343.2, 300 sec: 361.0). Total num frames: 4345856. Throughput: 0: 358.9. Samples: 4346384. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 20:57:02,571][187912] Avg episode reward: [(0, '1004.784')]
[31m[12176975 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12176975 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[12176975 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:57:07,589][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 347.9). Total num frames: 4345856. Throughput: 0: 354.9. Samples: 4348688. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 20:57:07,589][187912] Avg episode reward: [(0, '1035.332')]
[36m[2025-06-29 20:57:12,571][187912] Fps is (10 sec: 409.6, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 4349952. Throughput: 0: 357.9. Samples: 4349728. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:57:12,572][187912] Avg episode reward: [(0, '1025.933')]
[36m[2025-06-29 20:57:17,570][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4349952. Throughput: 0: 350.7. Samples: 4351744. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:57:17,570][187912] Avg episode reward: [(0, '1005.165')]
[36m[2025-06-29 20:57:22,777][187912] Fps is (10 sec: 401.3, 60 sec: 408.4, 300 sec: 360.7). Total num frames: 4354048. Throughput: 0: 330.4. Samples: 4352896. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:57:22,778][187912] Avg episode reward: [(0, '937.311')]
[36m[2025-06-29 20:57:27,590][187912] Fps is (10 sec: 408.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4354048. Throughput: 0: 355.8. Samples: 4355040. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:57:27,591][187912] Avg episode reward: [(0, '973.559')]
[36m[2025-06-29 20:57:32,570][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 4354048. Throughput: 0: 360.9. Samples: 4357392. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:57:32,570][187912] Avg episode reward: [(0, '961.840')]
[36m[2025-06-29 20:57:37,592][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4358144. Throughput: 0: 355.3. Samples: 4359328. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 20:57:37,592][187912] Avg episode reward: [(0, '994.188')]
[36m[2025-06-29 20:57:42,595][187912] Fps is (10 sec: 408.6, 60 sec: 341.2, 300 sec: 347.4). Total num frames: 4358144. Throughput: 0: 357.3. Samples: 4360512. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 20:57:42,595][187912] Avg episode reward: [(0, '1043.173')]
[36m[2025-06-29 20:57:47,558][187912] Fps is (10 sec: 411.0, 60 sec: 344.7, 300 sec: 361.0). Total num frames: 4362240. Throughput: 0: 361.7. Samples: 4362656. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 20:57:47,558][187912] Avg episode reward: [(0, '1032.541')]
[36m[2025-06-29 20:57:52,563][187912] Fps is (10 sec: 410.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4362240. Throughput: 0: 363.6. Samples: 4365040. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 20:57:52,563][187912] Avg episode reward: [(0, '1015.528')]
[33m[12226557 ms][navigation_task] - WARNING : Curriculum Level: 47, Curriculum progress fraction: 0.9142857142857143 (navigation_task.py:262)
[33m[12226557 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.82421875
[33mCrash Rate: 0.13623046875
[33mTimeout Rate: 0.03955078125 (navigation_task.py:265)
[33m[12226557 ms][navigation_task] - WARNING : 
[33mSuccesses: 1688
[33mCrashes : 279
[33mTimeouts: 81 (navigation_task.py:268)
[36m[2025-06-29 20:57:57,589][187912] Fps is (10 sec: 408.4, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 4366336. Throughput: 0: 365.4. Samples: 4366176. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 20:57:57,589][187912] Avg episode reward: [(0, '1038.272')]
[31m[12235575 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12235576 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[12235576 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:58:02,564][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4366336. Throughput: 0: 365.6. Samples: 4368192. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 20:58:02,565][187912] Avg episode reward: [(0, '1049.001')]
[36m[2025-06-29 20:58:07,703][187912] Fps is (10 sec: 405.0, 60 sec: 408.8, 300 sec: 360.8). Total num frames: 4370432. Throughput: 0: 365.0. Samples: 4369296. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 20:58:07,703][187912] Avg episode reward: [(0, '1053.605')]
[36m[2025-06-29 20:58:12,583][187912] Fps is (10 sec: 408.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4370432. Throughput: 0: 364.5. Samples: 4371440. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 20:58:12,583][187912] Avg episode reward: [(0, '1068.244')]
[37m[1m[2025-06-29 20:58:12,628][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000017072_4370432.pth...
[36m[2025-06-29 20:58:12,690][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000016736_4284416.pth
[31m[12247367 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12247367 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[12247367 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:58:17,584][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 4370432. Throughput: 0: 361.8. Samples: 4373680. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 20:58:17,584][187912] Avg episode reward: [(0, '991.180')]
[36m[2025-06-29 20:58:22,586][187912] Fps is (10 sec: 409.5, 60 sec: 342.4, 300 sec: 361.0). Total num frames: 4374528. Throughput: 0: 364.8. Samples: 4375744. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 20:58:22,586][187912] Avg episode reward: [(0, '984.156')]
[36m[2025-06-29 20:58:27,560][187912] Fps is (10 sec: 410.6, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4374528. Throughput: 0: 363.0. Samples: 4376832. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 20:58:27,560][187912] Avg episode reward: [(0, '1014.203')]
[36m[2025-06-29 20:58:32,594][187912] Fps is (10 sec: 409.3, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 4378624. Throughput: 0: 361.3. Samples: 4378928. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 20:58:32,594][187912] Avg episode reward: [(0, '950.341')]
[36m[2025-06-29 20:58:37,594][187912] Fps is (10 sec: 408.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4378624. Throughput: 0: 358.9. Samples: 4381200. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 20:58:37,594][187912] Avg episode reward: [(0, '977.021')]
[36m[2025-06-29 20:58:42,559][187912] Fps is (10 sec: 411.0, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 4382720. Throughput: 0: 360.1. Samples: 4382368. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:58:42,559][187912] Avg episode reward: [(0, '1025.362')]
[36m[2025-06-29 20:58:47,582][187912] Fps is (10 sec: 410.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4382720. Throughput: 0: 360.0. Samples: 4384400. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:58:47,582][187912] Avg episode reward: [(0, '1090.695')]
[31m[12285640 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12285641 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[12285641 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:58:52,599][187912] Fps is (10 sec: 0.0, 60 sec: 341.1, 300 sec: 347.1). Total num frames: 4382720. Throughput: 0: 386.3. Samples: 4386640. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:58:52,599][187912] Avg episode reward: [(0, '980.563')]
[36m[2025-06-29 20:58:57,617][187912] Fps is (10 sec: 408.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4386816. Throughput: 0: 357.8. Samples: 4387552. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:58:57,618][187912] Avg episode reward: [(0, '1042.934')]
[31m[12294121 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12294122 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[12294122 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:59:02,578][187912] Fps is (10 sec: 410.4, 60 sec: 341.3, 300 sec: 347.5). Total num frames: 4386816. Throughput: 0: 358.1. Samples: 4389792. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 20:59:02,579][187912] Avg episode reward: [(0, '1011.059')]
[31m[12297354 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12297355 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[12297355 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:59:07,582][187912] Fps is (10 sec: 411.1, 60 sec: 342.0, 300 sec: 361.0). Total num frames: 4390912. Throughput: 0: 354.9. Samples: 4391712. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-06-29 20:59:07,582][187912] Avg episode reward: [(0, '1032.382')]
[31m[12303477 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12303478 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[12303478 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 20:59:12,590][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4390912. Throughput: 0: 357.1. Samples: 4392912. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-06-29 20:59:12,590][187912] Avg episode reward: [(0, '933.764')]
[36m[2025-06-29 20:59:17,594][187912] Fps is (10 sec: 409.1, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 4395008. Throughput: 0: 358.0. Samples: 4395040. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-06-29 20:59:17,594][187912] Avg episode reward: [(0, '1018.277')]
[36m[2025-06-29 20:59:22,568][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4395008. Throughput: 0: 352.6. Samples: 4397056. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-06-29 20:59:22,568][187912] Avg episode reward: [(0, '992.062')]
[36m[2025-06-29 20:59:28,047][187912] Fps is (10 sec: 391.9, 60 sec: 406.3, 300 sec: 360.4). Total num frames: 4399104. Throughput: 0: 350.0. Samples: 4398288. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:59:28,047][187912] Avg episode reward: [(0, '1031.183')]
[36m[2025-06-29 20:59:32,566][187912] Fps is (10 sec: 409.7, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4399104. Throughput: 0: 355.0. Samples: 4400368. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:59:32,567][187912] Avg episode reward: [(0, '1052.409')]
[36m[2025-06-29 20:59:37,587][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.5). Total num frames: 4399104. Throughput: 0: 354.2. Samples: 4402576. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:59:37,587][187912] Avg episode reward: [(0, '1121.675')]
[36m[2025-06-29 20:59:42,587][187912] Fps is (10 sec: 408.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4403200. Throughput: 0: 352.9. Samples: 4403424. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:59:42,588][187912] Avg episode reward: [(0, '1124.987')]
[36m[2025-06-29 20:59:47,569][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4403200. Throughput: 0: 353.1. Samples: 4405680. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 20:59:47,569][187912] Avg episode reward: [(0, '1104.114')]
[36m[2025-06-29 20:59:52,589][187912] Fps is (10 sec: 409.5, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 4407296. Throughput: 0: 356.2. Samples: 4407744. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 20:59:52,589][187912] Avg episode reward: [(0, '1144.605')]
[36m[2025-06-29 20:59:57,569][187912] Fps is (10 sec: 409.6, 60 sec: 341.6, 300 sec: 361.0). Total num frames: 4407296. Throughput: 0: 357.9. Samples: 4409008. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 20:59:57,569][187912] Avg episode reward: [(0, '1116.473')]
[36m[2025-06-29 21:00:02,574][187912] Fps is (10 sec: 410.2, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 4411392. Throughput: 0: 359.6. Samples: 4411216. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 21:00:02,574][187912] Avg episode reward: [(0, '1097.797')]
[36m[2025-06-29 21:00:07,597][187912] Fps is (10 sec: 408.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4411392. Throughput: 0: 359.6. Samples: 4413248. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 21:00:07,597][187912] Avg episode reward: [(0, '967.170')]
[36m[2025-06-29 21:00:12,562][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 4411392. Throughput: 0: 361.2. Samples: 4414368. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 21:00:12,562][187912] Avg episode reward: [(0, '1021.317')]
[37m[1m[2025-06-29 21:00:12,613][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000017232_4411392.pth...
[36m[2025-06-29 21:00:12,677][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000016896_4325376.pth
[36m[2025-06-29 21:00:17,579][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4415488. Throughput: 0: 354.4. Samples: 4416320. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 21:00:17,579][187912] Avg episode reward: [(0, '974.557')]
[36m[2025-06-29 21:00:22,592][187912] Fps is (10 sec: 408.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4415488. Throughput: 0: 358.0. Samples: 4418688. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 21:00:22,592][187912] Avg episode reward: [(0, '1010.304')]
[31m[12376613 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12376613 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[12376613 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:00:27,568][187912] Fps is (10 sec: 410.0, 60 sec: 344.1, 300 sec: 361.0). Total num frames: 4419584. Throughput: 0: 360.0. Samples: 4419616. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 21:00:27,568][187912] Avg episode reward: [(0, '988.569')]
[36m[2025-06-29 21:00:32,569][187912] Fps is (10 sec: 410.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4419584. Throughput: 0: 362.7. Samples: 4422000. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 21:00:32,569][187912] Avg episode reward: [(0, '1030.329')]
[36m[2025-06-29 21:00:37,571][187912] Fps is (10 sec: 409.5, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 4423680. Throughput: 0: 358.5. Samples: 4423872. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:00:37,571][187912] Avg episode reward: [(0, '1042.199')]
[36m[2025-06-29 21:00:42,591][187912] Fps is (10 sec: 408.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4423680. Throughput: 0: 356.4. Samples: 4425056. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:00:42,592][187912] Avg episode reward: [(0, '1043.498')]
[36m[2025-06-29 21:00:48,155][187912] Fps is (10 sec: 387.0, 60 sec: 405.6, 300 sec: 360.3). Total num frames: 4427776. Throughput: 0: 353.5. Samples: 4427328. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:00:48,155][187912] Avg episode reward: [(0, '1045.382')]
[36m[2025-06-29 21:00:52,571][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4427776. Throughput: 0: 356.1. Samples: 4429264. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:00:52,571][187912] Avg episode reward: [(0, '1022.718')]
[36m[2025-06-29 21:00:57,571][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.5). Total num frames: 4427776. Throughput: 0: 358.0. Samples: 4430480. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:00:57,572][187912] Avg episode reward: [(0, '1100.773')]
[36m[2025-06-29 21:01:02,609][187912] Fps is (10 sec: 408.0, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 4431872. Throughput: 0: 358.2. Samples: 4432448. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 21:01:02,610][187912] Avg episode reward: [(0, '1005.170')]
[36m[2025-06-29 21:01:07,580][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4431872. Throughput: 0: 356.7. Samples: 4434736. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 21:01:07,580][187912] Avg episode reward: [(0, '1029.948')]
[36m[2025-06-29 21:01:12,588][187912] Fps is (10 sec: 410.5, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 4435968. Throughput: 0: 361.4. Samples: 4435888. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 21:01:12,588][187912] Avg episode reward: [(0, '981.349')]
[36m[2025-06-29 21:01:17,571][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4435968. Throughput: 0: 353.1. Samples: 4437888. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 21:01:17,571][187912] Avg episode reward: [(0, '1031.783')]
[36m[2025-06-29 21:01:22,561][187912] Fps is (10 sec: 410.7, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 4440064. Throughput: 0: 360.6. Samples: 4440096. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 21:01:22,561][187912] Avg episode reward: [(0, '998.495')]
[36m[2025-06-29 21:01:27,570][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4440064. Throughput: 0: 357.2. Samples: 4441120. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 21:01:27,570][187912] Avg episode reward: [(0, '1043.035')]
[36m[2025-06-29 21:01:32,565][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 4440064. Throughput: 0: 362.4. Samples: 4443424. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 21:01:32,565][187912] Avg episode reward: [(0, '1091.562')]
[36m[2025-06-29 21:01:37,570][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4444160. Throughput: 0: 360.9. Samples: 4445504. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 21:01:37,570][187912] Avg episode reward: [(0, '1091.508')]
[36m[2025-06-29 21:01:42,568][187912] Fps is (10 sec: 409.5, 60 sec: 341.5, 300 sec: 347.8). Total num frames: 4444160. Throughput: 0: 360.6. Samples: 4446704. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 21:01:42,568][187912] Avg episode reward: [(0, '1084.353')]
[36m[2025-06-29 21:01:47,567][187912] Fps is (10 sec: 409.7, 60 sec: 344.7, 300 sec: 361.0). Total num frames: 4448256. Throughput: 0: 363.0. Samples: 4448768. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 21:01:47,567][187912] Avg episode reward: [(0, '1051.747')]
[36m[2025-06-29 21:01:52,567][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4448256. Throughput: 0: 364.6. Samples: 4451136. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 21:01:52,567][187912] Avg episode reward: [(0, '1043.770')]
[36m[2025-06-29 21:01:57,568][187912] Fps is (10 sec: 409.6, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 4452352. Throughput: 0: 366.7. Samples: 4452384. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 21:01:57,568][187912] Avg episode reward: [(0, '1040.485')]
[31m[12474851 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12474851 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[12474851 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:02:02,560][187912] Fps is (10 sec: 409.9, 60 sec: 341.6, 300 sec: 361.0). Total num frames: 4452352. Throughput: 0: 365.2. Samples: 4454320. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 21:02:02,561][187912] Avg episode reward: [(0, '975.822')]
[36m[2025-06-29 21:02:08,190][187912] Fps is (10 sec: 385.6, 60 sec: 405.5, 300 sec: 360.2). Total num frames: 4456448. Throughput: 0: 356.3. Samples: 4456352. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 21:02:08,190][187912] Avg episode reward: [(0, '960.855')]
[36m[2025-06-29 21:02:12,579][187912] Fps is (10 sec: 408.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4456448. Throughput: 0: 357.3. Samples: 4457200. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 21:02:12,580][187912] Avg episode reward: [(0, '969.859')]
[37m[1m[2025-06-29 21:02:12,621][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000017408_4456448.pth...
[36m[2025-06-29 21:02:12,692][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000017072_4370432.pth
[36m[2025-06-29 21:02:17,587][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.3). Total num frames: 4456448. Throughput: 0: 357.2. Samples: 4459504. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 21:02:17,587][187912] Avg episode reward: [(0, '1006.794')]
[36m[2025-06-29 21:02:22,566][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4460544. Throughput: 0: 357.7. Samples: 4461600. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 21:02:22,566][187912] Avg episode reward: [(0, '1022.781')]
[36m[2025-06-29 21:02:27,568][187912] Fps is (10 sec: 410.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4460544. Throughput: 0: 358.0. Samples: 4462816. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 21:02:27,568][187912] Avg episode reward: [(0, '1056.541')]
[36m[2025-06-29 21:02:32,571][187912] Fps is (10 sec: 409.4, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 4464640. Throughput: 0: 356.9. Samples: 4464832. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:02:32,572][187912] Avg episode reward: [(0, '1090.177')]
[36m[2025-06-29 21:02:37,588][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4464640. Throughput: 0: 356.5. Samples: 4467184. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:02:37,588][187912] Avg episode reward: [(0, '1108.409')]
[36m[2025-06-29 21:02:42,588][187912] Fps is (10 sec: 408.9, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 4468736. Throughput: 0: 353.3. Samples: 4468288. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:02:42,589][187912] Avg episode reward: [(0, '1126.924')]
[36m[2025-06-29 21:02:47,577][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4468736. Throughput: 0: 357.9. Samples: 4470432. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:02:47,577][187912] Avg episode reward: [(0, '1105.903')]
[36m[2025-06-29 21:02:53,200][187912] Fps is (10 sec: 386.0, 60 sec: 405.3, 300 sec: 360.3). Total num frames: 4472832. Throughput: 0: 363.7. Samples: 4472720. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:02:53,200][187912] Avg episode reward: [(0, '1091.472')]
[31m[12528054 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12528054 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[12528055 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:02:57,589][187912] Fps is (10 sec: 409.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4472832. Throughput: 0: 365.8. Samples: 4473664. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:02:57,589][187912] Avg episode reward: [(0, '1101.651')]
[36m[2025-06-29 21:03:02,566][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.3). Total num frames: 4472832. Throughput: 0: 367.1. Samples: 4476016. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:03:02,566][187912] Avg episode reward: [(0, '1123.940')]
[36m[2025-06-29 21:03:07,563][187912] Fps is (10 sec: 410.7, 60 sec: 344.9, 300 sec: 361.0). Total num frames: 4476928. Throughput: 0: 365.2. Samples: 4478032. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:03:07,563][187912] Avg episode reward: [(0, '1106.503')]
[36m[2025-06-29 21:03:12,593][187912] Fps is (10 sec: 408.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4476928. Throughput: 0: 364.2. Samples: 4479216. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:03:12,593][187912] Avg episode reward: [(0, '1082.162')]
[31m[12549216 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12549216 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[12549216 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:03:17,597][187912] Fps is (10 sec: 408.2, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 4481024. Throughput: 0: 364.2. Samples: 4481232. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 21:03:17,597][187912] Avg episode reward: [(0, '1050.969')]
[36m[2025-06-29 21:03:22,587][187912] Fps is (10 sec: 409.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4481024. Throughput: 0: 363.0. Samples: 4483520. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 21:03:22,587][187912] Avg episode reward: [(0, '1054.547')]
[36m[2025-06-29 21:03:27,588][187912] Fps is (10 sec: 409.9, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 4485120. Throughput: 0: 365.2. Samples: 4484720. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 21:03:27,589][187912] Avg episode reward: [(0, '1041.415')]
[36m[2025-06-29 21:03:32,590][187912] Fps is (10 sec: 409.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4485120. Throughput: 0: 362.9. Samples: 4486768. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 21:03:32,590][187912] Avg episode reward: [(0, '1071.843')]
[36m[2025-06-29 21:03:37,905][187912] Fps is (10 sec: 397.0, 60 sec: 407.4, 300 sec: 360.6). Total num frames: 4489216. Throughput: 0: 369.0. Samples: 4489216. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 21:03:37,906][187912] Avg episode reward: [(0, '1091.541')]
[36m[2025-06-29 21:03:42,563][187912] Fps is (10 sec: 410.7, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4489216. Throughput: 0: 366.8. Samples: 4490160. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 21:03:42,563][187912] Avg episode reward: [(0, '1054.255')]
[36m[2025-06-29 21:03:47,580][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4489216. Throughput: 0: 366.1. Samples: 4492496. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 21:03:47,580][187912] Avg episode reward: [(0, '1089.212')]
[36m[2025-06-29 21:03:52,575][187912] Fps is (10 sec: 409.1, 60 sec: 344.9, 300 sec: 361.1). Total num frames: 4493312. Throughput: 0: 368.3. Samples: 4494608. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 21:03:52,575][187912] Avg episode reward: [(0, '1084.430')]
[36m[2025-06-29 21:03:57,593][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4493312. Throughput: 0: 367.3. Samples: 4495744. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 21:03:57,593][187912] Avg episode reward: [(0, '1067.456')]
[36m[2025-06-29 21:04:02,598][187912] Fps is (10 sec: 408.6, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 4497408. Throughput: 0: 370.5. Samples: 4497904. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 21:04:02,598][187912] Avg episode reward: [(0, '1048.354')]
[36m[2025-06-29 21:04:07,558][187912] Fps is (10 sec: 411.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4497408. Throughput: 0: 373.9. Samples: 4500336. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 21:04:07,558][187912] Avg episode reward: [(0, '1151.536')]
[37m[1m[2025-06-29 21:04:07,600][187912] Saving new best policy, reward=1151.536!
[36m[2025-06-29 21:04:12,575][187912] Fps is (10 sec: 410.6, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 4501504. Throughput: 0: 372.0. Samples: 4501456. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 21:04:12,575][187912] Avg episode reward: [(0, '1100.810')]
[37m[1m[2025-06-29 21:04:12,638][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000017584_4501504.pth...
[36m[2025-06-29 21:04:12,707][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000017232_4411392.pth
[36m[2025-06-29 21:04:17,564][187912] Fps is (10 sec: 409.4, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4501504. Throughput: 0: 367.1. Samples: 4503280. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 21:04:17,565][187912] Avg episode reward: [(0, '1060.690')]
[31m[12615726 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12615727 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[12615727 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:04:23,251][187912] Fps is (10 sec: 383.6, 60 sec: 405.1, 300 sec: 360.8). Total num frames: 4505600. Throughput: 0: 358.5. Samples: 4505472. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 21:04:23,252][187912] Avg episode reward: [(0, '989.818')]
[36m[2025-06-29 21:04:27,574][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4505600. Throughput: 0: 360.1. Samples: 4506368. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 21:04:27,574][187912] Avg episode reward: [(0, '961.317')]
[36m[2025-06-29 21:04:32,597][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4505600. Throughput: 0: 359.3. Samples: 4508672. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 21:04:32,597][187912] Avg episode reward: [(0, '887.184')]
[36m[2025-06-29 21:04:37,595][187912] Fps is (10 sec: 408.8, 60 sec: 343.1, 300 sec: 361.0). Total num frames: 4509696. Throughput: 0: 356.5. Samples: 4510656. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 21:04:37,595][187912] Avg episode reward: [(0, '899.325')]
[36m[2025-06-29 21:04:42,567][187912] Fps is (10 sec: 410.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4509696. Throughput: 0: 358.2. Samples: 4511856. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 21:04:42,568][187912] Avg episode reward: [(0, '887.613')]
[36m[2025-06-29 21:04:47,584][187912] Fps is (10 sec: 410.0, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 4513792. Throughput: 0: 358.2. Samples: 4514016. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:04:47,585][187912] Avg episode reward: [(0, '994.188')]
[36m[2025-06-29 21:04:52,588][187912] Fps is (10 sec: 408.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4513792. Throughput: 0: 358.2. Samples: 4516464. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:04:52,588][187912] Avg episode reward: [(0, '940.304')]
[36m[2025-06-29 21:04:57,566][187912] Fps is (10 sec: 410.4, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 4517888. Throughput: 0: 358.8. Samples: 4517600. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:04:57,566][187912] Avg episode reward: [(0, '974.994')]
[36m[2025-06-29 21:05:02,571][187912] Fps is (10 sec: 410.3, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4517888. Throughput: 0: 363.3. Samples: 4519632. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:05:02,571][187912] Avg episode reward: [(0, '1013.363')]
[36m[2025-06-29 21:05:07,967][187912] Fps is (10 sec: 393.8, 60 sec: 406.8, 300 sec: 374.4). Total num frames: 4521984. Throughput: 0: 368.9. Samples: 4521968. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:05:07,967][187912] Avg episode reward: [(0, '1072.654')]
[36m[2025-06-29 21:05:12,595][187912] Fps is (10 sec: 408.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4521984. Throughput: 0: 367.1. Samples: 4522896. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:05:12,595][187912] Avg episode reward: [(0, '987.641')]
[36m[2025-06-29 21:05:17,563][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4521984. Throughput: 0: 368.6. Samples: 4525248. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:05:17,563][187912] Avg episode reward: [(0, '1060.832')]
[36m[2025-06-29 21:05:22,610][187912] Fps is (10 sec: 409.0, 60 sec: 345.0, 300 sec: 361.0). Total num frames: 4526080. Throughput: 0: 367.9. Samples: 4527216. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:05:22,610][187912] Avg episode reward: [(0, '1030.134')]
[36m[2025-06-29 21:05:27,569][187912] Fps is (10 sec: 409.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4526080. Throughput: 0: 365.5. Samples: 4528304. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:05:27,570][187912] Avg episode reward: [(0, '976.620')]
[36m[2025-06-29 21:05:32,568][187912] Fps is (10 sec: 411.3, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 4530176. Throughput: 0: 360.3. Samples: 4530224. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:05:32,569][187912] Avg episode reward: [(0, '943.787')]
[36m[2025-06-29 21:05:37,564][187912] Fps is (10 sec: 409.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4530176. Throughput: 0: 358.6. Samples: 4532592. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:05:37,565][187912] Avg episode reward: [(0, '1042.220')]
[36m[2025-06-29 21:05:42,570][187912] Fps is (10 sec: 409.5, 60 sec: 409.6, 300 sec: 361.7). Total num frames: 4534272. Throughput: 0: 359.8. Samples: 4533792. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:05:42,570][187912] Avg episode reward: [(0, '969.836')]
[36m[2025-06-29 21:05:47,609][187912] Fps is (10 sec: 407.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4534272. Throughput: 0: 361.6. Samples: 4535920. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:05:47,609][187912] Avg episode reward: [(0, '1006.289')]
[36m[2025-06-29 21:05:52,564][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4534272. Throughput: 0: 362.7. Samples: 4538144. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:05:52,564][187912] Avg episode reward: [(0, '1034.811')]
[36m[2025-06-29 21:05:57,560][187912] Fps is (10 sec: 411.6, 60 sec: 341.4, 300 sec: 361.1). Total num frames: 4538368. Throughput: 0: 360.5. Samples: 4539104. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 21:05:57,560][187912] Avg episode reward: [(0, '1086.123')]
[36m[2025-06-29 21:06:02,562][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4538368. Throughput: 0: 356.3. Samples: 4541280. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 21:06:02,562][187912] Avg episode reward: [(0, '1029.105')]
[36m[2025-06-29 21:06:07,592][187912] Fps is (10 sec: 408.3, 60 sec: 343.5, 300 sec: 361.0). Total num frames: 4542464. Throughput: 0: 357.8. Samples: 4543312. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 21:06:07,593][187912] Avg episode reward: [(0, '1072.365')]
[36m[2025-06-29 21:06:12,583][187912] Fps is (10 sec: 408.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4542464. Throughput: 0: 361.1. Samples: 4544560. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 21:06:12,583][187912] Avg episode reward: [(0, '1028.353')]
[37m[1m[2025-06-29 21:06:12,624][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000017744_4542464.pth...
[36m[2025-06-29 21:06:12,684][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000017408_4456448.pth
[36m[2025-06-29 21:06:17,589][187912] Fps is (10 sec: 409.7, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 4546560. Throughput: 0: 365.0. Samples: 4546656. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:06:17,589][187912] Avg episode reward: [(0, '1098.221')]
[31m[12733437 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12733438 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[12733438 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:06:22,577][187912] Fps is (10 sec: 409.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4546560. Throughput: 0: 363.6. Samples: 4548960. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:06:22,577][187912] Avg episode reward: [(0, '1024.734')]
[36m[2025-06-29 21:06:27,587][187912] Fps is (10 sec: 409.7, 60 sec: 409.5, 300 sec: 374.9). Total num frames: 4550656. Throughput: 0: 363.2. Samples: 4550144. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:06:27,587][187912] Avg episode reward: [(0, '1049.386')]
[31m[12744299 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12744299 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[12744299 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:06:32,574][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4550656. Throughput: 0: 362.6. Samples: 4552224. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:06:32,574][187912] Avg episode reward: [(0, '1038.884')]
[33m[12748530 ms][navigation_task] - WARNING : Curriculum Level: 49, Curriculum progress fraction: 0.9714285714285714 (navigation_task.py:262)
[33m[12748530 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.81982421875
[33mCrash Rate: 0.15283203125
[33mTimeout Rate: 0.02734375 (navigation_task.py:265)
[33m[12748530 ms][navigation_task] - WARNING : 
[33mSuccesses: 1679
[33mCrashes : 313
[33mTimeouts: 56 (navigation_task.py:268)
[36m[2025-06-29 21:06:37,586][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4550656. Throughput: 0: 362.5. Samples: 4554464. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:06:37,586][187912] Avg episode reward: [(0, '1098.140')]
[36m[2025-06-29 21:06:42,567][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4554752. Throughput: 0: 362.3. Samples: 4555408. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 21:06:42,567][187912] Avg episode reward: [(0, '1094.997')]
[36m[2025-06-29 21:06:47,592][187912] Fps is (10 sec: 409.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4554752. Throughput: 0: 367.0. Samples: 4557808. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 21:06:47,593][187912] Avg episode reward: [(0, '1117.022')]
[36m[2025-06-29 21:06:52,570][187912] Fps is (10 sec: 409.5, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 4558848. Throughput: 0: 367.8. Samples: 4559856. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 21:06:52,570][187912] Avg episode reward: [(0, '1112.603')]
[36m[2025-06-29 21:06:57,596][187912] Fps is (10 sec: 409.5, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 4558848. Throughput: 0: 362.9. Samples: 4560896. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 21:06:57,596][187912] Avg episode reward: [(0, '1131.520')]
[36m[2025-06-29 21:07:02,586][187912] Fps is (10 sec: 408.9, 60 sec: 409.4, 300 sec: 361.7). Total num frames: 4562944. Throughput: 0: 362.7. Samples: 4562976. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:07:02,587][187912] Avg episode reward: [(0, '1119.512')]
[36m[2025-06-29 21:07:07,562][187912] Fps is (10 sec: 411.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4562944. Throughput: 0: 358.9. Samples: 4565104. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:07:07,563][187912] Avg episode reward: [(0, '1054.951')]
[36m[2025-06-29 21:07:13,315][187912] Fps is (10 sec: 381.8, 60 sec: 404.7, 300 sec: 374.0). Total num frames: 4567040. Throughput: 0: 351.3. Samples: 4566208. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:07:13,315][187912] Avg episode reward: [(0, '1067.947')]
[31m[12789216 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12789216 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[12789217 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:07:17,558][187912] Fps is (10 sec: 409.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4567040. Throughput: 0: 354.6. Samples: 4568176. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:07:17,558][187912] Avg episode reward: [(0, '1060.555')]
[36m[2025-06-29 21:07:22,588][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4567040. Throughput: 0: 353.4. Samples: 4570368. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:07:22,588][187912] Avg episode reward: [(0, '1004.077')]
[36m[2025-06-29 21:07:27,607][187912] Fps is (10 sec: 407.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4571136. Throughput: 0: 352.0. Samples: 4571264. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 21:07:27,608][187912] Avg episode reward: [(0, '1025.942')]
[36m[2025-06-29 21:07:32,587][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4571136. Throughput: 0: 350.3. Samples: 4573568. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 21:07:32,587][187912] Avg episode reward: [(0, '1060.580')]
[31m[12807420 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12807421 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[12807421 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:07:37,563][187912] Fps is (10 sec: 411.4, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 4575232. Throughput: 0: 352.8. Samples: 4575728. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 21:07:37,564][187912] Avg episode reward: [(0, '1043.795')]
[36m[2025-06-29 21:07:42,600][187912] Fps is (10 sec: 409.1, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 4575232. Throughput: 0: 354.8. Samples: 4576864. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 21:07:42,601][187912] Avg episode reward: [(0, '1097.062')]
[36m[2025-06-29 21:07:47,568][187912] Fps is (10 sec: 409.4, 60 sec: 409.8, 300 sec: 361.8). Total num frames: 4579328. Throughput: 0: 361.4. Samples: 4579232. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:07:47,568][187912] Avg episode reward: [(0, '1093.909')]
[36m[2025-06-29 21:07:52,585][187912] Fps is (10 sec: 410.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4579328. Throughput: 0: 358.9. Samples: 4581264. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:07:52,586][187912] Avg episode reward: [(0, '1051.239')]
[36m[2025-06-29 21:07:57,578][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4579328. Throughput: 0: 366.5. Samples: 4582432. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:07:57,578][187912] Avg episode reward: [(0, '1059.741')]
[36m[2025-06-29 21:08:02,565][187912] Fps is (10 sec: 410.4, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4583424. Throughput: 0: 358.7. Samples: 4584320. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:08:02,565][187912] Avg episode reward: [(0, '1057.583')]
[36m[2025-06-29 21:08:07,589][187912] Fps is (10 sec: 409.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4583424. Throughput: 0: 359.5. Samples: 4586544. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:08:07,589][187912] Avg episode reward: [(0, '984.802')]
[36m[2025-06-29 21:08:12,586][187912] Fps is (10 sec: 408.7, 60 sec: 345.5, 300 sec: 361.0). Total num frames: 4587520. Throughput: 0: 362.1. Samples: 4587552. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:08:12,586][187912] Avg episode reward: [(0, '1047.945')]
[37m[1m[2025-06-29 21:08:12,630][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000017920_4587520.pth...
[36m[2025-06-29 21:08:12,694][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000017584_4501504.pth
[36m[2025-06-29 21:08:17,591][187912] Fps is (10 sec: 409.5, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 4587520. Throughput: 0: 358.7. Samples: 4589712. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:08:17,591][187912] Avg episode reward: [(0, '1050.215')]
[31m[12851840 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12851840 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[12851841 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:08:22,558][187912] Fps is (10 sec: 410.8, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 4591616. Throughput: 0: 356.3. Samples: 4591760. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:08:22,558][187912] Avg episode reward: [(0, '961.809')]
[36m[2025-06-29 21:08:27,577][187912] Fps is (10 sec: 410.2, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4591616. Throughput: 0: 358.2. Samples: 4592976. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:08:27,585][187912] Avg episode reward: [(0, '978.365')]
[36m[2025-06-29 21:08:33,259][187912] Fps is (10 sec: 382.8, 60 sec: 405.1, 300 sec: 360.6). Total num frames: 4595712. Throughput: 0: 349.8. Samples: 4595216. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 21:08:33,259][187912] Avg episode reward: [(0, '1025.711')]
[36m[2025-06-29 21:08:37,563][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4595712. Throughput: 0: 356.1. Samples: 4597280. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 21:08:37,564][187912] Avg episode reward: [(0, '1059.967')]
[36m[2025-06-29 21:08:42,594][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4595712. Throughput: 0: 356.1. Samples: 4598464. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 21:08:42,594][187912] Avg episode reward: [(0, '994.406')]
[31m[12876324 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[12876325 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([12], device='cuda:0') (navigation_task.py:196)
[31m[12876325 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:08:47,578][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4599808. Throughput: 0: 359.4. Samples: 4600496. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 21:08:47,578][187912] Avg episode reward: [(0, '1022.155')]
[36m[2025-06-29 21:08:52,564][187912] Fps is (10 sec: 410.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4599808. Throughput: 0: 361.1. Samples: 4602784. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 21:08:52,564][187912] Avg episode reward: [(0, '1031.392')]
[36m[2025-06-29 21:08:57,580][187912] Fps is (10 sec: 409.5, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 4603904. Throughput: 0: 364.1. Samples: 4603936. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:08:57,580][187912] Avg episode reward: [(0, '1017.660')]
[36m[2025-06-29 21:09:02,583][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4603904. Throughput: 0: 364.9. Samples: 4606128. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:09:02,583][187912] Avg episode reward: [(0, '1066.539')]
[36m[2025-06-29 21:09:07,573][187912] Fps is (10 sec: 409.9, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 4608000. Throughput: 0: 361.5. Samples: 4608032. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:09:07,573][187912] Avg episode reward: [(0, '1076.728')]
[36m[2025-06-29 21:09:12,560][187912] Fps is (10 sec: 410.6, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4608000. Throughput: 0: 360.0. Samples: 4609168. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:09:12,560][187912] Avg episode reward: [(0, '1200.072')]
[37m[1m[2025-06-29 21:09:12,600][187912] Saving new best policy, reward=1200.072!
[36m[2025-06-29 21:09:17,611][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.9). Total num frames: 4608000. Throughput: 0: 364.0. Samples: 4611360. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:09:17,611][187912] Avg episode reward: [(0, '1163.889')]
[36m[2025-06-29 21:09:22,600][187912] Fps is (10 sec: 408.0, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 4612096. Throughput: 0: 357.4. Samples: 4613376. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 21:09:22,600][187912] Avg episode reward: [(0, '1185.082')]
[36m[2025-06-29 21:09:27,570][187912] Fps is (10 sec: 411.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4612096. Throughput: 0: 357.2. Samples: 4614528. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 21:09:27,571][187912] Avg episode reward: [(0, '1151.250')]
[36m[2025-06-29 21:09:32,572][187912] Fps is (10 sec: 410.7, 60 sec: 345.3, 300 sec: 361.0). Total num frames: 4616192. Throughput: 0: 355.2. Samples: 4616480. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 21:09:32,573][187912] Avg episode reward: [(0, '1136.829')]
[36m[2025-06-29 21:09:37,566][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4616192. Throughput: 0: 355.5. Samples: 4618784. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 21:09:37,566][187912] Avg episode reward: [(0, '1190.184')]
[36m[2025-06-29 21:09:42,578][187912] Fps is (10 sec: 409.4, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 4620288. Throughput: 0: 355.6. Samples: 4619936. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:09:42,578][187912] Avg episode reward: [(0, '1168.217')]
[36m[2025-06-29 21:09:47,560][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4620288. Throughput: 0: 352.9. Samples: 4622000. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:09:47,560][187912] Avg episode reward: [(0, '1097.754')]
[36m[2025-06-29 21:09:52,602][187912] Fps is (10 sec: 0.0, 60 sec: 341.1, 300 sec: 347.1). Total num frames: 4620288. Throughput: 0: 358.9. Samples: 4624192. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:09:52,602][187912] Avg episode reward: [(0, '1133.731')]
[36m[2025-06-29 21:09:57,566][187912] Fps is (10 sec: 409.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4624384. Throughput: 0: 351.9. Samples: 4625008. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:09:57,567][187912] Avg episode reward: [(0, '1122.289')]
[36m[2025-06-29 21:10:02,604][187912] Fps is (10 sec: 409.5, 60 sec: 341.2, 300 sec: 347.5). Total num frames: 4624384. Throughput: 0: 355.6. Samples: 4627360. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:10:02,604][187912] Avg episode reward: [(0, '1105.270')]
[36m[2025-06-29 21:10:07,562][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4628480. Throughput: 0: 355.9. Samples: 4629376. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:10:07,562][187912] Avg episode reward: [(0, '1088.249')]
[36m[2025-06-29 21:10:12,575][187912] Fps is (10 sec: 410.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4628480. Throughput: 0: 355.5. Samples: 4630528. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:10:12,575][187912] Avg episode reward: [(0, '1105.868')]
[37m[1m[2025-06-29 21:10:12,618][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000018080_4628480.pth...
[36m[2025-06-29 21:10:12,691][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000017744_4542464.pth
[36m[2025-06-29 21:10:17,585][187912] Fps is (10 sec: 408.7, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 4632576. Throughput: 0: 358.3. Samples: 4632608. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:10:17,585][187912] Avg episode reward: [(0, '1015.247')]
[36m[2025-06-29 21:10:22,580][187912] Fps is (10 sec: 409.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4632576. Throughput: 0: 359.0. Samples: 4634944. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:10:22,580][187912] Avg episode reward: [(0, '973.235')]
[36m[2025-06-29 21:10:27,579][187912] Fps is (10 sec: 409.8, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 4636672. Throughput: 0: 357.7. Samples: 4636032. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:10:27,579][187912] Avg episode reward: [(0, '977.703')]
[36m[2025-06-29 21:10:32,598][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4636672. Throughput: 0: 357.4. Samples: 4638096. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:10:32,599][187912] Avg episode reward: [(0, '946.981')]
[36m[2025-06-29 21:10:37,599][187912] Fps is (10 sec: 0.0, 60 sec: 341.1, 300 sec: 347.1). Total num frames: 4636672. Throughput: 0: 355.9. Samples: 4640208. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:10:37,599][187912] Avg episode reward: [(0, '987.846')]
[36m[2025-06-29 21:10:42,567][187912] Fps is (10 sec: 410.9, 60 sec: 341.4, 300 sec: 361.1). Total num frames: 4640768. Throughput: 0: 358.4. Samples: 4641136. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:10:42,567][187912] Avg episode reward: [(0, '1017.690')]
[36m[2025-06-29 21:10:47,563][187912] Fps is (10 sec: 411.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4640768. Throughput: 0: 357.3. Samples: 4643424. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:10:47,564][187912] Avg episode reward: [(0, '1111.560')]
[36m[2025-06-29 21:10:52,567][187912] Fps is (10 sec: 409.6, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 4644864. Throughput: 0: 358.7. Samples: 4645520. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:10:52,567][187912] Avg episode reward: [(0, '1031.130')]
[31m[13009304 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13009304 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[13009304 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:10:57,612][187912] Fps is (10 sec: 407.6, 60 sec: 341.1, 300 sec: 360.9). Total num frames: 4644864. Throughput: 0: 359.5. Samples: 4646720. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:10:57,612][187912] Avg episode reward: [(0, '1011.793')]
[36m[2025-06-29 21:11:02,596][187912] Fps is (10 sec: 408.4, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 4648960. Throughput: 0: 363.6. Samples: 4648976. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:11:02,597][187912] Avg episode reward: [(0, '1090.768')]
[36m[2025-06-29 21:11:07,577][187912] Fps is (10 sec: 411.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4648960. Throughput: 0: 358.1. Samples: 4651056. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:11:07,577][187912] Avg episode reward: [(0, '1136.213')]
[31m[13024453 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13024453 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[13024454 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:11:13,015][187912] Fps is (10 sec: 393.1, 60 sec: 406.6, 300 sec: 360.5). Total num frames: 4653056. Throughput: 0: 357.1. Samples: 4652256. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 21:11:13,015][187912] Avg episode reward: [(0, '1099.721')]
[31m[13029879 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13029879 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[13029880 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:11:17,583][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4653056. Throughput: 0: 361.4. Samples: 4654352. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 21:11:17,583][187912] Avg episode reward: [(0, '1118.796')]
[36m[2025-06-29 21:11:22,562][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 4653056. Throughput: 0: 367.9. Samples: 4656752. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 21:11:22,562][187912] Avg episode reward: [(0, '1068.062')]
[36m[2025-06-29 21:11:27,588][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4657152. Throughput: 0: 366.4. Samples: 4657632. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 21:11:27,588][187912] Avg episode reward: [(0, '1027.667')]
[36m[2025-06-29 21:11:32,583][187912] Fps is (10 sec: 408.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4657152. Throughput: 0: 368.5. Samples: 4660016. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 21:11:32,584][187912] Avg episode reward: [(0, '1037.507')]
[36m[2025-06-29 21:11:37,592][187912] Fps is (10 sec: 409.4, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 4661248. Throughput: 0: 365.3. Samples: 4661968. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:11:37,592][187912] Avg episode reward: [(0, '1034.682')]
[36m[2025-06-29 21:11:42,570][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4661248. Throughput: 0: 362.6. Samples: 4663024. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:11:42,570][187912] Avg episode reward: [(0, '1084.886')]
[31m[13060789 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13060789 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[13060790 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:11:47,564][187912] Fps is (10 sec: 410.7, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 4665344. Throughput: 0: 364.0. Samples: 4665344. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:11:47,564][187912] Avg episode reward: [(0, '1100.436')]
[36m[2025-06-29 21:11:52,566][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4665344. Throughput: 0: 366.0. Samples: 4667520. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:11:52,566][187912] Avg episode reward: [(0, '1146.981')]
[36m[2025-06-29 21:11:57,728][187912] Fps is (10 sec: 403.0, 60 sec: 408.8, 300 sec: 360.8). Total num frames: 4669440. Throughput: 0: 367.5. Samples: 4668688. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:11:57,729][187912] Avg episode reward: [(0, '1129.524')]
[36m[2025-06-29 21:12:02,591][187912] Fps is (10 sec: 408.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4669440. Throughput: 0: 366.5. Samples: 4670848. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:12:02,591][187912] Avg episode reward: [(0, '1079.963')]
[36m[2025-06-29 21:12:07,610][187912] Fps is (10 sec: 0.0, 60 sec: 341.1, 300 sec: 348.0). Total num frames: 4669440. Throughput: 0: 362.6. Samples: 4673088. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:12:07,610][187912] Avg episode reward: [(0, '1076.881')]
[36m[2025-06-29 21:12:12,567][187912] Fps is (10 sec: 410.6, 60 sec: 343.9, 300 sec: 361.0). Total num frames: 4673536. Throughput: 0: 361.4. Samples: 4673888. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:12:12,568][187912] Avg episode reward: [(0, '1007.450')]
[37m[1m[2025-06-29 21:12:12,618][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000018256_4673536.pth...
[36m[2025-06-29 21:12:12,693][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000017920_4587520.pth
[36m[2025-06-29 21:12:17,575][187912] Fps is (10 sec: 411.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4673536. Throughput: 0: 359.9. Samples: 4676208. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:12:17,575][187912] Avg episode reward: [(0, '1036.059')]
[36m[2025-06-29 21:12:22,591][187912] Fps is (10 sec: 408.7, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 4677632. Throughput: 0: 363.7. Samples: 4678336. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 21:12:22,591][187912] Avg episode reward: [(0, '1000.599')]
[36m[2025-06-29 21:12:27,596][187912] Fps is (10 sec: 408.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4677632. Throughput: 0: 368.5. Samples: 4679616. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 21:12:27,596][187912] Avg episode reward: [(0, '1053.329')]
[36m[2025-06-29 21:12:32,571][187912] Fps is (10 sec: 410.4, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 4681728. Throughput: 0: 364.7. Samples: 4681760. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 21:12:32,571][187912] Avg episode reward: [(0, '1102.580')]
[36m[2025-06-29 21:12:37,563][187912] Fps is (10 sec: 410.9, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4681728. Throughput: 0: 368.0. Samples: 4684080. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 21:12:37,564][187912] Avg episode reward: [(0, '1114.726')]
[36m[2025-06-29 21:12:42,562][187912] Fps is (10 sec: 410.0, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 4685824. Throughput: 0: 369.0. Samples: 4685232. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 21:12:42,562][187912] Avg episode reward: [(0, '1113.606')]
[36m[2025-06-29 21:12:47,576][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4685824. Throughput: 0: 366.0. Samples: 4687312. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 21:12:47,576][187912] Avg episode reward: [(0, '1046.922')]
[36m[2025-06-29 21:12:52,559][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4685824. Throughput: 0: 368.1. Samples: 4689632. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 21:12:52,559][187912] Avg episode reward: [(0, '1032.328')]
[36m[2025-06-29 21:12:57,594][187912] Fps is (10 sec: 408.9, 60 sec: 342.1, 300 sec: 361.0). Total num frames: 4689920. Throughput: 0: 369.6. Samples: 4690528. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 21:12:57,594][187912] Avg episode reward: [(0, '976.325')]
[36m[2025-06-29 21:13:02,568][187912] Fps is (10 sec: 409.2, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4689920. Throughput: 0: 370.2. Samples: 4692864. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 21:13:02,568][187912] Avg episode reward: [(0, '1018.536')]
[36m[2025-06-29 21:13:07,573][187912] Fps is (10 sec: 410.4, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 4694016. Throughput: 0: 367.8. Samples: 4694880. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 21:13:07,573][187912] Avg episode reward: [(0, '1021.244')]
[36m[2025-06-29 21:13:12,577][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4694016. Throughput: 0: 365.3. Samples: 4696048. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 21:13:12,578][187912] Avg episode reward: [(0, '1036.317')]
[36m[2025-06-29 21:13:17,558][187912] Fps is (10 sec: 410.2, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 4698112. Throughput: 0: 364.2. Samples: 4698144. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 21:13:17,558][187912] Avg episode reward: [(0, '1077.037')]
[36m[2025-06-29 21:13:22,576][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4698112. Throughput: 0: 361.9. Samples: 4700368. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 21:13:22,577][187912] Avg episode reward: [(0, '1061.612')]
[36m[2025-06-29 21:13:27,705][187912] Fps is (10 sec: 403.7, 60 sec: 408.9, 300 sec: 361.7). Total num frames: 4702208. Throughput: 0: 360.5. Samples: 4701504. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 21:13:27,705][187912] Avg episode reward: [(0, '1050.887')]
[36m[2025-06-29 21:13:32,566][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4702208. Throughput: 0: 362.0. Samples: 4703600. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 21:13:32,567][187912] Avg episode reward: [(0, '1073.927')]
[36m[2025-06-29 21:13:37,600][187912] Fps is (10 sec: 0.0, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 4702208. Throughput: 0: 360.2. Samples: 4705856. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 21:13:37,600][187912] Avg episode reward: [(0, '1094.014')]
[36m[2025-06-29 21:13:42,566][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4706304. Throughput: 0: 360.4. Samples: 4706736. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 21:13:42,566][187912] Avg episode reward: [(0, '1106.815')]
[36m[2025-06-29 21:13:47,589][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4706304. Throughput: 0: 361.8. Samples: 4709152. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 21:13:47,589][187912] Avg episode reward: [(0, '1118.846')]
[36m[2025-06-29 21:13:52,565][187912] Fps is (10 sec: 409.6, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 4710400. Throughput: 0: 362.4. Samples: 4711184. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:13:52,565][187912] Avg episode reward: [(0, '1128.982')]
[36m[2025-06-29 21:13:57,579][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4710400. Throughput: 0: 364.1. Samples: 4712432. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:13:57,579][187912] Avg episode reward: [(0, '1103.824')]
[36m[2025-06-29 21:14:02,587][187912] Fps is (10 sec: 408.7, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 4714496. Throughput: 0: 363.9. Samples: 4714528. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:14:02,588][187912] Avg episode reward: [(0, '1157.224')]
[36m[2025-06-29 21:14:07,572][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4714496. Throughput: 0: 364.5. Samples: 4716768. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:14:07,573][187912] Avg episode reward: [(0, '1112.305')]
[36m[2025-06-29 21:14:12,575][187912] Fps is (10 sec: 410.1, 60 sec: 409.6, 300 sec: 374.9). Total num frames: 4718592. Throughput: 0: 367.6. Samples: 4718000. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:14:12,575][187912] Avg episode reward: [(0, '1085.465')]
[37m[1m[2025-06-29 21:14:12,614][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000018432_4718592.pth...
[36m[2025-06-29 21:14:12,683][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000018080_4628480.pth
[36m[2025-06-29 21:14:17,590][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4718592. Throughput: 0: 366.4. Samples: 4720096. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:14:17,590][187912] Avg episode reward: [(0, '1122.176')]
[36m[2025-06-29 21:14:22,592][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4718592. Throughput: 0: 369.8. Samples: 4722496. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:14:22,592][187912] Avg episode reward: [(0, '1123.998')]
[36m[2025-06-29 21:14:27,596][187912] Fps is (10 sec: 409.4, 60 sec: 342.0, 300 sec: 361.0). Total num frames: 4722688. Throughput: 0: 369.9. Samples: 4723392. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:14:27,596][187912] Avg episode reward: [(0, '1099.417')]
[36m[2025-06-29 21:14:32,598][187912] Fps is (10 sec: 409.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4722688. Throughput: 0: 366.5. Samples: 4725648. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:14:32,599][187912] Avg episode reward: [(0, '1109.630')]
[36m[2025-06-29 21:14:37,611][187912] Fps is (10 sec: 409.0, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 4726784. Throughput: 0: 363.0. Samples: 4727536. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 21:14:37,611][187912] Avg episode reward: [(0, '1108.971')]
[36m[2025-06-29 21:14:42,590][187912] Fps is (10 sec: 409.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4726784. Throughput: 0: 361.5. Samples: 4728704. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 21:14:42,591][187912] Avg episode reward: [(0, '1097.016')]
[36m[2025-06-29 21:14:47,611][187912] Fps is (10 sec: 409.6, 60 sec: 409.4, 300 sec: 374.9). Total num frames: 4730880. Throughput: 0: 363.2. Samples: 4730880. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 21:14:47,612][187912] Avg episode reward: [(0, '1087.082')]
[36m[2025-06-29 21:14:52,585][187912] Fps is (10 sec: 409.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4730880. Throughput: 0: 356.9. Samples: 4732832. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 21:14:52,586][187912] Avg episode reward: [(0, '1053.131')]
[36m[2025-06-29 21:14:57,559][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 361.1). Total num frames: 4730880. Throughput: 0: 353.5. Samples: 4733904. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 21:14:57,559][187912] Avg episode reward: [(0, '1100.266')]
[36m[2025-06-29 21:15:02,571][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4734976. Throughput: 0: 352.1. Samples: 4735936. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 21:15:02,571][187912] Avg episode reward: [(0, '1126.692')]
[36m[2025-06-29 21:15:07,566][187912] Fps is (10 sec: 409.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4734976. Throughput: 0: 346.5. Samples: 4738080. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 21:15:07,566][187912] Avg episode reward: [(0, '1137.861')]
[36m[2025-06-29 21:15:12,563][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4739072. Throughput: 0: 349.4. Samples: 4739104. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 21:15:12,563][187912] Avg episode reward: [(0, '1150.790')]
[36m[2025-06-29 21:15:17,584][187912] Fps is (10 sec: 408.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4739072. Throughput: 0: 349.6. Samples: 4741376. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 21:15:17,584][187912] Avg episode reward: [(0, '1136.782')]
[33m[13273615 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[13273615 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.818359375
[33mCrash Rate: 0.14208984375
[33mTimeout Rate: 0.03955078125 (navigation_task.py:265)
[33m[13273615 ms][navigation_task] - WARNING : 
[33mSuccesses: 1676
[33mCrashes : 291
[33mTimeouts: 81 (navigation_task.py:268)
[36m[2025-06-29 21:15:22,587][187912] Fps is (10 sec: 408.6, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 4743168. Throughput: 0: 354.7. Samples: 4743488. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 21:15:22,587][187912] Avg episode reward: [(0, '1133.678')]
[31m[13277734 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13277734 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[13277735 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:15:27,596][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4743168. Throughput: 0: 355.2. Samples: 4744688. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 21:15:27,596][187912] Avg episode reward: [(0, '1132.642')]
[36m[2025-06-29 21:15:32,875][187912] Fps is (10 sec: 398.1, 60 sec: 407.7, 300 sec: 374.5). Total num frames: 4747264. Throughput: 0: 353.5. Samples: 4746880. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 21:15:32,875][187912] Avg episode reward: [(0, '1105.149')]
[36m[2025-06-29 21:15:37,579][187912] Fps is (10 sec: 410.3, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4747264. Throughput: 0: 359.9. Samples: 4749024. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 21:15:37,579][187912] Avg episode reward: [(0, '1062.080')]
[36m[2025-06-29 21:15:42,558][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4747264. Throughput: 0: 361.3. Samples: 4750160. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 21:15:42,558][187912] Avg episode reward: [(0, '1080.897')]
[36m[2025-06-29 21:15:47,614][187912] Fps is (10 sec: 408.2, 60 sec: 341.3, 300 sec: 360.9). Total num frames: 4751360. Throughput: 0: 362.7. Samples: 4752272. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 21:15:47,614][187912] Avg episode reward: [(0, '1049.521')]
[36m[2025-06-29 21:15:52,560][187912] Fps is (10 sec: 409.5, 60 sec: 341.5, 300 sec: 361.1). Total num frames: 4751360. Throughput: 0: 368.8. Samples: 4754672. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 21:15:52,560][187912] Avg episode reward: [(0, '988.467')]
[36m[2025-06-29 21:15:57,578][187912] Fps is (10 sec: 411.1, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 4755456. Throughput: 0: 366.5. Samples: 4755600. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 21:15:57,578][187912] Avg episode reward: [(0, '980.052')]
[36m[2025-06-29 21:16:02,584][187912] Fps is (10 sec: 408.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4755456. Throughput: 0: 367.3. Samples: 4757904. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 21:16:02,584][187912] Avg episode reward: [(0, '982.977')]
[36m[2025-06-29 21:16:07,590][187912] Fps is (10 sec: 409.1, 60 sec: 409.4, 300 sec: 361.5). Total num frames: 4759552. Throughput: 0: 364.8. Samples: 4759904. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 21:16:07,591][187912] Avg episode reward: [(0, '1012.195')]
[36m[2025-06-29 21:16:12,588][187912] Fps is (10 sec: 409.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4759552. Throughput: 0: 363.4. Samples: 4761040. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 21:16:12,588][187912] Avg episode reward: [(0, '1051.020')]
[37m[1m[2025-06-29 21:16:12,630][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000018592_4759552.pth...
[36m[2025-06-29 21:16:12,693][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000018256_4673536.pth
[36m[2025-06-29 21:16:18,221][187912] Fps is (10 sec: 385.3, 60 sec: 405.3, 300 sec: 374.1). Total num frames: 4763648. Throughput: 0: 359.5. Samples: 4763184. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 21:16:18,221][187912] Avg episode reward: [(0, '1065.214')]
[36m[2025-06-29 21:16:22,584][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4763648. Throughput: 0: 359.8. Samples: 4765216. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 21:16:22,584][187912] Avg episode reward: [(0, '1119.550')]
[36m[2025-06-29 21:16:27,586][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4763648. Throughput: 0: 361.7. Samples: 4766448. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 21:16:27,586][187912] Avg episode reward: [(0, '1092.535')]
[36m[2025-06-29 21:16:32,583][187912] Fps is (10 sec: 409.7, 60 sec: 343.0, 300 sec: 361.0). Total num frames: 4767744. Throughput: 0: 362.9. Samples: 4768592. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:16:32,583][187912] Avg episode reward: [(0, '1124.948')]
[36m[2025-06-29 21:16:37,609][187912] Fps is (10 sec: 408.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4767744. Throughput: 0: 360.9. Samples: 4770928. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:16:37,609][187912] Avg episode reward: [(0, '1127.361')]
[36m[2025-06-29 21:16:42,596][187912] Fps is (10 sec: 409.1, 60 sec: 409.3, 300 sec: 361.0). Total num frames: 4771840. Throughput: 0: 361.5. Samples: 4771872. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:16:42,596][187912] Avg episode reward: [(0, '1117.995')]
[36m[2025-06-29 21:16:47,605][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4771840. Throughput: 0: 360.4. Samples: 4774128. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:16:47,605][187912] Avg episode reward: [(0, '1228.990')]
[37m[1m[2025-06-29 21:16:47,657][187912] Saving new best policy, reward=1228.990!
[36m[2025-06-29 21:16:52,582][187912] Fps is (10 sec: 410.2, 60 sec: 409.4, 300 sec: 361.2). Total num frames: 4775936. Throughput: 0: 357.4. Samples: 4775984. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 21:16:52,582][187912] Avg episode reward: [(0, '1235.444')]
[37m[1m[2025-06-29 21:16:52,637][187912] Saving new best policy, reward=1235.444!
[36m[2025-06-29 21:16:57,590][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4775936. Throughput: 0: 358.0. Samples: 4777152. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 21:16:57,590][187912] Avg episode reward: [(0, '1138.470')]
[36m[2025-06-29 21:17:02,564][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 361.1). Total num frames: 4775936. Throughput: 0: 368.0. Samples: 4779504. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 21:17:02,564][187912] Avg episode reward: [(0, '1136.128')]
[36m[2025-06-29 21:17:07,596][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4780032. Throughput: 0: 361.1. Samples: 4781472. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 21:17:07,597][187912] Avg episode reward: [(0, '1104.736')]
[36m[2025-06-29 21:17:12,585][187912] Fps is (10 sec: 408.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4780032. Throughput: 0: 357.3. Samples: 4782528. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 21:17:12,586][187912] Avg episode reward: [(0, '1072.647')]
[36m[2025-06-29 21:17:17,568][187912] Fps is (10 sec: 410.7, 60 sec: 345.1, 300 sec: 361.0). Total num frames: 4784128. Throughput: 0: 353.9. Samples: 4784512. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:17:17,569][187912] Avg episode reward: [(0, '1108.912')]
[36m[2025-06-29 21:17:22,597][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4784128. Throughput: 0: 352.8. Samples: 4786800. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:17:22,597][187912] Avg episode reward: [(0, '1187.272')]
[36m[2025-06-29 21:17:27,575][187912] Fps is (10 sec: 409.3, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 4788224. Throughput: 0: 357.1. Samples: 4787936. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:17:27,575][187912] Avg episode reward: [(0, '1163.329')]
[36m[2025-06-29 21:17:32,564][187912] Fps is (10 sec: 410.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4788224. Throughput: 0: 353.0. Samples: 4790000. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:17:32,564][187912] Avg episode reward: [(0, '1118.701')]
[36m[2025-06-29 21:17:37,890][187912] Fps is (10 sec: 397.1, 60 sec: 407.7, 300 sec: 360.6). Total num frames: 4792320. Throughput: 0: 360.9. Samples: 4792336. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:17:37,890][187912] Avg episode reward: [(0, '1067.128')]
[36m[2025-06-29 21:17:42,574][187912] Fps is (10 sec: 409.2, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4792320. Throughput: 0: 357.1. Samples: 4793216. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:17:42,574][187912] Avg episode reward: [(0, '1053.818')]
[36m[2025-06-29 21:17:47,559][187912] Fps is (10 sec: 0.0, 60 sec: 341.6, 300 sec: 361.0). Total num frames: 4792320. Throughput: 0: 354.2. Samples: 4795440. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:17:47,560][187912] Avg episode reward: [(0, '1032.158')]
[36m[2025-06-29 21:17:52,561][187912] Fps is (10 sec: 410.1, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4796416. Throughput: 0: 352.6. Samples: 4797328. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:17:52,562][187912] Avg episode reward: [(0, '969.771')]
[36m[2025-06-29 21:17:57,576][187912] Fps is (10 sec: 408.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4796416. Throughput: 0: 352.4. Samples: 4798384. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:17:57,576][187912] Avg episode reward: [(0, '1017.442')]
[36m[2025-06-29 21:18:02,586][187912] Fps is (10 sec: 408.6, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 4800512. Throughput: 0: 356.1. Samples: 4800544. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:18:02,586][187912] Avg episode reward: [(0, '1018.313')]
[36m[2025-06-29 21:18:07,611][187912] Fps is (10 sec: 408.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4800512. Throughput: 0: 351.5. Samples: 4802624. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:18:07,611][187912] Avg episode reward: [(0, '1014.494')]
[36m[2025-06-29 21:18:12,574][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 4800512. Throughput: 0: 351.7. Samples: 4803760. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:18:12,574][187912] Avg episode reward: [(0, '1020.271')]
[37m[1m[2025-06-29 21:18:13,352][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000018768_4804608.pth...
[36m[2025-06-29 21:18:13,409][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000018432_4718592.pth
[36m[2025-06-29 21:18:17,582][187912] Fps is (10 sec: 410.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4804608. Throughput: 0: 347.2. Samples: 4805632. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:18:17,582][187912] Avg episode reward: [(0, '1114.632')]
[36m[2025-06-29 21:18:22,582][187912] Fps is (10 sec: 409.3, 60 sec: 341.4, 300 sec: 347.3). Total num frames: 4804608. Throughput: 0: 346.2. Samples: 4807808. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:18:22,582][187912] Avg episode reward: [(0, '1074.706')]
[36m[2025-06-29 21:18:27,590][187912] Fps is (10 sec: 409.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4808704. Throughput: 0: 344.8. Samples: 4808736. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:18:27,590][187912] Avg episode reward: [(0, '1150.227')]
[36m[2025-06-29 21:18:32,566][187912] Fps is (10 sec: 410.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4808704. Throughput: 0: 347.7. Samples: 4811088. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:18:32,566][187912] Avg episode reward: [(0, '1175.728')]
[31m[13467570 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13467570 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[13467570 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:18:37,568][187912] Fps is (10 sec: 410.5, 60 sec: 343.2, 300 sec: 361.0). Total num frames: 4812800. Throughput: 0: 349.8. Samples: 4813072. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:18:37,568][187912] Avg episode reward: [(0, '1173.535')]
[36m[2025-06-29 21:18:42,582][187912] Fps is (10 sec: 408.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4812800. Throughput: 0: 353.7. Samples: 4814304. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:18:42,583][187912] Avg episode reward: [(0, '1234.672')]
[36m[2025-06-29 21:18:47,783][187912] Fps is (10 sec: 401.0, 60 sec: 408.1, 300 sec: 360.7). Total num frames: 4816896. Throughput: 0: 354.4. Samples: 4816560. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 21:18:47,784][187912] Avg episode reward: [(0, '1262.156')]
[37m[1m[2025-06-29 21:18:47,825][187912] Saving new best policy, reward=1262.156!
[36m[2025-06-29 21:18:52,576][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4816896. Throughput: 0: 354.4. Samples: 4818560. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 21:18:52,576][187912] Avg episode reward: [(0, '1281.113')]
[37m[1m[2025-06-29 21:18:52,619][187912] Saving new best policy, reward=1281.113!
[36m[2025-06-29 21:18:57,598][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 4816896. Throughput: 0: 353.6. Samples: 4819680. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 21:18:57,598][187912] Avg episode reward: [(0, '1238.886')]
[36m[2025-06-29 21:19:02,571][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4820992. Throughput: 0: 354.2. Samples: 4821568. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 21:19:02,571][187912] Avg episode reward: [(0, '1174.256')]
[36m[2025-06-29 21:19:07,576][187912] Fps is (10 sec: 410.5, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 4820992. Throughput: 0: 357.0. Samples: 4823872. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 21:19:07,577][187912] Avg episode reward: [(0, '1175.419')]
[36m[2025-06-29 21:19:12,566][187912] Fps is (10 sec: 409.8, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 4825088. Throughput: 0: 363.2. Samples: 4825072. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:19:12,566][187912] Avg episode reward: [(0, '1129.555')]
[36m[2025-06-29 21:19:17,576][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4825088. Throughput: 0: 358.0. Samples: 4827200. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:19:17,576][187912] Avg episode reward: [(0, '1068.915')]
[36m[2025-06-29 21:19:22,564][187912] Fps is (10 sec: 409.7, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 4829184. Throughput: 0: 358.8. Samples: 4829216. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:19:22,564][187912] Avg episode reward: [(0, '1017.479')]
[36m[2025-06-29 21:19:27,584][187912] Fps is (10 sec: 409.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4829184. Throughput: 0: 358.4. Samples: 4830432. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:19:27,584][187912] Avg episode reward: [(0, '1068.320')]
[36m[2025-06-29 21:19:33,213][187912] Fps is (10 sec: 384.6, 60 sec: 405.2, 300 sec: 360.3). Total num frames: 4833280. Throughput: 0: 356.4. Samples: 4832752. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:19:33,213][187912] Avg episode reward: [(0, '1028.679')]
[36m[2025-06-29 21:19:37,570][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4833280. Throughput: 0: 362.4. Samples: 4834864. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:19:37,570][187912] Avg episode reward: [(0, '1065.669')]
[36m[2025-06-29 21:19:42,568][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 4833280. Throughput: 0: 364.3. Samples: 4836064. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:19:42,568][187912] Avg episode reward: [(0, '1100.809')]
[36m[2025-06-29 21:19:47,594][187912] Fps is (10 sec: 408.6, 60 sec: 342.4, 300 sec: 361.0). Total num frames: 4837376. Throughput: 0: 366.0. Samples: 4838048. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:19:47,594][187912] Avg episode reward: [(0, '1167.824')]
[36m[2025-06-29 21:19:52,620][187912] Fps is (10 sec: 407.5, 60 sec: 341.1, 300 sec: 360.9). Total num frames: 4837376. Throughput: 0: 368.7. Samples: 4840480. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:19:52,621][187912] Avg episode reward: [(0, '1085.009')]
[36m[2025-06-29 21:19:57,575][187912] Fps is (10 sec: 410.4, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 4841472. Throughput: 0: 365.1. Samples: 4841504. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:19:57,575][187912] Avg episode reward: [(0, '1082.728')]
[36m[2025-06-29 21:20:02,568][187912] Fps is (10 sec: 411.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4841472. Throughput: 0: 364.5. Samples: 4843600. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:20:02,568][187912] Avg episode reward: [(0, '1053.723')]
[36m[2025-06-29 21:20:07,572][187912] Fps is (10 sec: 409.7, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 4845568. Throughput: 0: 365.1. Samples: 4845648. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:20:07,572][187912] Avg episode reward: [(0, '1030.708')]
[36m[2025-06-29 21:20:12,574][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4845568. Throughput: 0: 365.2. Samples: 4846864. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:20:12,574][187912] Avg episode reward: [(0, '1030.529')]
[37m[1m[2025-06-29 21:20:12,627][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000018928_4845568.pth...
[36m[2025-06-29 21:20:12,689][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000018592_4759552.pth
[36m[2025-06-29 21:20:18,199][187912] Fps is (10 sec: 385.4, 60 sec: 405.4, 300 sec: 360.3). Total num frames: 4849664. Throughput: 0: 364.6. Samples: 4849152. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 21:20:18,199][187912] Avg episode reward: [(0, '1090.896')]
[36m[2025-06-29 21:20:22,572][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4849664. Throughput: 0: 362.6. Samples: 4851184. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 21:20:22,573][187912] Avg episode reward: [(0, '1139.425')]
[36m[2025-06-29 21:20:27,595][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.4). Total num frames: 4849664. Throughput: 0: 361.0. Samples: 4852320. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 21:20:27,595][187912] Avg episode reward: [(0, '1075.391')]
[31m[13585497 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13585497 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[13585498 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:20:32,576][187912] Fps is (10 sec: 409.5, 60 sec: 345.0, 300 sec: 361.0). Total num frames: 4853760. Throughput: 0: 363.9. Samples: 4854416. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 21:20:32,576][187912] Avg episode reward: [(0, '1090.774')]
[31m[13586042 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13586042 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[13586043 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:20:37,563][187912] Fps is (10 sec: 410.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4853760. Throughput: 0: 366.0. Samples: 4856928. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 21:20:37,563][187912] Avg episode reward: [(0, '1062.800')]
[36m[2025-06-29 21:20:42,583][187912] Fps is (10 sec: 409.3, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 4857856. Throughput: 0: 364.0. Samples: 4857888. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:20:42,583][187912] Avg episode reward: [(0, '1083.490')]
[36m[2025-06-29 21:20:47,563][187912] Fps is (10 sec: 409.6, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4857856. Throughput: 0: 364.1. Samples: 4859984. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:20:47,563][187912] Avg episode reward: [(0, '1081.116')]
[36m[2025-06-29 21:20:52,594][187912] Fps is (10 sec: 409.2, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 4861952. Throughput: 0: 363.2. Samples: 4862000. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:20:52,594][187912] Avg episode reward: [(0, '1136.181')]
[36m[2025-06-29 21:20:57,574][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4861952. Throughput: 0: 363.4. Samples: 4863216. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:20:57,575][187912] Avg episode reward: [(0, '1096.198')]
[36m[2025-06-29 21:21:02,585][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 4861952. Throughput: 0: 364.8. Samples: 4865344. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:21:02,585][187912] Avg episode reward: [(0, '1120.202')]
[36m[2025-06-29 21:21:07,572][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4866048. Throughput: 0: 360.2. Samples: 4867392. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:21:07,572][187912] Avg episode reward: [(0, '1132.806')]
[36m[2025-06-29 21:21:12,594][187912] Fps is (10 sec: 409.2, 60 sec: 341.2, 300 sec: 347.9). Total num frames: 4866048. Throughput: 0: 360.9. Samples: 4868560. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:21:12,594][187912] Avg episode reward: [(0, '1137.872')]
[36m[2025-06-29 21:21:17,561][187912] Fps is (10 sec: 410.0, 60 sec: 345.0, 300 sec: 361.0). Total num frames: 4870144. Throughput: 0: 357.8. Samples: 4870512. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:21:17,562][187912] Avg episode reward: [(0, '1120.416')]
[36m[2025-06-29 21:21:22,585][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4870144. Throughput: 0: 351.8. Samples: 4872768. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:21:22,586][187912] Avg episode reward: [(0, '1124.005')]
[36m[2025-06-29 21:21:27,579][187912] Fps is (10 sec: 408.9, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 4874240. Throughput: 0: 354.5. Samples: 4873840. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:21:27,579][187912] Avg episode reward: [(0, '1118.368')]
[36m[2025-06-29 21:21:32,570][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 361.1). Total num frames: 4874240. Throughput: 0: 350.9. Samples: 4875776. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:21:32,570][187912] Avg episode reward: [(0, '1083.232')]
[36m[2025-06-29 21:21:37,562][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.2). Total num frames: 4874240. Throughput: 0: 357.6. Samples: 4878080. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:21:37,562][187912] Avg episode reward: [(0, '1092.449')]
[36m[2025-06-29 21:21:42,591][187912] Fps is (10 sec: 408.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4878336. Throughput: 0: 350.1. Samples: 4878976. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:21:42,591][187912] Avg episode reward: [(0, '1124.455')]
[36m[2025-06-29 21:21:47,571][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 4878336. Throughput: 0: 355.0. Samples: 4881312. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:21:47,571][187912] Avg episode reward: [(0, '1146.934')]
[36m[2025-06-29 21:21:52,584][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4882432. Throughput: 0: 353.0. Samples: 4883280. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 21:21:52,584][187912] Avg episode reward: [(0, '1163.128')]
[36m[2025-06-29 21:21:57,596][187912] Fps is (10 sec: 408.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4882432. Throughput: 0: 351.6. Samples: 4884384. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 21:21:57,596][187912] Avg episode reward: [(0, '1223.382')]
[36m[2025-06-29 21:22:02,597][187912] Fps is (10 sec: 409.1, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 4886528. Throughput: 0: 356.3. Samples: 4886560. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 21:22:02,597][187912] Avg episode reward: [(0, '1226.144')]
[36m[2025-06-29 21:22:07,558][187912] Fps is (10 sec: 411.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4886528. Throughput: 0: 352.2. Samples: 4888608. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 21:22:07,558][187912] Avg episode reward: [(0, '1117.044')]
[31m[13681309 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13681310 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[13681310 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:22:13,241][187912] Fps is (10 sec: 384.8, 60 sec: 405.2, 300 sec: 360.2). Total num frames: 4890624. Throughput: 0: 349.0. Samples: 4889776. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:22:13,241][187912] Avg episode reward: [(0, '1142.544')]
[37m[1m[2025-06-29 21:22:13,303][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000019104_4890624.pth...
[36m[2025-06-29 21:22:13,370][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000018768_4804608.pth
[36m[2025-06-29 21:22:17,587][187912] Fps is (10 sec: 408.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4890624. Throughput: 0: 355.8. Samples: 4891792. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:22:17,587][187912] Avg episode reward: [(0, '1122.251')]
[36m[2025-06-29 21:22:22,583][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 4890624. Throughput: 0: 357.2. Samples: 4894160. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:22:22,583][187912] Avg episode reward: [(0, '1070.694')]
[36m[2025-06-29 21:22:27,582][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4894720. Throughput: 0: 358.5. Samples: 4895104. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:22:27,583][187912] Avg episode reward: [(0, '1074.421')]
[36m[2025-06-29 21:22:32,564][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 347.5). Total num frames: 4894720. Throughput: 0: 357.0. Samples: 4897376. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:22:32,564][187912] Avg episode reward: [(0, '1082.247')]
[36m[2025-06-29 21:22:37,565][187912] Fps is (10 sec: 410.3, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 4898816. Throughput: 0: 359.6. Samples: 4899456. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 21:22:37,566][187912] Avg episode reward: [(0, '1050.770')]
[36m[2025-06-29 21:22:42,587][187912] Fps is (10 sec: 408.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4898816. Throughput: 0: 362.0. Samples: 4900672. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 21:22:42,587][187912] Avg episode reward: [(0, '1085.942')]
[36m[2025-06-29 21:22:47,581][187912] Fps is (10 sec: 408.9, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 4902912. Throughput: 0: 362.8. Samples: 4902880. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 21:22:47,582][187912] Avg episode reward: [(0, '1042.896')]
[31m[13722973 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[13722973 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[13722974 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:22:52,605][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4902912. Throughput: 0: 364.4. Samples: 4905024. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 21:22:52,606][187912] Avg episode reward: [(0, '998.157')]
[36m[2025-06-29 21:22:57,591][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 4902912. Throughput: 0: 368.3. Samples: 4906112. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 21:22:57,591][187912] Avg episode reward: [(0, '1062.673')]
[36m[2025-06-29 21:23:02,599][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4907008. Throughput: 0: 361.1. Samples: 4908048. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:23:02,600][187912] Avg episode reward: [(0, '1042.376')]
[36m[2025-06-29 21:23:07,583][187912] Fps is (10 sec: 409.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4907008. Throughput: 0: 358.0. Samples: 4910272. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:23:07,583][187912] Avg episode reward: [(0, '1016.497')]
[36m[2025-06-29 21:23:12,575][187912] Fps is (10 sec: 410.6, 60 sec: 345.2, 300 sec: 361.0). Total num frames: 4911104. Throughput: 0: 356.3. Samples: 4911136. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:23:12,575][187912] Avg episode reward: [(0, '991.740')]
[36m[2025-06-29 21:23:17,563][187912] Fps is (10 sec: 410.4, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4911104. Throughput: 0: 349.2. Samples: 4913088. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:23:17,564][187912] Avg episode reward: [(0, '986.965')]
[36m[2025-06-29 21:23:22,740][187912] Fps is (10 sec: 402.9, 60 sec: 408.5, 300 sec: 360.8). Total num frames: 4915200. Throughput: 0: 324.4. Samples: 4914112. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:23:22,741][187912] Avg episode reward: [(0, '924.006')]
[36m[2025-06-29 21:23:27,592][187912] Fps is (10 sec: 408.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4915200. Throughput: 0: 340.2. Samples: 4915984. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:23:27,592][187912] Avg episode reward: [(0, '925.835')]
[36m[2025-06-29 21:23:32,597][187912] Fps is (10 sec: 0.0, 60 sec: 341.1, 300 sec: 347.1). Total num frames: 4915200. Throughput: 0: 338.7. Samples: 4918128. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:23:32,597][187912] Avg episode reward: [(0, '904.907')]
[36m[2025-06-29 21:23:37,580][187912] Fps is (10 sec: 410.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 4919296. Throughput: 0: 335.1. Samples: 4920096. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:23:37,580][187912] Avg episode reward: [(0, '917.169')]
[36m[2025-06-29 21:23:42,576][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 347.4). Total num frames: 4919296. Throughput: 0: 335.8. Samples: 4921216. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:23:42,576][187912] Avg episode reward: [(0, '902.426')]
[36m[2025-06-29 21:23:47,591][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 4923392. Throughput: 0: 337.1. Samples: 4923216. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 21:23:47,591][187912] Avg episode reward: [(0, '906.797')]
[36m[2025-06-29 21:23:52,582][187912] Fps is (10 sec: 409.3, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 4923392. Throughput: 0: 330.0. Samples: 4925120. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 21:23:52,582][187912] Avg episode reward: [(0, '986.766')]
[36m[2025-06-29 21:23:57,562][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 4923392. Throughput: 0: 336.8. Samples: 4926288. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 21:23:57,562][187912] Avg episode reward: [(0, '1028.629')]
[36m[2025-06-29 21:24:02,596][187912] Fps is (10 sec: 409.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4927488. Throughput: 0: 337.5. Samples: 4928288. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 21:24:02,596][187912] Avg episode reward: [(0, '1042.039')]
[36m[2025-06-29 21:24:07,582][187912] Fps is (10 sec: 408.8, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 4927488. Throughput: 0: 363.6. Samples: 4930416. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 21:24:07,583][187912] Avg episode reward: [(0, '1081.887')]
[33m[13804526 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[13804526 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.81103515625
[33mCrash Rate: 0.15478515625
[33mTimeout Rate: 0.0341796875 (navigation_task.py:265)
[33m[13804526 ms][navigation_task] - WARNING : 
[33mSuccesses: 1661
[33mCrashes : 317
[33mTimeouts: 70 (navigation_task.py:268)
[36m[2025-06-29 21:24:12,563][187912] Fps is (10 sec: 410.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 4931584. Throughput: 0: 345.5. Samples: 4931520. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:24:12,564][187912] Avg episode reward: [(0, '1046.452')]
[37m[1m[2025-06-29 21:24:12,605][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000019264_4931584.pth...
[36m[2025-06-29 21:24:12,674][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000018928_4845568.pth
[36m[2025-06-29 21:24:17,568][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 4931584. Throughput: 0: 335.5. Samples: 4933216. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:24:17,569][187912] Avg episode reward: [(0, '999.260')]
[36m[2025-06-29 21:24:22,611][187912] Fps is (10 sec: 0.0, 60 sec: 273.7, 300 sec: 347.1). Total num frames: 4931584. Throughput: 0: 334.4. Samples: 4935152. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:24:22,611][187912] Avg episode reward: [(0, '987.261')]
[36m[2025-06-29 21:24:27,581][187912] Fps is (10 sec: 409.1, 60 sec: 341.4, 300 sec: 347.9). Total num frames: 4935680. Throughput: 0: 323.5. Samples: 4935776. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:24:27,581][187912] Avg episode reward: [(0, '952.310')]
[36m[2025-06-29 21:24:32,587][187912] Fps is (10 sec: 410.6, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 4935680. Throughput: 0: 318.2. Samples: 4937536. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:24:32,587][187912] Avg episode reward: [(0, '989.441')]
[36m[2025-06-29 21:24:37,590][187912] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 347.1). Total num frames: 4935680. Throughput: 0: 315.7. Samples: 4939328. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:24:37,590][187912] Avg episode reward: [(0, '971.535')]
[36m[2025-06-29 21:24:42,573][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 4939776. Throughput: 0: 305.3. Samples: 4940032. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 21:24:42,573][187912] Avg episode reward: [(0, '991.478')]
[36m[2025-06-29 21:24:47,565][187912] Fps is (10 sec: 410.6, 60 sec: 273.2, 300 sec: 347.2). Total num frames: 4939776. Throughput: 0: 304.9. Samples: 4942000. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 21:24:47,566][187912] Avg episode reward: [(0, '1064.100')]
[36m[2025-06-29 21:24:52,677][187912] Fps is (10 sec: 405.4, 60 sec: 340.8, 300 sec: 347.0). Total num frames: 4943872. Throughput: 0: 278.9. Samples: 4942992. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 21:24:52,678][187912] Avg episode reward: [(0, '966.147')]
[36m[2025-06-29 21:24:57,595][187912] Fps is (10 sec: 408.4, 60 sec: 341.1, 300 sec: 347.1). Total num frames: 4943872. Throughput: 0: 294.5. Samples: 4944784. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 21:24:57,595][187912] Avg episode reward: [(0, '1019.849')]
[36m[2025-06-29 21:25:02,603][187912] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 4943872. Throughput: 0: 301.6. Samples: 4946800. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 21:25:02,603][187912] Avg episode reward: [(0, '1057.964')]
[36m[2025-06-29 21:25:07,586][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 4947968. Throughput: 0: 297.1. Samples: 4948512. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:25:07,586][187912] Avg episode reward: [(0, '1106.620')]
[36m[2025-06-29 21:25:12,598][187912] Fps is (10 sec: 409.8, 60 sec: 272.9, 300 sec: 333.9). Total num frames: 4947968. Throughput: 0: 303.9. Samples: 4949456. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:25:12,599][187912] Avg episode reward: [(0, '1056.457')]
[36m[2025-06-29 21:25:17,574][187912] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 333.2). Total num frames: 4947968. Throughput: 0: 307.6. Samples: 4951376. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:25:17,574][187912] Avg episode reward: [(0, '975.237')]
[36m[2025-06-29 21:25:22,562][187912] Fps is (10 sec: 411.1, 60 sec: 341.6, 300 sec: 347.2). Total num frames: 4952064. Throughput: 0: 310.2. Samples: 4953280. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:25:22,562][187912] Avg episode reward: [(0, '1148.615')]
[36m[2025-06-29 21:25:27,562][187912] Fps is (10 sec: 410.1, 60 sec: 273.2, 300 sec: 333.2). Total num frames: 4952064. Throughput: 0: 318.3. Samples: 4954352. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:25:27,562][187912] Avg episode reward: [(0, '1112.725')]
[36m[2025-06-29 21:25:32,591][187912] Fps is (10 sec: 408.4, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 4956160. Throughput: 0: 315.2. Samples: 4956192. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 21:25:32,591][187912] Avg episode reward: [(0, '1013.609')]
[36m[2025-06-29 21:25:37,586][187912] Fps is (10 sec: 408.6, 60 sec: 341.4, 300 sec: 333.2). Total num frames: 4956160. Throughput: 0: 344.2. Samples: 4958448. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 21:25:37,586][187912] Avg episode reward: [(0, '1015.914')]
[36m[2025-06-29 21:25:42,988][187912] Fps is (10 sec: 393.9, 60 sec: 339.0, 300 sec: 346.6). Total num frames: 4960256. Throughput: 0: 322.9. Samples: 4959440. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 21:25:42,988][187912] Avg episode reward: [(0, '1066.069')]
[36m[2025-06-29 21:25:47,586][187912] Fps is (10 sec: 409.6, 60 sec: 341.2, 300 sec: 333.2). Total num frames: 4960256. Throughput: 0: 325.1. Samples: 4961424. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 21:25:47,586][187912] Avg episode reward: [(0, '1026.730')]
[36m[2025-06-29 21:25:52,571][187912] Fps is (10 sec: 0.0, 60 sec: 273.6, 300 sec: 333.2). Total num frames: 4960256. Throughput: 0: 332.6. Samples: 4963472. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 21:25:52,571][187912] Avg episode reward: [(0, '963.198')]
[36m[2025-06-29 21:25:57,579][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 4964352. Throughput: 0: 331.9. Samples: 4964384. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 21:25:57,579][187912] Avg episode reward: [(0, '975.219')]
[36m[2025-06-29 21:26:02,561][187912] Fps is (10 sec: 410.0, 60 sec: 341.6, 300 sec: 333.2). Total num frames: 4964352. Throughput: 0: 336.1. Samples: 4966496. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 21:26:02,561][187912] Avg episode reward: [(0, '988.011')]
[36m[2025-06-29 21:26:07,839][187912] Fps is (10 sec: 399.2, 60 sec: 339.9, 300 sec: 346.8). Total num frames: 4968448. Throughput: 0: 312.0. Samples: 4967408. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 21:26:07,839][187912] Avg episode reward: [(0, '1012.783')]
[36m[2025-06-29 21:26:12,575][187912] Fps is (10 sec: 409.0, 60 sec: 341.5, 300 sec: 333.2). Total num frames: 4968448. Throughput: 0: 330.9. Samples: 4969248. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 21:26:12,575][187912] Avg episode reward: [(0, '1053.434')]
[37m[1m[2025-06-29 21:26:12,617][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000019408_4968448.pth...
[36m[2025-06-29 21:26:12,688][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000019104_4890624.pth
[36m[2025-06-29 21:26:17,559][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 333.3). Total num frames: 4968448. Throughput: 0: 338.0. Samples: 4971392. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 21:26:17,559][187912] Avg episode reward: [(0, '1119.653')]
[36m[2025-06-29 21:26:22,570][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 333.2). Total num frames: 4972544. Throughput: 0: 330.1. Samples: 4973296. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:26:22,570][187912] Avg episode reward: [(0, '1150.949')]
[36m[2025-06-29 21:26:27,562][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 333.2). Total num frames: 4972544. Throughput: 0: 334.2. Samples: 4974336. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:26:27,563][187912] Avg episode reward: [(0, '1200.851')]
[36m[2025-06-29 21:26:32,743][187912] Fps is (10 sec: 402.6, 60 sec: 340.5, 300 sec: 346.9). Total num frames: 4976640. Throughput: 0: 330.6. Samples: 4976352. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:26:32,743][187912] Avg episode reward: [(0, '1215.813')]
[36m[2025-06-29 21:26:37,561][187912] Fps is (10 sec: 409.6, 60 sec: 341.5, 300 sec: 333.3). Total num frames: 4976640. Throughput: 0: 331.8. Samples: 4978400. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:26:37,562][187912] Avg episode reward: [(0, '1165.516')]
[36m[2025-06-29 21:26:42,583][187912] Fps is (10 sec: 0.0, 60 sec: 274.9, 300 sec: 333.2). Total num frames: 4976640. Throughput: 0: 334.2. Samples: 4979424. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:26:42,583][187912] Avg episode reward: [(0, '1145.269')]
[36m[2025-06-29 21:26:47,589][187912] Fps is (10 sec: 408.5, 60 sec: 341.3, 300 sec: 333.2). Total num frames: 4980736. Throughput: 0: 329.7. Samples: 4981344. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:26:47,590][187912] Avg episode reward: [(0, '1120.325')]
[36m[2025-06-29 21:26:52,572][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 333.3). Total num frames: 4980736. Throughput: 0: 361.3. Samples: 4983568. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:26:52,572][187912] Avg episode reward: [(0, '1049.821')]
[36m[2025-06-29 21:26:57,584][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 333.2). Total num frames: 4984832. Throughput: 0: 344.1. Samples: 4984736. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:26:57,584][187912] Avg episode reward: [(0, '1013.840')]
[36m[2025-06-29 21:27:02,577][187912] Fps is (10 sec: 409.4, 60 sec: 341.2, 300 sec: 333.2). Total num frames: 4984832. Throughput: 0: 343.7. Samples: 4986864. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:27:02,577][187912] Avg episode reward: [(0, '957.934')]
[36m[2025-06-29 21:27:07,580][187912] Fps is (10 sec: 409.8, 60 sec: 342.8, 300 sec: 334.0). Total num frames: 4988928. Throughput: 0: 348.0. Samples: 4988960. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:27:07,580][187912] Avg episode reward: [(0, '1011.085')]
[36m[2025-06-29 21:27:12,578][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 333.2). Total num frames: 4988928. Throughput: 0: 352.2. Samples: 4990192. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:27:12,578][187912] Avg episode reward: [(0, '990.518')]
[36m[2025-06-29 21:27:18,166][187912] Fps is (10 sec: 386.9, 60 sec: 405.5, 300 sec: 346.4). Total num frames: 4993024. Throughput: 0: 355.8. Samples: 4992512. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:27:18,167][187912] Avg episode reward: [(0, '1052.020')]
[36m[2025-06-29 21:27:22,566][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 333.3). Total num frames: 4993024. Throughput: 0: 360.9. Samples: 4994640. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:27:22,566][187912] Avg episode reward: [(0, '1049.249')]
[36m[2025-06-29 21:27:27,559][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 333.2). Total num frames: 4993024. Throughput: 0: 363.6. Samples: 4995776. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:27:27,559][187912] Avg episode reward: [(0, '1112.967')]
[36m[2025-06-29 21:27:32,579][187912] Fps is (10 sec: 409.1, 60 sec: 342.3, 300 sec: 333.2). Total num frames: 4997120. Throughput: 0: 364.2. Samples: 4997728. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:27:32,580][187912] Avg episode reward: [(0, '1139.793')]
[36m[2025-06-29 21:27:37,575][187912] Fps is (10 sec: 408.9, 60 sec: 341.3, 300 sec: 333.2). Total num frames: 4997120. Throughput: 0: 368.0. Samples: 5000128. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:27:37,575][187912] Avg episode reward: [(0, '1128.238')]
[36m[2025-06-29 21:27:42,587][187912] Fps is (10 sec: 409.3, 60 sec: 409.6, 300 sec: 333.2). Total num frames: 5001216. Throughput: 0: 366.9. Samples: 5001248. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:27:42,588][187912] Avg episode reward: [(0, '1059.459')]
[31m[14020641 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14020641 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[14020641 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:27:47,581][187912] Fps is (10 sec: 409.3, 60 sec: 341.4, 300 sec: 333.3). Total num frames: 5001216. Throughput: 0: 367.6. Samples: 5003408. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:27:47,582][187912] Avg episode reward: [(0, '1013.855')]
[36m[2025-06-29 21:27:52,559][187912] Fps is (10 sec: 410.8, 60 sec: 409.7, 300 sec: 347.2). Total num frames: 5005312. Throughput: 0: 368.5. Samples: 5005536. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 21:27:52,559][187912] Avg episode reward: [(0, '1043.082')]
[36m[2025-06-29 21:27:57,585][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 333.2). Total num frames: 5005312. Throughput: 0: 367.9. Samples: 5006752. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 21:27:57,586][187912] Avg episode reward: [(0, '967.791')]
[36m[2025-06-29 21:28:02,978][187912] Fps is (10 sec: 393.1, 60 sec: 406.9, 300 sec: 346.7). Total num frames: 5009408. Throughput: 0: 368.1. Samples: 5009008. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 21:28:02,978][187912] Avg episode reward: [(0, '963.705')]
[36m[2025-06-29 21:28:07,565][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 333.2). Total num frames: 5009408. Throughput: 0: 365.5. Samples: 5011088. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 21:28:07,565][187912] Avg episode reward: [(0, '997.796')]
[36m[2025-06-29 21:28:12,583][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 333.2). Total num frames: 5009408. Throughput: 0: 367.4. Samples: 5012320. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 21:28:12,583][187912] Avg episode reward: [(0, '1058.299')]
[37m[1m[2025-06-29 21:28:12,629][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000019568_5009408.pth...
[36m[2025-06-29 21:28:12,695][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000019264_4931584.pth
[36m[2025-06-29 21:28:17,566][187912] Fps is (10 sec: 409.6, 60 sec: 344.8, 300 sec: 333.4). Total num frames: 5013504. Throughput: 0: 368.5. Samples: 5014304. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 21:28:17,566][187912] Avg episode reward: [(0, '995.435')]
[36m[2025-06-29 21:28:22,600][187912] Fps is (10 sec: 408.9, 60 sec: 341.1, 300 sec: 333.2). Total num frames: 5013504. Throughput: 0: 366.7. Samples: 5016640. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 21:28:22,600][187912] Avg episode reward: [(0, '986.391')]
[36m[2025-06-29 21:28:27,593][187912] Fps is (10 sec: 408.5, 60 sec: 409.4, 300 sec: 347.1). Total num frames: 5017600. Throughput: 0: 364.0. Samples: 5017632. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 21:28:27,593][187912] Avg episode reward: [(0, '1033.652')]
[36m[2025-06-29 21:28:32,602][187912] Fps is (10 sec: 409.5, 60 sec: 341.2, 300 sec: 333.2). Total num frames: 5017600. Throughput: 0: 362.9. Samples: 5019744. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 21:28:32,602][187912] Avg episode reward: [(0, '1044.002')]
[36m[2025-06-29 21:28:37,591][187912] Fps is (10 sec: 409.7, 60 sec: 409.5, 300 sec: 347.1). Total num frames: 5021696. Throughput: 0: 362.0. Samples: 5021840. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 21:28:37,591][187912] Avg episode reward: [(0, '972.982')]
[36m[2025-06-29 21:28:42,562][187912] Fps is (10 sec: 411.2, 60 sec: 341.5, 300 sec: 333.3). Total num frames: 5021696. Throughput: 0: 361.4. Samples: 5023008. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 21:28:42,563][187912] Avg episode reward: [(0, '988.057')]
[36m[2025-06-29 21:28:47,574][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 333.2). Total num frames: 5021696. Throughput: 0: 362.4. Samples: 5025168. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 21:28:47,574][187912] Avg episode reward: [(0, '980.381')]
[36m[2025-06-29 21:28:52,560][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5025792. Throughput: 0: 358.8. Samples: 5027232. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 21:28:52,560][187912] Avg episode reward: [(0, '1024.217')]
[36m[2025-06-29 21:28:57,563][187912] Fps is (10 sec: 410.0, 60 sec: 341.5, 300 sec: 333.3). Total num frames: 5025792. Throughput: 0: 355.7. Samples: 5028320. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 21:28:57,564][187912] Avg episode reward: [(0, '1084.039')]
[36m[2025-06-29 21:29:02,566][187912] Fps is (10 sec: 409.4, 60 sec: 343.7, 300 sec: 347.1). Total num frames: 5029888. Throughput: 0: 355.9. Samples: 5030320. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 21:29:02,566][187912] Avg episode reward: [(0, '1128.151')]
[36m[2025-06-29 21:29:07,567][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 333.2). Total num frames: 5029888. Throughput: 0: 356.9. Samples: 5032688. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 21:29:07,567][187912] Avg episode reward: [(0, '1119.560')]
[36m[2025-06-29 21:29:12,572][187912] Fps is (10 sec: 409.3, 60 sec: 409.7, 300 sec: 347.1). Total num frames: 5033984. Throughput: 0: 359.6. Samples: 5033808. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 21:29:12,572][187912] Avg episode reward: [(0, '1137.003')]
[31m[14106782 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14106783 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[14106783 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:29:17,572][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 347.2). Total num frames: 5033984. Throughput: 0: 356.5. Samples: 5035776. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 21:29:17,572][187912] Avg episode reward: [(0, '1128.325')]
[36m[2025-06-29 21:29:22,752][187912] Fps is (10 sec: 402.4, 60 sec: 408.6, 300 sec: 346.9). Total num frames: 5038080. Throughput: 0: 335.2. Samples: 5036976. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:29:22,752][187912] Avg episode reward: [(0, '1043.202')]
[31m[14118340 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14118340 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[14118340 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:29:27,589][187912] Fps is (10 sec: 408.9, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5038080. Throughput: 0: 356.4. Samples: 5039056. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:29:27,590][187912] Avg episode reward: [(0, '985.332')]
[36m[2025-06-29 21:29:32,604][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5038080. Throughput: 0: 357.1. Samples: 5041248. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:29:32,605][187912] Avg episode reward: [(0, '1052.145')]
[36m[2025-06-29 21:29:37,579][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5042176. Throughput: 0: 354.0. Samples: 5043168. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:29:37,579][187912] Avg episode reward: [(0, '1080.272')]
[36m[2025-06-29 21:29:42,564][187912] Fps is (10 sec: 411.2, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5042176. Throughput: 0: 354.8. Samples: 5044288. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:29:42,565][187912] Avg episode reward: [(0, '1054.851')]
[36m[2025-06-29 21:29:47,563][187912] Fps is (10 sec: 410.3, 60 sec: 409.7, 300 sec: 347.3). Total num frames: 5046272. Throughput: 0: 355.2. Samples: 5046304. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 21:29:47,563][187912] Avg episode reward: [(0, '1042.451')]
[36m[2025-06-29 21:29:52,567][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 347.2). Total num frames: 5046272. Throughput: 0: 353.1. Samples: 5048576. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 21:29:52,568][187912] Avg episode reward: [(0, '1046.772')]
[36m[2025-06-29 21:29:57,838][187912] Fps is (10 sec: 398.6, 60 sec: 407.7, 300 sec: 360.7). Total num frames: 5050368. Throughput: 0: 351.3. Samples: 5049712. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 21:29:57,838][187912] Avg episode reward: [(0, '1091.400')]
[36m[2025-06-29 21:30:02,588][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 5050368. Throughput: 0: 353.7. Samples: 5051696. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 21:30:02,588][187912] Avg episode reward: [(0, '1036.874')]
[31m[14159445 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14159446 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[14159446 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:30:07,579][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5050368. Throughput: 0: 378.7. Samples: 5053952. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 21:30:07,579][187912] Avg episode reward: [(0, '1079.570')]
[36m[2025-06-29 21:30:12,561][187912] Fps is (10 sec: 410.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5054464. Throughput: 0: 350.4. Samples: 5054816. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:30:12,561][187912] Avg episode reward: [(0, '1082.894')]
[37m[1m[2025-06-29 21:30:12,605][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000019744_5054464.pth...
[36m[2025-06-29 21:30:12,676][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000019408_4968448.pth
[36m[2025-06-29 21:30:17,568][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5054464. Throughput: 0: 353.4. Samples: 5057136. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:30:17,568][187912] Avg episode reward: [(0, '1118.267')]
[36m[2025-06-29 21:30:22,578][187912] Fps is (10 sec: 408.9, 60 sec: 342.3, 300 sec: 361.0). Total num frames: 5058560. Throughput: 0: 355.2. Samples: 5059152. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:30:22,578][187912] Avg episode reward: [(0, '1076.728')]
[36m[2025-06-29 21:30:27,586][187912] Fps is (10 sec: 408.9, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5058560. Throughput: 0: 356.8. Samples: 5060352. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:30:27,586][187912] Avg episode reward: [(0, '1080.585')]
[36m[2025-06-29 21:30:32,578][187912] Fps is (10 sec: 409.6, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 5062656. Throughput: 0: 362.9. Samples: 5062640. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:30:32,578][187912] Avg episode reward: [(0, '1038.522')]
[31m[14187118 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14187118 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[14187118 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:30:37,581][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 347.6). Total num frames: 5062656. Throughput: 0: 357.2. Samples: 5064656. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:30:37,581][187912] Avg episode reward: [(0, '1062.386')]
[36m[2025-06-29 21:30:42,585][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 5062656. Throughput: 0: 360.4. Samples: 5065840. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:30:42,585][187912] Avg episode reward: [(0, '1075.619')]
[36m[2025-06-29 21:30:47,566][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5066752. Throughput: 0: 358.2. Samples: 5067808. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:30:47,566][187912] Avg episode reward: [(0, '1054.032')]
[36m[2025-06-29 21:30:52,595][187912] Fps is (10 sec: 409.2, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 5066752. Throughput: 0: 358.3. Samples: 5070080. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:30:52,595][187912] Avg episode reward: [(0, '1114.578')]
[36m[2025-06-29 21:30:57,569][187912] Fps is (10 sec: 409.5, 60 sec: 342.9, 300 sec: 361.0). Total num frames: 5070848. Throughput: 0: 356.9. Samples: 5070880. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:30:57,569][187912] Avg episode reward: [(0, '1088.799')]
[36m[2025-06-29 21:31:02,570][187912] Fps is (10 sec: 410.6, 60 sec: 341.4, 300 sec: 347.4). Total num frames: 5070848. Throughput: 0: 357.0. Samples: 5073200. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:31:02,571][187912] Avg episode reward: [(0, '1142.243')]
[36m[2025-06-29 21:31:07,584][187912] Fps is (10 sec: 409.0, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 5074944. Throughput: 0: 356.6. Samples: 5075200. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:31:07,585][187912] Avg episode reward: [(0, '1120.381')]
[36m[2025-06-29 21:31:12,558][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5074944. Throughput: 0: 355.1. Samples: 5076320. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:31:12,558][187912] Avg episode reward: [(0, '1057.041')]
[36m[2025-06-29 21:31:18,315][187912] Fps is (10 sec: 381.7, 60 sec: 404.6, 300 sec: 360.1). Total num frames: 5079040. Throughput: 0: 346.3. Samples: 5078480. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 21:31:18,315][187912] Avg episode reward: [(0, '978.166')]
[36m[2025-06-29 21:31:22,558][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5079040. Throughput: 0: 353.2. Samples: 5080544. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 21:31:22,558][187912] Avg episode reward: [(0, '991.309')]
[36m[2025-06-29 21:31:27,561][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.3). Total num frames: 5079040. Throughput: 0: 352.2. Samples: 5081680. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 21:31:27,561][187912] Avg episode reward: [(0, '946.237')]
[36m[2025-06-29 21:31:32,567][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5083136. Throughput: 0: 353.4. Samples: 5083712. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 21:31:32,567][187912] Avg episode reward: [(0, '966.482')]
[36m[2025-06-29 21:31:37,565][187912] Fps is (10 sec: 409.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5083136. Throughput: 0: 353.3. Samples: 5085968. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 21:31:37,565][187912] Avg episode reward: [(0, '1045.986')]
[36m[2025-06-29 21:31:42,565][187912] Fps is (10 sec: 409.7, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 5087232. Throughput: 0: 359.9. Samples: 5087072. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:31:42,565][187912] Avg episode reward: [(0, '1042.697')]
[36m[2025-06-29 21:31:47,596][187912] Fps is (10 sec: 408.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5087232. Throughput: 0: 351.4. Samples: 5089024. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:31:47,596][187912] Avg episode reward: [(0, '1098.857')]
[36m[2025-06-29 21:31:53,070][187912] Fps is (10 sec: 389.9, 60 sec: 406.4, 300 sec: 360.4). Total num frames: 5091328. Throughput: 0: 353.2. Samples: 5091264. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:31:53,071][187912] Avg episode reward: [(0, '1057.479')]
[36m[2025-06-29 21:31:57,566][187912] Fps is (10 sec: 410.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5091328. Throughput: 0: 352.6. Samples: 5092192. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:31:57,566][187912] Avg episode reward: [(0, '998.309')]
[36m[2025-06-29 21:32:02,583][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5091328. Throughput: 0: 365.4. Samples: 5094656. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:32:02,584][187912] Avg episode reward: [(0, '1052.422')]
[36m[2025-06-29 21:32:07,562][187912] Fps is (10 sec: 409.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 5095424. Throughput: 0: 359.1. Samples: 5096704. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:32:07,562][187912] Avg episode reward: [(0, '1039.661')]
[36m[2025-06-29 21:32:12,592][187912] Fps is (10 sec: 409.2, 60 sec: 341.1, 300 sec: 347.8). Total num frames: 5095424. Throughput: 0: 358.5. Samples: 5097824. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:32:12,593][187912] Avg episode reward: [(0, '1057.292')]
[37m[1m[2025-06-29 21:32:12,641][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000019904_5095424.pth...
[36m[2025-06-29 21:32:12,704][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000019568_5009408.pth
[36m[2025-06-29 21:32:17,587][187912] Fps is (10 sec: 408.6, 60 sec: 345.5, 300 sec: 361.0). Total num frames: 5099520. Throughput: 0: 355.0. Samples: 5099696. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:32:17,587][187912] Avg episode reward: [(0, '1125.264')]
[36m[2025-06-29 21:32:22,590][187912] Fps is (10 sec: 409.7, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 5099520. Throughput: 0: 356.4. Samples: 5102016. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:32:22,591][187912] Avg episode reward: [(0, '1209.012')]
[36m[2025-06-29 21:32:27,565][187912] Fps is (10 sec: 410.5, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 5103616. Throughput: 0: 356.6. Samples: 5103120. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 21:32:27,566][187912] Avg episode reward: [(0, '1137.815')]
[36m[2025-06-29 21:32:32,580][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5103616. Throughput: 0: 359.9. Samples: 5105216. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 21:32:32,581][187912] Avg episode reward: [(0, '1079.243')]
[36m[2025-06-29 21:32:38,315][187912] Fps is (10 sec: 381.0, 60 sec: 404.5, 300 sec: 360.1). Total num frames: 5107712. Throughput: 0: 360.4. Samples: 5107568. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 21:32:38,315][187912] Avg episode reward: [(0, '1102.332')]
[36m[2025-06-29 21:32:42,575][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5107712. Throughput: 0: 362.2. Samples: 5108496. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 21:32:42,575][187912] Avg episode reward: [(0, '1082.498')]
[31m[14316782 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14316782 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[14316782 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:32:47,568][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 5107712. Throughput: 0: 361.7. Samples: 5110928. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 21:32:47,568][187912] Avg episode reward: [(0, '977.181')]
[36m[2025-06-29 21:32:52,567][187912] Fps is (10 sec: 409.9, 60 sec: 344.2, 300 sec: 361.0). Total num frames: 5111808. Throughput: 0: 362.6. Samples: 5113024. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 21:32:52,567][187912] Avg episode reward: [(0, '1086.733')]
[36m[2025-06-29 21:32:57,593][187912] Fps is (10 sec: 408.5, 60 sec: 341.2, 300 sec: 347.6). Total num frames: 5111808. Throughput: 0: 362.7. Samples: 5114144. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 21:32:57,594][187912] Avg episode reward: [(0, '1039.163')]
[36m[2025-06-29 21:33:02,584][187912] Fps is (10 sec: 408.9, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 5115904. Throughput: 0: 365.2. Samples: 5116128. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 21:33:02,584][187912] Avg episode reward: [(0, '995.789')]
[36m[2025-06-29 21:33:07,563][187912] Fps is (10 sec: 410.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5115904. Throughput: 0: 365.4. Samples: 5118448. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 21:33:07,563][187912] Avg episode reward: [(0, '915.583')]
[36m[2025-06-29 21:33:12,575][187912] Fps is (10 sec: 410.0, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 5120000. Throughput: 0: 365.1. Samples: 5119552. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 21:33:12,575][187912] Avg episode reward: [(0, '954.737')]
[36m[2025-06-29 21:33:17,558][187912] Fps is (10 sec: 409.8, 60 sec: 341.5, 300 sec: 361.1). Total num frames: 5120000. Throughput: 0: 363.6. Samples: 5121568. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 21:33:17,558][187912] Avg episode reward: [(0, '898.329')]
[33m[14353495 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[14353495 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.80126953125
[33mCrash Rate: 0.13623046875
[33mTimeout Rate: 0.0625 (navigation_task.py:265)
[33m[14353495 ms][navigation_task] - WARNING : 
[33mSuccesses: 1641
[33mCrashes : 279
[33mTimeouts: 128 (navigation_task.py:268)
[31m[14354187 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14354187 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[14354187 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:33:22,603][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5120000. Throughput: 0: 363.1. Samples: 5123648. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 21:33:22,603][187912] Avg episode reward: [(0, '915.732')]
[36m[2025-06-29 21:33:27,591][187912] Fps is (10 sec: 408.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5124096. Throughput: 0: 356.5. Samples: 5124544. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 21:33:27,591][187912] Avg episode reward: [(0, '876.297')]
[36m[2025-06-29 21:33:32,564][187912] Fps is (10 sec: 411.2, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 5124096. Throughput: 0: 351.3. Samples: 5126736. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 21:33:32,564][187912] Avg episode reward: [(0, '879.126')]
[36m[2025-06-29 21:33:37,560][187912] Fps is (10 sec: 410.9, 60 sec: 345.7, 300 sec: 361.0). Total num frames: 5128192. Throughput: 0: 349.6. Samples: 5128752. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:33:37,560][187912] Avg episode reward: [(0, '880.412')]
[36m[2025-06-29 21:33:42,570][187912] Fps is (10 sec: 409.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5128192. Throughput: 0: 352.2. Samples: 5129984. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:33:42,571][187912] Avg episode reward: [(0, '901.715')]
[36m[2025-06-29 21:33:47,560][187912] Fps is (10 sec: 409.6, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 5132288. Throughput: 0: 356.1. Samples: 5132144. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:33:47,560][187912] Avg episode reward: [(0, '869.218')]
[36m[2025-06-29 21:33:52,593][187912] Fps is (10 sec: 408.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5132288. Throughput: 0: 350.0. Samples: 5134208. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:33:52,593][187912] Avg episode reward: [(0, '904.615')]
[36m[2025-06-29 21:33:57,586][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5132288. Throughput: 0: 349.8. Samples: 5135296. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:33:57,586][187912] Avg episode reward: [(0, '1061.199')]
[36m[2025-06-29 21:34:02,559][187912] Fps is (10 sec: 411.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 5136384. Throughput: 0: 349.9. Samples: 5137312. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 21:34:02,559][187912] Avg episode reward: [(0, '1096.854')]
[36m[2025-06-29 21:34:07,582][187912] Fps is (10 sec: 409.8, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 5136384. Throughput: 0: 351.1. Samples: 5139440. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 21:34:07,582][187912] Avg episode reward: [(0, '1064.999')]
[36m[2025-06-29 21:34:12,593][187912] Fps is (10 sec: 408.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5140480. Throughput: 0: 354.8. Samples: 5140512. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 21:34:12,594][187912] Avg episode reward: [(0, '1128.775')]
[37m[1m[2025-06-29 21:34:12,597][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000020080_5140480.pth...
[36m[2025-06-29 21:34:12,652][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000019744_5054464.pth
[36m[2025-06-29 21:34:17,597][187912] Fps is (10 sec: 409.0, 60 sec: 341.1, 300 sec: 347.3). Total num frames: 5140480. Throughput: 0: 352.5. Samples: 5142608. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 21:34:17,597][187912] Avg episode reward: [(0, '1148.996')]
[36m[2025-06-29 21:34:22,574][187912] Fps is (10 sec: 410.4, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 5144576. Throughput: 0: 352.2. Samples: 5144608. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:34:22,574][187912] Avg episode reward: [(0, '1102.576')]
[36m[2025-06-29 21:34:27,588][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5144576. Throughput: 0: 349.7. Samples: 5145728. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:34:27,588][187912] Avg episode reward: [(0, '1057.260')]
[36m[2025-06-29 21:34:32,561][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5144576. Throughput: 0: 354.5. Samples: 5148096. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:34:32,561][187912] Avg episode reward: [(0, '1075.824')]
[36m[2025-06-29 21:34:37,577][187912] Fps is (10 sec: 410.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5148672. Throughput: 0: 355.3. Samples: 5150192. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:34:37,577][187912] Avg episode reward: [(0, '1008.277')]
[36m[2025-06-29 21:34:42,578][187912] Fps is (10 sec: 408.9, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5148672. Throughput: 0: 358.1. Samples: 5151408. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:34:42,578][187912] Avg episode reward: [(0, '1030.771')]
[36m[2025-06-29 21:34:47,560][187912] Fps is (10 sec: 410.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5152768. Throughput: 0: 357.0. Samples: 5153376. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 21:34:47,561][187912] Avg episode reward: [(0, '1022.497')]
[36m[2025-06-29 21:34:52,587][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 347.4). Total num frames: 5152768. Throughput: 0: 362.3. Samples: 5155744. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 21:34:52,587][187912] Avg episode reward: [(0, '1022.124')]
[36m[2025-06-29 21:34:57,580][187912] Fps is (10 sec: 408.8, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 5156864. Throughput: 0: 364.2. Samples: 5156896. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 21:34:57,580][187912] Avg episode reward: [(0, '1056.307')]
[36m[2025-06-29 21:35:02,565][187912] Fps is (10 sec: 410.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5156864. Throughput: 0: 363.6. Samples: 5158960. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 21:35:02,565][187912] Avg episode reward: [(0, '1099.076')]
[36m[2025-06-29 21:35:07,573][187912] Fps is (10 sec: 409.9, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 5160960. Throughput: 0: 364.1. Samples: 5160992. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 21:35:07,573][187912] Avg episode reward: [(0, '1059.958')]
[36m[2025-06-29 21:35:12,562][187912] Fps is (10 sec: 409.7, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 5160960. Throughput: 0: 362.9. Samples: 5162048. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 21:35:12,562][187912] Avg episode reward: [(0, '967.105')]
[31m[14469492 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14469493 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[14469493 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:35:17,574][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 5160960. Throughput: 0: 360.4. Samples: 5164320. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 21:35:17,574][187912] Avg episode reward: [(0, '968.826')]
[36m[2025-06-29 21:35:22,573][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5165056. Throughput: 0: 362.7. Samples: 5166512. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 21:35:22,573][187912] Avg episode reward: [(0, '1011.218')]
[36m[2025-06-29 21:35:27,584][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5165056. Throughput: 0: 360.5. Samples: 5167632. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 21:35:27,585][187912] Avg episode reward: [(0, '959.810')]
[36m[2025-06-29 21:35:32,594][187912] Fps is (10 sec: 408.8, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 5169152. Throughput: 0: 363.1. Samples: 5169728. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:35:32,594][187912] Avg episode reward: [(0, '1015.431')]
[36m[2025-06-29 21:35:37,589][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5169152. Throughput: 0: 360.5. Samples: 5171968. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:35:37,589][187912] Avg episode reward: [(0, '1104.724')]
[36m[2025-06-29 21:35:42,581][187912] Fps is (10 sec: 410.1, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 5173248. Throughput: 0: 357.7. Samples: 5172992. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:35:42,581][187912] Avg episode reward: [(0, '1118.227')]
[36m[2025-06-29 21:35:47,598][187912] Fps is (10 sec: 409.2, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 5173248. Throughput: 0: 354.6. Samples: 5174928. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:35:47,598][187912] Avg episode reward: [(0, '1069.760')]
[36m[2025-06-29 21:35:53,320][187912] Fps is (10 sec: 381.4, 60 sec: 404.7, 300 sec: 360.1). Total num frames: 5177344. Throughput: 0: 354.3. Samples: 5177200. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 21:35:53,320][187912] Avg episode reward: [(0, '1011.836')]
[31m[14510687 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14510687 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[14510687 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:35:57,575][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5177344. Throughput: 0: 355.5. Samples: 5178048. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 21:35:57,575][187912] Avg episode reward: [(0, '976.049')]
[36m[2025-06-29 21:36:02,593][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 5177344. Throughput: 0: 355.8. Samples: 5180336. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 21:36:02,593][187912] Avg episode reward: [(0, '880.842')]
[36m[2025-06-29 21:36:07,594][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5181440. Throughput: 0: 349.7. Samples: 5182256. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 21:36:07,594][187912] Avg episode reward: [(0, '909.589')]
[36m[2025-06-29 21:36:12,591][187912] Fps is (10 sec: 409.7, 60 sec: 341.2, 300 sec: 348.0). Total num frames: 5181440. Throughput: 0: 349.5. Samples: 5183360. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 21:36:12,591][187912] Avg episode reward: [(0, '931.198')]
[37m[1m[2025-06-29 21:36:12,632][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000020240_5181440.pth...
[36m[2025-06-29 21:36:12,688][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000019904_5095424.pth
[36m[2025-06-29 21:36:17,565][187912] Fps is (10 sec: 410.8, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 5185536. Throughput: 0: 352.2. Samples: 5185568. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:36:17,565][187912] Avg episode reward: [(0, '940.710')]
[36m[2025-06-29 21:36:22,558][187912] Fps is (10 sec: 410.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5185536. Throughput: 0: 346.2. Samples: 5187536. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:36:22,558][187912] Avg episode reward: [(0, '908.764')]
[36m[2025-06-29 21:36:27,577][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5185536. Throughput: 0: 347.4. Samples: 5188624. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:36:27,578][187912] Avg episode reward: [(0, '956.663')]
[36m[2025-06-29 21:36:32,577][187912] Fps is (10 sec: 408.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5189632. Throughput: 0: 350.4. Samples: 5190688. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:36:32,577][187912] Avg episode reward: [(0, '966.992')]
[36m[2025-06-29 21:36:37,559][187912] Fps is (10 sec: 410.4, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 5189632. Throughput: 0: 355.5. Samples: 5192928. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:36:37,559][187912] Avg episode reward: [(0, '944.626')]
[36m[2025-06-29 21:36:42,562][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5193728. Throughput: 0: 350.7. Samples: 5193824. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:36:42,562][187912] Avg episode reward: [(0, '1008.741')]
[36m[2025-06-29 21:36:47,581][187912] Fps is (10 sec: 408.7, 60 sec: 341.4, 300 sec: 347.7). Total num frames: 5193728. Throughput: 0: 353.5. Samples: 5196240. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:36:47,581][187912] Avg episode reward: [(0, '1031.043')]
[36m[2025-06-29 21:36:52,563][187912] Fps is (10 sec: 409.6, 60 sec: 345.7, 300 sec: 361.0). Total num frames: 5197824. Throughput: 0: 353.3. Samples: 5198144. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:36:52,563][187912] Avg episode reward: [(0, '997.538')]
[36m[2025-06-29 21:36:57,569][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5197824. Throughput: 0: 354.3. Samples: 5199296. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:36:57,569][187912] Avg episode reward: [(0, '970.354')]
[36m[2025-06-29 21:37:02,585][187912] Fps is (10 sec: 408.7, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 5201920. Throughput: 0: 357.5. Samples: 5201664. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:37:02,585][187912] Avg episode reward: [(0, '1029.494')]
[36m[2025-06-29 21:37:07,591][187912] Fps is (10 sec: 408.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5201920. Throughput: 0: 358.5. Samples: 5203680. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:37:07,591][187912] Avg episode reward: [(0, '965.077')]
[36m[2025-06-29 21:37:12,592][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5201920. Throughput: 0: 361.1. Samples: 5204880. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:37:12,593][187912] Avg episode reward: [(0, '1036.842')]
[31m[14587883 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14587883 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[14587883 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:37:17,576][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5206016. Throughput: 0: 361.2. Samples: 5206944. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:37:17,576][187912] Avg episode reward: [(0, '1041.166')]
[36m[2025-06-29 21:37:22,586][187912] Fps is (10 sec: 409.9, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 5206016. Throughput: 0: 360.7. Samples: 5209168. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:37:22,587][187912] Avg episode reward: [(0, '1029.573')]
[36m[2025-06-29 21:37:27,594][187912] Fps is (10 sec: 408.8, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 5210112. Throughput: 0: 362.4. Samples: 5210144. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 21:37:27,595][187912] Avg episode reward: [(0, '1035.097')]
[36m[2025-06-29 21:37:32,569][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 348.0). Total num frames: 5210112. Throughput: 0: 358.9. Samples: 5212384. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 21:37:32,569][187912] Avg episode reward: [(0, '1036.180')]
[36m[2025-06-29 21:37:37,588][187912] Fps is (10 sec: 409.9, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 5214208. Throughput: 0: 363.2. Samples: 5214496. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 21:37:37,588][187912] Avg episode reward: [(0, '951.642')]
[36m[2025-06-29 21:37:42,594][187912] Fps is (10 sec: 408.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5214208. Throughput: 0: 363.2. Samples: 5215648. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 21:37:42,594][187912] Avg episode reward: [(0, '942.847')]
[36m[2025-06-29 21:37:47,919][187912] Fps is (10 sec: 396.5, 60 sec: 407.3, 300 sec: 360.6). Total num frames: 5218304. Throughput: 0: 357.9. Samples: 5217888. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:37:47,919][187912] Avg episode reward: [(0, '988.292')]
[31m[14624392 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14624392 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[14624393 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:37:52,563][187912] Fps is (10 sec: 410.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5218304. Throughput: 0: 361.5. Samples: 5219936. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:37:52,563][187912] Avg episode reward: [(0, '929.860')]
[36m[2025-06-29 21:37:57,569][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5218304. Throughput: 0: 360.7. Samples: 5221104. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:37:57,569][187912] Avg episode reward: [(0, '912.600')]
[36m[2025-06-29 21:38:02,590][187912] Fps is (10 sec: 408.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5222400. Throughput: 0: 359.0. Samples: 5223104. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:38:02,590][187912] Avg episode reward: [(0, '961.550')]
[36m[2025-06-29 21:38:07,600][187912] Fps is (10 sec: 408.3, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5222400. Throughput: 0: 361.8. Samples: 5225456. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:38:07,600][187912] Avg episode reward: [(0, '996.774')]
[36m[2025-06-29 21:38:12,594][187912] Fps is (10 sec: 409.4, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 5226496. Throughput: 0: 364.1. Samples: 5226528. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 21:38:12,594][187912] Avg episode reward: [(0, '947.616')]
[37m[1m[2025-06-29 21:38:12,634][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000020416_5226496.pth...
[36m[2025-06-29 21:38:12,690][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000020080_5140480.pth
[36m[2025-06-29 21:38:17,573][187912] Fps is (10 sec: 410.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5226496. Throughput: 0: 358.7. Samples: 5228528. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 21:38:17,574][187912] Avg episode reward: [(0, '1001.746')]
[36m[2025-06-29 21:38:22,587][187912] Fps is (10 sec: 409.9, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 5230592. Throughput: 0: 358.4. Samples: 5230624. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 21:38:22,587][187912] Avg episode reward: [(0, '995.797')]
[36m[2025-06-29 21:38:27,560][187912] Fps is (10 sec: 410.1, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 5230592. Throughput: 0: 356.2. Samples: 5231664. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 21:38:27,561][187912] Avg episode reward: [(0, '1007.883')]
[36m[2025-06-29 21:38:32,586][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 5230592. Throughput: 0: 361.8. Samples: 5234048. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 21:38:32,587][187912] Avg episode reward: [(0, '940.180')]
[36m[2025-06-29 21:38:37,571][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5234688. Throughput: 0: 360.5. Samples: 5236160. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:38:37,571][187912] Avg episode reward: [(0, '949.055')]
[36m[2025-06-29 21:38:42,586][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5234688. Throughput: 0: 360.4. Samples: 5237328. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:38:42,587][187912] Avg episode reward: [(0, '1026.104')]
[36m[2025-06-29 21:38:47,576][187912] Fps is (10 sec: 409.4, 60 sec: 343.3, 300 sec: 361.0). Total num frames: 5238784. Throughput: 0: 361.7. Samples: 5239376. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:38:47,577][187912] Avg episode reward: [(0, '1028.019')]
[36m[2025-06-29 21:38:52,577][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5238784. Throughput: 0: 364.3. Samples: 5241840. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:38:52,577][187912] Avg episode reward: [(0, '1030.322')]
[36m[2025-06-29 21:38:57,580][187912] Fps is (10 sec: 409.4, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 5242880. Throughput: 0: 364.2. Samples: 5242912. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:38:57,580][187912] Avg episode reward: [(0, '1078.723')]
[36m[2025-06-29 21:39:02,586][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5242880. Throughput: 0: 366.1. Samples: 5245008. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:39:02,586][187912] Avg episode reward: [(0, '1062.088')]
[36m[2025-06-29 21:39:07,590][187912] Fps is (10 sec: 409.2, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 5246976. Throughput: 0: 364.1. Samples: 5247008. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:39:07,590][187912] Avg episode reward: [(0, '998.651')]
[36m[2025-06-29 21:39:12,580][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5246976. Throughput: 0: 366.8. Samples: 5248176. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:39:12,581][187912] Avg episode reward: [(0, '1009.685')]
[36m[2025-06-29 21:39:17,589][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 5246976. Throughput: 0: 365.1. Samples: 5250480. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:39:17,589][187912] Avg episode reward: [(0, '967.720')]
[36m[2025-06-29 21:39:22,586][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5251072. Throughput: 0: 362.9. Samples: 5252496. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:39:22,586][187912] Avg episode reward: [(0, '891.347')]
[36m[2025-06-29 21:39:27,562][187912] Fps is (10 sec: 410.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5251072. Throughput: 0: 360.4. Samples: 5253536. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:39:27,562][187912] Avg episode reward: [(0, '1001.898')]
[36m[2025-06-29 21:39:32,597][187912] Fps is (10 sec: 409.1, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 5255168. Throughput: 0: 357.9. Samples: 5255488. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:39:32,597][187912] Avg episode reward: [(0, '992.750')]
[36m[2025-06-29 21:39:37,574][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5255168. Throughput: 0: 352.4. Samples: 5257696. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:39:37,574][187912] Avg episode reward: [(0, '968.109')]
[36m[2025-06-29 21:39:42,565][187912] Fps is (10 sec: 410.9, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 5259264. Throughput: 0: 355.3. Samples: 5258896. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 21:39:42,566][187912] Avg episode reward: [(0, '968.919')]
[36m[2025-06-29 21:39:47,592][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5259264. Throughput: 0: 353.7. Samples: 5260928. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 21:39:47,592][187912] Avg episode reward: [(0, '978.445')]
[36m[2025-06-29 21:39:53,260][187912] Fps is (10 sec: 383.0, 60 sec: 405.0, 300 sec: 360.2). Total num frames: 5263360. Throughput: 0: 354.9. Samples: 5263216. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 21:39:53,261][187912] Avg episode reward: [(0, '935.663')]
[36m[2025-06-29 21:39:57,567][187912] Fps is (10 sec: 410.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5263360. Throughput: 0: 354.2. Samples: 5264112. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 21:39:57,567][187912] Avg episode reward: [(0, '898.278')]
[36m[2025-06-29 21:40:02,564][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 5263360. Throughput: 0: 356.5. Samples: 5266512. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 21:40:02,565][187912] Avg episode reward: [(0, '920.452')]
[36m[2025-06-29 21:40:07,574][187912] Fps is (10 sec: 409.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5267456. Throughput: 0: 356.4. Samples: 5268528. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 21:40:07,574][187912] Avg episode reward: [(0, '916.769')]
[36m[2025-06-29 21:40:12,559][187912] Fps is (10 sec: 409.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 5267456. Throughput: 0: 359.1. Samples: 5269696. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 21:40:12,560][187912] Avg episode reward: [(0, '931.918')]
[37m[1m[2025-06-29 21:40:12,614][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000020576_5267456.pth...
[36m[2025-06-29 21:40:12,684][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000020240_5181440.pth
[36m[2025-06-29 21:40:17,577][187912] Fps is (10 sec: 409.4, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 5271552. Throughput: 0: 357.8. Samples: 5271584. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 21:40:17,577][187912] Avg episode reward: [(0, '879.872')]
[31m[14774169 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14774169 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[14774170 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:40:22,579][187912] Fps is (10 sec: 408.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5271552. Throughput: 0: 359.4. Samples: 5273872. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 21:40:22,580][187912] Avg episode reward: [(0, '888.486')]
[36m[2025-06-29 21:40:27,996][187912] Fps is (10 sec: 393.1, 60 sec: 406.7, 300 sec: 360.5). Total num frames: 5275648. Throughput: 0: 352.5. Samples: 5274912. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:40:27,996][187912] Avg episode reward: [(0, '904.242')]
[36m[2025-06-29 21:40:32,596][187912] Fps is (10 sec: 408.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5275648. Throughput: 0: 355.9. Samples: 5276944. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:40:32,596][187912] Avg episode reward: [(0, '862.030')]
[36m[2025-06-29 21:40:37,593][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 5275648. Throughput: 0: 363.1. Samples: 5279312. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:40:37,594][187912] Avg episode reward: [(0, '880.702')]
[36m[2025-06-29 21:40:42,585][187912] Fps is (10 sec: 410.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5279744. Throughput: 0: 356.1. Samples: 5280144. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:40:42,585][187912] Avg episode reward: [(0, '925.090')]
[36m[2025-06-29 21:40:47,575][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 348.0). Total num frames: 5279744. Throughput: 0: 353.7. Samples: 5282432. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:40:47,575][187912] Avg episode reward: [(0, '974.935')]
[36m[2025-06-29 21:40:52,586][187912] Fps is (10 sec: 409.5, 60 sec: 345.2, 300 sec: 361.0). Total num frames: 5283840. Throughput: 0: 354.4. Samples: 5284480. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:40:52,587][187912] Avg episode reward: [(0, '1025.465')]
[36m[2025-06-29 21:40:57,563][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5283840. Throughput: 0: 353.7. Samples: 5285616. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:40:57,564][187912] Avg episode reward: [(0, '1048.297')]
[36m[2025-06-29 21:41:02,566][187912] Fps is (10 sec: 410.4, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 5287936. Throughput: 0: 362.4. Samples: 5287888. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:41:02,567][187912] Avg episode reward: [(0, '1010.540')]
[36m[2025-06-29 21:41:07,595][187912] Fps is (10 sec: 408.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5287936. Throughput: 0: 355.4. Samples: 5289872. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:41:07,595][187912] Avg episode reward: [(0, '1034.257')]
[36m[2025-06-29 21:41:12,588][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 5287936. Throughput: 0: 360.6. Samples: 5290992. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:41:12,588][187912] Avg episode reward: [(0, '1009.826')]
[36m[2025-06-29 21:41:17,595][187912] Fps is (10 sec: 409.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5292032. Throughput: 0: 359.8. Samples: 5293136. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:41:17,596][187912] Avg episode reward: [(0, '906.533')]
[36m[2025-06-29 21:41:22,567][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5292032. Throughput: 0: 357.5. Samples: 5295392. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:41:22,567][187912] Avg episode reward: [(0, '898.722')]
[36m[2025-06-29 21:41:27,584][187912] Fps is (10 sec: 410.1, 60 sec: 343.7, 300 sec: 361.0). Total num frames: 5296128. Throughput: 0: 358.4. Samples: 5296272. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:41:27,584][187912] Avg episode reward: [(0, '931.520')]
[36m[2025-06-29 21:41:32,590][187912] Fps is (10 sec: 408.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5296128. Throughput: 0: 359.7. Samples: 5298624. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:41:32,590][187912] Avg episode reward: [(0, '952.541')]
[36m[2025-06-29 21:41:37,577][187912] Fps is (10 sec: 409.9, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 5300224. Throughput: 0: 357.1. Samples: 5300544. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:41:37,577][187912] Avg episode reward: [(0, '956.846')]
[36m[2025-06-29 21:41:42,560][187912] Fps is (10 sec: 410.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 5300224. Throughput: 0: 356.7. Samples: 5301664. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:41:42,560][187912] Avg episode reward: [(0, '1022.100')]
[36m[2025-06-29 21:41:48,092][187912] Fps is (10 sec: 389.5, 60 sec: 406.1, 300 sec: 360.4). Total num frames: 5304320. Throughput: 0: 350.0. Samples: 5303824. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:41:48,092][187912] Avg episode reward: [(0, '1058.400')]
[36m[2025-06-29 21:41:52,578][187912] Fps is (10 sec: 408.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5304320. Throughput: 0: 356.0. Samples: 5305888. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:41:52,578][187912] Avg episode reward: [(0, '1152.867')]
[36m[2025-06-29 21:41:57,573][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5304320. Throughput: 0: 354.6. Samples: 5306944. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:41:57,574][187912] Avg episode reward: [(0, '1070.921')]
[36m[2025-06-29 21:42:02,587][187912] Fps is (10 sec: 409.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5308416. Throughput: 0: 350.6. Samples: 5308912. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:42:02,587][187912] Avg episode reward: [(0, '1009.382')]
[33m[14877378 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[14877378 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.76513671875
[33mCrash Rate: 0.166015625
[33mTimeout Rate: 0.06884765625 (navigation_task.py:265)
[33m[14877378 ms][navigation_task] - WARNING : 
[33mSuccesses: 1567
[33mCrashes : 340
[33mTimeouts: 141 (navigation_task.py:268)
[36m[2025-06-29 21:42:07,565][187912] Fps is (10 sec: 409.9, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 5308416. Throughput: 0: 351.3. Samples: 5311200. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:42:07,566][187912] Avg episode reward: [(0, '993.536')]
[31m[14884857 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14884858 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[14884858 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:42:12,587][187912] Fps is (10 sec: 409.6, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 5312512. Throughput: 0: 356.2. Samples: 5312304. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:42:12,587][187912] Avg episode reward: [(0, '888.904')]
[37m[1m[2025-06-29 21:42:12,626][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000020752_5312512.pth...
[36m[2025-06-29 21:42:12,696][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000020416_5226496.pth
[36m[2025-06-29 21:42:17,591][187912] Fps is (10 sec: 408.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5312512. Throughput: 0: 349.9. Samples: 5314368. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:42:17,591][187912] Avg episode reward: [(0, '853.242')]
[36m[2025-06-29 21:42:22,918][187912] Fps is (10 sec: 396.5, 60 sec: 407.2, 300 sec: 360.6). Total num frames: 5316608. Throughput: 0: 354.3. Samples: 5316608. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:42:22,928][187912] Avg episode reward: [(0, '972.482')]
[36m[2025-06-29 21:42:27,591][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5316608. Throughput: 0: 351.8. Samples: 5317504. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:42:27,591][187912] Avg episode reward: [(0, '966.467')]
[36m[2025-06-29 21:42:32,587][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5316608. Throughput: 0: 358.2. Samples: 5319760. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:42:32,587][187912] Avg episode reward: [(0, '1010.122')]
[36m[2025-06-29 21:42:37,573][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5320704. Throughput: 0: 353.8. Samples: 5321808. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:42:37,573][187912] Avg episode reward: [(0, '1104.789')]
[36m[2025-06-29 21:42:42,601][187912] Fps is (10 sec: 409.0, 60 sec: 341.1, 300 sec: 347.5). Total num frames: 5320704. Throughput: 0: 355.0. Samples: 5322928. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:42:42,602][187912] Avg episode reward: [(0, '1024.408')]
[36m[2025-06-29 21:42:47,585][187912] Fps is (10 sec: 409.1, 60 sec: 344.2, 300 sec: 361.0). Total num frames: 5324800. Throughput: 0: 355.9. Samples: 5324928. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-06-29 21:42:47,585][187912] Avg episode reward: [(0, '969.349')]
[36m[2025-06-29 21:42:52,585][187912] Fps is (10 sec: 410.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5324800. Throughput: 0: 355.8. Samples: 5327216. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-06-29 21:42:52,586][187912] Avg episode reward: [(0, '939.838')]
[36m[2025-06-29 21:42:57,577][187912] Fps is (10 sec: 409.9, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 5328896. Throughput: 0: 355.6. Samples: 5328304. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-06-29 21:42:57,577][187912] Avg episode reward: [(0, '964.449')]
[36m[2025-06-29 21:43:02,593][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5328896. Throughput: 0: 353.8. Samples: 5330288. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-06-29 21:43:02,593][187912] Avg episode reward: [(0, '891.873')]
[36m[2025-06-29 21:43:07,561][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 5328896. Throughput: 0: 355.2. Samples: 5332464. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-06-29 21:43:07,562][187912] Avg episode reward: [(0, '938.045')]
[36m[2025-06-29 21:43:12,577][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5332992. Throughput: 0: 351.0. Samples: 5333296. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:43:12,577][187912] Avg episode reward: [(0, '1041.775')]
[36m[2025-06-29 21:43:17,569][187912] Fps is (10 sec: 409.3, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 5332992. Throughput: 0: 353.9. Samples: 5335680. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:43:17,569][187912] Avg episode reward: [(0, '1077.364')]
[36m[2025-06-29 21:43:22,562][187912] Fps is (10 sec: 410.2, 60 sec: 343.4, 300 sec: 361.0). Total num frames: 5337088. Throughput: 0: 355.3. Samples: 5337792. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:43:22,562][187912] Avg episode reward: [(0, '1009.089')]
[36m[2025-06-29 21:43:27,560][187912] Fps is (10 sec: 410.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 5337088. Throughput: 0: 356.6. Samples: 5338960. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:43:27,560][187912] Avg episode reward: [(0, '1015.482')]
[36m[2025-06-29 21:43:32,562][187912] Fps is (10 sec: 409.6, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 5341184. Throughput: 0: 362.1. Samples: 5341216. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 21:43:32,562][187912] Avg episode reward: [(0, '975.345')]
[36m[2025-06-29 21:43:37,576][187912] Fps is (10 sec: 408.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5341184. Throughput: 0: 356.7. Samples: 5343264. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 21:43:37,577][187912] Avg episode reward: [(0, '849.528')]
[31m[14971854 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[14971854 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[14971854 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:43:42,575][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 5341184. Throughput: 0: 355.2. Samples: 5344288. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 21:43:42,575][187912] Avg episode reward: [(0, '835.993')]
[36m[2025-06-29 21:43:47,566][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5345280. Throughput: 0: 356.5. Samples: 5346320. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 21:43:47,566][187912] Avg episode reward: [(0, '881.338')]
[36m[2025-06-29 21:43:52,602][187912] Fps is (10 sec: 408.5, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 5345280. Throughput: 0: 358.4. Samples: 5348608. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 21:43:52,602][187912] Avg episode reward: [(0, '913.281')]
[36m[2025-06-29 21:43:57,610][187912] Fps is (10 sec: 407.8, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 5349376. Throughput: 0: 357.8. Samples: 5349408. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:43:57,611][187912] Avg episode reward: [(0, '952.078')]
[36m[2025-06-29 21:44:02,591][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5349376. Throughput: 0: 356.1. Samples: 5351712. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:44:02,591][187912] Avg episode reward: [(0, '949.898')]
[36m[2025-06-29 21:44:07,587][187912] Fps is (10 sec: 410.6, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 5353472. Throughput: 0: 355.4. Samples: 5353792. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:44:07,587][187912] Avg episode reward: [(0, '970.378')]
[36m[2025-06-29 21:44:12,581][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5353472. Throughput: 0: 356.5. Samples: 5355008. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:44:12,581][187912] Avg episode reward: [(0, '992.255')]
[37m[1m[2025-06-29 21:44:12,627][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000020912_5353472.pth...
[36m[2025-06-29 21:44:12,700][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000020576_5267456.pth
[36m[2025-06-29 21:44:17,819][187912] Fps is (10 sec: 400.3, 60 sec: 407.9, 300 sec: 360.7). Total num frames: 5357568. Throughput: 0: 353.9. Samples: 5357232. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:44:17,820][187912] Avg episode reward: [(0, '1081.053')]
[36m[2025-06-29 21:44:22,592][187912] Fps is (10 sec: 409.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5357568. Throughput: 0: 355.1. Samples: 5359248. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:44:22,593][187912] Avg episode reward: [(0, '1019.479')]
[36m[2025-06-29 21:44:27,580][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 5357568. Throughput: 0: 357.6. Samples: 5360384. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:44:27,580][187912] Avg episode reward: [(0, '1072.702')]
[36m[2025-06-29 21:44:32,597][187912] Fps is (10 sec: 409.4, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 5361664. Throughput: 0: 355.0. Samples: 5362304. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:44:32,597][187912] Avg episode reward: [(0, '930.436')]
[36m[2025-06-29 21:44:37,567][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5361664. Throughput: 0: 350.8. Samples: 5364384. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:44:37,567][187912] Avg episode reward: [(0, '931.584')]
[36m[2025-06-29 21:44:42,589][187912] Fps is (10 sec: 409.9, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 5365760. Throughput: 0: 357.9. Samples: 5365504. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:44:42,590][187912] Avg episode reward: [(0, '889.193')]
[36m[2025-06-29 21:44:47,585][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 347.9). Total num frames: 5365760. Throughput: 0: 352.8. Samples: 5367584. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:44:47,586][187912] Avg episode reward: [(0, '915.054')]
[36m[2025-06-29 21:44:52,622][187912] Fps is (10 sec: 408.3, 60 sec: 409.5, 300 sec: 360.9). Total num frames: 5369856. Throughput: 0: 332.2. Samples: 5368752. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:44:52,622][187912] Avg episode reward: [(0, '875.047')]
[36m[2025-06-29 21:44:57,589][187912] Fps is (10 sec: 409.5, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 5369856. Throughput: 0: 353.0. Samples: 5370896. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:44:57,589][187912] Avg episode reward: [(0, '945.454')]
[36m[2025-06-29 21:45:02,564][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 5369856. Throughput: 0: 357.6. Samples: 5373232. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:45:02,564][187912] Avg episode reward: [(0, '942.603')]
[36m[2025-06-29 21:45:07,580][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5373952. Throughput: 0: 355.7. Samples: 5375248. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:45:07,581][187912] Avg episode reward: [(0, '934.752')]
[36m[2025-06-29 21:45:12,579][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5373952. Throughput: 0: 353.4. Samples: 5376288. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:45:12,579][187912] Avg episode reward: [(0, '885.431')]
[36m[2025-06-29 21:45:17,562][187912] Fps is (10 sec: 410.4, 60 sec: 342.8, 300 sec: 361.0). Total num frames: 5378048. Throughput: 0: 358.3. Samples: 5378416. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:45:17,562][187912] Avg episode reward: [(0, '907.792')]
[36m[2025-06-29 21:45:22,566][187912] Fps is (10 sec: 410.1, 60 sec: 341.5, 300 sec: 347.6). Total num frames: 5378048. Throughput: 0: 361.6. Samples: 5380656. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:45:22,566][187912] Avg episode reward: [(0, '922.942')]
[36m[2025-06-29 21:45:27,590][187912] Fps is (10 sec: 408.4, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 5382144. Throughput: 0: 363.7. Samples: 5381872. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:45:27,590][187912] Avg episode reward: [(0, '1019.023')]
[36m[2025-06-29 21:45:32,590][187912] Fps is (10 sec: 408.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5382144. Throughput: 0: 363.0. Samples: 5383920. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:45:32,590][187912] Avg episode reward: [(0, '971.687')]
[36m[2025-06-29 21:45:37,559][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5382144. Throughput: 0: 385.6. Samples: 5386080. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:45:37,559][187912] Avg episode reward: [(0, '941.564')]
[36m[2025-06-29 21:45:42,591][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5386240. Throughput: 0: 357.0. Samples: 5386960. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:45:42,592][187912] Avg episode reward: [(0, '899.521')]
[36m[2025-06-29 21:45:47,571][187912] Fps is (10 sec: 409.1, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5386240. Throughput: 0: 355.1. Samples: 5389216. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:45:47,572][187912] Avg episode reward: [(0, '991.166')]
[31m[15102050 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15102050 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([12], device='cuda:0') (navigation_task.py:196)
[31m[15102050 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:45:52,591][187912] Fps is (10 sec: 409.6, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 5390336. Throughput: 0: 355.1. Samples: 5391232. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 21:45:52,591][187912] Avg episode reward: [(0, '877.870')]
[36m[2025-06-29 21:45:57,600][187912] Fps is (10 sec: 408.4, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5390336. Throughput: 0: 357.2. Samples: 5392368. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 21:45:57,600][187912] Avg episode reward: [(0, '847.154')]
[36m[2025-06-29 21:46:02,569][187912] Fps is (10 sec: 410.5, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 5394432. Throughput: 0: 356.6. Samples: 5394464. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 21:46:02,569][187912] Avg episode reward: [(0, '917.581')]
[36m[2025-06-29 21:46:07,558][187912] Fps is (10 sec: 411.3, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 5394432. Throughput: 0: 354.9. Samples: 5396624. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 21:46:07,559][187912] Avg episode reward: [(0, '986.714')]
[36m[2025-06-29 21:46:13,223][187912] Fps is (10 sec: 384.4, 60 sec: 405.2, 300 sec: 360.2). Total num frames: 5398528. Throughput: 0: 346.1. Samples: 5397664. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 21:46:13,223][187912] Avg episode reward: [(0, '963.197')]
[37m[1m[2025-06-29 21:46:13,268][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000021088_5398528.pth...
[36m[2025-06-29 21:46:13,333][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000020752_5312512.pth
[36m[2025-06-29 21:46:17,584][187912] Fps is (10 sec: 408.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5398528. Throughput: 0: 350.6. Samples: 5399696. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 21:46:17,585][187912] Avg episode reward: [(0, '992.662')]
[36m[2025-06-29 21:46:22,577][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5398528. Throughput: 0: 354.0. Samples: 5402016. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 21:46:22,577][187912] Avg episode reward: [(0, '1018.729')]
[31m[15136600 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15136600 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[15136600 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:46:27,562][187912] Fps is (10 sec: 410.5, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 5402624. Throughput: 0: 355.4. Samples: 5402944. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 21:46:27,562][187912] Avg episode reward: [(0, '942.204')]
[36m[2025-06-29 21:46:32,590][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5402624. Throughput: 0: 353.6. Samples: 5405136. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 21:46:32,590][187912] Avg episode reward: [(0, '881.544')]
[36m[2025-06-29 21:46:37,566][187912] Fps is (10 sec: 409.4, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 5406720. Throughput: 0: 352.5. Samples: 5407088. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 21:46:37,566][187912] Avg episode reward: [(0, '878.683')]
[36m[2025-06-29 21:46:42,591][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 347.7). Total num frames: 5406720. Throughput: 0: 355.6. Samples: 5408368. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 21:46:42,591][187912] Avg episode reward: [(0, '910.487')]
[36m[2025-06-29 21:46:47,979][187912] Fps is (10 sec: 393.4, 60 sec: 406.8, 300 sec: 360.5). Total num frames: 5410816. Throughput: 0: 352.0. Samples: 5410448. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 21:46:47,979][187912] Avg episode reward: [(0, '897.215')]
[31m[15164575 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15164575 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[15164575 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:46:52,582][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5410816. Throughput: 0: 352.5. Samples: 5412496. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 21:46:52,582][187912] Avg episode reward: [(0, '925.598')]
[36m[2025-06-29 21:46:57,592][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5410816. Throughput: 0: 359.5. Samples: 5413616. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 21:46:57,592][187912] Avg episode reward: [(0, '979.576')]
[36m[2025-06-29 21:47:02,570][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5414912. Throughput: 0: 353.5. Samples: 5415600. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:47:02,570][187912] Avg episode reward: [(0, '961.528')]
[36m[2025-06-29 21:47:07,581][187912] Fps is (10 sec: 410.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 5414912. Throughput: 0: 349.8. Samples: 5417760. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:47:07,581][187912] Avg episode reward: [(0, '948.919')]
[36m[2025-06-29 21:47:12,580][187912] Fps is (10 sec: 409.2, 60 sec: 345.0, 300 sec: 361.0). Total num frames: 5419008. Throughput: 0: 351.5. Samples: 5418768. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:47:12,580][187912] Avg episode reward: [(0, '950.962')]
[36m[2025-06-29 21:47:17,580][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 347.5). Total num frames: 5419008. Throughput: 0: 349.2. Samples: 5420848. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:47:17,580][187912] Avg episode reward: [(0, '984.609')]
[36m[2025-06-29 21:47:22,681][187912] Fps is (10 sec: 405.5, 60 sec: 408.9, 300 sec: 360.9). Total num frames: 5423104. Throughput: 0: 331.6. Samples: 5422048. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:47:22,681][187912] Avg episode reward: [(0, '1003.731')]
[36m[2025-06-29 21:47:27,582][187912] Fps is (10 sec: 409.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5423104. Throughput: 0: 351.0. Samples: 5424160. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:47:27,582][187912] Avg episode reward: [(0, '990.582')]
[31m[15203819 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15203820 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[15203820 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:47:32,585][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5423104. Throughput: 0: 358.7. Samples: 5426448. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:47:32,585][187912] Avg episode reward: [(0, '983.636')]
[36m[2025-06-29 21:47:37,569][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5427200. Throughput: 0: 356.7. Samples: 5428544. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:47:37,569][187912] Avg episode reward: [(0, '910.335')]
[36m[2025-06-29 21:47:42,569][187912] Fps is (10 sec: 410.3, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 5427200. Throughput: 0: 358.9. Samples: 5429760. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:47:42,569][187912] Avg episode reward: [(0, '910.788')]
[36m[2025-06-29 21:47:47,575][187912] Fps is (10 sec: 409.3, 60 sec: 343.6, 300 sec: 361.0). Total num frames: 5431296. Throughput: 0: 358.0. Samples: 5431712. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:47:47,575][187912] Avg episode reward: [(0, '877.605')]
[36m[2025-06-29 21:47:52,586][187912] Fps is (10 sec: 408.9, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5431296. Throughput: 0: 360.5. Samples: 5433984. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:47:52,586][187912] Avg episode reward: [(0, '897.950')]
[36m[2025-06-29 21:47:57,559][187912] Fps is (10 sec: 410.3, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 5435392. Throughput: 0: 362.8. Samples: 5435088. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:47:57,559][187912] Avg episode reward: [(0, '931.624')]
[36m[2025-06-29 21:48:02,567][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5435392. Throughput: 0: 364.9. Samples: 5437264. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:48:02,567][187912] Avg episode reward: [(0, '990.493')]
[36m[2025-06-29 21:48:07,924][187912] Fps is (10 sec: 395.2, 60 sec: 407.3, 300 sec: 360.6). Total num frames: 5439488. Throughput: 0: 385.5. Samples: 5439488. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 21:48:07,925][187912] Avg episode reward: [(0, '950.681')]
[36m[2025-06-29 21:48:12,580][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5439488. Throughput: 0: 359.5. Samples: 5440336. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 21:48:12,580][187912] Avg episode reward: [(0, '964.508')]
[37m[1m[2025-06-29 21:48:12,623][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000021248_5439488.pth...
[36m[2025-06-29 21:48:12,685][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000020912_5353472.pth
[36m[2025-06-29 21:48:17,574][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5439488. Throughput: 0: 357.4. Samples: 5442528. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 21:48:17,574][187912] Avg episode reward: [(0, '939.611')]
[36m[2025-06-29 21:48:22,584][187912] Fps is (10 sec: 409.4, 60 sec: 341.9, 300 sec: 361.0). Total num frames: 5443584. Throughput: 0: 355.8. Samples: 5444560. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 21:48:22,584][187912] Avg episode reward: [(0, '848.802')]
[31m[15260397 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15260398 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[15260398 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:48:27,558][187912] Fps is (10 sec: 410.2, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 5443584. Throughput: 0: 354.2. Samples: 5445696. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 21:48:27,558][187912] Avg episode reward: [(0, '932.881')]
[36m[2025-06-29 21:48:32,572][187912] Fps is (10 sec: 410.1, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 5447680. Throughput: 0: 355.6. Samples: 5447712. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:48:32,572][187912] Avg episode reward: [(0, '784.301')]
[36m[2025-06-29 21:48:37,568][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5447680. Throughput: 0: 355.0. Samples: 5449952. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:48:37,568][187912] Avg episode reward: [(0, '857.887')]
[36m[2025-06-29 21:48:43,000][187912] Fps is (10 sec: 392.8, 60 sec: 406.7, 300 sec: 360.5). Total num frames: 5451776. Throughput: 0: 351.4. Samples: 5451056. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:48:43,001][187912] Avg episode reward: [(0, '872.389')]
[36m[2025-06-29 21:48:47,575][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5451776. Throughput: 0: 349.1. Samples: 5452976. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:48:47,575][187912] Avg episode reward: [(0, '908.477')]
[36m[2025-06-29 21:48:52,579][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 5451776. Throughput: 0: 349.3. Samples: 5455088. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:48:52,580][187912] Avg episode reward: [(0, '865.757')]
[36m[2025-06-29 21:48:57,592][187912] Fps is (10 sec: 408.9, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 5455872. Throughput: 0: 346.6. Samples: 5455936. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:48:57,593][187912] Avg episode reward: [(0, '921.899')]
[36m[2025-06-29 21:49:02,558][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 5455872. Throughput: 0: 352.1. Samples: 5458368. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:49:02,559][187912] Avg episode reward: [(0, '951.757')]
[36m[2025-06-29 21:49:07,576][187912] Fps is (10 sec: 410.3, 60 sec: 343.3, 300 sec: 361.0). Total num frames: 5459968. Throughput: 0: 352.8. Samples: 5460432. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:49:07,576][187912] Avg episode reward: [(0, '928.484')]
[36m[2025-06-29 21:49:12,596][187912] Fps is (10 sec: 408.1, 60 sec: 341.2, 300 sec: 347.4). Total num frames: 5459968. Throughput: 0: 352.8. Samples: 5461584. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:49:12,597][187912] Avg episode reward: [(0, '902.295')]
[36m[2025-06-29 21:49:17,583][187912] Fps is (10 sec: 409.3, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 5464064. Throughput: 0: 359.0. Samples: 5463872. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:49:17,583][187912] Avg episode reward: [(0, '862.691')]
[36m[2025-06-29 21:49:22,586][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5464064. Throughput: 0: 355.1. Samples: 5465936. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:49:22,586][187912] Avg episode reward: [(0, '938.590')]
[36m[2025-06-29 21:49:27,593][187912] Fps is (10 sec: 0.0, 60 sec: 341.1, 300 sec: 347.1). Total num frames: 5464064. Throughput: 0: 359.5. Samples: 5467088. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:49:27,594][187912] Avg episode reward: [(0, '889.224')]
[36m[2025-06-29 21:49:32,582][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5468160. Throughput: 0: 355.5. Samples: 5468976. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:49:32,582][187912] Avg episode reward: [(0, '880.110')]
[36m[2025-06-29 21:49:37,590][187912] Fps is (10 sec: 409.7, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 5468160. Throughput: 0: 362.2. Samples: 5471392. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:49:37,591][187912] Avg episode reward: [(0, '888.910')]
[36m[2025-06-29 21:49:42,570][187912] Fps is (10 sec: 410.1, 60 sec: 343.8, 300 sec: 361.0). Total num frames: 5472256. Throughput: 0: 364.3. Samples: 5472320. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:49:42,570][187912] Avg episode reward: [(0, '890.477')]
[36m[2025-06-29 21:49:47,564][187912] Fps is (10 sec: 410.7, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 5472256. Throughput: 0: 360.1. Samples: 5474576. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:49:47,564][187912] Avg episode reward: [(0, '811.087')]
[36m[2025-06-29 21:49:52,584][187912] Fps is (10 sec: 409.1, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 5476352. Throughput: 0: 359.4. Samples: 5476608. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:49:52,584][187912] Avg episode reward: [(0, '902.174')]
[36m[2025-06-29 21:49:57,578][187912] Fps is (10 sec: 409.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5476352. Throughput: 0: 358.9. Samples: 5477728. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:49:57,578][187912] Avg episode reward: [(0, '867.466')]
[31m[15354426 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15354427 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[15354427 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:50:02,582][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 5476352. Throughput: 0: 353.1. Samples: 5479760. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:50:02,582][187912] Avg episode reward: [(0, '894.555')]
[36m[2025-06-29 21:50:07,569][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5480448. Throughput: 0: 353.6. Samples: 5481840. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:50:07,569][187912] Avg episode reward: [(0, '918.025')]
[36m[2025-06-29 21:50:12,584][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5480448. Throughput: 0: 354.2. Samples: 5483024. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:50:12,584][187912] Avg episode reward: [(0, '823.031')]
[37m[1m[2025-06-29 21:50:12,625][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000021408_5480448.pth...
[36m[2025-06-29 21:50:12,696][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000021088_5398528.pth
[36m[2025-06-29 21:50:17,570][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5484544. Throughput: 0: 358.1. Samples: 5485088. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:50:17,571][187912] Avg episode reward: [(0, '805.406')]
[36m[2025-06-29 21:50:22,576][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5484544. Throughput: 0: 357.1. Samples: 5487456. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:50:22,577][187912] Avg episode reward: [(0, '848.729')]
[36m[2025-06-29 21:50:27,559][187912] Fps is (10 sec: 410.1, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 5488640. Throughput: 0: 363.5. Samples: 5488672. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:50:27,559][187912] Avg episode reward: [(0, '857.478')]
[31m[15385087 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15385087 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[15385087 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:50:32,575][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5488640. Throughput: 0: 361.9. Samples: 5490864. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:50:32,576][187912] Avg episode reward: [(0, '829.905')]
[36m[2025-06-29 21:50:37,581][187912] Fps is (10 sec: 408.7, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 5492736. Throughput: 0: 359.1. Samples: 5492768. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:50:37,581][187912] Avg episode reward: [(0, '870.421')]
[33m[15394829 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[15394829 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.72021484375
[33mCrash Rate: 0.18798828125
[33mTimeout Rate: 0.091796875 (navigation_task.py:265)
[33m[15394829 ms][navigation_task] - WARNING : 
[33mSuccesses: 1475
[33mCrashes : 385
[33mTimeouts: 188 (navigation_task.py:268)
[36m[2025-06-29 21:50:42,567][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5492736. Throughput: 0: 361.7. Samples: 5494000. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:50:42,567][187912] Avg episode reward: [(0, '836.360')]
[36m[2025-06-29 21:50:47,588][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 5492736. Throughput: 0: 365.8. Samples: 5496224. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:50:47,588][187912] Avg episode reward: [(0, '973.703')]
[36m[2025-06-29 21:50:52,565][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5496832. Throughput: 0: 364.1. Samples: 5498224. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:50:52,566][187912] Avg episode reward: [(0, '900.876')]
[36m[2025-06-29 21:50:57,558][187912] Fps is (10 sec: 410.8, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5496832. Throughput: 0: 361.1. Samples: 5499264. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:50:57,558][187912] Avg episode reward: [(0, '913.730')]
[36m[2025-06-29 21:51:02,585][187912] Fps is (10 sec: 408.8, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 5500928. Throughput: 0: 357.9. Samples: 5501200. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:51:02,585][187912] Avg episode reward: [(0, '985.864')]
[36m[2025-06-29 21:51:07,583][187912] Fps is (10 sec: 408.6, 60 sec: 341.3, 300 sec: 347.9). Total num frames: 5500928. Throughput: 0: 356.2. Samples: 5503488. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:51:07,584][187912] Avg episode reward: [(0, '1004.620')]
[36m[2025-06-29 21:51:12,591][187912] Fps is (10 sec: 409.3, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 5505024. Throughput: 0: 353.5. Samples: 5504592. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 21:51:12,591][187912] Avg episode reward: [(0, '938.059')]
[36m[2025-06-29 21:51:17,592][187912] Fps is (10 sec: 409.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5505024. Throughput: 0: 349.4. Samples: 5506592. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 21:51:17,593][187912] Avg episode reward: [(0, '853.305')]
[36m[2025-06-29 21:51:23,236][187912] Fps is (10 sec: 384.8, 60 sec: 405.1, 300 sec: 360.2). Total num frames: 5509120. Throughput: 0: 355.0. Samples: 5508976. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 21:51:23,236][187912] Avg episode reward: [(0, '881.251')]
[36m[2025-06-29 21:51:27,566][187912] Fps is (10 sec: 410.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5509120. Throughput: 0: 353.4. Samples: 5509904. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 21:51:27,566][187912] Avg episode reward: [(0, '830.130')]
[36m[2025-06-29 21:51:32,560][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5509120. Throughput: 0: 354.0. Samples: 5512144. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 21:51:32,560][187912] Avg episode reward: [(0, '902.494')]
[36m[2025-06-29 21:51:37,596][187912] Fps is (10 sec: 408.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5513216. Throughput: 0: 356.4. Samples: 5514272. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:51:37,597][187912] Avg episode reward: [(0, '950.405')]
[36m[2025-06-29 21:51:42,592][187912] Fps is (10 sec: 408.3, 60 sec: 341.2, 300 sec: 347.6). Total num frames: 5513216. Throughput: 0: 356.7. Samples: 5515328. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:51:42,593][187912] Avg episode reward: [(0, '1044.679')]
[36m[2025-06-29 21:51:47,596][187912] Fps is (10 sec: 409.6, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 5517312. Throughput: 0: 358.7. Samples: 5517344. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:51:47,596][187912] Avg episode reward: [(0, '1115.891')]
[36m[2025-06-29 21:51:52,591][187912] Fps is (10 sec: 409.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5517312. Throughput: 0: 354.8. Samples: 5519456. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:51:52,591][187912] Avg episode reward: [(0, '1086.022')]
[36m[2025-06-29 21:51:57,958][187912] Fps is (10 sec: 395.3, 60 sec: 406.9, 300 sec: 360.5). Total num frames: 5521408. Throughput: 0: 353.0. Samples: 5520608. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 21:51:57,958][187912] Avg episode reward: [(0, '1066.037')]
[31m[15472413 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15472413 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[15472414 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:52:02,562][187912] Fps is (10 sec: 410.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 5521408. Throughput: 0: 357.2. Samples: 5522656. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 21:52:02,562][187912] Avg episode reward: [(0, '1088.558')]
[36m[2025-06-29 21:52:07,588][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5521408. Throughput: 0: 359.3. Samples: 5524912. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 21:52:07,588][187912] Avg episode reward: [(0, '1040.286')]
[36m[2025-06-29 21:52:12,593][187912] Fps is (10 sec: 408.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5525504. Throughput: 0: 351.8. Samples: 5525744. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 21:52:12,593][187912] Avg episode reward: [(0, '1025.157')]
[37m[1m[2025-06-29 21:52:12,634][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000021584_5525504.pth...
[36m[2025-06-29 21:52:12,698][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000021248_5439488.pth
[36m[2025-06-29 21:52:17,567][187912] Fps is (10 sec: 410.5, 60 sec: 341.5, 300 sec: 347.3). Total num frames: 5525504. Throughput: 0: 354.8. Samples: 5528112. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 21:52:17,567][187912] Avg episode reward: [(0, '977.435')]
[36m[2025-06-29 21:52:22,566][187912] Fps is (10 sec: 410.7, 60 sec: 345.2, 300 sec: 361.0). Total num frames: 5529600. Throughput: 0: 356.5. Samples: 5530304. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:52:22,567][187912] Avg episode reward: [(0, '971.963')]
[36m[2025-06-29 21:52:27,576][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5529600. Throughput: 0: 357.8. Samples: 5531424. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:52:27,576][187912] Avg episode reward: [(0, '896.325')]
[36m[2025-06-29 21:52:32,595][187912] Fps is (10 sec: 408.4, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 5533696. Throughput: 0: 363.4. Samples: 5533696. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:52:32,595][187912] Avg episode reward: [(0, '948.082')]
[36m[2025-06-29 21:52:37,591][187912] Fps is (10 sec: 409.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5533696. Throughput: 0: 362.0. Samples: 5535744. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:52:37,591][187912] Avg episode reward: [(0, '863.228')]
[36m[2025-06-29 21:52:42,586][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5533696. Throughput: 0: 365.0. Samples: 5536896. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:52:42,587][187912] Avg episode reward: [(0, '898.914')]
[36m[2025-06-29 21:52:47,598][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5537792. Throughput: 0: 358.5. Samples: 5538800. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 21:52:47,598][187912] Avg episode reward: [(0, '906.822')]
[36m[2025-06-29 21:52:52,571][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5537792. Throughput: 0: 356.4. Samples: 5540944. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 21:52:52,572][187912] Avg episode reward: [(0, '917.088')]
[36m[2025-06-29 21:52:57,576][187912] Fps is (10 sec: 410.5, 60 sec: 343.5, 300 sec: 361.0). Total num frames: 5541888. Throughput: 0: 359.6. Samples: 5541920. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 21:52:57,576][187912] Avg episode reward: [(0, '847.094')]
[36m[2025-06-29 21:53:02,591][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 347.5). Total num frames: 5541888. Throughput: 0: 356.4. Samples: 5544160. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 21:53:02,591][187912] Avg episode reward: [(0, '954.130')]
[36m[2025-06-29 21:53:07,586][187912] Fps is (10 sec: 409.2, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 5545984. Throughput: 0: 352.6. Samples: 5546176. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:53:07,587][187912] Avg episode reward: [(0, '977.619')]
[36m[2025-06-29 21:53:12,580][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5545984. Throughput: 0: 352.7. Samples: 5547296. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:53:12,580][187912] Avg episode reward: [(0, '957.271')]
[36m[2025-06-29 21:53:18,031][187912] Fps is (10 sec: 392.2, 60 sec: 406.5, 300 sec: 360.5). Total num frames: 5550080. Throughput: 0: 350.7. Samples: 5549632. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:53:18,031][187912] Avg episode reward: [(0, '906.220')]
[36m[2025-06-29 21:53:22,607][187912] Fps is (10 sec: 408.5, 60 sec: 341.1, 300 sec: 360.9). Total num frames: 5550080. Throughput: 0: 356.1. Samples: 5551776. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:53:22,607][187912] Avg episode reward: [(0, '963.470')]
[36m[2025-06-29 21:53:27,599][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 5550080. Throughput: 0: 356.2. Samples: 5552928. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:53:27,599][187912] Avg episode reward: [(0, '930.314')]
[36m[2025-06-29 21:53:32,580][187912] Fps is (10 sec: 410.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5554176. Throughput: 0: 360.0. Samples: 5554992. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 21:53:32,580][187912] Avg episode reward: [(0, '931.814')]
[36m[2025-06-29 21:53:37,565][187912] Fps is (10 sec: 411.0, 60 sec: 341.5, 300 sec: 347.6). Total num frames: 5554176. Throughput: 0: 361.3. Samples: 5557200. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 21:53:37,565][187912] Avg episode reward: [(0, '923.859')]
[36m[2025-06-29 21:53:42,576][187912] Fps is (10 sec: 409.7, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 5558272. Throughput: 0: 364.1. Samples: 5558304. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 21:53:42,576][187912] Avg episode reward: [(0, '959.413')]
[31m[15580248 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15580248 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([12], device='cuda:0') (navigation_task.py:196)
[31m[15580249 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:53:47,567][187912] Fps is (10 sec: 409.5, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 5558272. Throughput: 0: 363.2. Samples: 5560496. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 21:53:47,568][187912] Avg episode reward: [(0, '1025.528')]
[36m[2025-06-29 21:53:52,600][187912] Fps is (10 sec: 408.6, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 5562368. Throughput: 0: 360.4. Samples: 5562400. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:53:52,601][187912] Avg episode reward: [(0, '1023.178')]
[36m[2025-06-29 21:53:57,593][187912] Fps is (10 sec: 408.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5562368. Throughput: 0: 362.2. Samples: 5563600. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:53:57,593][187912] Avg episode reward: [(0, '990.269')]
[36m[2025-06-29 21:54:02,576][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5562368. Throughput: 0: 365.3. Samples: 5565904. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:54:02,576][187912] Avg episode reward: [(0, '950.049')]
[36m[2025-06-29 21:54:07,559][187912] Fps is (10 sec: 411.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 5566464. Throughput: 0: 362.0. Samples: 5568048. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:54:07,559][187912] Avg episode reward: [(0, '949.957')]
[36m[2025-06-29 21:54:12,582][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5566464. Throughput: 0: 360.7. Samples: 5569152. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:54:12,582][187912] Avg episode reward: [(0, '903.183')]
[37m[1m[2025-06-29 21:54:12,626][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000021744_5566464.pth...
[36m[2025-06-29 21:54:12,691][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000021408_5480448.pth
[36m[2025-06-29 21:54:17,564][187912] Fps is (10 sec: 409.4, 60 sec: 344.0, 300 sec: 361.0). Total num frames: 5570560. Throughput: 0: 358.9. Samples: 5571136. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:54:17,565][187912] Avg episode reward: [(0, '909.601')]
[36m[2025-06-29 21:54:22,586][187912] Fps is (10 sec: 409.4, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 5570560. Throughput: 0: 359.3. Samples: 5573376. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:54:22,586][187912] Avg episode reward: [(0, '915.812')]
[36m[2025-06-29 21:54:27,590][187912] Fps is (10 sec: 408.5, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 5574656. Throughput: 0: 359.4. Samples: 5574480. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:54:27,590][187912] Avg episode reward: [(0, '897.326')]
[31m[15621675 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15621675 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[15621675 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:54:32,582][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5574656. Throughput: 0: 357.2. Samples: 5576576. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:54:32,582][187912] Avg episode reward: [(0, '944.041')]
[36m[2025-06-29 21:54:37,687][187912] Fps is (10 sec: 405.7, 60 sec: 408.8, 300 sec: 360.9). Total num frames: 5578752. Throughput: 0: 339.6. Samples: 5577712. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:54:37,687][187912] Avg episode reward: [(0, '900.025')]
[36m[2025-06-29 21:54:42,582][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5578752. Throughput: 0: 358.8. Samples: 5579744. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:54:42,583][187912] Avg episode reward: [(0, '817.840')]
[36m[2025-06-29 21:54:47,563][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5578752. Throughput: 0: 358.2. Samples: 5582016. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:54:47,563][187912] Avg episode reward: [(0, '852.672')]
[36m[2025-06-29 21:54:52,558][187912] Fps is (10 sec: 410.6, 60 sec: 341.6, 300 sec: 361.0). Total num frames: 5582848. Throughput: 0: 355.9. Samples: 5584064. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:54:52,558][187912] Avg episode reward: [(0, '790.973')]
[36m[2025-06-29 21:54:57,584][187912] Fps is (10 sec: 408.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5582848. Throughput: 0: 357.0. Samples: 5585216. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 21:54:57,584][187912] Avg episode reward: [(0, '820.616')]
[36m[2025-06-29 21:55:02,595][187912] Fps is (10 sec: 408.1, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 5586944. Throughput: 0: 359.6. Samples: 5587328. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:55:02,595][187912] Avg episode reward: [(0, '812.044')]
[36m[2025-06-29 21:55:07,565][187912] Fps is (10 sec: 410.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5586944. Throughput: 0: 361.8. Samples: 5589648. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:55:07,565][187912] Avg episode reward: [(0, '806.218')]
[36m[2025-06-29 21:55:12,612][187912] Fps is (10 sec: 408.9, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 5591040. Throughput: 0: 359.6. Samples: 5590672. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:55:12,612][187912] Avg episode reward: [(0, '796.300')]
[31m[15669853 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15669854 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[15669854 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:55:17,591][187912] Fps is (10 sec: 408.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5591040. Throughput: 0: 356.9. Samples: 5592640. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:55:17,591][187912] Avg episode reward: [(0, '819.838')]
[36m[2025-06-29 21:55:23,223][187912] Fps is (10 sec: 386.0, 60 sec: 405.3, 300 sec: 360.2). Total num frames: 5595136. Throughput: 0: 379.8. Samples: 5595008. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 21:55:23,224][187912] Avg episode reward: [(0, '887.141')]
[36m[2025-06-29 21:55:27,579][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5595136. Throughput: 0: 359.1. Samples: 5595904. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 21:55:27,579][187912] Avg episode reward: [(0, '938.666')]
[36m[2025-06-29 21:55:32,559][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 5595136. Throughput: 0: 356.3. Samples: 5598048. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 21:55:32,559][187912] Avg episode reward: [(0, '989.894')]
[36m[2025-06-29 21:55:37,584][187912] Fps is (10 sec: 409.4, 60 sec: 341.9, 300 sec: 361.0). Total num frames: 5599232. Throughput: 0: 355.4. Samples: 5600064. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 21:55:37,584][187912] Avg episode reward: [(0, '964.424')]
[36m[2025-06-29 21:55:42,583][187912] Fps is (10 sec: 408.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5599232. Throughput: 0: 355.6. Samples: 5601216. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 21:55:42,583][187912] Avg episode reward: [(0, '929.023')]
[36m[2025-06-29 21:55:47,569][187912] Fps is (10 sec: 410.2, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 5603328. Throughput: 0: 356.5. Samples: 5603360. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:55:47,570][187912] Avg episode reward: [(0, '959.563')]
[36m[2025-06-29 21:55:52,566][187912] Fps is (10 sec: 410.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5603328. Throughput: 0: 351.3. Samples: 5605456. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:55:52,566][187912] Avg episode reward: [(0, '942.351')]
[36m[2025-06-29 21:55:58,080][187912] Fps is (10 sec: 389.7, 60 sec: 406.2, 300 sec: 360.4). Total num frames: 5607424. Throughput: 0: 349.8. Samples: 5606576. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:55:58,080][187912] Avg episode reward: [(0, '919.528')]
[36m[2025-06-29 21:56:02,564][187912] Fps is (10 sec: 409.7, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 5607424. Throughput: 0: 356.1. Samples: 5608656. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:56:02,564][187912] Avg episode reward: [(0, '962.807')]
[36m[2025-06-29 21:56:07,572][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5607424. Throughput: 0: 359.7. Samples: 5610960. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:56:07,573][187912] Avg episode reward: [(0, '939.483')]
[36m[2025-06-29 21:56:12,561][187912] Fps is (10 sec: 409.7, 60 sec: 341.6, 300 sec: 361.0). Total num frames: 5611520. Throughput: 0: 353.9. Samples: 5611824. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:56:12,562][187912] Avg episode reward: [(0, '930.032')]
[37m[1m[2025-06-29 21:56:12,636][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000021920_5611520.pth...
[36m[2025-06-29 21:56:12,707][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000021584_5525504.pth
[36m[2025-06-29 21:56:17,579][187912] Fps is (10 sec: 409.3, 60 sec: 341.4, 300 sec: 347.9). Total num frames: 5611520. Throughput: 0: 355.4. Samples: 5614048. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:56:17,580][187912] Avg episode reward: [(0, '942.300')]
[31m[15735065 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15735065 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[15735065 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:56:22,582][187912] Fps is (10 sec: 408.8, 60 sec: 345.0, 300 sec: 361.0). Total num frames: 5615616. Throughput: 0: 354.2. Samples: 5616000. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:56:22,582][187912] Avg episode reward: [(0, '988.051')]
[36m[2025-06-29 21:56:27,571][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5615616. Throughput: 0: 354.9. Samples: 5617184. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:56:27,571][187912] Avg episode reward: [(0, '954.396')]
[36m[2025-06-29 21:56:32,918][187912] Fps is (10 sec: 396.3, 60 sec: 407.2, 300 sec: 360.6). Total num frames: 5619712. Throughput: 0: 352.8. Samples: 5619360. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:56:32,918][187912] Avg episode reward: [(0, '1054.793')]
[36m[2025-06-29 21:56:37,579][187912] Fps is (10 sec: 409.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5619712. Throughput: 0: 355.1. Samples: 5621440. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:56:37,579][187912] Avg episode reward: [(0, '1010.496')]
[36m[2025-06-29 21:56:42,591][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5619712. Throughput: 0: 357.7. Samples: 5622496. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:56:42,591][187912] Avg episode reward: [(0, '1037.380')]
[36m[2025-06-29 21:56:47,583][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5623808. Throughput: 0: 352.9. Samples: 5624544. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:56:47,583][187912] Avg episode reward: [(0, '1009.196')]
[36m[2025-06-29 21:56:52,568][187912] Fps is (10 sec: 410.6, 60 sec: 341.3, 300 sec: 347.6). Total num frames: 5623808. Throughput: 0: 354.5. Samples: 5626912. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:56:52,568][187912] Avg episode reward: [(0, '1052.469')]
[36m[2025-06-29 21:56:57,583][187912] Fps is (10 sec: 409.6, 60 sec: 344.2, 300 sec: 361.0). Total num frames: 5627904. Throughput: 0: 357.9. Samples: 5627936. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:56:57,583][187912] Avg episode reward: [(0, '1027.832')]
[31m[15770889 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15770889 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[15770889 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:57:02,571][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5627904. Throughput: 0: 358.1. Samples: 5630160. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:57:02,571][187912] Avg episode reward: [(0, '1051.627')]
[36m[2025-06-29 21:57:07,561][187912] Fps is (10 sec: 410.5, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 5632000. Throughput: 0: 357.5. Samples: 5632080. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:57:07,561][187912] Avg episode reward: [(0, '1079.024')]
[36m[2025-06-29 21:57:12,573][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5632000. Throughput: 0: 358.4. Samples: 5633312. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:57:12,573][187912] Avg episode reward: [(0, '1076.541')]
[36m[2025-06-29 21:57:18,074][187912] Fps is (10 sec: 389.6, 60 sec: 406.2, 300 sec: 360.4). Total num frames: 5636096. Throughput: 0: 360.0. Samples: 5635616. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:57:18,075][187912] Avg episode reward: [(0, '988.405')]
[36m[2025-06-29 21:57:22,562][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5636096. Throughput: 0: 362.1. Samples: 5637728. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:57:22,562][187912] Avg episode reward: [(0, '969.045')]
[36m[2025-06-29 21:57:27,579][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5636096. Throughput: 0: 362.4. Samples: 5638800. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:57:27,579][187912] Avg episode reward: [(0, '940.575')]
[36m[2025-06-29 21:57:32,598][187912] Fps is (10 sec: 408.1, 60 sec: 343.2, 300 sec: 361.0). Total num frames: 5640192. Throughput: 0: 363.3. Samples: 5640896. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:57:32,599][187912] Avg episode reward: [(0, '890.948')]
[36m[2025-06-29 21:57:37,605][187912] Fps is (10 sec: 408.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5640192. Throughput: 0: 357.7. Samples: 5643024. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 21:57:37,606][187912] Avg episode reward: [(0, '877.136')]
[36m[2025-06-29 21:57:42,594][187912] Fps is (10 sec: 409.8, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 5644288. Throughput: 0: 359.4. Samples: 5644112. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 21:57:42,594][187912] Avg episode reward: [(0, '902.599')]
[36m[2025-06-29 21:57:47,562][187912] Fps is (10 sec: 411.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5644288. Throughput: 0: 358.5. Samples: 5646288. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 21:57:47,562][187912] Avg episode reward: [(0, '976.677')]
[36m[2025-06-29 21:57:52,593][187912] Fps is (10 sec: 409.6, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 5648384. Throughput: 0: 362.8. Samples: 5648416. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 21:57:52,593][187912] Avg episode reward: [(0, '984.696')]
[36m[2025-06-29 21:57:57,595][187912] Fps is (10 sec: 408.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5648384. Throughput: 0: 360.0. Samples: 5649520. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 21:57:57,595][187912] Avg episode reward: [(0, '871.049')]
[36m[2025-06-29 21:58:02,582][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5648384. Throughput: 0: 359.5. Samples: 5651616. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 21:58:02,582][187912] Avg episode reward: [(0, '900.387')]
[36m[2025-06-29 21:58:07,563][187912] Fps is (10 sec: 410.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5652480. Throughput: 0: 352.7. Samples: 5653600. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 21:58:07,563][187912] Avg episode reward: [(0, '857.174')]
[36m[2025-06-29 21:58:12,570][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 347.7). Total num frames: 5652480. Throughput: 0: 353.5. Samples: 5654704. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 21:58:12,571][187912] Avg episode reward: [(0, '866.187')]
[37m[1m[2025-06-29 21:58:12,626][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000022080_5652480.pth...
[36m[2025-06-29 21:58:12,697][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000021744_5566464.pth
[36m[2025-06-29 21:58:17,568][187912] Fps is (10 sec: 409.4, 60 sec: 344.2, 300 sec: 361.1). Total num frames: 5656576. Throughput: 0: 350.5. Samples: 5656656. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 21:58:17,568][187912] Avg episode reward: [(0, '832.454')]
[36m[2025-06-29 21:58:22,572][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5656576. Throughput: 0: 353.3. Samples: 5658912. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 21:58:22,572][187912] Avg episode reward: [(0, '931.090')]
[36m[2025-06-29 21:58:27,673][187912] Fps is (10 sec: 405.3, 60 sec: 409.0, 300 sec: 360.9). Total num frames: 5660672. Throughput: 0: 353.5. Samples: 5660048. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 21:58:27,674][187912] Avg episode reward: [(0, '956.059')]
[31m[15865843 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15865843 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[15865843 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:58:32,578][187912] Fps is (10 sec: 409.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5660672. Throughput: 0: 351.9. Samples: 5662128. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 21:58:32,578][187912] Avg episode reward: [(0, '957.203')]
[36m[2025-06-29 21:58:37,588][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5660672. Throughput: 0: 354.5. Samples: 5664368. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 21:58:37,588][187912] Avg episode reward: [(0, '886.877')]
[36m[2025-06-29 21:58:42,589][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5664768. Throughput: 0: 348.1. Samples: 5665184. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 21:58:42,589][187912] Avg episode reward: [(0, '907.191')]
[36m[2025-06-29 21:58:47,564][187912] Fps is (10 sec: 410.6, 60 sec: 341.3, 300 sec: 347.2). Total num frames: 5664768. Throughput: 0: 354.3. Samples: 5667552. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 21:58:47,564][187912] Avg episode reward: [(0, '936.602')]
[31m[15884834 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15884834 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[15884834 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 21:58:52,575][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5668864. Throughput: 0: 357.2. Samples: 5669680. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 21:58:52,575][187912] Avg episode reward: [(0, '904.176')]
[36m[2025-06-29 21:58:57,593][187912] Fps is (10 sec: 408.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5668864. Throughput: 0: 359.3. Samples: 5670880. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 21:58:57,593][187912] Avg episode reward: [(0, '924.374')]
[36m[2025-06-29 21:59:02,570][187912] Fps is (10 sec: 409.8, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 5672960. Throughput: 0: 363.0. Samples: 5672992. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 21:59:02,570][187912] Avg episode reward: [(0, '956.516')]
[36m[2025-06-29 21:59:07,602][187912] Fps is (10 sec: 409.2, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 5672960. Throughput: 0: 367.0. Samples: 5675440. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 21:59:07,602][187912] Avg episode reward: [(0, '940.164')]
[36m[2025-06-29 21:59:12,572][187912] Fps is (10 sec: 409.5, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 5677056. Throughput: 0: 367.0. Samples: 5676528. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 21:59:12,572][187912] Avg episode reward: [(0, '966.042')]
[36m[2025-06-29 21:59:17,567][187912] Fps is (10 sec: 411.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5677056. Throughput: 0: 366.3. Samples: 5678608. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 21:59:17,567][187912] Avg episode reward: [(0, '955.226')]
[36m[2025-06-29 21:59:22,564][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5677056. Throughput: 0: 368.5. Samples: 5680944. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 21:59:22,564][187912] Avg episode reward: [(0, '917.694')]
[36m[2025-06-29 21:59:27,559][187912] Fps is (10 sec: 409.9, 60 sec: 342.0, 300 sec: 361.0). Total num frames: 5681152. Throughput: 0: 370.7. Samples: 5681856. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 21:59:27,559][187912] Avg episode reward: [(0, '894.989')]
[33m[15922395 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[15922395 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.7154709696769714
[33mCrash Rate: 0.16398243606090546
[33mTimeout Rate: 0.1205466091632843 (navigation_task.py:265)
[33m[15922395 ms][navigation_task] - WARNING : 
[33mSuccesses: 1466
[33mCrashes : 336
[33mTimeouts: 247 (navigation_task.py:268)
[36m[2025-06-29 21:59:32,563][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 347.3). Total num frames: 5681152. Throughput: 0: 371.2. Samples: 5684256. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 21:59:32,564][187912] Avg episode reward: [(0, '907.228')]
[36m[2025-06-29 21:59:37,559][187912] Fps is (10 sec: 409.6, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 5685248. Throughput: 0: 367.8. Samples: 5686224. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 21:59:37,559][187912] Avg episode reward: [(0, '847.575')]
[36m[2025-06-29 21:59:42,582][187912] Fps is (10 sec: 408.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5685248. Throughput: 0: 365.6. Samples: 5687328. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 21:59:42,583][187912] Avg episode reward: [(0, '913.710')]
[36m[2025-06-29 21:59:47,595][187912] Fps is (10 sec: 408.1, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 5689344. Throughput: 0: 363.9. Samples: 5689376. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 21:59:47,595][187912] Avg episode reward: [(0, '903.486')]
[36m[2025-06-29 21:59:52,561][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5689344. Throughput: 0: 356.6. Samples: 5691472. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 21:59:52,561][187912] Avg episode reward: [(0, '931.790')]
[36m[2025-06-29 21:59:57,560][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.2). Total num frames: 5689344. Throughput: 0: 356.0. Samples: 5692544. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 21:59:57,560][187912] Avg episode reward: [(0, '1011.793')]
[36m[2025-06-29 22:00:02,559][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5693440. Throughput: 0: 355.3. Samples: 5694592. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:00:02,559][187912] Avg episode reward: [(0, '1012.434')]
[36m[2025-06-29 22:00:07,558][187912] Fps is (10 sec: 409.7, 60 sec: 341.6, 300 sec: 347.2). Total num frames: 5693440. Throughput: 0: 354.2. Samples: 5696880. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:00:07,558][187912] Avg episode reward: [(0, '931.634')]
[36m[2025-06-29 22:00:12,578][187912] Fps is (10 sec: 408.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5697536. Throughput: 0: 354.0. Samples: 5697792. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:00:12,578][187912] Avg episode reward: [(0, '835.334')]
[37m[1m[2025-06-29 22:00:12,618][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000022256_5697536.pth...
[36m[2025-06-29 22:00:12,680][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000021920_5611520.pth
[36m[2025-06-29 22:00:17,598][187912] Fps is (10 sec: 408.0, 60 sec: 341.2, 300 sec: 347.9). Total num frames: 5697536. Throughput: 0: 350.7. Samples: 5700048. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:00:17,598][187912] Avg episode reward: [(0, '955.474')]
[36m[2025-06-29 22:00:22,608][187912] Fps is (10 sec: 408.4, 60 sec: 409.3, 300 sec: 361.0). Total num frames: 5701632. Throughput: 0: 348.4. Samples: 5701920. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:00:22,608][187912] Avg episode reward: [(0, '936.700')]
[36m[2025-06-29 22:00:27,590][187912] Fps is (10 sec: 409.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5701632. Throughput: 0: 350.5. Samples: 5703104. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:00:27,590][187912] Avg episode reward: [(0, '1031.996')]
[36m[2025-06-29 22:00:32,873][187912] Fps is (10 sec: 399.0, 60 sec: 407.5, 300 sec: 360.7). Total num frames: 5705728. Throughput: 0: 353.4. Samples: 5705376. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:00:32,873][187912] Avg episode reward: [(0, '1083.638')]
[36m[2025-06-29 22:00:37,591][187912] Fps is (10 sec: 409.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5705728. Throughput: 0: 355.0. Samples: 5707456. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:00:37,591][187912] Avg episode reward: [(0, '1041.608')]
[36m[2025-06-29 22:00:42,570][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5705728. Throughput: 0: 358.3. Samples: 5708672. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:00:42,570][187912] Avg episode reward: [(0, '994.729')]
[36m[2025-06-29 22:00:47,568][187912] Fps is (10 sec: 410.5, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 5709824. Throughput: 0: 358.7. Samples: 5710736. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:00:47,569][187912] Avg episode reward: [(0, '995.635')]
[36m[2025-06-29 22:00:52,572][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 347.7). Total num frames: 5709824. Throughput: 0: 358.3. Samples: 5713008. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:00:52,572][187912] Avg episode reward: [(0, '915.635')]
[36m[2025-06-29 22:00:57,564][187912] Fps is (10 sec: 409.8, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 5713920. Throughput: 0: 359.2. Samples: 5713952. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:00:57,565][187912] Avg episode reward: [(0, '919.008')]
[36m[2025-06-29 22:01:02,588][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5713920. Throughput: 0: 356.3. Samples: 5716080. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:01:02,588][187912] Avg episode reward: [(0, '916.547')]
[36m[2025-06-29 22:01:07,566][187912] Fps is (10 sec: 409.5, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 5718016. Throughput: 0: 360.2. Samples: 5718112. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 22:01:07,567][187912] Avg episode reward: [(0, '938.810')]
[36m[2025-06-29 22:01:12,612][187912] Fps is (10 sec: 408.6, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 5718016. Throughput: 0: 357.9. Samples: 5719216. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 22:01:12,613][187912] Avg episode reward: [(0, '935.530')]
[36m[2025-06-29 22:01:17,588][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5718016. Throughput: 0: 359.6. Samples: 5721456. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 22:01:17,588][187912] Avg episode reward: [(0, '877.020')]
[31m[16035391 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16035392 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[16035392 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:01:22,588][187912] Fps is (10 sec: 410.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5722112. Throughput: 0: 359.1. Samples: 5723616. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 22:01:22,588][187912] Avg episode reward: [(0, '893.621')]
[36m[2025-06-29 22:01:27,587][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 347.5). Total num frames: 5722112. Throughput: 0: 357.9. Samples: 5724784. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 22:01:27,587][187912] Avg episode reward: [(0, '979.494')]
[36m[2025-06-29 22:01:32,566][187912] Fps is (10 sec: 410.5, 60 sec: 343.1, 300 sec: 361.0). Total num frames: 5726208. Throughput: 0: 357.0. Samples: 5726800. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 22:01:32,566][187912] Avg episode reward: [(0, '893.092')]
[36m[2025-06-29 22:01:37,583][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5726208. Throughput: 0: 356.2. Samples: 5729040. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 22:01:37,583][187912] Avg episode reward: [(0, '912.210')]
[36m[2025-06-29 22:01:42,569][187912] Fps is (10 sec: 409.5, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 5730304. Throughput: 0: 360.9. Samples: 5730192. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 22:01:42,569][187912] Avg episode reward: [(0, '864.701')]
[36m[2025-06-29 22:01:47,587][187912] Fps is (10 sec: 409.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5730304. Throughput: 0: 360.9. Samples: 5732320. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 22:01:47,587][187912] Avg episode reward: [(0, '891.377')]
[36m[2025-06-29 22:01:52,680][187912] Fps is (10 sec: 405.1, 60 sec: 408.9, 300 sec: 360.9). Total num frames: 5734400. Throughput: 0: 339.1. Samples: 5733408. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 22:01:52,681][187912] Avg episode reward: [(0, '791.845')]
[36m[2025-06-29 22:01:57,575][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5734400. Throughput: 0: 359.4. Samples: 5735376. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 22:01:57,575][187912] Avg episode reward: [(0, '777.058')]
[31m[16072503 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16072503 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[16072503 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:02:02,578][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5734400. Throughput: 0: 358.8. Samples: 5737600. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 22:02:02,578][187912] Avg episode reward: [(0, '750.958')]
[36m[2025-06-29 22:02:07,587][187912] Fps is (10 sec: 409.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5738496. Throughput: 0: 353.8. Samples: 5739536. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 22:02:07,588][187912] Avg episode reward: [(0, '717.003')]
[36m[2025-06-29 22:02:12,584][187912] Fps is (10 sec: 409.3, 60 sec: 341.5, 300 sec: 347.7). Total num frames: 5738496. Throughput: 0: 351.0. Samples: 5740576. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 22:02:12,584][187912] Avg episode reward: [(0, '710.938')]
[37m[1m[2025-06-29 22:02:12,635][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000022416_5738496.pth...
[36m[2025-06-29 22:02:12,700][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000022080_5652480.pth
[36m[2025-06-29 22:02:17,558][187912] Fps is (10 sec: 410.8, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 5742592. Throughput: 0: 351.7. Samples: 5742624. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 22:02:17,559][187912] Avg episode reward: [(0, '769.111')]
[36m[2025-06-29 22:02:22,590][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5742592. Throughput: 0: 354.1. Samples: 5744976. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 22:02:22,590][187912] Avg episode reward: [(0, '861.508')]
[36m[2025-06-29 22:02:27,674][187912] Fps is (10 sec: 404.9, 60 sec: 409.0, 300 sec: 360.9). Total num frames: 5746688. Throughput: 0: 352.6. Samples: 5746096. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 22:02:27,674][187912] Avg episode reward: [(0, '821.085')]
[36m[2025-06-29 22:02:32,592][187912] Fps is (10 sec: 409.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5746688. Throughput: 0: 348.8. Samples: 5748016. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 22:02:32,592][187912] Avg episode reward: [(0, '902.289')]
[36m[2025-06-29 22:02:37,566][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 5746688. Throughput: 0: 378.2. Samples: 5750384. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 22:02:37,566][187912] Avg episode reward: [(0, '955.173')]
[36m[2025-06-29 22:02:42,569][187912] Fps is (10 sec: 410.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5750784. Throughput: 0: 351.7. Samples: 5751200. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:02:42,570][187912] Avg episode reward: [(0, '963.246')]
[36m[2025-06-29 22:02:47,572][187912] Fps is (10 sec: 409.4, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5750784. Throughput: 0: 356.7. Samples: 5753648. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:02:47,572][187912] Avg episode reward: [(0, '941.965')]
[36m[2025-06-29 22:02:52,592][187912] Fps is (10 sec: 408.7, 60 sec: 341.8, 300 sec: 361.0). Total num frames: 5754880. Throughput: 0: 359.4. Samples: 5755712. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:02:52,592][187912] Avg episode reward: [(0, '965.354')]
[36m[2025-06-29 22:02:57,593][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5754880. Throughput: 0: 362.6. Samples: 5756896. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:02:57,593][187912] Avg episode reward: [(0, '941.637')]
[36m[2025-06-29 22:03:02,560][187912] Fps is (10 sec: 410.9, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 5758976. Throughput: 0: 364.1. Samples: 5759008. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:03:02,560][187912] Avg episode reward: [(0, '979.697')]
[36m[2025-06-29 22:03:07,591][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5758976. Throughput: 0: 359.5. Samples: 5761152. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:03:07,591][187912] Avg episode reward: [(0, '980.449')]
[36m[2025-06-29 22:03:13,200][187912] Fps is (10 sec: 385.0, 60 sec: 405.4, 300 sec: 360.2). Total num frames: 5763072. Throughput: 0: 354.3. Samples: 5762224. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:03:13,200][187912] Avg episode reward: [(0, '966.355')]
[36m[2025-06-29 22:03:17,594][187912] Fps is (10 sec: 409.5, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 5763072. Throughput: 0: 359.1. Samples: 5764176. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:03:17,595][187912] Avg episode reward: [(0, '905.602')]
[36m[2025-06-29 22:03:22,569][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.2). Total num frames: 5763072. Throughput: 0: 358.4. Samples: 5766512. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:03:22,569][187912] Avg episode reward: [(0, '941.433')]
[36m[2025-06-29 22:03:27,577][187912] Fps is (10 sec: 410.3, 60 sec: 341.9, 300 sec: 361.0). Total num frames: 5767168. Throughput: 0: 358.3. Samples: 5767328. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 22:03:27,577][187912] Avg episode reward: [(0, '978.877')]
[36m[2025-06-29 22:03:32,574][187912] Fps is (10 sec: 409.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5767168. Throughput: 0: 358.0. Samples: 5769760. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 22:03:32,574][187912] Avg episode reward: [(0, '950.242')]
[36m[2025-06-29 22:03:37,597][187912] Fps is (10 sec: 408.8, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 5771264. Throughput: 0: 356.2. Samples: 5771744. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 22:03:37,597][187912] Avg episode reward: [(0, '917.899')]
[36m[2025-06-29 22:03:42,574][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5771264. Throughput: 0: 356.1. Samples: 5772912. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 22:03:42,574][187912] Avg episode reward: [(0, '1028.000')]
[36m[2025-06-29 22:03:47,590][187912] Fps is (10 sec: 409.9, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 5775360. Throughput: 0: 359.6. Samples: 5775200. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:03:47,590][187912] Avg episode reward: [(0, '1103.955')]
[36m[2025-06-29 22:03:52,572][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5775360. Throughput: 0: 356.8. Samples: 5777200. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:03:52,573][187912] Avg episode reward: [(0, '1096.368')]
[36m[2025-06-29 22:03:57,587][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5775360. Throughput: 0: 362.6. Samples: 5778320. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:03:57,588][187912] Avg episode reward: [(0, '1060.019')]
[36m[2025-06-29 22:04:02,561][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.1). Total num frames: 5779456. Throughput: 0: 356.5. Samples: 5780208. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:04:02,561][187912] Avg episode reward: [(0, '1106.838')]
[36m[2025-06-29 22:04:07,561][187912] Fps is (10 sec: 410.7, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 5779456. Throughput: 0: 355.3. Samples: 5782496. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:04:07,561][187912] Avg episode reward: [(0, '1072.017')]
[31m[16203203 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16203204 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[16203204 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:04:12,589][187912] Fps is (10 sec: 408.4, 60 sec: 344.8, 300 sec: 361.0). Total num frames: 5783552. Throughput: 0: 361.1. Samples: 5783584. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 22:04:12,589][187912] Avg episode reward: [(0, '971.830')]
[37m[1m[2025-06-29 22:04:12,628][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000022592_5783552.pth...
[36m[2025-06-29 22:04:12,701][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000022256_5697536.pth
[31m[16209811 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16209811 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([12], device='cuda:0') (navigation_task.py:196)
[31m[16209811 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:04:17,577][187912] Fps is (10 sec: 408.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5783552. Throughput: 0: 350.2. Samples: 5785520. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 22:04:17,577][187912] Avg episode reward: [(0, '966.289')]
[36m[2025-06-29 22:04:22,764][187912] Fps is (10 sec: 402.6, 60 sec: 408.3, 300 sec: 360.8). Total num frames: 5787648. Throughput: 0: 329.8. Samples: 5786640. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 22:04:22,764][187912] Avg episode reward: [(0, '908.444')]
[36m[2025-06-29 22:04:27,588][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5787648. Throughput: 0: 347.3. Samples: 5788544. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 22:04:27,588][187912] Avg episode reward: [(0, '942.399')]
[36m[2025-06-29 22:04:32,589][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 5787648. Throughput: 0: 345.6. Samples: 5790752. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 22:04:32,589][187912] Avg episode reward: [(0, '928.187')]
[36m[2025-06-29 22:04:37,564][187912] Fps is (10 sec: 410.6, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 5791744. Throughput: 0: 347.1. Samples: 5792816. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 22:04:37,564][187912] Avg episode reward: [(0, '997.133')]
[36m[2025-06-29 22:04:42,577][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5791744. Throughput: 0: 347.5. Samples: 5793952. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 22:04:42,577][187912] Avg episode reward: [(0, '918.261')]
[36m[2025-06-29 22:04:47,581][187912] Fps is (10 sec: 408.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5795840. Throughput: 0: 348.3. Samples: 5795888. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 22:04:47,581][187912] Avg episode reward: [(0, '977.195')]
[36m[2025-06-29 22:04:52,571][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5795840. Throughput: 0: 349.4. Samples: 5798224. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 22:04:52,571][187912] Avg episode reward: [(0, '1011.032')]
[36m[2025-06-29 22:04:57,686][187912] Fps is (10 sec: 405.3, 60 sec: 408.9, 300 sec: 360.8). Total num frames: 5799936. Throughput: 0: 349.5. Samples: 5799344. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:04:57,687][187912] Avg episode reward: [(0, '998.956')]
[36m[2025-06-29 22:05:02,582][187912] Fps is (10 sec: 409.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5799936. Throughput: 0: 349.8. Samples: 5801264. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:05:02,582][187912] Avg episode reward: [(0, '941.919')]
[36m[2025-06-29 22:05:07,595][187912] Fps is (10 sec: 0.0, 60 sec: 341.1, 300 sec: 347.1). Total num frames: 5799936. Throughput: 0: 377.2. Samples: 5803552. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:05:07,595][187912] Avg episode reward: [(0, '992.556')]
[36m[2025-06-29 22:05:12,585][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5804032. Throughput: 0: 352.7. Samples: 5804416. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:05:12,585][187912] Avg episode reward: [(0, '979.238')]
[36m[2025-06-29 22:05:17,598][187912] Fps is (10 sec: 409.5, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 5804032. Throughput: 0: 351.9. Samples: 5806592. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:05:17,598][187912] Avg episode reward: [(0, '974.443')]
[36m[2025-06-29 22:05:22,566][187912] Fps is (10 sec: 410.4, 60 sec: 342.5, 300 sec: 361.0). Total num frames: 5808128. Throughput: 0: 350.2. Samples: 5808576. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:05:22,566][187912] Avg episode reward: [(0, '913.343')]
[31m[16280740 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16280740 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[16280740 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:05:27,573][187912] Fps is (10 sec: 410.6, 60 sec: 341.4, 300 sec: 347.5). Total num frames: 5808128. Throughput: 0: 352.4. Samples: 5809808. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:05:27,573][187912] Avg episode reward: [(0, '960.194')]
[31m[16282920 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16282921 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[16282921 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[16283971 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16283971 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[16283971 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:05:32,567][187912] Fps is (10 sec: 409.6, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 5812224. Throughput: 0: 359.9. Samples: 5812080. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:05:32,567][187912] Avg episode reward: [(0, '795.890')]
[36m[2025-06-29 22:05:37,593][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5812224. Throughput: 0: 352.9. Samples: 5814112. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:05:37,594][187912] Avg episode reward: [(0, '831.385')]
[36m[2025-06-29 22:05:42,572][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5812224. Throughput: 0: 355.0. Samples: 5815280. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:05:42,572][187912] Avg episode reward: [(0, '780.373')]
[36m[2025-06-29 22:05:47,583][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5816320. Throughput: 0: 358.7. Samples: 5817408. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 22:05:47,584][187912] Avg episode reward: [(0, '795.530')]
[36m[2025-06-29 22:05:52,595][187912] Fps is (10 sec: 408.7, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 5816320. Throughput: 0: 360.9. Samples: 5819792. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 22:05:52,595][187912] Avg episode reward: [(0, '792.087')]
[36m[2025-06-29 22:05:57,590][187912] Fps is (10 sec: 409.3, 60 sec: 341.9, 300 sec: 361.0). Total num frames: 5820416. Throughput: 0: 359.1. Samples: 5820576. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 22:05:57,590][187912] Avg episode reward: [(0, '824.656')]
[36m[2025-06-29 22:06:02,581][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5820416. Throughput: 0: 365.6. Samples: 5823040. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 22:06:02,582][187912] Avg episode reward: [(0, '794.707')]
[36m[2025-06-29 22:06:07,599][187912] Fps is (10 sec: 409.2, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 5824512. Throughput: 0: 367.4. Samples: 5825120. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:06:07,599][187912] Avg episode reward: [(0, '906.487')]
[36m[2025-06-29 22:06:12,559][187912] Fps is (10 sec: 410.5, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 5824512. Throughput: 0: 364.9. Samples: 5826224. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:06:12,559][187912] Avg episode reward: [(0, '910.621')]
[37m[1m[2025-06-29 22:06:12,603][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000022752_5824512.pth...
[36m[2025-06-29 22:06:12,674][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000022416_5738496.pth
[36m[2025-06-29 22:06:17,560][187912] Fps is (10 sec: 411.2, 60 sec: 409.9, 300 sec: 361.0). Total num frames: 5828608. Throughput: 0: 362.7. Samples: 5828400. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:06:17,560][187912] Avg episode reward: [(0, '831.093')]
[36m[2025-06-29 22:06:22,580][187912] Fps is (10 sec: 408.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5828608. Throughput: 0: 365.3. Samples: 5830544. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:06:22,580][187912] Avg episode reward: [(0, '907.703')]
[31m[16340485 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16340486 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[16340486 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:06:27,583][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5828608. Throughput: 0: 364.0. Samples: 5831664. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:06:27,583][187912] Avg episode reward: [(0, '941.082')]
[36m[2025-06-29 22:06:32,559][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5832704. Throughput: 0: 363.6. Samples: 5833760. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:06:32,560][187912] Avg episode reward: [(0, '788.398')]
[36m[2025-06-29 22:06:37,596][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5832704. Throughput: 0: 359.5. Samples: 5835968. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:06:37,596][187912] Avg episode reward: [(0, '826.791')]
[31m[16354573 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16354573 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[16354573 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:06:42,568][187912] Fps is (10 sec: 409.2, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 5836800. Throughput: 0: 361.4. Samples: 5836832. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:06:42,569][187912] Avg episode reward: [(0, '872.507')]
[31m[16356068 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16356069 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[16356069 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:06:47,569][187912] Fps is (10 sec: 410.7, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 5836800. Throughput: 0: 360.6. Samples: 5839264. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:06:47,569][187912] Avg episode reward: [(0, '921.048')]
[36m[2025-06-29 22:06:52,572][187912] Fps is (10 sec: 409.5, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 5840896. Throughput: 0: 360.7. Samples: 5841344. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:06:52,572][187912] Avg episode reward: [(0, '921.134')]
[36m[2025-06-29 22:06:57,582][187912] Fps is (10 sec: 409.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5840896. Throughput: 0: 362.5. Samples: 5842544. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:06:57,582][187912] Avg episode reward: [(0, '981.365')]
[36m[2025-06-29 22:07:02,779][187912] Fps is (10 sec: 401.3, 60 sec: 408.3, 300 sec: 360.8). Total num frames: 5844992. Throughput: 0: 360.2. Samples: 5844688. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:07:02,779][187912] Avg episode reward: [(0, '865.148')]
[36m[2025-06-29 22:07:07,567][187912] Fps is (10 sec: 410.2, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 5844992. Throughput: 0: 357.8. Samples: 5846640. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:07:07,567][187912] Avg episode reward: [(0, '865.077')]
[36m[2025-06-29 22:07:12,581][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 5844992. Throughput: 0: 357.3. Samples: 5847744. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:07:12,581][187912] Avg episode reward: [(0, '791.671')]
[36m[2025-06-29 22:07:17,585][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5849088. Throughput: 0: 357.8. Samples: 5849872. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:07:17,585][187912] Avg episode reward: [(0, '777.065')]
[36m[2025-06-29 22:07:22,571][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 5849088. Throughput: 0: 359.3. Samples: 5852128. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:07:22,571][187912] Avg episode reward: [(0, '748.379')]
[36m[2025-06-29 22:07:27,591][187912] Fps is (10 sec: 409.4, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 5853184. Throughput: 0: 362.8. Samples: 5853168. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:07:27,591][187912] Avg episode reward: [(0, '799.394')]
[36m[2025-06-29 22:07:32,576][187912] Fps is (10 sec: 409.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5853184. Throughput: 0: 352.7. Samples: 5855136. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:07:32,576][187912] Avg episode reward: [(0, '822.928')]
[36m[2025-06-29 22:07:37,738][187912] Fps is (10 sec: 403.7, 60 sec: 408.6, 300 sec: 360.8). Total num frames: 5857280. Throughput: 0: 328.7. Samples: 5856192. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:07:37,738][187912] Avg episode reward: [(0, '860.392')]
[36m[2025-06-29 22:07:42,567][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5857280. Throughput: 0: 347.1. Samples: 5858160. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:07:42,567][187912] Avg episode reward: [(0, '796.917')]
[36m[2025-06-29 22:07:47,582][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5857280. Throughput: 0: 352.5. Samples: 5860480. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:07:47,582][187912] Avg episode reward: [(0, '878.339')]
[36m[2025-06-29 22:07:52,595][187912] Fps is (10 sec: 408.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5861376. Throughput: 0: 352.9. Samples: 5862528. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:07:52,595][187912] Avg episode reward: [(0, '830.211')]
[36m[2025-06-29 22:07:57,565][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5861376. Throughput: 0: 355.3. Samples: 5863728. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:07:57,565][187912] Avg episode reward: [(0, '806.253')]
[36m[2025-06-29 22:08:02,563][187912] Fps is (10 sec: 410.9, 60 sec: 342.6, 300 sec: 361.0). Total num frames: 5865472. Throughput: 0: 351.8. Samples: 5865696. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 22:08:02,563][187912] Avg episode reward: [(0, '837.820')]
[31m[16439286 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16439286 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[16439286 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:08:07,585][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 347.8). Total num frames: 5865472. Throughput: 0: 353.0. Samples: 5868016. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 22:08:07,585][187912] Avg episode reward: [(0, '877.154')]
[36m[2025-06-29 22:08:12,585][187912] Fps is (10 sec: 408.7, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 5869568. Throughput: 0: 354.2. Samples: 5869104. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 22:08:12,585][187912] Avg episode reward: [(0, '865.929')]
[37m[1m[2025-06-29 22:08:12,627][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000022928_5869568.pth...
[36m[2025-06-29 22:08:12,697][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000022592_5783552.pth
[36m[2025-06-29 22:08:17,592][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5869568. Throughput: 0: 354.7. Samples: 5871104. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 22:08:17,593][187912] Avg episode reward: [(0, '845.091')]
[33m[16452023 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[16452023 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.71875
[33mCrash Rate: 0.169921875
[33mTimeout Rate: 0.111328125 (navigation_task.py:265)
[33m[16452023 ms][navigation_task] - WARNING : 
[33mSuccesses: 1472
[33mCrashes : 348
[33mTimeouts: 228 (navigation_task.py:268)
[36m[2025-06-29 22:08:22,558][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5869568. Throughput: 0: 384.5. Samples: 5873424. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 22:08:22,558][187912] Avg episode reward: [(0, '792.517')]
[36m[2025-06-29 22:08:27,592][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5873664. Throughput: 0: 358.9. Samples: 5874320. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:08:27,592][187912] Avg episode reward: [(0, '839.789')]
[36m[2025-06-29 22:08:32,583][187912] Fps is (10 sec: 408.6, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5873664. Throughput: 0: 361.6. Samples: 5876752. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:08:32,583][187912] Avg episode reward: [(0, '857.761')]
[36m[2025-06-29 22:08:37,590][187912] Fps is (10 sec: 409.7, 60 sec: 342.2, 300 sec: 361.0). Total num frames: 5877760. Throughput: 0: 360.6. Samples: 5878752. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:08:37,590][187912] Avg episode reward: [(0, '825.792')]
[36m[2025-06-29 22:08:42,560][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 5877760. Throughput: 0: 360.9. Samples: 5879968. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:08:42,561][187912] Avg episode reward: [(0, '970.102')]
[36m[2025-06-29 22:08:47,572][187912] Fps is (10 sec: 410.3, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 5881856. Throughput: 0: 364.4. Samples: 5882096. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 22:08:47,573][187912] Avg episode reward: [(0, '940.084')]
[36m[2025-06-29 22:08:52,578][187912] Fps is (10 sec: 408.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5881856. Throughput: 0: 364.5. Samples: 5884416. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 22:08:52,578][187912] Avg episode reward: [(0, '896.426')]
[31m[16487524 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16487524 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[16487524 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:08:57,564][187912] Fps is (10 sec: 410.0, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 5885952. Throughput: 0: 363.9. Samples: 5885472. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 22:08:57,564][187912] Avg episode reward: [(0, '899.909')]
[36m[2025-06-29 22:09:02,583][187912] Fps is (10 sec: 409.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5885952. Throughput: 0: 363.8. Samples: 5887472. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 22:09:02,583][187912] Avg episode reward: [(0, '866.470')]
[36m[2025-06-29 22:09:07,590][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5885952. Throughput: 0: 359.6. Samples: 5889616. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 22:09:07,590][187912] Avg episode reward: [(0, '810.733')]
[36m[2025-06-29 22:09:12,571][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5890048. Throughput: 0: 359.6. Samples: 5890496. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 22:09:12,571][187912] Avg episode reward: [(0, '867.003')]
[36m[2025-06-29 22:09:17,576][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 347.3). Total num frames: 5890048. Throughput: 0: 357.7. Samples: 5892848. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 22:09:17,576][187912] Avg episode reward: [(0, '861.734')]
[36m[2025-06-29 22:09:22,571][187912] Fps is (10 sec: 409.6, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 5894144. Throughput: 0: 355.4. Samples: 5894736. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 22:09:22,571][187912] Avg episode reward: [(0, '884.771')]
[36m[2025-06-29 22:09:27,579][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5894144. Throughput: 0: 354.0. Samples: 5895904. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 22:09:27,579][187912] Avg episode reward: [(0, '877.475')]
[36m[2025-06-29 22:09:32,591][187912] Fps is (10 sec: 408.8, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 5898240. Throughput: 0: 355.0. Samples: 5898080. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:09:32,592][187912] Avg episode reward: [(0, '854.136')]
[36m[2025-06-29 22:09:37,564][187912] Fps is (10 sec: 410.2, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 5898240. Throughput: 0: 348.9. Samples: 5900112. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:09:37,564][187912] Avg episode reward: [(0, '843.112')]
[36m[2025-06-29 22:09:42,569][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5898240. Throughput: 0: 350.9. Samples: 5901264. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:09:42,570][187912] Avg episode reward: [(0, '858.747')]
[36m[2025-06-29 22:09:47,560][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5902336. Throughput: 0: 352.5. Samples: 5903328. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:09:47,560][187912] Avg episode reward: [(0, '854.784')]
[36m[2025-06-29 22:09:52,583][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 347.2). Total num frames: 5902336. Throughput: 0: 358.5. Samples: 5905744. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:09:52,583][187912] Avg episode reward: [(0, '868.504')]
[36m[2025-06-29 22:09:57,578][187912] Fps is (10 sec: 408.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5906432. Throughput: 0: 359.8. Samples: 5906688. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:09:57,578][187912] Avg episode reward: [(0, '928.899')]
[36m[2025-06-29 22:10:02,577][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5906432. Throughput: 0: 356.3. Samples: 5908880. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:10:02,577][187912] Avg episode reward: [(0, '1003.387')]
[36m[2025-06-29 22:10:07,588][187912] Fps is (10 sec: 409.2, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 5910528. Throughput: 0: 360.0. Samples: 5910944. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:10:07,588][187912] Avg episode reward: [(0, '990.675')]
[36m[2025-06-29 22:10:12,592][187912] Fps is (10 sec: 409.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5910528. Throughput: 0: 360.1. Samples: 5912112. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:10:12,592][187912] Avg episode reward: [(0, '967.485')]
[37m[1m[2025-06-29 22:10:12,636][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000023088_5910528.pth...
[36m[2025-06-29 22:10:12,707][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000022752_5824512.pth
[36m[2025-06-29 22:10:17,579][187912] Fps is (10 sec: 410.0, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 5914624. Throughput: 0: 364.9. Samples: 5914496. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:10:17,579][187912] Avg episode reward: [(0, '958.710')]
[36m[2025-06-29 22:10:22,570][187912] Fps is (10 sec: 410.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5914624. Throughput: 0: 367.9. Samples: 5916672. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:10:22,570][187912] Avg episode reward: [(0, '903.136')]
[36m[2025-06-29 22:10:27,590][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 5914624. Throughput: 0: 367.1. Samples: 5917792. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:10:27,590][187912] Avg episode reward: [(0, '893.696')]
[36m[2025-06-29 22:10:32,595][187912] Fps is (10 sec: 408.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5918720. Throughput: 0: 366.6. Samples: 5919840. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:10:32,595][187912] Avg episode reward: [(0, '952.896')]
[31m[16586461 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16586461 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[16586461 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:10:37,584][187912] Fps is (10 sec: 409.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5918720. Throughput: 0: 364.1. Samples: 5922128. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:10:37,584][187912] Avg episode reward: [(0, '954.823')]
[36m[2025-06-29 22:10:42,561][187912] Fps is (10 sec: 411.0, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 5922816. Throughput: 0: 362.8. Samples: 5923008. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-06-29 22:10:42,562][187912] Avg episode reward: [(0, '952.987')]
[31m[16596971 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16596971 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[16596972 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:10:47,588][187912] Fps is (10 sec: 409.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5922816. Throughput: 0: 368.3. Samples: 5925456. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-06-29 22:10:47,589][187912] Avg episode reward: [(0, '960.257')]
[36m[2025-06-29 22:10:52,560][187912] Fps is (10 sec: 409.6, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 5926912. Throughput: 0: 370.4. Samples: 5927600. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-06-29 22:10:52,561][187912] Avg episode reward: [(0, '1000.232')]
[36m[2025-06-29 22:10:57,569][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5926912. Throughput: 0: 369.6. Samples: 5928736. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-06-29 22:10:57,569][187912] Avg episode reward: [(0, '906.821')]
[36m[2025-06-29 22:11:02,577][187912] Fps is (10 sec: 408.9, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 5931008. Throughput: 0: 362.0. Samples: 5930784. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:11:02,577][187912] Avg episode reward: [(0, '971.293')]
[36m[2025-06-29 22:11:07,564][187912] Fps is (10 sec: 409.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 5931008. Throughput: 0: 357.7. Samples: 5932768. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:11:07,564][187912] Avg episode reward: [(0, '956.095')]
[36m[2025-06-29 22:11:12,559][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 5931008. Throughput: 0: 358.3. Samples: 5933904. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:11:12,559][187912] Avg episode reward: [(0, '1042.243')]
[36m[2025-06-29 22:11:17,572][187912] Fps is (10 sec: 409.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5935104. Throughput: 0: 355.7. Samples: 5935840. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:11:17,572][187912] Avg episode reward: [(0, '1001.585')]
[36m[2025-06-29 22:11:22,588][187912] Fps is (10 sec: 408.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5935104. Throughput: 0: 356.6. Samples: 5938176. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:11:22,588][187912] Avg episode reward: [(0, '1069.361')]
[36m[2025-06-29 22:11:27,561][187912] Fps is (10 sec: 410.0, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 5939200. Throughput: 0: 360.5. Samples: 5939232. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 22:11:27,562][187912] Avg episode reward: [(0, '1022.870')]
[31m[16643762 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16643763 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[16643763 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:11:32,596][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5939200. Throughput: 0: 353.4. Samples: 5941360. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 22:11:32,596][187912] Avg episode reward: [(0, '996.553')]
[36m[2025-06-29 22:11:37,578][187912] Fps is (10 sec: 408.9, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 5943296. Throughput: 0: 352.2. Samples: 5943456. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 22:11:37,578][187912] Avg episode reward: [(0, '943.899')]
[36m[2025-06-29 22:11:42,586][187912] Fps is (10 sec: 410.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5943296. Throughput: 0: 355.1. Samples: 5944720. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 22:11:42,586][187912] Avg episode reward: [(0, '961.207')]
[31m[16658004 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16658005 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[16658005 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:11:47,807][187912] Fps is (10 sec: 400.4, 60 sec: 408.1, 300 sec: 360.7). Total num frames: 5947392. Throughput: 0: 359.4. Samples: 5947040. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:11:47,807][187912] Avg episode reward: [(0, '864.362')]
[36m[2025-06-29 22:11:52,564][187912] Fps is (10 sec: 410.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5947392. Throughput: 0: 364.4. Samples: 5949168. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:11:52,564][187912] Avg episode reward: [(0, '981.893')]
[36m[2025-06-29 22:11:57,564][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.4). Total num frames: 5947392. Throughput: 0: 364.8. Samples: 5950320. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:11:57,564][187912] Avg episode reward: [(0, '972.287')]
[36m[2025-06-29 22:12:02,593][187912] Fps is (10 sec: 408.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5951488. Throughput: 0: 365.7. Samples: 5952304. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:12:02,594][187912] Avg episode reward: [(0, '980.335')]
[36m[2025-06-29 22:12:07,559][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5951488. Throughput: 0: 361.5. Samples: 5954432. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:12:07,559][187912] Avg episode reward: [(0, '891.189')]
[36m[2025-06-29 22:12:12,562][187912] Fps is (10 sec: 410.9, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 5955584. Throughput: 0: 359.5. Samples: 5955408. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 22:12:12,562][187912] Avg episode reward: [(0, '894.657')]
[37m[1m[2025-06-29 22:12:12,601][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000023264_5955584.pth...
[36m[2025-06-29 22:12:12,672][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000022928_5869568.pth
[36m[2025-06-29 22:12:17,560][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5955584. Throughput: 0: 355.8. Samples: 5957360. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 22:12:17,560][187912] Avg episode reward: [(0, '877.146')]
[31m[16691416 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16691416 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[16691417 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:12:22,933][187912] Fps is (10 sec: 394.9, 60 sec: 407.3, 300 sec: 360.6). Total num frames: 5959680. Throughput: 0: 357.4. Samples: 5959664. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 22:12:22,933][187912] Avg episode reward: [(0, '873.444')]
[36m[2025-06-29 22:12:27,594][187912] Fps is (10 sec: 408.2, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 5959680. Throughput: 0: 353.0. Samples: 5960608. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 22:12:27,594][187912] Avg episode reward: [(0, '828.248')]
[31m[16705318 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16705318 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[16705318 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:12:32,598][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.3). Total num frames: 5959680. Throughput: 0: 354.0. Samples: 5962896. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 22:12:32,598][187912] Avg episode reward: [(0, '872.632')]
[36m[2025-06-29 22:12:37,587][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5963776. Throughput: 0: 352.9. Samples: 5965056. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 22:12:37,587][187912] Avg episode reward: [(0, '863.392')]
[36m[2025-06-29 22:12:42,570][187912] Fps is (10 sec: 410.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5963776. Throughput: 0: 353.7. Samples: 5966240. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 22:12:42,570][187912] Avg episode reward: [(0, '853.130')]
[36m[2025-06-29 22:12:47,593][187912] Fps is (10 sec: 409.4, 60 sec: 342.6, 300 sec: 361.0). Total num frames: 5967872. Throughput: 0: 354.1. Samples: 5968240. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 22:12:47,593][187912] Avg episode reward: [(0, '809.936')]
[36m[2025-06-29 22:12:52,590][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5967872. Throughput: 0: 355.7. Samples: 5970448. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 22:12:52,590][187912] Avg episode reward: [(0, '878.861')]
[31m[16726145 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16726145 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[16726145 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:12:57,592][187912] Fps is (10 sec: 409.6, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 5971968. Throughput: 0: 359.2. Samples: 5971584. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:12:57,592][187912] Avg episode reward: [(0, '817.144')]
[36m[2025-06-29 22:13:02,559][187912] Fps is (10 sec: 410.9, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 5971968. Throughput: 0: 361.6. Samples: 5973632. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:13:02,559][187912] Avg episode reward: [(0, '886.140')]
[31m[16737800 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16737800 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[16737800 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:13:08,308][187912] Fps is (10 sec: 382.2, 60 sec: 404.6, 300 sec: 360.1). Total num frames: 5976064. Throughput: 0: 358.3. Samples: 5975920. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:13:08,308][187912] Avg episode reward: [(0, '864.649')]
[36m[2025-06-29 22:13:12,572][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5976064. Throughput: 0: 360.0. Samples: 5976800. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:13:12,572][187912] Avg episode reward: [(0, '907.980')]
[36m[2025-06-29 22:13:17,590][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5976064. Throughput: 0: 360.2. Samples: 5979104. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:13:17,591][187912] Avg episode reward: [(0, '853.248')]
[36m[2025-06-29 22:13:22,591][187912] Fps is (10 sec: 408.8, 60 sec: 343.3, 300 sec: 361.0). Total num frames: 5980160. Throughput: 0: 355.9. Samples: 5981072. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:13:22,592][187912] Avg episode reward: [(0, '877.136')]
[36m[2025-06-29 22:13:27,589][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5980160. Throughput: 0: 351.5. Samples: 5982064. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:13:27,589][187912] Avg episode reward: [(0, '852.590')]
[36m[2025-06-29 22:13:32,561][187912] Fps is (10 sec: 410.8, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 5984256. Throughput: 0: 356.9. Samples: 5984288. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:13:32,562][187912] Avg episode reward: [(0, '925.670')]
[36m[2025-06-29 22:13:37,591][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 5984256. Throughput: 0: 354.5. Samples: 5986400. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:13:37,592][187912] Avg episode reward: [(0, '863.996')]
[36m[2025-06-29 22:13:43,009][187912] Fps is (10 sec: 392.1, 60 sec: 406.6, 300 sec: 360.5). Total num frames: 5988352. Throughput: 0: 352.3. Samples: 5987584. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:13:43,009][187912] Avg episode reward: [(0, '917.440')]
[36m[2025-06-29 22:13:47,575][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5988352. Throughput: 0: 355.4. Samples: 5989632. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:13:47,575][187912] Avg episode reward: [(0, '999.121')]
[36m[2025-06-29 22:13:52,574][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 5988352. Throughput: 0: 359.6. Samples: 5991840. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:13:52,574][187912] Avg episode reward: [(0, '1005.225')]
[36m[2025-06-29 22:13:57,573][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5992448. Throughput: 0: 353.4. Samples: 5992704. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:13:57,573][187912] Avg episode reward: [(0, '1027.888')]
[36m[2025-06-29 22:14:02,580][187912] Fps is (10 sec: 409.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 5992448. Throughput: 0: 353.1. Samples: 5994992. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:14:02,580][187912] Avg episode reward: [(0, '1089.822')]
[36m[2025-06-29 22:14:07,566][187912] Fps is (10 sec: 409.9, 60 sec: 345.6, 300 sec: 361.0). Total num frames: 5996544. Throughput: 0: 355.8. Samples: 5997072. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:14:07,566][187912] Avg episode reward: [(0, '1117.311')]
[36m[2025-06-29 22:14:12,564][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 5996544. Throughput: 0: 360.4. Samples: 5998272. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:14:12,564][187912] Avg episode reward: [(0, '1003.657')]
[37m[1m[2025-06-29 22:14:12,605][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000023424_5996544.pth...
[36m[2025-06-29 22:14:12,686][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000023088_5910528.pth
[36m[2025-06-29 22:14:17,590][187912] Fps is (10 sec: 408.6, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6000640. Throughput: 0: 361.7. Samples: 6000576. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:14:17,591][187912] Avg episode reward: [(0, '1065.972')]
[36m[2025-06-29 22:14:22,558][187912] Fps is (10 sec: 409.9, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 6000640. Throughput: 0: 360.1. Samples: 6002592. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:14:22,558][187912] Avg episode reward: [(0, '1005.906')]
[36m[2025-06-29 22:14:28,283][187912] Fps is (10 sec: 383.1, 60 sec: 404.9, 300 sec: 360.2). Total num frames: 6004736. Throughput: 0: 357.6. Samples: 6003776. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 22:14:28,283][187912] Avg episode reward: [(0, '991.534')]
[36m[2025-06-29 22:14:32,593][187912] Fps is (10 sec: 408.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6004736. Throughput: 0: 359.7. Samples: 6005824. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 22:14:32,594][187912] Avg episode reward: [(0, '942.792')]
[36m[2025-06-29 22:14:37,577][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6004736. Throughput: 0: 361.6. Samples: 6008112. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 22:14:37,577][187912] Avg episode reward: [(0, '1005.007')]
[36m[2025-06-29 22:14:42,561][187912] Fps is (10 sec: 410.9, 60 sec: 343.9, 300 sec: 361.0). Total num frames: 6008832. Throughput: 0: 362.4. Samples: 6009008. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 22:14:42,561][187912] Avg episode reward: [(0, '966.810')]
[36m[2025-06-29 22:14:47,560][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6008832. Throughput: 0: 364.6. Samples: 6011392. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 22:14:47,560][187912] Avg episode reward: [(0, '930.264')]
[36m[2025-06-29 22:14:52,587][187912] Fps is (10 sec: 408.6, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 6012928. Throughput: 0: 366.1. Samples: 6013552. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 22:14:52,587][187912] Avg episode reward: [(0, '964.364')]
[36m[2025-06-29 22:14:57,586][187912] Fps is (10 sec: 408.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6012928. Throughput: 0: 366.8. Samples: 6014784. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 22:14:57,586][187912] Avg episode reward: [(0, '906.029')]
[36m[2025-06-29 22:15:02,590][187912] Fps is (10 sec: 409.4, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 6017024. Throughput: 0: 366.2. Samples: 6017056. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 22:15:02,591][187912] Avg episode reward: [(0, '921.732')]
[36m[2025-06-29 22:15:07,599][187912] Fps is (10 sec: 409.1, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 6017024. Throughput: 0: 368.7. Samples: 6019200. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 22:15:07,599][187912] Avg episode reward: [(0, '867.809')]
[36m[2025-06-29 22:15:12,783][187912] Fps is (10 sec: 401.9, 60 sec: 408.1, 300 sec: 360.8). Total num frames: 6021120. Throughput: 0: 372.9. Samples: 6020368. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:15:12,783][187912] Avg episode reward: [(0, '860.725')]
[36m[2025-06-29 22:15:17,578][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6021120. Throughput: 0: 369.5. Samples: 6022448. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:15:17,578][187912] Avg episode reward: [(0, '899.192')]
[36m[2025-06-29 22:15:22,576][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6021120. Throughput: 0: 371.9. Samples: 6024848. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:15:22,576][187912] Avg episode reward: [(0, '913.718')]
[31m[16876415 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16876415 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[16876415 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:15:27,577][187912] Fps is (10 sec: 409.6, 60 sec: 345.4, 300 sec: 361.0). Total num frames: 6025216. Throughput: 0: 371.8. Samples: 6025744. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:15:27,578][187912] Avg episode reward: [(0, '900.205')]
[36m[2025-06-29 22:15:32,592][187912] Fps is (10 sec: 408.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6025216. Throughput: 0: 370.2. Samples: 6028064. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:15:32,592][187912] Avg episode reward: [(0, '987.166')]
[31m[16890351 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[16890351 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[16890351 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:15:37,591][187912] Fps is (10 sec: 409.0, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 6029312. Throughput: 0: 368.3. Samples: 6030128. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:15:37,592][187912] Avg episode reward: [(0, '1021.432')]
[36m[2025-06-29 22:15:42,589][187912] Fps is (10 sec: 409.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6029312. Throughput: 0: 366.6. Samples: 6031280. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:15:42,589][187912] Avg episode reward: [(0, '1008.344')]
[36m[2025-06-29 22:15:47,575][187912] Fps is (10 sec: 410.3, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 6033408. Throughput: 0: 364.2. Samples: 6033440. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:15:47,575][187912] Avg episode reward: [(0, '1062.145')]
[36m[2025-06-29 22:15:52,576][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6033408. Throughput: 0: 365.7. Samples: 6035648. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:15:52,576][187912] Avg episode reward: [(0, '1062.503')]
[36m[2025-06-29 22:15:57,905][187912] Fps is (10 sec: 396.5, 60 sec: 407.4, 300 sec: 360.6). Total num frames: 6037504. Throughput: 0: 363.1. Samples: 6036752. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 22:15:57,905][187912] Avg episode reward: [(0, '1053.567')]
[36m[2025-06-29 22:16:02,584][187912] Fps is (10 sec: 409.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6037504. Throughput: 0: 364.4. Samples: 6038848. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 22:16:02,584][187912] Avg episode reward: [(0, '1066.601')]
[36m[2025-06-29 22:16:07,602][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6037504. Throughput: 0: 363.5. Samples: 6041216. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 22:16:07,602][187912] Avg episode reward: [(0, '1099.951')]
[36m[2025-06-29 22:16:12,577][187912] Fps is (10 sec: 409.9, 60 sec: 342.5, 300 sec: 361.0). Total num frames: 6041600. Throughput: 0: 362.7. Samples: 6042064. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 22:16:12,577][187912] Avg episode reward: [(0, '1077.814')]
[37m[1m[2025-06-29 22:16:12,627][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000023600_6041600.pth...
[36m[2025-06-29 22:16:12,704][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000023264_5955584.pth
[36m[2025-06-29 22:16:17,567][187912] Fps is (10 sec: 411.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6041600. Throughput: 0: 362.2. Samples: 6044352. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 22:16:17,567][187912] Avg episode reward: [(0, '1009.901')]
[36m[2025-06-29 22:16:22,594][187912] Fps is (10 sec: 408.9, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 6045696. Throughput: 0: 362.6. Samples: 6046448. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:16:22,595][187912] Avg episode reward: [(0, '959.531')]
[36m[2025-06-29 22:16:27,562][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6045696. Throughput: 0: 363.6. Samples: 6047632. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:16:27,563][187912] Avg episode reward: [(0, '912.068')]
[36m[2025-06-29 22:16:32,584][187912] Fps is (10 sec: 410.0, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 6049792. Throughput: 0: 364.0. Samples: 6049824. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:16:32,584][187912] Avg episode reward: [(0, '826.725')]
[36m[2025-06-29 22:16:37,573][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6049792. Throughput: 0: 359.1. Samples: 6051808. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:16:37,573][187912] Avg episode reward: [(0, '826.780')]
[36m[2025-06-29 22:16:42,558][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.4). Total num frames: 6049792. Throughput: 0: 362.3. Samples: 6052928. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:16:42,558][187912] Avg episode reward: [(0, '866.120')]
[36m[2025-06-29 22:16:47,581][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6053888. Throughput: 0: 359.1. Samples: 6055008. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:16:47,581][187912] Avg episode reward: [(0, '909.921')]
[36m[2025-06-29 22:16:52,587][187912] Fps is (10 sec: 408.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6053888. Throughput: 0: 357.5. Samples: 6057296. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:16:52,588][187912] Avg episode reward: [(0, '933.414')]
[36m[2025-06-29 22:16:57,567][187912] Fps is (10 sec: 410.2, 60 sec: 343.3, 300 sec: 361.0). Total num frames: 6057984. Throughput: 0: 356.7. Samples: 6058112. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:16:57,567][187912] Avg episode reward: [(0, '985.171')]
[33m[16975143 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[16975143 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.72998046875
[33mCrash Rate: 0.166015625
[33mTimeout Rate: 0.10400390625 (navigation_task.py:265)
[33m[16975143 ms][navigation_task] - WARNING : 
[33mSuccesses: 1495
[33mCrashes : 340
[33mTimeouts: 213 (navigation_task.py:268)
[36m[2025-06-29 22:17:02,564][187912] Fps is (10 sec: 410.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6057984. Throughput: 0: 357.7. Samples: 6060448. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:17:02,564][187912] Avg episode reward: [(0, '956.691')]
[36m[2025-06-29 22:17:07,566][187912] Fps is (10 sec: 409.6, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 6062080. Throughput: 0: 355.4. Samples: 6062432. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:17:07,567][187912] Avg episode reward: [(0, '953.311')]
[36m[2025-06-29 22:17:12,596][187912] Fps is (10 sec: 408.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6062080. Throughput: 0: 356.4. Samples: 6063680. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:17:12,596][187912] Avg episode reward: [(0, '946.049')]
[36m[2025-06-29 22:17:17,573][187912] Fps is (10 sec: 409.3, 60 sec: 409.6, 300 sec: 361.4). Total num frames: 6066176. Throughput: 0: 361.0. Samples: 6066064. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:17:17,573][187912] Avg episode reward: [(0, '931.378')]
[36m[2025-06-29 22:17:22,581][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6066176. Throughput: 0: 360.8. Samples: 6068048. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:17:22,582][187912] Avg episode reward: [(0, '915.292')]
[36m[2025-06-29 22:17:27,593][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6066176. Throughput: 0: 360.3. Samples: 6069152. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:17:27,593][187912] Avg episode reward: [(0, '879.820')]
[36m[2025-06-29 22:17:32,579][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6070272. Throughput: 0: 359.8. Samples: 6071200. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 22:17:32,580][187912] Avg episode reward: [(0, '873.277')]
[36m[2025-06-29 22:17:37,568][187912] Fps is (10 sec: 410.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6070272. Throughput: 0: 362.1. Samples: 6073584. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 22:17:37,573][187912] Avg episode reward: [(0, '806.733')]
[36m[2025-06-29 22:17:42,596][187912] Fps is (10 sec: 408.9, 60 sec: 409.3, 300 sec: 361.0). Total num frames: 6074368. Throughput: 0: 362.1. Samples: 6074416. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 22:17:42,596][187912] Avg episode reward: [(0, '844.012')]
[36m[2025-06-29 22:17:47,601][187912] Fps is (10 sec: 408.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6074368. Throughput: 0: 362.7. Samples: 6076784. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 22:17:47,602][187912] Avg episode reward: [(0, '869.130')]
[36m[2025-06-29 22:17:52,576][187912] Fps is (10 sec: 410.4, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 6078464. Throughput: 0: 362.2. Samples: 6078736. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:17:52,576][187912] Avg episode reward: [(0, '909.344')]
[36m[2025-06-29 22:17:57,582][187912] Fps is (10 sec: 410.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6078464. Throughput: 0: 360.3. Samples: 6079888. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:17:57,582][187912] Avg episode reward: [(0, '933.841')]
[36m[2025-06-29 22:18:03,130][187912] Fps is (10 sec: 388.1, 60 sec: 405.8, 300 sec: 361.2). Total num frames: 6082560. Throughput: 0: 352.3. Samples: 6082112. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:18:03,131][187912] Avg episode reward: [(0, '1018.954')]
[36m[2025-06-29 22:18:07,601][187912] Fps is (10 sec: 408.8, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 6082560. Throughput: 0: 357.2. Samples: 6084128. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:18:07,602][187912] Avg episode reward: [(0, '959.101')]
[36m[2025-06-29 22:18:12,585][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6082560. Throughput: 0: 356.7. Samples: 6085200. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:18:12,585][187912] Avg episode reward: [(0, '1030.281')]
[37m[1m[2025-06-29 22:18:12,646][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000023760_6082560.pth...
[36m[2025-06-29 22:18:12,715][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000023424_5996544.pth
[36m[2025-06-29 22:18:17,585][187912] Fps is (10 sec: 410.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6086656. Throughput: 0: 352.0. Samples: 6087040. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 22:18:17,585][187912] Avg episode reward: [(0, '1066.666')]
[36m[2025-06-29 22:18:22,605][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6086656. Throughput: 0: 352.1. Samples: 6089440. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 22:18:22,605][187912] Avg episode reward: [(0, '987.373')]
[36m[2025-06-29 22:18:27,574][187912] Fps is (10 sec: 410.0, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 6090752. Throughput: 0: 357.5. Samples: 6090496. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 22:18:27,574][187912] Avg episode reward: [(0, '951.620')]
[36m[2025-06-29 22:18:32,574][187912] Fps is (10 sec: 410.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6090752. Throughput: 0: 349.4. Samples: 6092496. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 22:18:32,574][187912] Avg episode reward: [(0, '914.938')]
[36m[2025-06-29 22:18:37,725][187912] Fps is (10 sec: 403.5, 60 sec: 408.5, 300 sec: 361.4). Total num frames: 6094848. Throughput: 0: 331.3. Samples: 6093696. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:18:37,726][187912] Avg episode reward: [(0, '946.534')]
[36m[2025-06-29 22:18:42,588][187912] Fps is (10 sec: 409.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6094848. Throughput: 0: 354.4. Samples: 6095840. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:18:42,588][187912] Avg episode reward: [(0, '956.513')]
[36m[2025-06-29 22:18:47,565][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 6094848. Throughput: 0: 360.1. Samples: 6098112. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:18:47,565][187912] Avg episode reward: [(0, '898.617')]
[36m[2025-06-29 22:18:52,561][187912] Fps is (10 sec: 410.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6098944. Throughput: 0: 355.5. Samples: 6100112. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:18:52,561][187912] Avg episode reward: [(0, '1001.642')]
[31m[17086535 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[17086535 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[17086535 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:18:57,581][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6098944. Throughput: 0: 358.4. Samples: 6101328. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:18:57,581][187912] Avg episode reward: [(0, '1048.300')]
[36m[2025-06-29 22:19:02,574][187912] Fps is (10 sec: 409.1, 60 sec: 344.5, 300 sec: 361.0). Total num frames: 6103040. Throughput: 0: 358.5. Samples: 6103168. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:19:02,575][187912] Avg episode reward: [(0, '1088.489')]
[36m[2025-06-29 22:19:07,569][187912] Fps is (10 sec: 410.1, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 6103040. Throughput: 0: 357.6. Samples: 6105520. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:19:07,569][187912] Avg episode reward: [(0, '963.481')]
[36m[2025-06-29 22:19:12,577][187912] Fps is (10 sec: 409.5, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6107136. Throughput: 0: 360.9. Samples: 6106736. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:19:12,578][187912] Avg episode reward: [(0, '995.001')]
[36m[2025-06-29 22:19:17,592][187912] Fps is (10 sec: 408.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6107136. Throughput: 0: 362.5. Samples: 6108816. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:19:17,592][187912] Avg episode reward: [(0, '977.600')]
[36m[2025-06-29 22:19:23,329][187912] Fps is (10 sec: 381.0, 60 sec: 404.7, 300 sec: 360.9). Total num frames: 6111232. Throughput: 0: 380.7. Samples: 6111056. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 22:19:23,329][187912] Avg episode reward: [(0, '995.109')]
[36m[2025-06-29 22:19:27,587][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6111232. Throughput: 0: 358.1. Samples: 6111952. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 22:19:27,587][187912] Avg episode reward: [(0, '913.454')]
[36m[2025-06-29 22:19:32,582][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6111232. Throughput: 0: 358.3. Samples: 6114240. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 22:19:32,582][187912] Avg episode reward: [(0, '945.308')]
[36m[2025-06-29 22:19:37,575][187912] Fps is (10 sec: 410.1, 60 sec: 342.2, 300 sec: 361.0). Total num frames: 6115328. Throughput: 0: 359.4. Samples: 6116288. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 22:19:37,575][187912] Avg episode reward: [(0, '947.958')]
[36m[2025-06-29 22:19:42,596][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6115328. Throughput: 0: 358.6. Samples: 6117472. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 22:19:42,596][187912] Avg episode reward: [(0, '975.913')]
[36m[2025-06-29 22:19:47,562][187912] Fps is (10 sec: 410.1, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6119424. Throughput: 0: 364.5. Samples: 6119568. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 22:19:47,562][187912] Avg episode reward: [(0, '999.603')]
[36m[2025-06-29 22:19:52,562][187912] Fps is (10 sec: 411.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6119424. Throughput: 0: 364.1. Samples: 6121904. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 22:19:52,562][187912] Avg episode reward: [(0, '1046.019')]
[36m[2025-06-29 22:19:57,573][187912] Fps is (10 sec: 409.1, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 6123520. Throughput: 0: 360.6. Samples: 6122960. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 22:19:57,573][187912] Avg episode reward: [(0, '1090.174')]
[36m[2025-06-29 22:20:02,560][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.1). Total num frames: 6123520. Throughput: 0: 361.1. Samples: 6125056. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 22:20:02,561][187912] Avg episode reward: [(0, '994.787')]
[36m[2025-06-29 22:20:07,600][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.3). Total num frames: 6123520. Throughput: 0: 368.6. Samples: 6127376. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 22:20:07,600][187912] Avg episode reward: [(0, '919.161')]
[36m[2025-06-29 22:20:12,559][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6127616. Throughput: 0: 361.8. Samples: 6128224. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:20:12,559][187912] Avg episode reward: [(0, '868.444')]
[37m[1m[2025-06-29 22:20:12,605][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000023936_6127616.pth...
[36m[2025-06-29 22:20:12,678][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000023600_6041600.pth
[36m[2025-06-29 22:20:17,583][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6127616. Throughput: 0: 360.5. Samples: 6130464. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:20:17,583][187912] Avg episode reward: [(0, '925.339')]
[36m[2025-06-29 22:20:22,590][187912] Fps is (10 sec: 408.3, 60 sec: 345.6, 300 sec: 361.0). Total num frames: 6131712. Throughput: 0: 361.1. Samples: 6132544. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:20:22,590][187912] Avg episode reward: [(0, '822.657')]
[36m[2025-06-29 22:20:27,579][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6131712. Throughput: 0: 361.4. Samples: 6133728. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:20:27,579][187912] Avg episode reward: [(0, '940.951')]
[36m[2025-06-29 22:20:32,602][187912] Fps is (10 sec: 409.1, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 6135808. Throughput: 0: 361.3. Samples: 6135840. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:20:32,602][187912] Avg episode reward: [(0, '885.498')]
[36m[2025-06-29 22:20:37,575][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6135808. Throughput: 0: 355.1. Samples: 6137888. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:20:37,575][187912] Avg episode reward: [(0, '865.537')]
[31m[17193157 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[17193157 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[17193157 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:20:42,560][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 6135808. Throughput: 0: 356.0. Samples: 6138976. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:20:42,560][187912] Avg episode reward: [(0, '787.106')]
[36m[2025-06-29 22:20:47,582][187912] Fps is (10 sec: 409.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6139904. Throughput: 0: 352.2. Samples: 6140912. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:20:47,583][187912] Avg episode reward: [(0, '826.016')]
[36m[2025-06-29 22:20:52,576][187912] Fps is (10 sec: 408.9, 60 sec: 341.3, 300 sec: 347.5). Total num frames: 6139904. Throughput: 0: 354.7. Samples: 6143328. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:20:52,577][187912] Avg episode reward: [(0, '894.101')]
[36m[2025-06-29 22:20:57,592][187912] Fps is (10 sec: 409.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6144000. Throughput: 0: 356.4. Samples: 6144272. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 22:20:57,592][187912] Avg episode reward: [(0, '880.620')]
[36m[2025-06-29 22:21:02,570][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6144000. Throughput: 0: 358.5. Samples: 6146592. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 22:21:02,571][187912] Avg episode reward: [(0, '975.995')]
[36m[2025-06-29 22:21:07,580][187912] Fps is (10 sec: 410.1, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 6148096. Throughput: 0: 357.1. Samples: 6148608. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 22:21:07,580][187912] Avg episode reward: [(0, '1025.896')]
[36m[2025-06-29 22:21:12,570][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6148096. Throughput: 0: 354.9. Samples: 6149696. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 22:21:12,570][187912] Avg episode reward: [(0, '989.022')]
[36m[2025-06-29 22:21:17,584][187912] Fps is (10 sec: 409.4, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6152192. Throughput: 0: 358.2. Samples: 6151952. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:21:17,585][187912] Avg episode reward: [(0, '947.182')]
[36m[2025-06-29 22:21:22,571][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6152192. Throughput: 0: 360.6. Samples: 6154112. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:21:22,571][187912] Avg episode reward: [(0, '965.154')]
[36m[2025-06-29 22:21:27,559][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 6152192. Throughput: 0: 362.0. Samples: 6155264. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:21:27,559][187912] Avg episode reward: [(0, '916.903')]
[36m[2025-06-29 22:21:32,568][187912] Fps is (10 sec: 409.7, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 6156288. Throughput: 0: 363.8. Samples: 6157280. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:21:32,569][187912] Avg episode reward: [(0, '890.430')]
[36m[2025-06-29 22:21:37,580][187912] Fps is (10 sec: 408.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6156288. Throughput: 0: 360.9. Samples: 6159568. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:21:37,580][187912] Avg episode reward: [(0, '894.617')]
[36m[2025-06-29 22:21:42,575][187912] Fps is (10 sec: 409.3, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 6160384. Throughput: 0: 358.9. Samples: 6160416. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 22:21:42,575][187912] Avg episode reward: [(0, '876.474')]
[36m[2025-06-29 22:21:47,586][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6160384. Throughput: 0: 360.1. Samples: 6162800. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 22:21:47,586][187912] Avg episode reward: [(0, '873.084')]
[36m[2025-06-29 22:21:52,579][187912] Fps is (10 sec: 409.4, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6164480. Throughput: 0: 357.0. Samples: 6164672. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 22:21:52,579][187912] Avg episode reward: [(0, '885.423')]
[36m[2025-06-29 22:21:57,572][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6164480. Throughput: 0: 358.4. Samples: 6165824. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 22:21:57,572][187912] Avg episode reward: [(0, '954.167')]
[36m[2025-06-29 22:22:03,113][187912] Fps is (10 sec: 388.8, 60 sec: 405.9, 300 sec: 360.3). Total num frames: 6168576. Throughput: 0: 355.6. Samples: 6168144. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 22:22:03,114][187912] Avg episode reward: [(0, '1018.148')]
[36m[2025-06-29 22:22:07,580][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6168576. Throughput: 0: 357.3. Samples: 6170192. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 22:22:07,580][187912] Avg episode reward: [(0, '950.405')]
[36m[2025-06-29 22:22:12,577][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 6168576. Throughput: 0: 357.2. Samples: 6171344. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 22:22:12,578][187912] Avg episode reward: [(0, '930.163')]
[37m[1m[2025-06-29 22:22:12,621][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000024096_6168576.pth...
[36m[2025-06-29 22:22:12,691][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000023760_6082560.pth
[36m[2025-06-29 22:22:17,565][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6172672. Throughput: 0: 355.6. Samples: 6173280. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 22:22:17,565][187912] Avg episode reward: [(0, '964.069')]
[36m[2025-06-29 22:22:22,597][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6172672. Throughput: 0: 357.9. Samples: 6175680. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 22:22:22,598][187912] Avg episode reward: [(0, '923.936')]
[36m[2025-06-29 22:22:27,592][187912] Fps is (10 sec: 408.5, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 6176768. Throughput: 0: 364.0. Samples: 6176800. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:22:27,592][187912] Avg episode reward: [(0, '878.538')]
[36m[2025-06-29 22:22:32,581][187912] Fps is (10 sec: 410.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6176768. Throughput: 0: 358.1. Samples: 6178912. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:22:32,581][187912] Avg episode reward: [(0, '881.605')]
[36m[2025-06-29 22:22:37,590][187912] Fps is (10 sec: 409.7, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 6180864. Throughput: 0: 361.5. Samples: 6180944. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:22:37,590][187912] Avg episode reward: [(0, '871.299')]
[36m[2025-06-29 22:22:42,566][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6180864. Throughput: 0: 361.6. Samples: 6182096. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:22:42,566][187912] Avg episode reward: [(0, '791.047')]
[36m[2025-06-29 22:22:47,588][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 6180864. Throughput: 0: 363.4. Samples: 6184304. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:22:47,588][187912] Avg episode reward: [(0, '833.648')]
[36m[2025-06-29 22:22:52,607][187912] Fps is (10 sec: 407.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6184960. Throughput: 0: 358.5. Samples: 6186336. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:22:52,608][187912] Avg episode reward: [(0, '897.175')]
[36m[2025-06-29 22:22:57,561][187912] Fps is (10 sec: 410.7, 60 sec: 341.4, 300 sec: 347.8). Total num frames: 6184960. Throughput: 0: 356.4. Samples: 6187376. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:22:57,561][187912] Avg episode reward: [(0, '854.208')]
[36m[2025-06-29 22:23:02,565][187912] Fps is (10 sec: 411.4, 60 sec: 344.5, 300 sec: 361.0). Total num frames: 6189056. Throughput: 0: 357.0. Samples: 6189344. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:23:02,565][187912] Avg episode reward: [(0, '893.177')]
[36m[2025-06-29 22:23:07,571][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6189056. Throughput: 0: 352.6. Samples: 6191536. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:23:07,571][187912] Avg episode reward: [(0, '944.458')]
[36m[2025-06-29 22:23:12,566][187912] Fps is (10 sec: 409.6, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 6193152. Throughput: 0: 354.0. Samples: 6192720. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 22:23:12,566][187912] Avg episode reward: [(0, '946.095')]
[36m[2025-06-29 22:23:17,594][187912] Fps is (10 sec: 408.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6193152. Throughput: 0: 350.1. Samples: 6194672. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 22:23:17,594][187912] Avg episode reward: [(0, '921.818')]
[36m[2025-06-29 22:23:23,203][187912] Fps is (10 sec: 385.1, 60 sec: 405.5, 300 sec: 360.2). Total num frames: 6197248. Throughput: 0: 354.6. Samples: 6197120. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 22:23:23,203][187912] Avg episode reward: [(0, '979.761')]
[36m[2025-06-29 22:23:27,595][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6197248. Throughput: 0: 353.6. Samples: 6198016. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 22:23:27,595][187912] Avg episode reward: [(0, '981.212')]
[36m[2025-06-29 22:23:32,578][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.3). Total num frames: 6197248. Throughput: 0: 355.6. Samples: 6200304. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 22:23:32,578][187912] Avg episode reward: [(0, '965.593')]
[36m[2025-06-29 22:23:37,583][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6201344. Throughput: 0: 357.9. Samples: 6202432. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 22:23:37,583][187912] Avg episode reward: [(0, '999.186')]
[36m[2025-06-29 22:23:42,575][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6201344. Throughput: 0: 362.2. Samples: 6203680. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 22:23:42,575][187912] Avg episode reward: [(0, '945.545')]
[36m[2025-06-29 22:23:47,571][187912] Fps is (10 sec: 410.1, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 6205440. Throughput: 0: 365.1. Samples: 6205776. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 22:23:47,572][187912] Avg episode reward: [(0, '899.146')]
[36m[2025-06-29 22:23:52,593][187912] Fps is (10 sec: 408.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6205440. Throughput: 0: 369.2. Samples: 6208160. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 22:23:52,593][187912] Avg episode reward: [(0, '879.809')]
[36m[2025-06-29 22:23:57,558][187912] Fps is (10 sec: 410.2, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6209536. Throughput: 0: 368.1. Samples: 6209280. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:23:57,558][187912] Avg episode reward: [(0, '873.010')]
[36m[2025-06-29 22:24:02,607][187912] Fps is (10 sec: 409.0, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 6209536. Throughput: 0: 370.0. Samples: 6211328. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:24:02,608][187912] Avg episode reward: [(0, '844.936')]
[36m[2025-06-29 22:24:07,959][187912] Fps is (10 sec: 393.8, 60 sec: 407.0, 300 sec: 360.5). Total num frames: 6213632. Throughput: 0: 368.6. Samples: 6213616. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:24:07,960][187912] Avg episode reward: [(0, '886.692')]
[36m[2025-06-29 22:24:12,566][187912] Fps is (10 sec: 411.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6213632. Throughput: 0: 367.2. Samples: 6214528. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:24:12,567][187912] Avg episode reward: [(0, '879.851')]
[37m[1m[2025-06-29 22:24:12,634][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000024272_6213632.pth...
[36m[2025-06-29 22:24:12,717][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000023936_6127616.pth
[36m[2025-06-29 22:24:17,559][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 348.0). Total num frames: 6213632. Throughput: 0: 365.7. Samples: 6216752. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:24:17,559][187912] Avg episode reward: [(0, '889.888')]
[36m[2025-06-29 22:24:22,587][187912] Fps is (10 sec: 408.8, 60 sec: 344.9, 300 sec: 361.0). Total num frames: 6217728. Throughput: 0: 363.7. Samples: 6218800. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 22:24:22,587][187912] Avg episode reward: [(0, '852.588')]
[36m[2025-06-29 22:24:27,587][187912] Fps is (10 sec: 408.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6217728. Throughput: 0: 362.6. Samples: 6220000. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 22:24:27,588][187912] Avg episode reward: [(0, '845.693')]
[36m[2025-06-29 22:24:32,566][187912] Fps is (10 sec: 410.5, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 6221824. Throughput: 0: 360.6. Samples: 6222000. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 22:24:32,566][187912] Avg episode reward: [(0, '781.712')]
[36m[2025-06-29 22:24:37,590][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6221824. Throughput: 0: 359.1. Samples: 6224320. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 22:24:37,590][187912] Avg episode reward: [(0, '731.413')]
[36m[2025-06-29 22:24:42,578][187912] Fps is (10 sec: 409.1, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6225920. Throughput: 0: 361.1. Samples: 6225536. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:24:42,578][187912] Avg episode reward: [(0, '715.059')]
[36m[2025-06-29 22:24:47,573][187912] Fps is (10 sec: 410.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6225920. Throughput: 0: 361.9. Samples: 6227600. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:24:47,573][187912] Avg episode reward: [(0, '779.304')]
[36m[2025-06-29 22:24:53,301][187912] Fps is (10 sec: 382.0, 60 sec: 404.8, 300 sec: 360.1). Total num frames: 6230016. Throughput: 0: 358.5. Samples: 6229872. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:24:53,301][187912] Avg episode reward: [(0, '795.961')]
[36m[2025-06-29 22:24:57,571][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6230016. Throughput: 0: 360.1. Samples: 6230736. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:24:57,571][187912] Avg episode reward: [(0, '797.620')]
[36m[2025-06-29 22:25:02,564][187912] Fps is (10 sec: 0.0, 60 sec: 341.6, 300 sec: 361.0). Total num frames: 6230016. Throughput: 0: 362.3. Samples: 6233056. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:25:02,565][187912] Avg episode reward: [(0, '868.785')]
[36m[2025-06-29 22:25:07,579][187912] Fps is (10 sec: 409.3, 60 sec: 343.5, 300 sec: 361.0). Total num frames: 6234112. Throughput: 0: 360.6. Samples: 6235024. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 22:25:07,580][187912] Avg episode reward: [(0, '883.963')]
[36m[2025-06-29 22:25:12,589][187912] Fps is (10 sec: 408.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6234112. Throughput: 0: 359.1. Samples: 6236160. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 22:25:12,589][187912] Avg episode reward: [(0, '827.511')]
[36m[2025-06-29 22:25:17,577][187912] Fps is (10 sec: 409.7, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 6238208. Throughput: 0: 360.8. Samples: 6238240. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 22:25:17,578][187912] Avg episode reward: [(0, '828.532')]
[36m[2025-06-29 22:25:22,593][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6238208. Throughput: 0: 363.0. Samples: 6240656. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 22:25:22,594][187912] Avg episode reward: [(0, '909.747')]
[31m[17480665 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[17480665 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[17480665 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:25:27,602][187912] Fps is (10 sec: 408.6, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 6242304. Throughput: 0: 362.1. Samples: 6241840. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 22:25:27,603][187912] Avg episode reward: [(0, '906.571')]
[36m[2025-06-29 22:25:32,566][187912] Fps is (10 sec: 410.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6242304. Throughput: 0: 364.1. Samples: 6243984. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 22:25:32,566][187912] Avg episode reward: [(0, '885.907')]
[36m[2025-06-29 22:25:37,584][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6242304. Throughput: 0: 367.5. Samples: 6246144. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 22:25:37,584][187912] Avg episode reward: [(0, '943.360')]
[36m[2025-06-29 22:25:42,595][187912] Fps is (10 sec: 408.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6246400. Throughput: 0: 360.3. Samples: 6246960. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 22:25:42,595][187912] Avg episode reward: [(0, '873.459')]
[31m[17500574 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[17500574 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[17500574 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[33m[17500690 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[17500691 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.74609375
[33mCrash Rate: 0.16552734375
[33mTimeout Rate: 0.08837890625 (navigation_task.py:265)
[33m[17500691 ms][navigation_task] - WARNING : 
[33mSuccesses: 1528
[33mCrashes : 339
[33mTimeouts: 181 (navigation_task.py:268)
[36m[2025-06-29 22:25:47,584][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6246400. Throughput: 0: 356.8. Samples: 6249120. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 22:25:47,584][187912] Avg episode reward: [(0, '758.471')]
[36m[2025-06-29 22:25:52,577][187912] Fps is (10 sec: 410.3, 60 sec: 345.5, 300 sec: 361.0). Total num frames: 6250496. Throughput: 0: 360.6. Samples: 6251248. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:25:52,578][187912] Avg episode reward: [(0, '837.858')]
[36m[2025-06-29 22:25:57,600][187912] Fps is (10 sec: 409.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6250496. Throughput: 0: 362.2. Samples: 6252464. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:25:57,600][187912] Avg episode reward: [(0, '846.887')]
[36m[2025-06-29 22:26:02,566][187912] Fps is (10 sec: 410.1, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6254592. Throughput: 0: 364.2. Samples: 6254624. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:26:02,567][187912] Avg episode reward: [(0, '847.179')]
[36m[2025-06-29 22:26:07,558][187912] Fps is (10 sec: 411.3, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 6254592. Throughput: 0: 359.0. Samples: 6256800. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:26:07,558][187912] Avg episode reward: [(0, '920.722')]
[36m[2025-06-29 22:26:12,854][187912] Fps is (10 sec: 398.2, 60 sec: 407.8, 300 sec: 360.7). Total num frames: 6258688. Throughput: 0: 356.8. Samples: 6257984. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:26:12,854][187912] Avg episode reward: [(0, '974.040')]
[37m[1m[2025-06-29 22:26:12,897][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000024448_6258688.pth...
[36m[2025-06-29 22:26:12,970][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000024096_6168576.pth
[36m[2025-06-29 22:26:17,596][187912] Fps is (10 sec: 408.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6258688. Throughput: 0: 354.6. Samples: 6259952. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:26:17,596][187912] Avg episode reward: [(0, '962.334')]
[36m[2025-06-29 22:26:22,565][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 6258688. Throughput: 0: 358.9. Samples: 6262288. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:26:22,566][187912] Avg episode reward: [(0, '953.217')]
[36m[2025-06-29 22:26:27,588][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6262784. Throughput: 0: 359.9. Samples: 6263152. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:26:27,588][187912] Avg episode reward: [(0, '871.717')]
[36m[2025-06-29 22:26:32,575][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6262784. Throughput: 0: 362.0. Samples: 6265408. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:26:32,575][187912] Avg episode reward: [(0, '810.809')]
[36m[2025-06-29 22:26:37,576][187912] Fps is (10 sec: 410.1, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 6266880. Throughput: 0: 359.8. Samples: 6267440. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:26:37,576][187912] Avg episode reward: [(0, '840.134')]
[36m[2025-06-29 22:26:42,578][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6266880. Throughput: 0: 358.9. Samples: 6268608. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:26:42,578][187912] Avg episode reward: [(0, '873.730')]
[36m[2025-06-29 22:26:47,581][187912] Fps is (10 sec: 409.4, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6270976. Throughput: 0: 358.6. Samples: 6270768. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:26:47,582][187912] Avg episode reward: [(0, '775.448')]
[36m[2025-06-29 22:26:52,559][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6270976. Throughput: 0: 354.5. Samples: 6272752. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:26:52,560][187912] Avg episode reward: [(0, '906.727')]
[36m[2025-06-29 22:26:57,592][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.7). Total num frames: 6270976. Throughput: 0: 357.3. Samples: 6273968. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:26:57,592][187912] Avg episode reward: [(0, '1018.699')]
[36m[2025-06-29 22:27:02,607][187912] Fps is (10 sec: 407.7, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 6275072. Throughput: 0: 358.3. Samples: 6276080. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:27:02,607][187912] Avg episode reward: [(0, '919.213')]
[36m[2025-06-29 22:27:07,568][187912] Fps is (10 sec: 410.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6275072. Throughput: 0: 356.2. Samples: 6278320. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:27:07,568][187912] Avg episode reward: [(0, '890.734')]
[36m[2025-06-29 22:27:12,576][187912] Fps is (10 sec: 410.9, 60 sec: 342.9, 300 sec: 361.0). Total num frames: 6279168. Throughput: 0: 356.7. Samples: 6279200. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:27:12,576][187912] Avg episode reward: [(0, '928.765')]
[36m[2025-06-29 22:27:17,586][187912] Fps is (10 sec: 408.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6279168. Throughput: 0: 355.1. Samples: 6281392. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:27:17,586][187912] Avg episode reward: [(0, '921.779')]
[31m[17595191 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[17595191 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[17595191 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:27:22,589][187912] Fps is (10 sec: 409.1, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 6283264. Throughput: 0: 356.5. Samples: 6283488. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:27:22,589][187912] Avg episode reward: [(0, '822.565')]
[36m[2025-06-29 22:27:27,591][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6283264. Throughput: 0: 357.6. Samples: 6284704. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:27:27,592][187912] Avg episode reward: [(0, '852.466')]
[36m[2025-06-29 22:27:33,157][187912] Fps is (10 sec: 387.6, 60 sec: 405.7, 300 sec: 360.3). Total num frames: 6287360. Throughput: 0: 353.2. Samples: 6286864. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:27:33,158][187912] Avg episode reward: [(0, '853.836')]
[36m[2025-06-29 22:27:37,579][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6287360. Throughput: 0: 359.7. Samples: 6288944. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:27:37,579][187912] Avg episode reward: [(0, '872.204')]
[31m[17614854 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[17614855 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[17614855 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:27:42,572][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6287360. Throughput: 0: 356.8. Samples: 6290016. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:27:42,572][187912] Avg episode reward: [(0, '871.429')]
[36m[2025-06-29 22:27:47,594][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6291456. Throughput: 0: 353.9. Samples: 6292000. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:27:47,595][187912] Avg episode reward: [(0, '875.652')]
[36m[2025-06-29 22:27:52,589][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6291456. Throughput: 0: 351.5. Samples: 6294144. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:27:52,589][187912] Avg episode reward: [(0, '889.461')]
[36m[2025-06-29 22:27:57,597][187912] Fps is (10 sec: 409.5, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6295552. Throughput: 0: 356.1. Samples: 6295232. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:27:57,597][187912] Avg episode reward: [(0, '944.349')]
[36m[2025-06-29 22:28:02,579][187912] Fps is (10 sec: 410.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 6295552. Throughput: 0: 352.8. Samples: 6297264. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:28:02,579][187912] Avg episode reward: [(0, '963.587')]
[36m[2025-06-29 22:28:07,596][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 6295552. Throughput: 0: 353.7. Samples: 6299408. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:28:07,596][187912] Avg episode reward: [(0, '994.504')]
[36m[2025-06-29 22:28:12,559][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6299648. Throughput: 0: 345.5. Samples: 6300240. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:28:12,559][187912] Avg episode reward: [(0, '928.781')]
[37m[1m[2025-06-29 22:28:12,599][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000024608_6299648.pth...
[36m[2025-06-29 22:28:12,671][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000024272_6213632.pth
[36m[2025-06-29 22:28:17,565][187912] Fps is (10 sec: 410.9, 60 sec: 341.5, 300 sec: 347.9). Total num frames: 6299648. Throughput: 0: 351.3. Samples: 6302464. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:28:17,565][187912] Avg episode reward: [(0, '918.910')]
[36m[2025-06-29 22:28:22,568][187912] Fps is (10 sec: 409.2, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 6303744. Throughput: 0: 343.6. Samples: 6304400. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:28:22,568][187912] Avg episode reward: [(0, '905.561')]
[36m[2025-06-29 22:28:27,583][187912] Fps is (10 sec: 408.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6303744. Throughput: 0: 346.2. Samples: 6305600. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:28:27,584][187912] Avg episode reward: [(0, '834.966')]
[36m[2025-06-29 22:28:32,602][187912] Fps is (10 sec: 408.2, 60 sec: 344.5, 300 sec: 361.0). Total num frames: 6307840. Throughput: 0: 350.5. Samples: 6307776. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:28:32,602][187912] Avg episode reward: [(0, '859.727')]
[36m[2025-06-29 22:28:37,561][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6307840. Throughput: 0: 347.6. Samples: 6309776. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:28:37,562][187912] Avg episode reward: [(0, '847.071')]
[36m[2025-06-29 22:28:42,568][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 6307840. Throughput: 0: 349.4. Samples: 6310944. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:28:42,568][187912] Avg episode reward: [(0, '874.857')]
[36m[2025-06-29 22:28:47,559][187912] Fps is (10 sec: 409.7, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 6311936. Throughput: 0: 350.0. Samples: 6313008. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:28:47,559][187912] Avg episode reward: [(0, '868.219')]
[36m[2025-06-29 22:28:52,592][187912] Fps is (10 sec: 408.6, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 6311936. Throughput: 0: 355.9. Samples: 6315424. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:28:52,592][187912] Avg episode reward: [(0, '916.892')]
[36m[2025-06-29 22:28:57,593][187912] Fps is (10 sec: 408.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6316032. Throughput: 0: 357.4. Samples: 6316336. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:28:57,593][187912] Avg episode reward: [(0, '914.082')]
[36m[2025-06-29 22:29:02,559][187912] Fps is (10 sec: 410.9, 60 sec: 341.4, 300 sec: 347.6). Total num frames: 6316032. Throughput: 0: 359.5. Samples: 6318640. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:29:02,559][187912] Avg episode reward: [(0, '953.466')]
[36m[2025-06-29 22:29:07,573][187912] Fps is (10 sec: 410.4, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 6320128. Throughput: 0: 361.2. Samples: 6320656. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:29:07,573][187912] Avg episode reward: [(0, '980.048')]
[36m[2025-06-29 22:29:12,567][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6320128. Throughput: 0: 360.3. Samples: 6321808. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:29:12,567][187912] Avg episode reward: [(0, '1022.929')]
[36m[2025-06-29 22:29:17,574][187912] Fps is (10 sec: 409.6, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 6324224. Throughput: 0: 362.9. Samples: 6324096. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 22:29:17,574][187912] Avg episode reward: [(0, '993.707')]
[36m[2025-06-29 22:29:22,580][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6324224. Throughput: 0: 362.9. Samples: 6326112. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 22:29:22,580][187912] Avg episode reward: [(0, '974.184')]
[36m[2025-06-29 22:29:27,565][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 6324224. Throughput: 0: 362.3. Samples: 6327248. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 22:29:27,566][187912] Avg episode reward: [(0, '946.697')]
[36m[2025-06-29 22:29:32,563][187912] Fps is (10 sec: 410.3, 60 sec: 341.6, 300 sec: 361.0). Total num frames: 6328320. Throughput: 0: 360.5. Samples: 6329232. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 22:29:32,563][187912] Avg episode reward: [(0, '907.672')]
[36m[2025-06-29 22:29:37,568][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 6328320. Throughput: 0: 357.9. Samples: 6331520. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 22:29:37,568][187912] Avg episode reward: [(0, '971.934')]
[36m[2025-06-29 22:29:42,564][187912] Fps is (10 sec: 409.6, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6332416. Throughput: 0: 358.3. Samples: 6332448. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:29:42,564][187912] Avg episode reward: [(0, '918.787')]
[36m[2025-06-29 22:29:47,577][187912] Fps is (10 sec: 409.2, 60 sec: 341.2, 300 sec: 348.0). Total num frames: 6332416. Throughput: 0: 356.8. Samples: 6334704. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:29:47,577][187912] Avg episode reward: [(0, '942.410')]
[36m[2025-06-29 22:29:52,583][187912] Fps is (10 sec: 408.8, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 6336512. Throughput: 0: 354.8. Samples: 6336624. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:29:52,583][187912] Avg episode reward: [(0, '866.967')]
[36m[2025-06-29 22:29:57,581][187912] Fps is (10 sec: 409.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6336512. Throughput: 0: 355.4. Samples: 6337808. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:29:57,581][187912] Avg episode reward: [(0, '872.788')]
[36m[2025-06-29 22:30:02,973][187912] Fps is (10 sec: 394.2, 60 sec: 406.8, 300 sec: 360.5). Total num frames: 6340608. Throughput: 0: 354.9. Samples: 6340208. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 22:30:02,973][187912] Avg episode reward: [(0, '813.100')]
[31m[17758146 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[17758146 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[17758146 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:30:07,597][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6340608. Throughput: 0: 358.3. Samples: 6342240. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 22:30:07,597][187912] Avg episode reward: [(0, '847.513')]
[36m[2025-06-29 22:30:12,595][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 6340608. Throughput: 0: 356.0. Samples: 6343280. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 22:30:12,595][187912] Avg episode reward: [(0, '870.467')]
[37m[1m[2025-06-29 22:30:12,638][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000024768_6340608.pth...
[36m[2025-06-29 22:30:12,711][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000024448_6258688.pth
[36m[2025-06-29 22:30:17,583][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6344704. Throughput: 0: 353.6. Samples: 6345152. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 22:30:17,583][187912] Avg episode reward: [(0, '870.158')]
[36m[2025-06-29 22:30:22,567][187912] Fps is (10 sec: 410.8, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 6344704. Throughput: 0: 354.1. Samples: 6347456. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 22:30:22,567][187912] Avg episode reward: [(0, '886.446')]
[36m[2025-06-29 22:30:27,579][187912] Fps is (10 sec: 409.8, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 6348800. Throughput: 0: 359.3. Samples: 6348624. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:30:27,579][187912] Avg episode reward: [(0, '881.241')]
[36m[2025-06-29 22:30:32,572][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6348800. Throughput: 0: 354.9. Samples: 6350672. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:30:32,573][187912] Avg episode reward: [(0, '884.322')]
[31m[17789947 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[17789947 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([12], device='cuda:0') (navigation_task.py:196)
[31m[17789947 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:30:38,120][187912] Fps is (10 sec: 388.6, 60 sec: 405.9, 300 sec: 360.4). Total num frames: 6352896. Throughput: 0: 355.9. Samples: 6352832. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:30:38,120][187912] Avg episode reward: [(0, '811.474')]
[36m[2025-06-29 22:30:42,584][187912] Fps is (10 sec: 409.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6352896. Throughput: 0: 353.0. Samples: 6353696. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:30:42,585][187912] Avg episode reward: [(0, '808.266')]
[36m[2025-06-29 22:30:47,579][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 6352896. Throughput: 0: 352.6. Samples: 6355936. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:30:47,579][187912] Avg episode reward: [(0, '768.222')]
[36m[2025-06-29 22:30:52,561][187912] Fps is (10 sec: 410.5, 60 sec: 341.5, 300 sec: 361.1). Total num frames: 6356992. Throughput: 0: 347.7. Samples: 6357872. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:30:52,562][187912] Avg episode reward: [(0, '787.380')]
[36m[2025-06-29 22:30:57,576][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 6356992. Throughput: 0: 352.2. Samples: 6359120. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:30:57,576][187912] Avg episode reward: [(0, '749.146')]
[36m[2025-06-29 22:31:02,580][187912] Fps is (10 sec: 408.8, 60 sec: 343.6, 300 sec: 361.0). Total num frames: 6361088. Throughput: 0: 355.9. Samples: 6361168. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:31:02,581][187912] Avg episode reward: [(0, '842.326')]
[36m[2025-06-29 22:31:07,565][187912] Fps is (10 sec: 410.1, 60 sec: 341.5, 300 sec: 347.5). Total num frames: 6361088. Throughput: 0: 354.2. Samples: 6363392. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:31:07,565][187912] Avg episode reward: [(0, '817.951')]
[31m[17824736 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[17824736 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[17824736 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:31:12,682][187912] Fps is (10 sec: 405.5, 60 sec: 409.0, 300 sec: 360.9). Total num frames: 6365184. Throughput: 0: 353.3. Samples: 6364560. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 22:31:12,682][187912] Avg episode reward: [(0, '860.923')]
[31m[17826527 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[17826528 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[17826528 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:31:17,559][187912] Fps is (10 sec: 409.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 6365184. Throughput: 0: 352.5. Samples: 6366528. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 22:31:17,560][187912] Avg episode reward: [(0, '840.061')]
[36m[2025-06-29 22:31:22,594][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 6365184. Throughput: 0: 358.3. Samples: 6368768. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 22:31:22,595][187912] Avg episode reward: [(0, '868.668')]
[36m[2025-06-29 22:31:27,604][187912] Fps is (10 sec: 407.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6369280. Throughput: 0: 354.7. Samples: 6369664. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 22:31:27,604][187912] Avg episode reward: [(0, '874.154')]
[36m[2025-06-29 22:31:32,588][187912] Fps is (10 sec: 409.9, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 6369280. Throughput: 0: 355.5. Samples: 6371936. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 22:31:32,588][187912] Avg episode reward: [(0, '893.753')]
[36m[2025-06-29 22:31:37,571][187912] Fps is (10 sec: 410.9, 60 sec: 344.5, 300 sec: 361.0). Total num frames: 6373376. Throughput: 0: 355.1. Samples: 6373856. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:31:37,571][187912] Avg episode reward: [(0, '861.688')]
[36m[2025-06-29 22:31:42,582][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 6373376. Throughput: 0: 354.1. Samples: 6375056. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:31:42,583][187912] Avg episode reward: [(0, '909.881')]
[36m[2025-06-29 22:31:47,710][187912] Fps is (10 sec: 404.0, 60 sec: 408.7, 300 sec: 360.8). Total num frames: 6377472. Throughput: 0: 354.5. Samples: 6377168. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:31:47,710][187912] Avg episode reward: [(0, '914.995')]
[36m[2025-06-29 22:31:52,600][187912] Fps is (10 sec: 408.9, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 6377472. Throughput: 0: 350.3. Samples: 6379168. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:31:52,601][187912] Avg episode reward: [(0, '892.648')]
[36m[2025-06-29 22:31:57,582][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 6377472. Throughput: 0: 348.9. Samples: 6380224. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:31:57,583][187912] Avg episode reward: [(0, '871.601')]
[36m[2025-06-29 22:32:02,582][187912] Fps is (10 sec: 410.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6381568. Throughput: 0: 349.7. Samples: 6382272. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:32:02,582][187912] Avg episode reward: [(0, '899.430')]
[36m[2025-06-29 22:32:07,561][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 6381568. Throughput: 0: 350.5. Samples: 6384528. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:32:07,561][187912] Avg episode reward: [(0, '846.340')]
[36m[2025-06-29 22:32:12,589][187912] Fps is (10 sec: 409.3, 60 sec: 341.9, 300 sec: 361.0). Total num frames: 6385664. Throughput: 0: 355.3. Samples: 6385648. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:32:12,590][187912] Avg episode reward: [(0, '858.425')]
[37m[1m[2025-06-29 22:32:12,639][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000024944_6385664.pth...
[36m[2025-06-29 22:32:12,715][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000024608_6299648.pth
[36m[2025-06-29 22:32:17,578][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 6385664. Throughput: 0: 347.5. Samples: 6387568. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:32:17,579][187912] Avg episode reward: [(0, '934.604')]
[36m[2025-06-29 22:32:23,172][187912] Fps is (10 sec: 387.1, 60 sec: 405.7, 300 sec: 360.3). Total num frames: 6389760. Throughput: 0: 347.4. Samples: 6389696. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:32:23,173][187912] Avg episode reward: [(0, '922.580')]
[36m[2025-06-29 22:32:27,591][187912] Fps is (10 sec: 409.1, 60 sec: 341.4, 300 sec: 347.8). Total num frames: 6389760. Throughput: 0: 343.4. Samples: 6390512. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:32:27,591][187912] Avg episode reward: [(0, '952.230')]
[31m[17905379 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[17905379 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[17905379 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:32:32,558][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 6389760. Throughput: 0: 350.3. Samples: 6392880. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:32:32,558][187912] Avg episode reward: [(0, '982.514')]
[36m[2025-06-29 22:32:37,571][187912] Fps is (10 sec: 410.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6393856. Throughput: 0: 349.4. Samples: 6394880. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:32:37,571][187912] Avg episode reward: [(0, '1049.939')]
[36m[2025-06-29 22:32:42,558][187912] Fps is (10 sec: 409.6, 60 sec: 341.5, 300 sec: 347.2). Total num frames: 6393856. Throughput: 0: 350.8. Samples: 6396000. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:32:42,558][187912] Avg episode reward: [(0, '1034.019')]
[36m[2025-06-29 22:32:47,571][187912] Fps is (10 sec: 409.6, 60 sec: 342.1, 300 sec: 361.0). Total num frames: 6397952. Throughput: 0: 349.2. Samples: 6397984. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:32:47,571][187912] Avg episode reward: [(0, '1005.045')]
[36m[2025-06-29 22:32:52,568][187912] Fps is (10 sec: 409.2, 60 sec: 341.5, 300 sec: 347.2). Total num frames: 6397952. Throughput: 0: 351.9. Samples: 6400368. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:32:52,568][187912] Avg episode reward: [(0, '1026.304')]
[36m[2025-06-29 22:32:57,589][187912] Fps is (10 sec: 408.9, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6402048. Throughput: 0: 352.7. Samples: 6401520. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:32:57,589][187912] Avg episode reward: [(0, '1010.661')]
[36m[2025-06-29 22:33:02,581][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6402048. Throughput: 0: 354.5. Samples: 6403520. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:33:02,581][187912] Avg episode reward: [(0, '1002.078')]
[36m[2025-06-29 22:33:07,576][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 6402048. Throughput: 0: 362.8. Samples: 6405808. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:33:07,577][187912] Avg episode reward: [(0, '985.680')]
[36m[2025-06-29 22:33:12,558][187912] Fps is (10 sec: 410.5, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 6406144. Throughput: 0: 360.1. Samples: 6406704. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:33:12,559][187912] Avg episode reward: [(0, '1034.801')]
[36m[2025-06-29 22:33:17,589][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 6406144. Throughput: 0: 359.9. Samples: 6409088. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:33:17,589][187912] Avg episode reward: [(0, '1051.432')]
[36m[2025-06-29 22:33:22,576][187912] Fps is (10 sec: 408.9, 60 sec: 344.8, 300 sec: 361.0). Total num frames: 6410240. Throughput: 0: 363.0. Samples: 6411216. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:33:22,576][187912] Avg episode reward: [(0, '1044.743')]
[36m[2025-06-29 22:33:27,603][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 6410240. Throughput: 0: 362.7. Samples: 6412336. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:33:27,603][187912] Avg episode reward: [(0, '1012.981')]
[36m[2025-06-29 22:33:32,575][187912] Fps is (10 sec: 409.6, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 6414336. Throughput: 0: 364.1. Samples: 6414368. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 22:33:32,575][187912] Avg episode reward: [(0, '968.266')]
[36m[2025-06-29 22:33:37,618][187912] Fps is (10 sec: 409.0, 60 sec: 341.1, 300 sec: 360.9). Total num frames: 6414336. Throughput: 0: 357.3. Samples: 6416464. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 22:33:37,619][187912] Avg episode reward: [(0, '1011.598')]
[31m[17975368 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[17975368 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[17975368 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:33:42,583][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 6414336. Throughput: 0: 354.9. Samples: 6417488. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 22:33:42,584][187912] Avg episode reward: [(0, '966.607')]
[36m[2025-06-29 22:33:47,590][187912] Fps is (10 sec: 410.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6418432. Throughput: 0: 354.8. Samples: 6419488. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 22:33:47,591][187912] Avg episode reward: [(0, '945.165')]
[36m[2025-06-29 22:33:52,571][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 6418432. Throughput: 0: 357.0. Samples: 6421872. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 22:33:52,571][187912] Avg episode reward: [(0, '857.430')]
[36m[2025-06-29 22:33:57,571][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6422528. Throughput: 0: 354.7. Samples: 6422672. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 22:33:57,571][187912] Avg episode reward: [(0, '848.703')]
[36m[2025-06-29 22:34:02,566][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 6422528. Throughput: 0: 351.8. Samples: 6424912. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 22:34:02,566][187912] Avg episode reward: [(0, '767.082')]
[36m[2025-06-29 22:34:07,570][187912] Fps is (10 sec: 409.6, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6426624. Throughput: 0: 351.3. Samples: 6427024. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 22:34:07,570][187912] Avg episode reward: [(0, '758.780')]
[36m[2025-06-29 22:34:12,558][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 6426624. Throughput: 0: 354.1. Samples: 6428256. Policy #0 lag: (min: 3.0, avg: 3.0, max: 19.0)
[36m[2025-06-29 22:34:12,558][187912] Avg episode reward: [(0, '784.491')]
[37m[1m[2025-06-29 22:34:12,599][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000025104_6426624.pth...
[36m[2025-06-29 22:34:12,671][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000024768_6340608.pth
[36m[2025-06-29 22:34:17,764][187912] Fps is (10 sec: 401.8, 60 sec: 408.4, 300 sec: 360.8). Total num frames: 6430720. Throughput: 0: 354.1. Samples: 6430368. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:34:17,765][187912] Avg episode reward: [(0, '758.522')]
[36m[2025-06-29 22:34:22,586][187912] Fps is (10 sec: 408.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6430720. Throughput: 0: 356.9. Samples: 6432512. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:34:22,586][187912] Avg episode reward: [(0, '829.552')]
[36m[2025-06-29 22:34:27,586][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 6430720. Throughput: 0: 361.6. Samples: 6433760. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:34:27,586][187912] Avg episode reward: [(0, '856.763')]
[36m[2025-06-29 22:34:32,565][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6434816. Throughput: 0: 363.9. Samples: 6435856. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:34:32,565][187912] Avg episode reward: [(0, '809.445')]
[36m[2025-06-29 22:34:37,568][187912] Fps is (10 sec: 410.3, 60 sec: 341.6, 300 sec: 347.1). Total num frames: 6434816. Throughput: 0: 362.7. Samples: 6438192. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:34:37,569][187912] Avg episode reward: [(0, '899.944')]
[33m[18031648 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[18031648 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.74169921875
[33mCrash Rate: 0.17138671875
[33mTimeout Rate: 0.0869140625 (navigation_task.py:265)
[33m[18031648 ms][navigation_task] - WARNING : 
[33mSuccesses: 1519
[33mCrashes : 351
[33mTimeouts: 178 (navigation_task.py:268)
[36m[2025-06-29 22:34:42,597][187912] Fps is (10 sec: 408.3, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 6438912. Throughput: 0: 362.8. Samples: 6439008. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:34:42,598][187912] Avg episode reward: [(0, '962.880')]
[36m[2025-06-29 22:34:47,584][187912] Fps is (10 sec: 409.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 6438912. Throughput: 0: 366.1. Samples: 6441392. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:34:47,585][187912] Avg episode reward: [(0, '941.527')]
[36m[2025-06-29 22:34:52,562][187912] Fps is (10 sec: 411.0, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 6443008. Throughput: 0: 363.4. Samples: 6443376. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:34:52,563][187912] Avg episode reward: [(0, '933.854')]
[36m[2025-06-29 22:34:57,570][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 347.6). Total num frames: 6443008. Throughput: 0: 362.6. Samples: 6444576. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:34:57,570][187912] Avg episode reward: [(0, '965.275')]
[36m[2025-06-29 22:35:02,563][187912] Fps is (10 sec: 409.6, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6447104. Throughput: 0: 367.9. Samples: 6446848. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:35:02,563][187912] Avg episode reward: [(0, '987.564')]
[36m[2025-06-29 22:35:07,579][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6447104. Throughput: 0: 367.7. Samples: 6449056. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:35:07,579][187912] Avg episode reward: [(0, '977.514')]
[36m[2025-06-29 22:35:12,578][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 6447104. Throughput: 0: 365.9. Samples: 6450224. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:35:12,578][187912] Avg episode reward: [(0, '1001.421')]
[36m[2025-06-29 22:35:17,588][187912] Fps is (10 sec: 409.2, 60 sec: 342.3, 300 sec: 361.0). Total num frames: 6451200. Throughput: 0: 365.3. Samples: 6452304. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:35:17,588][187912] Avg episode reward: [(0, '1024.467')]
[36m[2025-06-29 22:35:22,562][187912] Fps is (10 sec: 410.3, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 6451200. Throughput: 0: 363.8. Samples: 6454560. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:35:22,562][187912] Avg episode reward: [(0, '1013.136')]
[36m[2025-06-29 22:35:27,561][187912] Fps is (10 sec: 410.7, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 6455296. Throughput: 0: 366.5. Samples: 6455488. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:35:27,561][187912] Avg episode reward: [(0, '951.749')]
[36m[2025-06-29 22:35:32,565][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 347.8). Total num frames: 6455296. Throughput: 0: 366.0. Samples: 6457856. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:35:32,565][187912] Avg episode reward: [(0, '879.898')]
[36m[2025-06-29 22:35:37,565][187912] Fps is (10 sec: 409.4, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6459392. Throughput: 0: 367.6. Samples: 6459920. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:35:37,566][187912] Avg episode reward: [(0, '865.033')]
[36m[2025-06-29 22:35:42,588][187912] Fps is (10 sec: 408.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6459392. Throughput: 0: 367.9. Samples: 6461136. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:35:42,589][187912] Avg episode reward: [(0, '818.251')]
[36m[2025-06-29 22:35:47,621][187912] Fps is (10 sec: 407.3, 60 sec: 409.4, 300 sec: 360.9). Total num frames: 6463488. Throughput: 0: 364.0. Samples: 6463248. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 22:35:47,622][187912] Avg episode reward: [(0, '905.996')]
[36m[2025-06-29 22:35:52,593][187912] Fps is (10 sec: 409.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6463488. Throughput: 0: 360.1. Samples: 6465264. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 22:35:52,593][187912] Avg episode reward: [(0, '976.752')]
[36m[2025-06-29 22:35:57,579][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 6463488. Throughput: 0: 360.5. Samples: 6466448. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 22:35:57,579][187912] Avg episode reward: [(0, '981.768')]
[36m[2025-06-29 22:36:02,564][187912] Fps is (10 sec: 410.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6467584. Throughput: 0: 363.2. Samples: 6468640. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 22:36:02,564][187912] Avg episode reward: [(0, '1059.015')]
[36m[2025-06-29 22:36:07,595][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 347.2). Total num frames: 6467584. Throughput: 0: 362.7. Samples: 6470896. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 22:36:07,596][187912] Avg episode reward: [(0, '1117.132')]
[36m[2025-06-29 22:36:12,575][187912] Fps is (10 sec: 409.1, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6471680. Throughput: 0: 360.4. Samples: 6471712. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:36:12,575][187912] Avg episode reward: [(0, '1035.766')]
[37m[1m[2025-06-29 22:36:12,616][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000025280_6471680.pth...
[36m[2025-06-29 22:36:12,686][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000024944_6385664.pth
[36m[2025-06-29 22:36:17,575][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6471680. Throughput: 0: 355.5. Samples: 6473856. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:36:17,575][187912] Avg episode reward: [(0, '986.452')]
[31m[18131541 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18131542 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[18131542 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:36:22,558][187912] Fps is (10 sec: 410.3, 60 sec: 409.6, 300 sec: 361.1). Total num frames: 6475776. Throughput: 0: 357.4. Samples: 6476000. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:36:22,558][187912] Avg episode reward: [(0, '983.750')]
[36m[2025-06-29 22:36:27,566][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6475776. Throughput: 0: 357.2. Samples: 6477200. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:36:27,566][187912] Avg episode reward: [(0, '965.811')]
[36m[2025-06-29 22:36:32,940][187912] Fps is (10 sec: 394.5, 60 sec: 407.1, 300 sec: 360.6). Total num frames: 6479872. Throughput: 0: 357.6. Samples: 6479456. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:36:32,941][187912] Avg episode reward: [(0, '925.849')]
[36m[2025-06-29 22:36:37,569][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6479872. Throughput: 0: 362.9. Samples: 6481584. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:36:37,569][187912] Avg episode reward: [(0, '886.074')]
[36m[2025-06-29 22:36:42,610][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.2). Total num frames: 6479872. Throughput: 0: 359.6. Samples: 6482640. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:36:42,610][187912] Avg episode reward: [(0, '885.712')]
[36m[2025-06-29 22:36:47,566][187912] Fps is (10 sec: 409.7, 60 sec: 341.6, 300 sec: 361.0). Total num frames: 6483968. Throughput: 0: 355.2. Samples: 6484624. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:36:47,566][187912] Avg episode reward: [(0, '939.027')]
[36m[2025-06-29 22:36:52,596][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6483968. Throughput: 0: 354.8. Samples: 6486864. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:36:52,596][187912] Avg episode reward: [(0, '904.112')]
[36m[2025-06-29 22:36:57,570][187912] Fps is (10 sec: 409.4, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 6488064. Throughput: 0: 363.1. Samples: 6488048. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:36:57,571][187912] Avg episode reward: [(0, '932.910')]
[36m[2025-06-29 22:37:02,584][187912] Fps is (10 sec: 410.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6488064. Throughput: 0: 359.7. Samples: 6490048. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:37:02,584][187912] Avg episode reward: [(0, '947.864')]
[36m[2025-06-29 22:37:07,669][187912] Fps is (10 sec: 405.6, 60 sec: 409.1, 300 sec: 360.9). Total num frames: 6492160. Throughput: 0: 336.2. Samples: 6491168. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:37:07,669][187912] Avg episode reward: [(0, '966.784')]
[36m[2025-06-29 22:37:12,583][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6492160. Throughput: 0: 354.7. Samples: 6493168. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:37:12,584][187912] Avg episode reward: [(0, '946.094')]
[36m[2025-06-29 22:37:17,586][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.8). Total num frames: 6492160. Throughput: 0: 358.0. Samples: 6495440. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:37:17,586][187912] Avg episode reward: [(0, '971.877')]
[36m[2025-06-29 22:37:22,569][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6496256. Throughput: 0: 353.8. Samples: 6497504. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 22:37:22,569][187912] Avg episode reward: [(0, '1001.055')]
[36m[2025-06-29 22:37:27,563][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6496256. Throughput: 0: 353.1. Samples: 6498512. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 22:37:27,563][187912] Avg episode reward: [(0, '1060.329')]
[36m[2025-06-29 22:37:32,561][187912] Fps is (10 sec: 409.9, 60 sec: 343.5, 300 sec: 361.0). Total num frames: 6500352. Throughput: 0: 351.3. Samples: 6500432. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 22:37:32,561][187912] Avg episode reward: [(0, '1030.463')]
[36m[2025-06-29 22:37:37,592][187912] Fps is (10 sec: 408.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6500352. Throughput: 0: 354.9. Samples: 6502832. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 22:37:37,592][187912] Avg episode reward: [(0, '930.166')]
[36m[2025-06-29 22:37:42,584][187912] Fps is (10 sec: 408.7, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 6504448. Throughput: 0: 354.7. Samples: 6504016. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:37:42,584][187912] Avg episode reward: [(0, '927.990')]
[36m[2025-06-29 22:37:47,569][187912] Fps is (10 sec: 410.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6504448. Throughput: 0: 357.8. Samples: 6506144. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:37:47,570][187912] Avg episode reward: [(0, '956.301')]
[36m[2025-06-29 22:37:52,571][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 6504448. Throughput: 0: 383.1. Samples: 6508368. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:37:52,572][187912] Avg episode reward: [(0, '930.644')]
[36m[2025-06-29 22:37:57,591][187912] Fps is (10 sec: 408.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6508544. Throughput: 0: 358.0. Samples: 6509280. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:37:57,591][187912] Avg episode reward: [(0, '995.745')]
[36m[2025-06-29 22:38:02,594][187912] Fps is (10 sec: 408.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6508544. Throughput: 0: 361.9. Samples: 6511728. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:38:02,594][187912] Avg episode reward: [(0, '1034.808')]
[36m[2025-06-29 22:38:07,563][187912] Fps is (10 sec: 410.7, 60 sec: 341.9, 300 sec: 361.0). Total num frames: 6512640. Throughput: 0: 362.0. Samples: 6513792. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 22:38:07,563][187912] Avg episode reward: [(0, '1106.933')]
[36m[2025-06-29 22:38:12,582][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6512640. Throughput: 0: 364.6. Samples: 6514928. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 22:38:12,582][187912] Avg episode reward: [(0, '1102.955')]
[37m[1m[2025-06-29 22:38:12,628][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000025440_6512640.pth...
[36m[2025-06-29 22:38:12,699][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000025104_6426624.pth
[36m[2025-06-29 22:38:17,575][187912] Fps is (10 sec: 409.1, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 6516736. Throughput: 0: 363.6. Samples: 6516800. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 22:38:17,575][187912] Avg episode reward: [(0, '1064.999')]
[36m[2025-06-29 22:38:22,578][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6516736. Throughput: 0: 362.4. Samples: 6519136. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 22:38:22,578][187912] Avg episode reward: [(0, '1004.251')]
[36m[2025-06-29 22:38:27,579][187912] Fps is (10 sec: 409.4, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 6520832. Throughput: 0: 361.6. Samples: 6520288. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:38:27,579][187912] Avg episode reward: [(0, '951.524')]
[36m[2025-06-29 22:38:32,588][187912] Fps is (10 sec: 409.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6520832. Throughput: 0: 359.7. Samples: 6522336. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:38:32,588][187912] Avg episode reward: [(0, '934.048')]
[31m[18270758 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18270758 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[18270758 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:38:37,569][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 6520832. Throughput: 0: 358.4. Samples: 6524496. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:38:37,569][187912] Avg episode reward: [(0, '910.042')]
[36m[2025-06-29 22:38:42,582][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6524928. Throughput: 0: 358.8. Samples: 6525424. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:38:42,582][187912] Avg episode reward: [(0, '842.758')]
[36m[2025-06-29 22:38:47,592][187912] Fps is (10 sec: 408.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6524928. Throughput: 0: 354.1. Samples: 6527664. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:38:47,592][187912] Avg episode reward: [(0, '907.691')]
[36m[2025-06-29 22:38:52,559][187912] Fps is (10 sec: 410.5, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 6529024. Throughput: 0: 352.7. Samples: 6529664. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 22:38:52,559][187912] Avg episode reward: [(0, '898.422')]
[36m[2025-06-29 22:38:57,570][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6529024. Throughput: 0: 354.9. Samples: 6530896. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 22:38:57,571][187912] Avg episode reward: [(0, '926.875')]
[36m[2025-06-29 22:39:02,561][187912] Fps is (10 sec: 409.5, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 6533120. Throughput: 0: 360.3. Samples: 6533008. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 22:39:02,561][187912] Avg episode reward: [(0, '929.532')]
[36m[2025-06-29 22:39:07,582][187912] Fps is (10 sec: 409.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6533120. Throughput: 0: 354.1. Samples: 6535072. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 22:39:07,583][187912] Avg episode reward: [(0, '954.739')]
[36m[2025-06-29 22:39:12,592][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.3). Total num frames: 6533120. Throughput: 0: 353.3. Samples: 6536192. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 22:39:12,592][187912] Avg episode reward: [(0, '942.840')]
[31m[18308264 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18308264 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([12], device='cuda:0') (navigation_task.py:196)
[31m[18308264 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[18308739 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18308740 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[18308740 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:39:17,560][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6537216. Throughput: 0: 352.6. Samples: 6538192. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 22:39:17,560][187912] Avg episode reward: [(0, '890.156')]
[36m[2025-06-29 22:39:22,597][187912] Fps is (10 sec: 409.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6537216. Throughput: 0: 353.2. Samples: 6540400. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 22:39:22,598][187912] Avg episode reward: [(0, '848.750')]
[36m[2025-06-29 22:39:27,563][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6541312. Throughput: 0: 353.9. Samples: 6541344. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 22:39:27,563][187912] Avg episode reward: [(0, '821.225')]
[36m[2025-06-29 22:39:32,561][187912] Fps is (10 sec: 411.1, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 6541312. Throughput: 0: 357.2. Samples: 6543728. Policy #0 lag: (min: 6.0, avg: 6.0, max: 22.0)
[36m[2025-06-29 22:39:32,561][187912] Avg episode reward: [(0, '853.983')]
[36m[2025-06-29 22:39:37,565][187912] Fps is (10 sec: 409.5, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6545408. Throughput: 0: 356.9. Samples: 6545728. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:39:37,565][187912] Avg episode reward: [(0, '901.811')]
[31m[18332281 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18332281 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[18332281 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:39:42,591][187912] Fps is (10 sec: 408.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6545408. Throughput: 0: 354.3. Samples: 6546848. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:39:42,592][187912] Avg episode reward: [(0, '987.032')]
[36m[2025-06-29 22:39:47,630][187912] Fps is (10 sec: 406.9, 60 sec: 409.3, 300 sec: 360.9). Total num frames: 6549504. Throughput: 0: 359.3. Samples: 6549200. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:39:47,630][187912] Avg episode reward: [(0, '1066.444')]
[36m[2025-06-29 22:39:52,580][187912] Fps is (10 sec: 410.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6549504. Throughput: 0: 361.3. Samples: 6551328. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:39:52,580][187912] Avg episode reward: [(0, '1095.843')]
[36m[2025-06-29 22:39:57,599][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 6549504. Throughput: 0: 361.5. Samples: 6552464. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:39:57,599][187912] Avg episode reward: [(0, '1170.483')]
[36m[2025-06-29 22:40:02,586][187912] Fps is (10 sec: 409.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6553600. Throughput: 0: 358.9. Samples: 6554352. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 22:40:02,586][187912] Avg episode reward: [(0, '1085.985')]
[36m[2025-06-29 22:40:07,586][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6553600. Throughput: 0: 362.4. Samples: 6556704. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 22:40:07,586][187912] Avg episode reward: [(0, '1067.197')]
[36m[2025-06-29 22:40:12,588][187912] Fps is (10 sec: 409.5, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6557696. Throughput: 0: 363.9. Samples: 6557728. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 22:40:12,588][187912] Avg episode reward: [(0, '959.689')]
[37m[1m[2025-06-29 22:40:12,629][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000025616_6557696.pth...
[36m[2025-06-29 22:40:12,700][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000025280_6471680.pth
[31m[18366165 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18366166 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[18366166 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:40:17,586][187912] Fps is (10 sec: 409.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6557696. Throughput: 0: 358.6. Samples: 6559872. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 22:40:17,587][187912] Avg episode reward: [(0, '938.287')]
[36m[2025-06-29 22:40:22,600][187912] Fps is (10 sec: 409.1, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6561792. Throughput: 0: 357.4. Samples: 6561824. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 22:40:22,601][187912] Avg episode reward: [(0, '937.560')]
[36m[2025-06-29 22:40:27,588][187912] Fps is (10 sec: 409.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6561792. Throughput: 0: 357.0. Samples: 6562912. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 22:40:27,588][187912] Avg episode reward: [(0, '923.914')]
[31m[18380953 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18380954 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[18380954 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:40:32,588][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 6561792. Throughput: 0: 356.2. Samples: 6565216. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 22:40:32,588][187912] Avg episode reward: [(0, '935.927')]
[36m[2025-06-29 22:40:37,580][187912] Fps is (10 sec: 409.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6565888. Throughput: 0: 354.5. Samples: 6567280. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 22:40:37,580][187912] Avg episode reward: [(0, '933.471')]
[36m[2025-06-29 22:40:42,598][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 6565888. Throughput: 0: 354.1. Samples: 6568400. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 22:40:42,599][187912] Avg episode reward: [(0, '978.802')]
[36m[2025-06-29 22:40:47,558][187912] Fps is (10 sec: 410.5, 60 sec: 341.7, 300 sec: 361.0). Total num frames: 6569984. Throughput: 0: 355.8. Samples: 6570352. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:40:47,558][187912] Avg episode reward: [(0, '964.366')]
[36m[2025-06-29 22:40:52,591][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6569984. Throughput: 0: 353.4. Samples: 6572608. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:40:52,592][187912] Avg episode reward: [(0, '915.140')]
[31m[18408218 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18408218 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([12], device='cuda:0') (navigation_task.py:196)
[31m[18408218 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:40:57,565][187912] Fps is (10 sec: 409.3, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 6574080. Throughput: 0: 355.0. Samples: 6573696. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:40:57,565][187912] Avg episode reward: [(0, '867.650')]
[36m[2025-06-29 22:41:02,567][187912] Fps is (10 sec: 410.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6574080. Throughput: 0: 351.4. Samples: 6575680. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:41:02,567][187912] Avg episode reward: [(0, '874.201')]
[36m[2025-06-29 22:41:07,570][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 6574080. Throughput: 0: 359.4. Samples: 6577984. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:41:07,570][187912] Avg episode reward: [(0, '902.453')]
[36m[2025-06-29 22:41:12,601][187912] Fps is (10 sec: 408.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6578176. Throughput: 0: 355.1. Samples: 6578896. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 22:41:12,601][187912] Avg episode reward: [(0, '895.710')]
[36m[2025-06-29 22:41:17,623][187912] Fps is (10 sec: 407.5, 60 sec: 341.1, 300 sec: 347.0). Total num frames: 6578176. Throughput: 0: 355.6. Samples: 6581232. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 22:41:17,623][187912] Avg episode reward: [(0, '989.236')]
[36m[2025-06-29 22:41:22,570][187912] Fps is (10 sec: 410.9, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 6582272. Throughput: 0: 355.3. Samples: 6583264. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 22:41:22,570][187912] Avg episode reward: [(0, '957.254')]
[36m[2025-06-29 22:41:27,595][187912] Fps is (10 sec: 410.7, 60 sec: 341.3, 300 sec: 347.5). Total num frames: 6582272. Throughput: 0: 354.9. Samples: 6584368. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 22:41:27,596][187912] Avg episode reward: [(0, '945.484')]
[36m[2025-06-29 22:41:32,563][187912] Fps is (10 sec: 409.9, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 6586368. Throughput: 0: 357.3. Samples: 6586432. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 22:41:32,563][187912] Avg episode reward: [(0, '890.040')]
[36m[2025-06-29 22:41:37,583][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6586368. Throughput: 0: 361.0. Samples: 6588848. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 22:41:37,583][187912] Avg episode reward: [(0, '928.897')]
[36m[2025-06-29 22:41:42,583][187912] Fps is (10 sec: 408.8, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 6590464. Throughput: 0: 362.5. Samples: 6590016. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 22:41:42,583][187912] Avg episode reward: [(0, '915.844')]
[36m[2025-06-29 22:41:47,568][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6590464. Throughput: 0: 364.8. Samples: 6592096. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 22:41:47,569][187912] Avg episode reward: [(0, '1003.492')]
[31m[18464357 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18464357 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[18464357 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:41:53,082][187912] Fps is (10 sec: 390.1, 60 sec: 406.3, 300 sec: 360.4). Total num frames: 6594560. Throughput: 0: 362.5. Samples: 6594480. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 22:41:53,082][187912] Avg episode reward: [(0, '1006.642')]
[36m[2025-06-29 22:41:57,566][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6594560. Throughput: 0: 367.2. Samples: 6595408. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 22:41:57,567][187912] Avg episode reward: [(0, '1037.909')]
[36m[2025-06-29 22:42:02,591][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.2). Total num frames: 6594560. Throughput: 0: 366.1. Samples: 6597696. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 22:42:02,592][187912] Avg episode reward: [(0, '974.875')]
[36m[2025-06-29 22:42:07,581][187912] Fps is (10 sec: 409.0, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 6598656. Throughput: 0: 366.8. Samples: 6599776. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 22:42:07,581][187912] Avg episode reward: [(0, '1023.986')]
[36m[2025-06-29 22:42:12,566][187912] Fps is (10 sec: 410.6, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 6598656. Throughput: 0: 368.2. Samples: 6600928. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 22:42:12,566][187912] Avg episode reward: [(0, '1003.390')]
[37m[1m[2025-06-29 22:42:12,616][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000025776_6598656.pth...
[36m[2025-06-29 22:42:12,691][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000025440_6512640.pth
[31m[18489779 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18489780 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[18489780 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:42:17,577][187912] Fps is (10 sec: 409.8, 60 sec: 409.9, 300 sec: 361.0). Total num frames: 6602752. Throughput: 0: 366.5. Samples: 6602928. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:42:17,577][187912] Avg episode reward: [(0, '991.983')]
[31m[18494751 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18494751 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[18494751 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:42:22,585][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6602752. Throughput: 0: 363.0. Samples: 6605184. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:42:22,585][187912] Avg episode reward: [(0, '984.697')]
[36m[2025-06-29 22:42:27,560][187912] Fps is (10 sec: 410.3, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 6606848. Throughput: 0: 363.9. Samples: 6606384. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:42:27,560][187912] Avg episode reward: [(0, '1013.690')]
[36m[2025-06-29 22:42:32,570][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6606848. Throughput: 0: 364.1. Samples: 6608480. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:42:32,570][187912] Avg episode reward: [(0, '871.349')]
[36m[2025-06-29 22:42:37,563][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 6606848. Throughput: 0: 365.8. Samples: 6610752. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:42:37,563][187912] Avg episode reward: [(0, '883.344')]
[36m[2025-06-29 22:42:42,559][187912] Fps is (10 sec: 410.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 6610944. Throughput: 0: 361.3. Samples: 6611664. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:42:42,559][187912] Avg episode reward: [(0, '967.043')]
[36m[2025-06-29 22:42:47,575][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6610944. Throughput: 0: 362.4. Samples: 6614000. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:42:47,575][187912] Avg episode reward: [(0, '923.085')]
[36m[2025-06-29 22:42:52,593][187912] Fps is (10 sec: 408.2, 60 sec: 344.1, 300 sec: 361.0). Total num frames: 6615040. Throughput: 0: 361.9. Samples: 6616064. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:42:52,593][187912] Avg episode reward: [(0, '896.272')]
[36m[2025-06-29 22:42:57,581][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6615040. Throughput: 0: 362.5. Samples: 6617248. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:42:57,581][187912] Avg episode reward: [(0, '975.489')]
[36m[2025-06-29 22:43:02,590][187912] Fps is (10 sec: 409.7, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6619136. Throughput: 0: 361.5. Samples: 6619200. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:43:02,590][187912] Avg episode reward: [(0, '945.007')]
[36m[2025-06-29 22:43:07,590][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6619136. Throughput: 0: 364.0. Samples: 6621568. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:43:07,591][187912] Avg episode reward: [(0, '894.422')]
[36m[2025-06-29 22:43:12,593][187912] Fps is (10 sec: 409.5, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 6623232. Throughput: 0: 364.5. Samples: 6622800. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:43:12,593][187912] Avg episode reward: [(0, '827.871')]
[36m[2025-06-29 22:43:17,574][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6623232. Throughput: 0: 364.4. Samples: 6624880. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:43:17,574][187912] Avg episode reward: [(0, '858.902')]
[33m[18552249 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[18552249 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.75927734375
[33mCrash Rate: 0.18017578125
[33mTimeout Rate: 0.060546875 (navigation_task.py:265)
[33m[18552249 ms][navigation_task] - WARNING : 
[33mSuccesses: 1555
[33mCrashes : 369
[33mTimeouts: 124 (navigation_task.py:268)
[36m[2025-06-29 22:43:22,569][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 6623232. Throughput: 0: 364.0. Samples: 6627136. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:43:22,569][187912] Avg episode reward: [(0, '792.692')]
[36m[2025-06-29 22:43:27,590][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6627328. Throughput: 0: 363.8. Samples: 6628048. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 22:43:27,591][187912] Avg episode reward: [(0, '805.124')]
[36m[2025-06-29 22:43:32,572][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6627328. Throughput: 0: 365.9. Samples: 6630464. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 22:43:32,572][187912] Avg episode reward: [(0, '843.734')]
[36m[2025-06-29 22:43:37,572][187912] Fps is (10 sec: 410.3, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 6631424. Throughput: 0: 365.7. Samples: 6632512. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 22:43:37,573][187912] Avg episode reward: [(0, '916.371')]
[36m[2025-06-29 22:43:42,576][187912] Fps is (10 sec: 409.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6631424. Throughput: 0: 365.2. Samples: 6633680. Policy #0 lag: (min: 8.0, avg: 8.0, max: 24.0)
[36m[2025-06-29 22:43:42,576][187912] Avg episode reward: [(0, '937.659')]
[36m[2025-06-29 22:43:47,579][187912] Fps is (10 sec: 409.3, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6635520. Throughput: 0: 365.6. Samples: 6635648. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:43:47,579][187912] Avg episode reward: [(0, '1004.808')]
[36m[2025-06-29 22:43:52,573][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6635520. Throughput: 0: 363.5. Samples: 6637920. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:43:52,574][187912] Avg episode reward: [(0, '987.329')]
[36m[2025-06-29 22:43:57,611][187912] Fps is (10 sec: 408.3, 60 sec: 409.4, 300 sec: 360.9). Total num frames: 6639616. Throughput: 0: 359.0. Samples: 6638960. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:43:57,611][187912] Avg episode reward: [(0, '1006.629')]
[36m[2025-06-29 22:44:02,559][187912] Fps is (10 sec: 410.2, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 6639616. Throughput: 0: 359.9. Samples: 6641072. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:44:02,560][187912] Avg episode reward: [(0, '958.606')]
[36m[2025-06-29 22:44:07,594][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6639616. Throughput: 0: 360.7. Samples: 6643376. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:44:07,594][187912] Avg episode reward: [(0, '1002.114')]
[36m[2025-06-29 22:44:12,566][187912] Fps is (10 sec: 409.3, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 6643712. Throughput: 0: 360.4. Samples: 6644256. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:44:12,566][187912] Avg episode reward: [(0, '983.647')]
[37m[1m[2025-06-29 22:44:12,607][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000025952_6643712.pth...
[36m[2025-06-29 22:44:12,680][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000025616_6557696.pth
[31m[18608723 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18608723 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[18608724 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:44:17,597][187912] Fps is (10 sec: 409.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6643712. Throughput: 0: 356.1. Samples: 6646496. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:44:17,597][187912] Avg episode reward: [(0, '1055.932')]
[36m[2025-06-29 22:44:22,585][187912] Fps is (10 sec: 408.8, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 6647808. Throughput: 0: 355.5. Samples: 6648512. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:44:22,586][187912] Avg episode reward: [(0, '1043.979')]
[36m[2025-06-29 22:44:27,580][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6647808. Throughput: 0: 356.2. Samples: 6649712. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:44:27,580][187912] Avg episode reward: [(0, '1094.403')]
[36m[2025-06-29 22:44:32,589][187912] Fps is (10 sec: 409.4, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 6651904. Throughput: 0: 361.9. Samples: 6651936. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:44:32,590][187912] Avg episode reward: [(0, '1012.450')]
[36m[2025-06-29 22:44:37,587][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6651904. Throughput: 0: 358.3. Samples: 6654048. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:44:37,587][187912] Avg episode reward: [(0, '959.342')]
[36m[2025-06-29 22:44:42,936][187912] Fps is (10 sec: 395.9, 60 sec: 407.2, 300 sec: 360.6). Total num frames: 6656000. Throughput: 0: 359.4. Samples: 6655248. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:44:42,936][187912] Avg episode reward: [(0, '958.116')]
[36m[2025-06-29 22:44:47,589][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6656000. Throughput: 0: 359.9. Samples: 6657280. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:44:47,589][187912] Avg episode reward: [(0, '979.206')]
[36m[2025-06-29 22:44:52,564][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6656000. Throughput: 0: 358.6. Samples: 6659504. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:44:52,564][187912] Avg episode reward: [(0, '920.159')]
[36m[2025-06-29 22:44:57,578][187912] Fps is (10 sec: 410.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 6660096. Throughput: 0: 357.2. Samples: 6660336. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:44:57,578][187912] Avg episode reward: [(0, '976.351')]
[36m[2025-06-29 22:45:02,576][187912] Fps is (10 sec: 409.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6660096. Throughput: 0: 361.4. Samples: 6662752. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:45:02,576][187912] Avg episode reward: [(0, '1051.424')]
[36m[2025-06-29 22:45:07,584][187912] Fps is (10 sec: 409.4, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 6664192. Throughput: 0: 357.7. Samples: 6664608. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:45:07,584][187912] Avg episode reward: [(0, '1021.046')]
[36m[2025-06-29 22:45:12,592][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6664192. Throughput: 0: 357.6. Samples: 6665808. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:45:12,592][187912] Avg episode reward: [(0, '1023.694')]
[36m[2025-06-29 22:45:17,563][187912] Fps is (10 sec: 410.4, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 6668288. Throughput: 0: 361.1. Samples: 6668176. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:45:17,564][187912] Avg episode reward: [(0, '1017.066')]
[31m[18671017 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18671018 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[18671018 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:45:22,571][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6668288. Throughput: 0: 361.0. Samples: 6670288. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:45:22,571][187912] Avg episode reward: [(0, '1017.901')]
[36m[2025-06-29 22:45:27,567][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6668288. Throughput: 0: 359.9. Samples: 6671312. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:45:27,567][187912] Avg episode reward: [(0, '1042.500')]
[36m[2025-06-29 22:45:32,559][187912] Fps is (10 sec: 410.1, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 6672384. Throughput: 0: 357.6. Samples: 6673360. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:45:32,559][187912] Avg episode reward: [(0, '1055.094')]
[36m[2025-06-29 22:45:37,589][187912] Fps is (10 sec: 408.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6672384. Throughput: 0: 356.8. Samples: 6675568. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:45:37,589][187912] Avg episode reward: [(0, '1042.305')]
[36m[2025-06-29 22:45:42,559][187912] Fps is (10 sec: 409.6, 60 sec: 343.5, 300 sec: 361.0). Total num frames: 6676480. Throughput: 0: 359.6. Samples: 6676512. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 22:45:42,559][187912] Avg episode reward: [(0, '1027.504')]
[36m[2025-06-29 22:45:47,569][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6676480. Throughput: 0: 356.7. Samples: 6678800. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 22:45:47,570][187912] Avg episode reward: [(0, '1010.749')]
[36m[2025-06-29 22:45:52,559][187912] Fps is (10 sec: 409.6, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6680576. Throughput: 0: 358.9. Samples: 6680752. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 22:45:52,559][187912] Avg episode reward: [(0, '1028.091')]
[36m[2025-06-29 22:45:57,560][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6680576. Throughput: 0: 357.6. Samples: 6681888. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 22:45:57,560][187912] Avg episode reward: [(0, '979.932')]
[36m[2025-06-29 22:46:03,182][187912] Fps is (10 sec: 385.6, 60 sec: 405.5, 300 sec: 374.1). Total num frames: 6684672. Throughput: 0: 351.8. Samples: 6684224. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 22:46:03,183][187912] Avg episode reward: [(0, '986.171')]
[36m[2025-06-29 22:46:07,585][187912] Fps is (10 sec: 408.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6684672. Throughput: 0: 354.0. Samples: 6686224. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 22:46:07,585][187912] Avg episode reward: [(0, '1063.111')]
[36m[2025-06-29 22:46:12,559][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 361.1). Total num frames: 6684672. Throughput: 0: 356.3. Samples: 6687344. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 22:46:12,559][187912] Avg episode reward: [(0, '1013.044')]
[37m[1m[2025-06-29 22:46:12,601][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000026112_6684672.pth...
[36m[2025-06-29 22:46:12,676][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000025776_6598656.pth
[36m[2025-06-29 22:46:17,566][187912] Fps is (10 sec: 410.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6688768. Throughput: 0: 356.2. Samples: 6689392. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 22:46:17,566][187912] Avg episode reward: [(0, '1077.819')]
[36m[2025-06-29 22:46:22,573][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6688768. Throughput: 0: 358.5. Samples: 6691696. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 22:46:22,573][187912] Avg episode reward: [(0, '1069.598')]
[36m[2025-06-29 22:46:27,570][187912] Fps is (10 sec: 409.4, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6692864. Throughput: 0: 362.2. Samples: 6692816. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:46:27,570][187912] Avg episode reward: [(0, '1041.986')]
[36m[2025-06-29 22:46:32,586][187912] Fps is (10 sec: 409.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6692864. Throughput: 0: 357.9. Samples: 6694912. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:46:32,586][187912] Avg episode reward: [(0, '1001.685')]
[36m[2025-06-29 22:46:37,592][187912] Fps is (10 sec: 408.7, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6696960. Throughput: 0: 360.6. Samples: 6696992. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:46:37,593][187912] Avg episode reward: [(0, '1097.204')]
[36m[2025-06-29 22:46:42,574][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6696960. Throughput: 0: 359.7. Samples: 6698080. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:46:42,574][187912] Avg episode reward: [(0, '1058.070')]
[36m[2025-06-29 22:46:47,559][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.7). Total num frames: 6696960. Throughput: 0: 366.7. Samples: 6700496. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:46:47,559][187912] Avg episode reward: [(0, '1001.866')]
[36m[2025-06-29 22:46:52,571][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6701056. Throughput: 0: 362.4. Samples: 6702528. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:46:52,571][187912] Avg episode reward: [(0, '1000.382')]
[36m[2025-06-29 22:46:57,621][187912] Fps is (10 sec: 407.1, 60 sec: 341.0, 300 sec: 361.0). Total num frames: 6701056. Throughput: 0: 361.8. Samples: 6703648. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:46:57,621][187912] Avg episode reward: [(0, '1038.552')]
[36m[2025-06-29 22:47:02,587][187912] Fps is (10 sec: 408.9, 60 sec: 344.8, 300 sec: 361.0). Total num frames: 6705152. Throughput: 0: 358.9. Samples: 6705552. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:47:02,587][187912] Avg episode reward: [(0, '1039.541')]
[36m[2025-06-29 22:47:07,562][187912] Fps is (10 sec: 412.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 6705152. Throughput: 0: 361.3. Samples: 6707952. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:47:07,562][187912] Avg episode reward: [(0, '1044.828')]
[36m[2025-06-29 22:47:12,610][187912] Fps is (10 sec: 408.7, 60 sec: 409.3, 300 sec: 361.0). Total num frames: 6709248. Throughput: 0: 359.5. Samples: 6709008. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:47:12,610][187912] Avg episode reward: [(0, '1059.125')]
[31m[18787572 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18787572 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[18787572 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:47:17,572][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6709248. Throughput: 0: 357.1. Samples: 6710976. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:47:17,572][187912] Avg episode reward: [(0, '1123.255')]
[36m[2025-06-29 22:47:23,238][187912] Fps is (10 sec: 385.4, 60 sec: 405.1, 300 sec: 360.2). Total num frames: 6713344. Throughput: 0: 355.4. Samples: 6713216. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:47:23,238][187912] Avg episode reward: [(0, '1055.070')]
[36m[2025-06-29 22:47:27,605][187912] Fps is (10 sec: 408.3, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 6713344. Throughput: 0: 356.0. Samples: 6714112. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:47:27,605][187912] Avg episode reward: [(0, '1049.500')]
[36m[2025-06-29 22:47:32,584][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6713344. Throughput: 0: 350.4. Samples: 6716272. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:47:32,584][187912] Avg episode reward: [(0, '1004.857')]
[36m[2025-06-29 22:47:37,587][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6717440. Throughput: 0: 350.1. Samples: 6718288. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 22:47:37,587][187912] Avg episode reward: [(0, '1015.643')]
[36m[2025-06-29 22:47:42,558][187912] Fps is (10 sec: 410.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6717440. Throughput: 0: 351.8. Samples: 6719456. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 22:47:42,558][187912] Avg episode reward: [(0, '1002.638')]
[36m[2025-06-29 22:47:47,581][187912] Fps is (10 sec: 409.8, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 6721536. Throughput: 0: 356.0. Samples: 6721568. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 22:47:47,582][187912] Avg episode reward: [(0, '989.339')]
[36m[2025-06-29 22:47:52,568][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6721536. Throughput: 0: 351.2. Samples: 6723760. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 22:47:52,568][187912] Avg episode reward: [(0, '1038.358')]
[31m[18827035 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18827035 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[18827035 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:47:58,079][187912] Fps is (10 sec: 390.2, 60 sec: 406.5, 300 sec: 360.4). Total num frames: 6725632. Throughput: 0: 348.7. Samples: 6724864. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 22:47:58,080][187912] Avg episode reward: [(0, '1031.914')]
[36m[2025-06-29 22:48:02,586][187912] Fps is (10 sec: 408.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6725632. Throughput: 0: 353.0. Samples: 6726864. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 22:48:02,586][187912] Avg episode reward: [(0, '1037.448')]
[36m[2025-06-29 22:48:07,582][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 6725632. Throughput: 0: 361.2. Samples: 6729232. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 22:48:07,582][187912] Avg episode reward: [(0, '1021.299')]
[36m[2025-06-29 22:48:12,577][187912] Fps is (10 sec: 410.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 6729728. Throughput: 0: 356.1. Samples: 6730128. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 22:48:12,577][187912] Avg episode reward: [(0, '1051.687')]
[37m[1m[2025-06-29 22:48:12,616][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000026288_6729728.pth...
[36m[2025-06-29 22:48:12,679][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000025952_6643712.pth
[36m[2025-06-29 22:48:17,584][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6729728. Throughput: 0: 357.3. Samples: 6732352. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 22:48:17,584][187912] Avg episode reward: [(0, '1098.818')]
[36m[2025-06-29 22:48:22,598][187912] Fps is (10 sec: 408.8, 60 sec: 345.0, 300 sec: 361.0). Total num frames: 6733824. Throughput: 0: 358.0. Samples: 6734400. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 22:48:22,598][187912] Avg episode reward: [(0, '1031.659')]
[36m[2025-06-29 22:48:27,570][187912] Fps is (10 sec: 410.2, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 6733824. Throughput: 0: 358.0. Samples: 6735568. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 22:48:27,570][187912] Avg episode reward: [(0, '1019.393')]
[36m[2025-06-29 22:48:32,574][187912] Fps is (10 sec: 410.5, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 6737920. Throughput: 0: 363.1. Samples: 6737904. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 22:48:32,575][187912] Avg episode reward: [(0, '998.760')]
[36m[2025-06-29 22:48:37,595][187912] Fps is (10 sec: 408.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6737920. Throughput: 0: 356.8. Samples: 6739824. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 22:48:37,595][187912] Avg episode reward: [(0, '978.470')]
[36m[2025-06-29 22:48:42,590][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 6737920. Throughput: 0: 361.3. Samples: 6740944. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 22:48:42,591][187912] Avg episode reward: [(0, '960.968')]
[36m[2025-06-29 22:48:47,583][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6742016. Throughput: 0: 357.7. Samples: 6742960. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:48:47,584][187912] Avg episode reward: [(0, '1076.787')]
[36m[2025-06-29 22:48:52,559][187912] Fps is (10 sec: 410.9, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 6742016. Throughput: 0: 354.7. Samples: 6745184. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:48:52,559][187912] Avg episode reward: [(0, '1037.943')]
[36m[2025-06-29 22:48:57,562][187912] Fps is (10 sec: 410.5, 60 sec: 344.3, 300 sec: 361.0). Total num frames: 6746112. Throughput: 0: 356.0. Samples: 6746144. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:48:57,562][187912] Avg episode reward: [(0, '985.244')]
[36m[2025-06-29 22:49:02,576][187912] Fps is (10 sec: 408.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6746112. Throughput: 0: 354.6. Samples: 6748304. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:49:02,576][187912] Avg episode reward: [(0, '1066.037')]
[36m[2025-06-29 22:49:07,596][187912] Fps is (10 sec: 408.2, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 6750208. Throughput: 0: 352.7. Samples: 6750272. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:49:07,597][187912] Avg episode reward: [(0, '962.915')]
[31m[18902797 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[18902797 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[18902797 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:49:12,590][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6750208. Throughput: 0: 352.6. Samples: 6751440. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:49:12,590][187912] Avg episode reward: [(0, '874.332')]
[36m[2025-06-29 22:49:17,559][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.2). Total num frames: 6750208. Throughput: 0: 351.8. Samples: 6753728. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:49:17,559][187912] Avg episode reward: [(0, '894.084')]
[36m[2025-06-29 22:49:22,561][187912] Fps is (10 sec: 410.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 6754304. Throughput: 0: 355.5. Samples: 6755808. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:49:22,561][187912] Avg episode reward: [(0, '941.351')]
[36m[2025-06-29 22:49:27,563][187912] Fps is (10 sec: 409.4, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 6754304. Throughput: 0: 355.8. Samples: 6756944. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:49:27,563][187912] Avg episode reward: [(0, '959.373')]
[36m[2025-06-29 22:49:32,596][187912] Fps is (10 sec: 408.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6758400. Throughput: 0: 357.2. Samples: 6759040. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:49:32,596][187912] Avg episode reward: [(0, '1006.632')]
[36m[2025-06-29 22:49:37,572][187912] Fps is (10 sec: 409.2, 60 sec: 341.5, 300 sec: 347.5). Total num frames: 6758400. Throughput: 0: 360.4. Samples: 6761408. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:49:37,572][187912] Avg episode reward: [(0, '1094.049')]
[36m[2025-06-29 22:49:42,579][187912] Fps is (10 sec: 410.3, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 6762496. Throughput: 0: 364.0. Samples: 6762528. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:49:42,579][187912] Avg episode reward: [(0, '1097.924')]
[36m[2025-06-29 22:49:47,585][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6762496. Throughput: 0: 363.3. Samples: 6764656. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:49:47,585][187912] Avg episode reward: [(0, '1062.084')]
[36m[2025-06-29 22:49:52,584][187912] Fps is (10 sec: 409.4, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 6766592. Throughput: 0: 364.2. Samples: 6766656. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:49:52,585][187912] Avg episode reward: [(0, '1032.074')]
[36m[2025-06-29 22:49:57,562][187912] Fps is (10 sec: 410.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6766592. Throughput: 0: 363.6. Samples: 6767792. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:49:57,562][187912] Avg episode reward: [(0, '1003.200')]
[36m[2025-06-29 22:50:02,586][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 6766592. Throughput: 0: 363.5. Samples: 6770096. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:50:02,586][187912] Avg episode reward: [(0, '962.709')]
[36m[2025-06-29 22:50:07,558][187912] Fps is (10 sec: 409.7, 60 sec: 341.6, 300 sec: 361.0). Total num frames: 6770688. Throughput: 0: 364.1. Samples: 6772192. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:50:07,558][187912] Avg episode reward: [(0, '962.505')]
[36m[2025-06-29 22:50:12,568][187912] Fps is (10 sec: 410.3, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 6770688. Throughput: 0: 365.1. Samples: 6773376. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:50:12,568][187912] Avg episode reward: [(0, '904.635')]
[37m[1m[2025-06-29 22:50:12,615][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000026448_6770688.pth...
[36m[2025-06-29 22:50:12,682][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000026112_6684672.pth
[36m[2025-06-29 22:50:17,580][187912] Fps is (10 sec: 408.7, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 6774784. Throughput: 0: 361.0. Samples: 6775280. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 22:50:17,581][187912] Avg episode reward: [(0, '908.073')]
[36m[2025-06-29 22:50:22,587][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6774784. Throughput: 0: 360.1. Samples: 6777616. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 22:50:22,587][187912] Avg episode reward: [(0, '942.354')]
[36m[2025-06-29 22:50:27,566][187912] Fps is (10 sec: 410.2, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6778880. Throughput: 0: 360.3. Samples: 6778736. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 22:50:27,566][187912] Avg episode reward: [(0, '935.953')]
[36m[2025-06-29 22:50:32,587][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6778880. Throughput: 0: 358.4. Samples: 6780784. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 22:50:32,587][187912] Avg episode reward: [(0, '1026.993')]
[36m[2025-06-29 22:50:37,887][187912] Fps is (10 sec: 396.9, 60 sec: 407.5, 300 sec: 360.6). Total num frames: 6782976. Throughput: 0: 360.2. Samples: 6782976. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:50:37,887][187912] Avg episode reward: [(0, '988.541')]
[36m[2025-06-29 22:50:42,572][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6782976. Throughput: 0: 356.5. Samples: 6783840. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:50:42,572][187912] Avg episode reward: [(0, '970.737')]
[36m[2025-06-29 22:50:47,595][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 6782976. Throughput: 0: 354.1. Samples: 6786032. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:50:47,595][187912] Avg episode reward: [(0, '885.174')]
[31m[19005138 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19005138 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[19005138 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:50:52,562][187912] Fps is (10 sec: 410.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 6787072. Throughput: 0: 354.1. Samples: 6788128. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:50:52,562][187912] Avg episode reward: [(0, '821.457')]
[36m[2025-06-29 22:50:57,587][187912] Fps is (10 sec: 409.9, 60 sec: 341.2, 300 sec: 347.8). Total num frames: 6787072. Throughput: 0: 351.5. Samples: 6789200. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:50:57,588][187912] Avg episode reward: [(0, '810.843')]
[36m[2025-06-29 22:51:02,580][187912] Fps is (10 sec: 408.9, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6791168. Throughput: 0: 353.8. Samples: 6791200. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 22:51:02,580][187912] Avg episode reward: [(0, '847.048')]
[36m[2025-06-29 22:51:07,589][187912] Fps is (10 sec: 409.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6791168. Throughput: 0: 353.1. Samples: 6793504. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 22:51:07,589][187912] Avg episode reward: [(0, '931.140')]
[36m[2025-06-29 22:51:12,566][187912] Fps is (10 sec: 410.2, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6795264. Throughput: 0: 354.8. Samples: 6794704. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 22:51:12,566][187912] Avg episode reward: [(0, '978.849')]
[36m[2025-06-29 22:51:17,561][187912] Fps is (10 sec: 410.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6795264. Throughput: 0: 356.5. Samples: 6796816. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 22:51:17,561][187912] Avg episode reward: [(0, '1055.171')]
[36m[2025-06-29 22:51:22,593][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 6795264. Throughput: 0: 360.0. Samples: 6799072. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 22:51:22,593][187912] Avg episode reward: [(0, '1066.072')]
[36m[2025-06-29 22:51:27,571][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6799360. Throughput: 0: 358.8. Samples: 6799984. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 22:51:27,571][187912] Avg episode reward: [(0, '1086.260')]
[36m[2025-06-29 22:51:32,583][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 6799360. Throughput: 0: 363.1. Samples: 6802368. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 22:51:32,583][187912] Avg episode reward: [(0, '1034.423')]
[36m[2025-06-29 22:51:37,584][187912] Fps is (10 sec: 409.1, 60 sec: 343.1, 300 sec: 361.0). Total num frames: 6803456. Throughput: 0: 364.3. Samples: 6804528. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 22:51:37,584][187912] Avg episode reward: [(0, '1078.896')]
[36m[2025-06-29 22:51:42,592][187912] Fps is (10 sec: 409.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6803456. Throughput: 0: 365.1. Samples: 6805632. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 22:51:42,592][187912] Avg episode reward: [(0, '1084.471')]
[36m[2025-06-29 22:51:47,587][187912] Fps is (10 sec: 409.5, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 6807552. Throughput: 0: 364.0. Samples: 6807584. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:51:47,587][187912] Avg episode reward: [(0, '1146.639')]
[36m[2025-06-29 22:51:52,560][187912] Fps is (10 sec: 410.9, 60 sec: 341.3, 300 sec: 361.1). Total num frames: 6807552. Throughput: 0: 361.1. Samples: 6809744. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:51:52,560][187912] Avg episode reward: [(0, '1132.561')]
[36m[2025-06-29 22:51:57,688][187912] Fps is (10 sec: 405.5, 60 sec: 408.9, 300 sec: 360.9). Total num frames: 6811648. Throughput: 0: 360.6. Samples: 6810976. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:51:57,688][187912] Avg episode reward: [(0, '1112.965')]
[36m[2025-06-29 22:52:02,597][187912] Fps is (10 sec: 408.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6811648. Throughput: 0: 358.8. Samples: 6812976. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:52:02,597][187912] Avg episode reward: [(0, '1156.324')]
[36m[2025-06-29 22:52:07,585][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 6811648. Throughput: 0: 358.5. Samples: 6815200. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:52:07,585][187912] Avg episode reward: [(0, '1058.737')]
[33m[19084103 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[19084103 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.78662109375
[33mCrash Rate: 0.15087890625
[33mTimeout Rate: 0.0625 (navigation_task.py:265)
[33m[19084103 ms][navigation_task] - WARNING : 
[33mSuccesses: 1611
[33mCrashes : 309
[33mTimeouts: 128 (navigation_task.py:268)
[36m[2025-06-29 22:52:12,571][187912] Fps is (10 sec: 410.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6815744. Throughput: 0: 358.1. Samples: 6816096. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 22:52:12,571][187912] Avg episode reward: [(0, '1026.193')]
[37m[1m[2025-06-29 22:52:12,622][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000026624_6815744.pth...
[36m[2025-06-29 22:52:12,686][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000026288_6729728.pth
[36m[2025-06-29 22:52:17,592][187912] Fps is (10 sec: 409.3, 60 sec: 341.2, 300 sec: 347.9). Total num frames: 6815744. Throughput: 0: 356.9. Samples: 6818432. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 22:52:17,592][187912] Avg episode reward: [(0, '982.389')]
[36m[2025-06-29 22:52:22,592][187912] Fps is (10 sec: 408.7, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6819840. Throughput: 0: 351.9. Samples: 6820368. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 22:52:22,592][187912] Avg episode reward: [(0, '1040.150')]
[36m[2025-06-29 22:52:27,580][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6819840. Throughput: 0: 355.3. Samples: 6821616. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 22:52:27,580][187912] Avg episode reward: [(0, '1037.135')]
[36m[2025-06-29 22:52:32,564][187912] Fps is (10 sec: 410.7, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 6823936. Throughput: 0: 362.1. Samples: 6823872. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:52:32,565][187912] Avg episode reward: [(0, '1112.785')]
[36m[2025-06-29 22:52:37,569][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6823936. Throughput: 0: 359.7. Samples: 6825936. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:52:37,570][187912] Avg episode reward: [(0, '1087.219')]
[36m[2025-06-29 22:52:43,282][187912] Fps is (10 sec: 382.2, 60 sec: 404.9, 300 sec: 360.1). Total num frames: 6828032. Throughput: 0: 354.8. Samples: 6827152. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:52:43,283][187912] Avg episode reward: [(0, '1105.214')]
[36m[2025-06-29 22:52:47,576][187912] Fps is (10 sec: 409.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6828032. Throughput: 0: 361.4. Samples: 6829232. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:52:47,576][187912] Avg episode reward: [(0, '1071.996')]
[36m[2025-06-29 22:52:52,574][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.7). Total num frames: 6828032. Throughput: 0: 361.7. Samples: 6831472. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:52:52,574][187912] Avg episode reward: [(0, '1110.645')]
[36m[2025-06-29 22:52:57,565][187912] Fps is (10 sec: 410.1, 60 sec: 342.0, 300 sec: 361.0). Total num frames: 6832128. Throughput: 0: 362.0. Samples: 6832384. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 22:52:57,565][187912] Avg episode reward: [(0, '1064.884')]
[36m[2025-06-29 22:53:02,564][187912] Fps is (10 sec: 410.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 6832128. Throughput: 0: 362.2. Samples: 6834720. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 22:53:02,564][187912] Avg episode reward: [(0, '1099.378')]
[36m[2025-06-29 22:53:07,592][187912] Fps is (10 sec: 408.5, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6836224. Throughput: 0: 361.6. Samples: 6836640. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 22:53:07,592][187912] Avg episode reward: [(0, '997.388')]
[36m[2025-06-29 22:53:12,595][187912] Fps is (10 sec: 408.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6836224. Throughput: 0: 358.6. Samples: 6837760. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 22:53:12,596][187912] Avg episode reward: [(0, '1014.798')]
[36m[2025-06-29 22:53:17,823][187912] Fps is (10 sec: 400.3, 60 sec: 408.0, 300 sec: 360.7). Total num frames: 6840320. Throughput: 0: 356.0. Samples: 6839984. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 22:53:17,823][187912] Avg episode reward: [(0, '990.028')]
[31m[19155774 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19155774 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[19155774 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:53:22,586][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6840320. Throughput: 0: 357.2. Samples: 6842016. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 22:53:22,586][187912] Avg episode reward: [(0, '905.728')]
[36m[2025-06-29 22:53:27,586][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 6840320. Throughput: 0: 362.6. Samples: 6843216. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 22:53:27,586][187912] Avg episode reward: [(0, '900.059')]
[36m[2025-06-29 22:53:32,576][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6844416. Throughput: 0: 356.3. Samples: 6845264. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 22:53:32,576][187912] Avg episode reward: [(0, '951.140')]
[36m[2025-06-29 22:53:37,609][187912] Fps is (10 sec: 408.7, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 6844416. Throughput: 0: 351.7. Samples: 6847312. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 22:53:37,609][187912] Avg episode reward: [(0, '876.690')]
[36m[2025-06-29 22:53:42,581][187912] Fps is (10 sec: 409.4, 60 sec: 345.4, 300 sec: 361.0). Total num frames: 6848512. Throughput: 0: 354.0. Samples: 6848320. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:53:42,582][187912] Avg episode reward: [(0, '949.688')]
[36m[2025-06-29 22:53:47,575][187912] Fps is (10 sec: 411.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6848512. Throughput: 0: 348.7. Samples: 6850416. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:53:47,575][187912] Avg episode reward: [(0, '978.945')]
[36m[2025-06-29 22:53:52,562][187912] Fps is (10 sec: 410.4, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 6852608. Throughput: 0: 355.8. Samples: 6852640. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:53:52,562][187912] Avg episode reward: [(0, '1010.771')]
[36m[2025-06-29 22:53:57,595][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6852608. Throughput: 0: 354.8. Samples: 6853728. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:53:57,595][187912] Avg episode reward: [(0, '978.415')]
[36m[2025-06-29 22:54:02,565][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.2). Total num frames: 6852608. Throughput: 0: 359.0. Samples: 6856048. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:54:02,565][187912] Avg episode reward: [(0, '986.960')]
[36m[2025-06-29 22:54:07,562][187912] Fps is (10 sec: 411.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 6856704. Throughput: 0: 359.0. Samples: 6858160. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:54:07,562][187912] Avg episode reward: [(0, '984.096')]
[31m[19201596 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19201596 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[19201596 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:54:12,598][187912] Fps is (10 sec: 408.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6856704. Throughput: 0: 359.7. Samples: 6859408. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:54:12,598][187912] Avg episode reward: [(0, '905.051')]
[37m[1m[2025-06-29 22:54:12,653][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000026784_6856704.pth...
[36m[2025-06-29 22:54:12,707][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000026448_6770688.pth
[31m[19208778 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19208778 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[19208778 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:54:17,565][187912] Fps is (10 sec: 409.5, 60 sec: 342.8, 300 sec: 361.0). Total num frames: 6860800. Throughput: 0: 357.8. Samples: 6861360. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:54:17,565][187912] Avg episode reward: [(0, '879.386')]
[31m[19214046 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19214046 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[19214046 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:54:22,591][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6860800. Throughput: 0: 361.0. Samples: 6863552. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:54:22,591][187912] Avg episode reward: [(0, '870.091')]
[36m[2025-06-29 22:54:27,573][187912] Fps is (10 sec: 409.3, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 6864896. Throughput: 0: 363.8. Samples: 6864688. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:54:27,573][187912] Avg episode reward: [(0, '901.930')]
[36m[2025-06-29 22:54:32,589][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6864896. Throughput: 0: 363.3. Samples: 6866768. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:54:32,590][187912] Avg episode reward: [(0, '842.719')]
[31m[19229949 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19229949 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[19229949 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:54:38,216][187912] Fps is (10 sec: 384.9, 60 sec: 405.5, 300 sec: 360.2). Total num frames: 6868992. Throughput: 0: 355.7. Samples: 6868880. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:54:38,216][187912] Avg episode reward: [(0, '906.866')]
[36m[2025-06-29 22:54:42,574][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6868992. Throughput: 0: 356.1. Samples: 6869744. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:54:42,574][187912] Avg episode reward: [(0, '1004.229')]
[31m[19239583 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19239583 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[19239584 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:54:47,583][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 6868992. Throughput: 0: 356.8. Samples: 6872112. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:54:47,584][187912] Avg episode reward: [(0, '1044.401')]
[36m[2025-06-29 22:54:52,588][187912] Fps is (10 sec: 409.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6873088. Throughput: 0: 352.9. Samples: 6874048. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 22:54:52,589][187912] Avg episode reward: [(0, '998.475')]
[36m[2025-06-29 22:54:57,560][187912] Fps is (10 sec: 410.6, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 6873088. Throughput: 0: 351.6. Samples: 6875216. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 22:54:57,560][187912] Avg episode reward: [(0, '1044.027')]
[36m[2025-06-29 22:55:02,568][187912] Fps is (10 sec: 410.4, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6877184. Throughput: 0: 352.7. Samples: 6877232. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 22:55:02,568][187912] Avg episode reward: [(0, '1105.182')]
[36m[2025-06-29 22:55:07,565][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6877184. Throughput: 0: 356.5. Samples: 6879584. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 22:55:07,565][187912] Avg episode reward: [(0, '1037.889')]
[36m[2025-06-29 22:55:12,579][187912] Fps is (10 sec: 409.2, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 6881280. Throughput: 0: 355.9. Samples: 6880704. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:55:12,579][187912] Avg episode reward: [(0, '1048.386')]
[36m[2025-06-29 22:55:17,560][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6881280. Throughput: 0: 354.4. Samples: 6882704. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:55:17,560][187912] Avg episode reward: [(0, '1096.443')]
[36m[2025-06-29 22:55:22,592][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 6881280. Throughput: 0: 361.6. Samples: 6884928. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:55:22,593][187912] Avg episode reward: [(0, '1055.890')]
[36m[2025-06-29 22:55:27,575][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6885376. Throughput: 0: 358.0. Samples: 6885856. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:55:27,575][187912] Avg episode reward: [(0, '1066.078')]
[36m[2025-06-29 22:55:32,583][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 347.5). Total num frames: 6885376. Throughput: 0: 353.1. Samples: 6888000. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:55:32,584][187912] Avg episode reward: [(0, '1014.748')]
[36m[2025-06-29 22:55:37,589][187912] Fps is (10 sec: 409.0, 60 sec: 344.9, 300 sec: 361.0). Total num frames: 6889472. Throughput: 0: 353.8. Samples: 6889968. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:55:37,589][187912] Avg episode reward: [(0, '957.680')]
[36m[2025-06-29 22:55:42,587][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6889472. Throughput: 0: 351.1. Samples: 6891024. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:55:42,587][187912] Avg episode reward: [(0, '968.901')]
[36m[2025-06-29 22:55:47,655][187912] Fps is (10 sec: 406.9, 60 sec: 409.1, 300 sec: 360.9). Total num frames: 6893568. Throughput: 0: 357.0. Samples: 6893328. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:55:47,655][187912] Avg episode reward: [(0, '977.530')]
[36m[2025-06-29 22:55:52,559][187912] Fps is (10 sec: 410.7, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 6893568. Throughput: 0: 348.1. Samples: 6895248. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:55:52,560][187912] Avg episode reward: [(0, '937.351')]
[36m[2025-06-29 22:55:57,591][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 6893568. Throughput: 0: 346.2. Samples: 6896288. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:55:57,591][187912] Avg episode reward: [(0, '1030.509')]
[36m[2025-06-29 22:56:02,580][187912] Fps is (10 sec: 408.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6897664. Throughput: 0: 344.4. Samples: 6898208. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:56:02,580][187912] Avg episode reward: [(0, '1051.602')]
[36m[2025-06-29 22:56:07,569][187912] Fps is (10 sec: 410.5, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 6897664. Throughput: 0: 350.4. Samples: 6900688. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:56:07,570][187912] Avg episode reward: [(0, '1035.632')]
[36m[2025-06-29 22:56:12,563][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6901760. Throughput: 0: 354.2. Samples: 6901792. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:56:12,563][187912] Avg episode reward: [(0, '1081.394')]
[37m[1m[2025-06-29 22:56:12,605][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000026960_6901760.pth...
[36m[2025-06-29 22:56:12,666][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000026624_6815744.pth
[36m[2025-06-29 22:56:17,585][187912] Fps is (10 sec: 409.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6901760. Throughput: 0: 355.2. Samples: 6903984. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:56:17,585][187912] Avg episode reward: [(0, '1066.248')]
[36m[2025-06-29 22:56:22,594][187912] Fps is (10 sec: 408.3, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6905856. Throughput: 0: 353.7. Samples: 6905888. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:56:22,594][187912] Avg episode reward: [(0, '984.599')]
[36m[2025-06-29 22:56:27,558][187912] Fps is (10 sec: 410.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6905856. Throughput: 0: 355.8. Samples: 6907024. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:56:27,558][187912] Avg episode reward: [(0, '1011.299')]
[36m[2025-06-29 22:56:32,576][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 6905856. Throughput: 0: 354.0. Samples: 6909232. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:56:32,576][187912] Avg episode reward: [(0, '1074.509')]
[36m[2025-06-29 22:56:37,571][187912] Fps is (10 sec: 409.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6909952. Throughput: 0: 354.8. Samples: 6911216. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:56:37,571][187912] Avg episode reward: [(0, '1036.534')]
[36m[2025-06-29 22:56:42,579][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 6909952. Throughput: 0: 357.8. Samples: 6912384. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:56:42,580][187912] Avg episode reward: [(0, '1069.167')]
[36m[2025-06-29 22:56:47,576][187912] Fps is (10 sec: 409.4, 60 sec: 341.8, 300 sec: 361.0). Total num frames: 6914048. Throughput: 0: 356.7. Samples: 6914256. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 22:56:47,576][187912] Avg episode reward: [(0, '1085.822')]
[36m[2025-06-29 22:56:52,558][187912] Fps is (10 sec: 410.5, 60 sec: 341.3, 300 sec: 347.3). Total num frames: 6914048. Throughput: 0: 350.7. Samples: 6916464. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 22:56:52,558][187912] Avg episode reward: [(0, '1079.868')]
[36m[2025-06-29 22:56:57,584][187912] Fps is (10 sec: 409.3, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6918144. Throughput: 0: 352.2. Samples: 6917648. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 22:56:57,584][187912] Avg episode reward: [(0, '1039.413')]
[36m[2025-06-29 22:57:02,568][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6918144. Throughput: 0: 349.3. Samples: 6919696. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 22:57:02,568][187912] Avg episode reward: [(0, '1033.613')]
[36m[2025-06-29 22:57:07,595][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 6918144. Throughput: 0: 356.6. Samples: 6921936. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 22:57:07,595][187912] Avg episode reward: [(0, '1010.599')]
[36m[2025-06-29 22:57:12,570][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6922240. Throughput: 0: 352.3. Samples: 6922880. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 22:57:12,570][187912] Avg episode reward: [(0, '1042.187')]
[36m[2025-06-29 22:57:17,579][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 6922240. Throughput: 0: 355.2. Samples: 6925216. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 22:57:17,579][187912] Avg episode reward: [(0, '975.490')]
[36m[2025-06-29 22:57:22,579][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6926336. Throughput: 0: 354.4. Samples: 6927168. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 22:57:22,579][187912] Avg episode reward: [(0, '1001.091')]
[36m[2025-06-29 22:57:27,591][187912] Fps is (10 sec: 409.1, 60 sec: 341.1, 300 sec: 347.1). Total num frames: 6926336. Throughput: 0: 353.0. Samples: 6928272. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 22:57:27,592][187912] Avg episode reward: [(0, '891.708')]
[36m[2025-06-29 22:57:32,569][187912] Fps is (10 sec: 410.0, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 6930432. Throughput: 0: 360.2. Samples: 6930464. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:57:32,569][187912] Avg episode reward: [(0, '927.604')]
[36m[2025-06-29 22:57:37,595][187912] Fps is (10 sec: 409.4, 60 sec: 341.2, 300 sec: 347.9). Total num frames: 6930432. Throughput: 0: 359.9. Samples: 6932672. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:57:37,596][187912] Avg episode reward: [(0, '897.034')]
[36m[2025-06-29 22:57:42,581][187912] Fps is (10 sec: 409.1, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6934528. Throughput: 0: 361.3. Samples: 6933904. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:57:42,581][187912] Avg episode reward: [(0, '936.850')]
[36m[2025-06-29 22:57:47,576][187912] Fps is (10 sec: 410.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6934528. Throughput: 0: 363.0. Samples: 6936032. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:57:47,576][187912] Avg episode reward: [(0, '970.445')]
[31m[19422348 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19422349 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[19422349 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 22:57:52,573][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 6934528. Throughput: 0: 365.7. Samples: 6938384. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:57:52,573][187912] Avg episode reward: [(0, '1026.678')]
[36m[2025-06-29 22:57:57,595][187912] Fps is (10 sec: 408.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6938624. Throughput: 0: 362.5. Samples: 6939200. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 22:57:57,595][187912] Avg episode reward: [(0, '1008.942')]
[36m[2025-06-29 22:58:02,579][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 6938624. Throughput: 0: 361.2. Samples: 6941472. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 22:58:02,579][187912] Avg episode reward: [(0, '955.641')]
[36m[2025-06-29 22:58:07,580][187912] Fps is (10 sec: 410.2, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 6942720. Throughput: 0: 364.4. Samples: 6943568. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 22:58:07,580][187912] Avg episode reward: [(0, '994.988')]
[36m[2025-06-29 22:58:12,591][187912] Fps is (10 sec: 409.1, 60 sec: 341.2, 300 sec: 347.4). Total num frames: 6942720. Throughput: 0: 366.9. Samples: 6944784. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 22:58:12,591][187912] Avg episode reward: [(0, '997.841')]
[37m[1m[2025-06-29 22:58:12,639][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000027120_6942720.pth...
[36m[2025-06-29 22:58:12,709][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000026784_6856704.pth
[36m[2025-06-29 22:58:17,570][187912] Fps is (10 sec: 410.0, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 6946816. Throughput: 0: 364.1. Samples: 6946848. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:58:17,570][187912] Avg episode reward: [(0, '1024.732')]
[36m[2025-06-29 22:58:22,567][187912] Fps is (10 sec: 410.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6946816. Throughput: 0: 364.0. Samples: 6949040. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:58:22,567][187912] Avg episode reward: [(0, '1005.674')]
[36m[2025-06-29 22:58:27,599][187912] Fps is (10 sec: 408.4, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 6950912. Throughput: 0: 362.9. Samples: 6950240. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:58:27,599][187912] Avg episode reward: [(0, '1061.993')]
[36m[2025-06-29 22:58:32,559][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 361.1). Total num frames: 6950912. Throughput: 0: 362.5. Samples: 6952336. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:58:32,559][187912] Avg episode reward: [(0, '983.737')]
[36m[2025-06-29 22:58:37,576][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 6950912. Throughput: 0: 357.7. Samples: 6954480. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:58:37,576][187912] Avg episode reward: [(0, '1004.755')]
[36m[2025-06-29 22:58:42,587][187912] Fps is (10 sec: 408.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6955008. Throughput: 0: 357.4. Samples: 6955280. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:58:42,588][187912] Avg episode reward: [(0, '972.130')]
[36m[2025-06-29 22:58:47,611][187912] Fps is (10 sec: 408.2, 60 sec: 341.1, 300 sec: 347.1). Total num frames: 6955008. Throughput: 0: 358.1. Samples: 6957600. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:58:47,611][187912] Avg episode reward: [(0, '963.255')]
[36m[2025-06-29 22:58:52,590][187912] Fps is (10 sec: 409.5, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 6959104. Throughput: 0: 358.0. Samples: 6959680. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:58:52,590][187912] Avg episode reward: [(0, '947.681')]
[36m[2025-06-29 22:58:57,576][187912] Fps is (10 sec: 411.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6959104. Throughput: 0: 357.4. Samples: 6960864. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:58:57,577][187912] Avg episode reward: [(0, '965.163')]
[36m[2025-06-29 22:59:02,571][187912] Fps is (10 sec: 410.4, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 6963200. Throughput: 0: 360.2. Samples: 6963056. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:59:02,571][187912] Avg episode reward: [(0, '968.089')]
[36m[2025-06-29 22:59:07,609][187912] Fps is (10 sec: 408.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6963200. Throughput: 0: 355.9. Samples: 6965072. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:59:07,610][187912] Avg episode reward: [(0, '993.738')]
[36m[2025-06-29 22:59:12,593][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 6963200. Throughput: 0: 352.8. Samples: 6966112. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:59:12,594][187912] Avg episode reward: [(0, '1023.235')]
[36m[2025-06-29 22:59:17,575][187912] Fps is (10 sec: 411.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6967296. Throughput: 0: 354.0. Samples: 6968272. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:59:17,575][187912] Avg episode reward: [(0, '1007.418')]
[36m[2025-06-29 22:59:22,579][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 6967296. Throughput: 0: 354.1. Samples: 6970416. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 22:59:22,579][187912] Avg episode reward: [(0, '980.052')]
[36m[2025-06-29 22:59:27,566][187912] Fps is (10 sec: 410.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 6971392. Throughput: 0: 358.9. Samples: 6971424. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:59:27,566][187912] Avg episode reward: [(0, '937.332')]
[36m[2025-06-29 22:59:32,561][187912] Fps is (10 sec: 410.3, 60 sec: 341.3, 300 sec: 347.9). Total num frames: 6971392. Throughput: 0: 358.1. Samples: 6973696. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:59:32,562][187912] Avg episode reward: [(0, '990.695')]
[36m[2025-06-29 22:59:37,567][187912] Fps is (10 sec: 409.5, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 6975488. Throughput: 0: 356.1. Samples: 6975696. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:59:37,567][187912] Avg episode reward: [(0, '979.413')]
[36m[2025-06-29 22:59:42,592][187912] Fps is (10 sec: 408.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 6975488. Throughput: 0: 356.1. Samples: 6976896. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:59:42,592][187912] Avg episode reward: [(0, '947.327')]
[36m[2025-06-29 22:59:47,979][187912] Fps is (10 sec: 393.4, 60 sec: 407.1, 300 sec: 360.5). Total num frames: 6979584. Throughput: 0: 355.9. Samples: 6979216. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:59:47,979][187912] Avg episode reward: [(0, '964.434')]
[36m[2025-06-29 22:59:52,577][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6979584. Throughput: 0: 358.3. Samples: 6981184. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:59:52,577][187912] Avg episode reward: [(0, '988.952')]
[36m[2025-06-29 22:59:57,565][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 6979584. Throughput: 0: 361.8. Samples: 6982384. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 22:59:57,565][187912] Avg episode reward: [(0, '999.205')]
[36m[2025-06-29 23:00:02,568][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 6983680. Throughput: 0: 359.9. Samples: 6984464. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:00:02,568][187912] Avg episode reward: [(0, '983.158')]
[36m[2025-06-29 23:00:07,591][187912] Fps is (10 sec: 408.5, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 6983680. Throughput: 0: 365.1. Samples: 6986848. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:00:07,591][187912] Avg episode reward: [(0, '1009.861')]
[36m[2025-06-29 23:00:12,594][187912] Fps is (10 sec: 408.5, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6987776. Throughput: 0: 363.9. Samples: 6987808. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:00:12,594][187912] Avg episode reward: [(0, '1063.562')]
[37m[1m[2025-06-29 23:00:12,649][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000027296_6987776.pth...
[36m[2025-06-29 23:00:12,719][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000026960_6901760.pth
[31m[19570605 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19570606 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[19570606 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:00:17,594][187912] Fps is (10 sec: 409.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6987776. Throughput: 0: 360.3. Samples: 6989920. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:00:17,594][187912] Avg episode reward: [(0, '1040.828')]
[36m[2025-06-29 23:00:22,583][187912] Fps is (10 sec: 410.0, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 6991872. Throughput: 0: 360.1. Samples: 6991904. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:00:22,583][187912] Avg episode reward: [(0, '985.355')]
[36m[2025-06-29 23:00:27,585][187912] Fps is (10 sec: 410.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6991872. Throughput: 0: 361.0. Samples: 6993136. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:00:27,585][187912] Avg episode reward: [(0, '978.778')]
[36m[2025-06-29 23:00:33,124][187912] Fps is (10 sec: 388.6, 60 sec: 405.8, 300 sec: 360.4). Total num frames: 6995968. Throughput: 0: 360.4. Samples: 6995488. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 23:00:33,124][187912] Avg episode reward: [(0, '972.884')]
[36m[2025-06-29 23:00:37,590][187912] Fps is (10 sec: 409.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 6995968. Throughput: 0: 363.3. Samples: 6997536. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 23:00:37,590][187912] Avg episode reward: [(0, '966.996')]
[36m[2025-06-29 23:00:42,572][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.2). Total num frames: 6995968. Throughput: 0: 362.6. Samples: 6998704. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 23:00:42,572][187912] Avg episode reward: [(0, '972.333')]
[36m[2025-06-29 23:00:47,569][187912] Fps is (10 sec: 410.5, 60 sec: 343.7, 300 sec: 361.0). Total num frames: 7000064. Throughput: 0: 359.8. Samples: 7000656. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 23:00:47,569][187912] Avg episode reward: [(0, '986.546')]
[36m[2025-06-29 23:00:52,571][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7000064. Throughput: 0: 359.6. Samples: 7003024. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 23:00:52,571][187912] Avg episode reward: [(0, '945.944')]
[36m[2025-06-29 23:00:57,563][187912] Fps is (10 sec: 409.8, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7004160. Throughput: 0: 364.3. Samples: 7004192. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 23:00:57,564][187912] Avg episode reward: [(0, '1009.788')]
[33m[19614497 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[19614497 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.78271484375
[33mCrash Rate: 0.15625
[33mTimeout Rate: 0.06103515625 (navigation_task.py:265)
[33m[19614497 ms][navigation_task] - WARNING : 
[33mSuccesses: 1603
[33mCrashes : 320
[33mTimeouts: 125 (navigation_task.py:268)
[36m[2025-06-29 23:01:02,561][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7004160. Throughput: 0: 365.1. Samples: 7006336. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 23:01:02,561][187912] Avg episode reward: [(0, '988.409')]
[36m[2025-06-29 23:01:07,575][187912] Fps is (10 sec: 409.1, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 7008256. Throughput: 0: 367.0. Samples: 7008416. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 23:01:07,575][187912] Avg episode reward: [(0, '946.987')]
[36m[2025-06-29 23:01:12,584][187912] Fps is (10 sec: 408.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7008256. Throughput: 0: 364.1. Samples: 7009520. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 23:01:12,585][187912] Avg episode reward: [(0, '987.175')]
[36m[2025-06-29 23:01:17,566][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.2). Total num frames: 7008256. Throughput: 0: 365.4. Samples: 7011728. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 23:01:17,566][187912] Avg episode reward: [(0, '1018.996')]
[36m[2025-06-29 23:01:22,562][187912] Fps is (10 sec: 410.5, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7012352. Throughput: 0: 360.8. Samples: 7013760. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:01:22,562][187912] Avg episode reward: [(0, '966.724')]
[36m[2025-06-29 23:01:27,603][187912] Fps is (10 sec: 408.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7012352. Throughput: 0: 359.9. Samples: 7014912. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:01:27,603][187912] Avg episode reward: [(0, '994.615')]
[36m[2025-06-29 23:01:32,600][187912] Fps is (10 sec: 408.0, 60 sec: 344.3, 300 sec: 361.0). Total num frames: 7016448. Throughput: 0: 362.1. Samples: 7016960. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:01:32,601][187912] Avg episode reward: [(0, '1012.537')]
[36m[2025-06-29 23:01:37,569][187912] Fps is (10 sec: 411.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7016448. Throughput: 0: 361.6. Samples: 7019296. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:01:37,569][187912] Avg episode reward: [(0, '1010.567')]
[36m[2025-06-29 23:01:42,559][187912] Fps is (10 sec: 411.3, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 7020544. Throughput: 0: 360.9. Samples: 7020432. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:01:42,565][187912] Avg episode reward: [(0, '1024.968')]
[36m[2025-06-29 23:01:47,582][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7020544. Throughput: 0: 359.7. Samples: 7022528. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:01:47,582][187912] Avg episode reward: [(0, '1057.978')]
[36m[2025-06-29 23:01:52,592][187912] Fps is (10 sec: 408.3, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 7024640. Throughput: 0: 361.1. Samples: 7024672. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:01:52,592][187912] Avg episode reward: [(0, '986.584')]
[36m[2025-06-29 23:01:57,576][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7024640. Throughput: 0: 362.0. Samples: 7025808. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:01:57,576][187912] Avg episode reward: [(0, '1036.652')]
[36m[2025-06-29 23:02:03,286][187912] Fps is (10 sec: 383.0, 60 sec: 404.7, 300 sec: 374.0). Total num frames: 7028736. Throughput: 0: 359.8. Samples: 7028176. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:02:03,286][187912] Avg episode reward: [(0, '968.860')]
[36m[2025-06-29 23:02:07,569][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7028736. Throughput: 0: 364.0. Samples: 7030144. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:02:07,569][187912] Avg episode reward: [(0, '945.344')]
[36m[2025-06-29 23:02:12,562][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7028736. Throughput: 0: 363.4. Samples: 7031248. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:02:12,562][187912] Avg episode reward: [(0, '904.641')]
[37m[1m[2025-06-29 23:02:12,609][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000027456_7028736.pth...
[36m[2025-06-29 23:02:12,683][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000027120_6942720.pth
[36m[2025-06-29 23:02:17,570][187912] Fps is (10 sec: 409.6, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7032832. Throughput: 0: 361.8. Samples: 7033232. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:02:17,570][187912] Avg episode reward: [(0, '926.550')]
[36m[2025-06-29 23:02:22,573][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7032832. Throughput: 0: 363.3. Samples: 7035648. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:02:22,573][187912] Avg episode reward: [(0, '964.475')]
[36m[2025-06-29 23:02:27,572][187912] Fps is (10 sec: 409.5, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 7036928. Throughput: 0: 362.6. Samples: 7036752. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:02:27,572][187912] Avg episode reward: [(0, '965.073')]
[31m[19704695 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19704695 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[19704696 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:02:32,568][187912] Fps is (10 sec: 409.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7036928. Throughput: 0: 362.1. Samples: 7038816. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:02:32,568][187912] Avg episode reward: [(0, '967.901')]
[36m[2025-06-29 23:02:37,621][187912] Fps is (10 sec: 407.6, 60 sec: 409.2, 300 sec: 361.0). Total num frames: 7041024. Throughput: 0: 341.5. Samples: 7040048. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:02:37,621][187912] Avg episode reward: [(0, '968.716')]
[36m[2025-06-29 23:02:42,565][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7041024. Throughput: 0: 361.3. Samples: 7042064. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:02:42,566][187912] Avg episode reward: [(0, '986.320')]
[36m[2025-06-29 23:02:47,567][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7041024. Throughput: 0: 368.2. Samples: 7044480. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:02:47,567][187912] Avg episode reward: [(0, '926.628')]
[36m[2025-06-29 23:02:52,563][187912] Fps is (10 sec: 409.7, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7045120. Throughput: 0: 363.8. Samples: 7046512. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 23:02:52,563][187912] Avg episode reward: [(0, '1010.396')]
[36m[2025-06-29 23:02:57,562][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7045120. Throughput: 0: 363.0. Samples: 7047584. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 23:02:57,563][187912] Avg episode reward: [(0, '1044.406')]
[31m[19731806 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19731806 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[19731806 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:03:02,588][187912] Fps is (10 sec: 408.6, 60 sec: 345.3, 300 sec: 361.0). Total num frames: 7049216. Throughput: 0: 363.6. Samples: 7049600. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 23:03:02,589][187912] Avg episode reward: [(0, '1018.531')]
[36m[2025-06-29 23:03:07,572][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7049216. Throughput: 0: 359.1. Samples: 7051808. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 23:03:07,572][187912] Avg episode reward: [(0, '985.804')]
[36m[2025-06-29 23:03:12,569][187912] Fps is (10 sec: 410.4, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7053312. Throughput: 0: 359.1. Samples: 7052912. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:03:12,569][187912] Avg episode reward: [(0, '985.883')]
[36m[2025-06-29 23:03:17,589][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7053312. Throughput: 0: 356.1. Samples: 7054848. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:03:17,590][187912] Avg episode reward: [(0, '1002.460')]
[36m[2025-06-29 23:03:22,584][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 7053312. Throughput: 0: 376.5. Samples: 7056976. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:03:22,584][187912] Avg episode reward: [(0, '925.605')]
[36m[2025-06-29 23:03:27,588][187912] Fps is (10 sec: 409.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7057408. Throughput: 0: 350.8. Samples: 7057856. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:03:27,589][187912] Avg episode reward: [(0, '1017.883')]
[36m[2025-06-29 23:03:32,563][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7057408. Throughput: 0: 350.6. Samples: 7060256. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:03:32,563][187912] Avg episode reward: [(0, '1050.144')]
[36m[2025-06-29 23:03:37,576][187912] Fps is (10 sec: 410.1, 60 sec: 341.6, 300 sec: 361.0). Total num frames: 7061504. Throughput: 0: 348.0. Samples: 7062176. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 23:03:37,576][187912] Avg episode reward: [(0, '1079.970')]
[36m[2025-06-29 23:03:42,576][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7061504. Throughput: 0: 350.5. Samples: 7063360. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 23:03:42,577][187912] Avg episode reward: [(0, '982.293')]
[31m[19778046 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19778047 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[19778047 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:03:47,594][187912] Fps is (10 sec: 408.9, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 7065600. Throughput: 0: 351.2. Samples: 7065408. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 23:03:47,594][187912] Avg episode reward: [(0, '979.480')]
[31m[19785232 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19785232 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[19785233 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:03:52,570][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7065600. Throughput: 0: 346.0. Samples: 7067376. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 23:03:52,570][187912] Avg episode reward: [(0, '992.033')]
[36m[2025-06-29 23:03:57,567][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 7065600. Throughput: 0: 343.8. Samples: 7068384. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 23:03:57,567][187912] Avg episode reward: [(0, '994.883')]
[36m[2025-06-29 23:04:02,593][187912] Fps is (10 sec: 408.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7069696. Throughput: 0: 341.3. Samples: 7070208. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 23:04:02,593][187912] Avg episode reward: [(0, '973.295')]
[36m[2025-06-29 23:04:07,570][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7069696. Throughput: 0: 347.8. Samples: 7072624. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 23:04:07,570][187912] Avg episode reward: [(0, '1011.933')]
[36m[2025-06-29 23:04:12,589][187912] Fps is (10 sec: 409.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7073792. Throughput: 0: 353.1. Samples: 7073744. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 23:04:12,590][187912] Avg episode reward: [(0, '1000.650')]
[37m[1m[2025-06-29 23:04:12,629][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000027632_7073792.pth...
[36m[2025-06-29 23:04:12,699][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000027296_6987776.pth
[36m[2025-06-29 23:04:17,565][187912] Fps is (10 sec: 409.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7073792. Throughput: 0: 345.9. Samples: 7075824. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 23:04:17,565][187912] Avg episode reward: [(0, '1044.050')]
[36m[2025-06-29 23:04:22,601][187912] Fps is (10 sec: 409.1, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 7077888. Throughput: 0: 349.7. Samples: 7077920. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:04:22,602][187912] Avg episode reward: [(0, '1002.245')]
[36m[2025-06-29 23:04:27,568][187912] Fps is (10 sec: 409.5, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7077888. Throughput: 0: 348.5. Samples: 7079040. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:04:27,568][187912] Avg episode reward: [(0, '918.538')]
[36m[2025-06-29 23:04:32,601][187912] Fps is (10 sec: 0.0, 60 sec: 341.1, 300 sec: 347.1). Total num frames: 7077888. Throughput: 0: 354.4. Samples: 7081360. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:04:32,601][187912] Avg episode reward: [(0, '963.159')]
[36m[2025-06-29 23:04:37,567][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7081984. Throughput: 0: 358.4. Samples: 7083504. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:04:37,568][187912] Avg episode reward: [(0, '1002.205')]
[36m[2025-06-29 23:04:42,576][187912] Fps is (10 sec: 410.6, 60 sec: 341.3, 300 sec: 347.6). Total num frames: 7081984. Throughput: 0: 361.2. Samples: 7084640. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:04:42,576][187912] Avg episode reward: [(0, '927.942')]
[36m[2025-06-29 23:04:47,590][187912] Fps is (10 sec: 408.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7086080. Throughput: 0: 365.9. Samples: 7086672. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 23:04:47,590][187912] Avg episode reward: [(0, '930.048')]
[36m[2025-06-29 23:04:52,596][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7086080. Throughput: 0: 367.4. Samples: 7089168. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 23:04:52,596][187912] Avg episode reward: [(0, '950.384')]
[36m[2025-06-29 23:04:57,560][187912] Fps is (10 sec: 410.8, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7090176. Throughput: 0: 366.1. Samples: 7090208. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 23:04:57,560][187912] Avg episode reward: [(0, '918.605')]
[31m[19855069 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19855069 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[19855069 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:05:02,574][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7090176. Throughput: 0: 366.2. Samples: 7092304. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 23:05:02,574][187912] Avg episode reward: [(0, '880.599')]
[36m[2025-06-29 23:05:07,580][187912] Fps is (10 sec: 408.8, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 7094272. Throughput: 0: 365.3. Samples: 7094352. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:05:07,580][187912] Avg episode reward: [(0, '923.656')]
[36m[2025-06-29 23:05:12,562][187912] Fps is (10 sec: 410.1, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7094272. Throughput: 0: 368.0. Samples: 7095600. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:05:12,563][187912] Avg episode reward: [(0, '1036.674')]
[36m[2025-06-29 23:05:18,044][187912] Fps is (10 sec: 391.4, 60 sec: 406.4, 300 sec: 360.4). Total num frames: 7098368. Throughput: 0: 364.1. Samples: 7097904. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:05:18,044][187912] Avg episode reward: [(0, '963.682')]
[36m[2025-06-29 23:05:22,575][187912] Fps is (10 sec: 409.1, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7098368. Throughput: 0: 367.9. Samples: 7100064. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:05:22,576][187912] Avg episode reward: [(0, '983.062')]
[36m[2025-06-29 23:05:27,562][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.8). Total num frames: 7098368. Throughput: 0: 366.3. Samples: 7101120. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:05:27,562][187912] Avg episode reward: [(0, '941.969')]
[36m[2025-06-29 23:05:32,585][187912] Fps is (10 sec: 409.2, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 7102464. Throughput: 0: 363.8. Samples: 7103040. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 23:05:32,585][187912] Avg episode reward: [(0, '941.465')]
[36m[2025-06-29 23:05:37,562][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7102464. Throughput: 0: 361.2. Samples: 7105408. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 23:05:37,562][187912] Avg episode reward: [(0, '851.812')]
[31m[19894472 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19894472 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([12], device='cuda:0') (navigation_task.py:196)
[31m[19894472 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:05:42,559][187912] Fps is (10 sec: 410.6, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 7106560. Throughput: 0: 362.7. Samples: 7106528. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 23:05:42,560][187912] Avg episode reward: [(0, '907.912')]
[36m[2025-06-29 23:05:47,562][187912] Fps is (10 sec: 409.6, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7106560. Throughput: 0: 362.4. Samples: 7108608. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 23:05:47,562][187912] Avg episode reward: [(0, '1020.702')]
[36m[2025-06-29 23:05:52,578][187912] Fps is (10 sec: 408.8, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 7110656. Throughput: 0: 363.4. Samples: 7110704. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:05:52,578][187912] Avg episode reward: [(0, '985.880')]
[36m[2025-06-29 23:05:57,586][187912] Fps is (10 sec: 408.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7110656. Throughput: 0: 362.5. Samples: 7111920. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:05:57,586][187912] Avg episode reward: [(0, '1049.359')]
[36m[2025-06-29 23:06:03,256][187912] Fps is (10 sec: 383.6, 60 sec: 405.0, 300 sec: 360.2). Total num frames: 7114752. Throughput: 0: 360.6. Samples: 7114208. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:06:03,257][187912] Avg episode reward: [(0, '1122.326')]
[36m[2025-06-29 23:06:07,584][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7114752. Throughput: 0: 361.2. Samples: 7116320. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:06:07,584][187912] Avg episode reward: [(0, '1102.325')]
[36m[2025-06-29 23:06:12,595][187912] Fps is (10 sec: 0.0, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 7114752. Throughput: 0: 363.5. Samples: 7117488. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:06:12,595][187912] Avg episode reward: [(0, '993.175')]
[37m[1m[2025-06-29 23:06:12,639][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000027792_7114752.pth...
[36m[2025-06-29 23:06:12,709][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000027456_7028736.pth
[36m[2025-06-29 23:06:17,577][187912] Fps is (10 sec: 409.9, 60 sec: 344.0, 300 sec: 361.0). Total num frames: 7118848. Throughput: 0: 367.0. Samples: 7119552. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:06:17,577][187912] Avg episode reward: [(0, '969.337')]
[36m[2025-06-29 23:06:22,595][187912] Fps is (10 sec: 409.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7118848. Throughput: 0: 365.2. Samples: 7121856. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:06:22,595][187912] Avg episode reward: [(0, '904.682')]
[36m[2025-06-29 23:06:27,558][187912] Fps is (10 sec: 410.4, 60 sec: 409.6, 300 sec: 361.1). Total num frames: 7122944. Throughput: 0: 365.5. Samples: 7122976. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:06:27,558][187912] Avg episode reward: [(0, '942.268')]
[36m[2025-06-29 23:06:32,564][187912] Fps is (10 sec: 410.9, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7122944. Throughput: 0: 367.6. Samples: 7125152. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:06:32,564][187912] Avg episode reward: [(0, '858.212')]
[36m[2025-06-29 23:06:37,586][187912] Fps is (10 sec: 408.5, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 7127040. Throughput: 0: 363.7. Samples: 7127072. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:06:37,586][187912] Avg episode reward: [(0, '892.213')]
[36m[2025-06-29 23:06:42,565][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7127040. Throughput: 0: 360.0. Samples: 7128112. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:06:42,565][187912] Avg episode reward: [(0, '964.681')]
[36m[2025-06-29 23:06:47,581][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 7127040. Throughput: 0: 367.1. Samples: 7130480. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:06:47,581][187912] Avg episode reward: [(0, '1002.269')]
[36m[2025-06-29 23:06:52,566][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7131136. Throughput: 0: 363.9. Samples: 7132688. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:06:52,567][187912] Avg episode reward: [(0, '984.653')]
[36m[2025-06-29 23:06:57,592][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 347.9). Total num frames: 7131136. Throughput: 0: 362.3. Samples: 7133792. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:06:57,593][187912] Avg episode reward: [(0, '1012.576')]
[31m[19975164 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19975164 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([12], device='cuda:0') (navigation_task.py:196)
[31m[19975164 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:07:02,581][187912] Fps is (10 sec: 409.0, 60 sec: 345.2, 300 sec: 361.0). Total num frames: 7135232. Throughput: 0: 362.6. Samples: 7135872. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 23:07:02,581][187912] Avg episode reward: [(0, '958.676')]
[36m[2025-06-29 23:07:07,579][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7135232. Throughput: 0: 361.7. Samples: 7138128. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 23:07:07,579][187912] Avg episode reward: [(0, '1003.386')]
[31m[19984335 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[19984335 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[19984335 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:07:12,589][187912] Fps is (10 sec: 409.3, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7139328. Throughput: 0: 361.7. Samples: 7139264. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 23:07:12,589][187912] Avg episode reward: [(0, '954.844')]
[36m[2025-06-29 23:07:17,582][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7139328. Throughput: 0: 361.1. Samples: 7141408. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 23:07:17,582][187912] Avg episode reward: [(0, '909.776')]
[36m[2025-06-29 23:07:22,586][187912] Fps is (10 sec: 409.7, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 7143424. Throughput: 0: 364.1. Samples: 7143456. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 23:07:22,586][187912] Avg episode reward: [(0, '1013.156')]
[36m[2025-06-29 23:07:27,590][187912] Fps is (10 sec: 409.3, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 7143424. Throughput: 0: 366.0. Samples: 7144592. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 23:07:27,591][187912] Avg episode reward: [(0, '1020.906')]
[36m[2025-06-29 23:07:32,576][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.2). Total num frames: 7143424. Throughput: 0: 360.9. Samples: 7146720. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 23:07:32,576][187912] Avg episode reward: [(0, '1048.153')]
[36m[2025-06-29 23:07:37,568][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7147520. Throughput: 0: 359.1. Samples: 7148848. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 23:07:37,568][187912] Avg episode reward: [(0, '1045.452')]
[36m[2025-06-29 23:07:42,588][187912] Fps is (10 sec: 409.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7147520. Throughput: 0: 360.2. Samples: 7150000. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 23:07:42,589][187912] Avg episode reward: [(0, '997.631')]
[36m[2025-06-29 23:07:47,594][187912] Fps is (10 sec: 408.5, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 7151616. Throughput: 0: 359.0. Samples: 7152032. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 23:07:47,594][187912] Avg episode reward: [(0, '947.131')]
[36m[2025-06-29 23:07:52,571][187912] Fps is (10 sec: 410.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7151616. Throughput: 0: 362.0. Samples: 7154416. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 23:07:52,571][187912] Avg episode reward: [(0, '983.166')]
[36m[2025-06-29 23:07:57,589][187912] Fps is (10 sec: 409.8, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7155712. Throughput: 0: 360.2. Samples: 7155472. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 23:07:57,589][187912] Avg episode reward: [(0, '946.331')]
[36m[2025-06-29 23:08:02,586][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7155712. Throughput: 0: 358.7. Samples: 7157552. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 23:08:02,587][187912] Avg episode reward: [(0, '993.031')]
[36m[2025-06-29 23:08:07,758][187912] Fps is (10 sec: 402.8, 60 sec: 408.4, 300 sec: 360.8). Total num frames: 7159808. Throughput: 0: 338.6. Samples: 7158752. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:08:07,758][187912] Avg episode reward: [(0, '1050.598')]
[36m[2025-06-29 23:08:12,559][187912] Fps is (10 sec: 410.7, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7159808. Throughput: 0: 360.1. Samples: 7160784. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:08:12,559][187912] Avg episode reward: [(0, '1034.575')]
[37m[1m[2025-06-29 23:08:12,602][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000027968_7159808.pth...
[36m[2025-06-29 23:08:12,671][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000027632_7073792.pth
[36m[2025-06-29 23:08:17,583][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7159808. Throughput: 0: 359.8. Samples: 7162912. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:08:17,583][187912] Avg episode reward: [(0, '1008.037')]
[36m[2025-06-29 23:08:22,587][187912] Fps is (10 sec: 408.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7163904. Throughput: 0: 356.8. Samples: 7164912. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:08:22,587][187912] Avg episode reward: [(0, '989.147')]
[36m[2025-06-29 23:08:27,566][187912] Fps is (10 sec: 410.3, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7163904. Throughput: 0: 356.4. Samples: 7166032. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:08:27,567][187912] Avg episode reward: [(0, '995.847')]
[36m[2025-06-29 23:08:32,578][187912] Fps is (10 sec: 409.9, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7168000. Throughput: 0: 355.7. Samples: 7168032. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:08:32,578][187912] Avg episode reward: [(0, '972.605')]
[36m[2025-06-29 23:08:37,595][187912] Fps is (10 sec: 408.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7168000. Throughput: 0: 354.7. Samples: 7170384. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:08:37,595][187912] Avg episode reward: [(0, '953.616')]
[36m[2025-06-29 23:08:42,670][187912] Fps is (10 sec: 405.9, 60 sec: 409.0, 300 sec: 360.9). Total num frames: 7172096. Throughput: 0: 354.6. Samples: 7171456. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:08:42,670][187912] Avg episode reward: [(0, '849.106')]
[36m[2025-06-29 23:08:47,564][187912] Fps is (10 sec: 410.9, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7172096. Throughput: 0: 354.7. Samples: 7173504. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:08:47,564][187912] Avg episode reward: [(0, '908.055')]
[36m[2025-06-29 23:08:52,586][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7172096. Throughput: 0: 381.9. Samples: 7175872. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:08:52,586][187912] Avg episode reward: [(0, '869.001')]
[36m[2025-06-29 23:08:57,588][187912] Fps is (10 sec: 408.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7176192. Throughput: 0: 354.3. Samples: 7176736. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:08:57,588][187912] Avg episode reward: [(0, '928.009')]
[36m[2025-06-29 23:09:02,571][187912] Fps is (10 sec: 410.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7176192. Throughput: 0: 357.4. Samples: 7178992. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:09:02,571][187912] Avg episode reward: [(0, '936.870')]
[36m[2025-06-29 23:09:07,569][187912] Fps is (10 sec: 410.4, 60 sec: 342.4, 300 sec: 361.0). Total num frames: 7180288. Throughput: 0: 359.3. Samples: 7181072. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:09:07,569][187912] Avg episode reward: [(0, '1039.444')]
[31m[20102875 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20102875 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[20102876 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:09:12,580][187912] Fps is (10 sec: 409.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7180288. Throughput: 0: 359.4. Samples: 7182208. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:09:12,581][187912] Avg episode reward: [(0, '1011.929')]
[31m[20107518 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20107518 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[20107518 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:09:17,580][187912] Fps is (10 sec: 409.2, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7184384. Throughput: 0: 364.1. Samples: 7184416. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 23:09:17,580][187912] Avg episode reward: [(0, '1000.802')]
[36m[2025-06-29 23:09:22,588][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7184384. Throughput: 0: 358.5. Samples: 7186512. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 23:09:22,588][187912] Avg episode reward: [(0, '949.142')]
[36m[2025-06-29 23:09:28,093][187912] Fps is (10 sec: 389.6, 60 sec: 406.0, 300 sec: 374.3). Total num frames: 7188480. Throughput: 0: 356.8. Samples: 7187664. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 23:09:28,093][187912] Avg episode reward: [(0, '876.408')]
[31m[20123670 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20123670 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[20123670 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:09:32,590][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7188480. Throughput: 0: 359.3. Samples: 7189680. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 23:09:32,590][187912] Avg episode reward: [(0, '798.147')]
[36m[2025-06-29 23:09:37,577][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7188480. Throughput: 0: 357.4. Samples: 7191952. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 23:09:37,577][187912] Avg episode reward: [(0, '754.171')]
[33m[20135074 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[20135075 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.7666015625
[33mCrash Rate: 0.1748046875
[33mTimeout Rate: 0.05859375 (navigation_task.py:265)
[33m[20135075 ms][navigation_task] - WARNING : 
[33mSuccesses: 1570
[33mCrashes : 358
[33mTimeouts: 120 (navigation_task.py:268)
[36m[2025-06-29 23:09:42,587][187912] Fps is (10 sec: 409.7, 60 sec: 341.8, 300 sec: 361.0). Total num frames: 7192576. Throughput: 0: 356.6. Samples: 7192784. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:09:42,587][187912] Avg episode reward: [(0, '801.859')]
[36m[2025-06-29 23:09:47,592][187912] Fps is (10 sec: 409.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7192576. Throughput: 0: 360.7. Samples: 7195232. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:09:47,593][187912] Avg episode reward: [(0, '779.481')]
[36m[2025-06-29 23:09:52,571][187912] Fps is (10 sec: 410.3, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 7196672. Throughput: 0: 357.7. Samples: 7197168. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:09:52,571][187912] Avg episode reward: [(0, '804.507')]
[36m[2025-06-29 23:09:57,584][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7196672. Throughput: 0: 358.4. Samples: 7198336. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:09:57,584][187912] Avg episode reward: [(0, '853.016')]
[36m[2025-06-29 23:10:02,561][187912] Fps is (10 sec: 410.0, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 7200768. Throughput: 0: 359.6. Samples: 7200592. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:10:02,561][187912] Avg episode reward: [(0, '891.464')]
[36m[2025-06-29 23:10:07,579][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7200768. Throughput: 0: 360.2. Samples: 7202720. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:10:07,579][187912] Avg episode reward: [(0, '936.808')]
[36m[2025-06-29 23:10:12,576][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.7). Total num frames: 7200768. Throughput: 0: 363.6. Samples: 7203840. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:10:12,576][187912] Avg episode reward: [(0, '952.603')]
[37m[1m[2025-06-29 23:10:12,616][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000028128_7200768.pth...
[36m[2025-06-29 23:10:12,686][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000027792_7114752.pth
[36m[2025-06-29 23:10:17,568][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7204864. Throughput: 0: 362.1. Samples: 7205968. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:10:17,569][187912] Avg episode reward: [(0, '939.295')]
[31m[20172636 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20172636 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[20172637 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:10:22,559][187912] Fps is (10 sec: 410.3, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7204864. Throughput: 0: 362.1. Samples: 7208240. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:10:22,559][187912] Avg episode reward: [(0, '902.090')]
[36m[2025-06-29 23:10:27,592][187912] Fps is (10 sec: 408.6, 60 sec: 344.2, 300 sec: 361.0). Total num frames: 7208960. Throughput: 0: 362.6. Samples: 7209104. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:10:27,592][187912] Avg episode reward: [(0, '897.310')]
[36m[2025-06-29 23:10:32,572][187912] Fps is (10 sec: 409.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7208960. Throughput: 0: 359.3. Samples: 7211392. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:10:32,572][187912] Avg episode reward: [(0, '906.914')]
[36m[2025-06-29 23:10:37,578][187912] Fps is (10 sec: 410.2, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7213056. Throughput: 0: 360.5. Samples: 7213392. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:10:37,578][187912] Avg episode reward: [(0, '900.396')]
[31m[20195212 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20195212 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[20195212 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:10:42,569][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7213056. Throughput: 0: 361.4. Samples: 7214592. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:10:42,569][187912] Avg episode reward: [(0, '902.479')]
[36m[2025-06-29 23:10:47,857][187912] Fps is (10 sec: 398.5, 60 sec: 407.8, 300 sec: 360.7). Total num frames: 7217152. Throughput: 0: 357.5. Samples: 7216784. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 23:10:47,857][187912] Avg episode reward: [(0, '895.915')]
[31m[20203878 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20203878 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[20203878 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:10:52,561][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7217152. Throughput: 0: 356.4. Samples: 7218752. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 23:10:52,561][187912] Avg episode reward: [(0, '908.541')]
[36m[2025-06-29 23:10:57,565][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.9). Total num frames: 7217152. Throughput: 0: 355.6. Samples: 7219840. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 23:10:57,565][187912] Avg episode reward: [(0, '896.772')]
[36m[2025-06-29 23:11:02,587][187912] Fps is (10 sec: 408.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7221248. Throughput: 0: 349.7. Samples: 7221712. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 23:11:02,587][187912] Avg episode reward: [(0, '845.037')]
[36m[2025-06-29 23:11:07,593][187912] Fps is (10 sec: 408.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7221248. Throughput: 0: 351.4. Samples: 7224064. Policy #0 lag: (min: 13.0, avg: 13.0, max: 29.0)
[36m[2025-06-29 23:11:07,593][187912] Avg episode reward: [(0, '837.428')]
[36m[2025-06-29 23:11:12,578][187912] Fps is (10 sec: 410.0, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7225344. Throughput: 0: 359.9. Samples: 7225296. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 23:11:12,578][187912] Avg episode reward: [(0, '866.124')]
[36m[2025-06-29 23:11:17,590][187912] Fps is (10 sec: 409.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7225344. Throughput: 0: 355.4. Samples: 7227392. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 23:11:17,590][187912] Avg episode reward: [(0, '846.247')]
[36m[2025-06-29 23:11:22,563][187912] Fps is (10 sec: 410.2, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7229440. Throughput: 0: 357.4. Samples: 7229472. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 23:11:22,563][187912] Avg episode reward: [(0, '878.784')]
[36m[2025-06-29 23:11:27,595][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7229440. Throughput: 0: 356.4. Samples: 7230640. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 23:11:27,595][187912] Avg episode reward: [(0, '1014.379')]
[36m[2025-06-29 23:11:32,568][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 7229440. Throughput: 0: 360.0. Samples: 7232880. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 23:11:32,568][187912] Avg episode reward: [(0, '1019.469')]
[31m[20249083 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20249083 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[20249084 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:11:37,582][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7233536. Throughput: 0: 358.2. Samples: 7234880. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 23:11:37,583][187912] Avg episode reward: [(0, '958.596')]
[36m[2025-06-29 23:11:42,618][187912] Fps is (10 sec: 407.6, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 7233536. Throughput: 0: 360.5. Samples: 7236080. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 23:11:42,618][187912] Avg episode reward: [(0, '964.465')]
[36m[2025-06-29 23:11:47,571][187912] Fps is (10 sec: 410.1, 60 sec: 343.0, 300 sec: 361.0). Total num frames: 7237632. Throughput: 0: 364.9. Samples: 7238128. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 23:11:47,571][187912] Avg episode reward: [(0, '935.834')]
[36m[2025-06-29 23:11:52,560][187912] Fps is (10 sec: 412.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7237632. Throughput: 0: 364.7. Samples: 7240464. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 23:11:52,560][187912] Avg episode reward: [(0, '793.376')]
[36m[2025-06-29 23:11:57,568][187912] Fps is (10 sec: 409.7, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7241728. Throughput: 0: 363.1. Samples: 7241632. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:11:57,569][187912] Avg episode reward: [(0, '805.353')]
[36m[2025-06-29 23:12:02,579][187912] Fps is (10 sec: 408.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7241728. Throughput: 0: 359.9. Samples: 7243584. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:12:02,579][187912] Avg episode reward: [(0, '900.746')]
[36m[2025-06-29 23:12:07,856][187912] Fps is (10 sec: 398.1, 60 sec: 407.8, 300 sec: 360.7). Total num frames: 7245824. Throughput: 0: 361.4. Samples: 7245840. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:12:07,857][187912] Avg episode reward: [(0, '889.159')]
[36m[2025-06-29 23:12:12,574][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7245824. Throughput: 0: 359.3. Samples: 7246800. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:12:12,574][187912] Avg episode reward: [(0, '947.819')]
[37m[1m[2025-06-29 23:12:12,614][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000028304_7245824.pth...
[36m[2025-06-29 23:12:12,687][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000027968_7159808.pth
[36m[2025-06-29 23:12:17,563][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 7245824. Throughput: 0: 357.4. Samples: 7248960. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:12:17,564][187912] Avg episode reward: [(0, '1031.761')]
[36m[2025-06-29 23:12:22,587][187912] Fps is (10 sec: 409.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7249920. Throughput: 0: 357.7. Samples: 7250976. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 23:12:22,587][187912] Avg episode reward: [(0, '1093.987')]
[36m[2025-06-29 23:12:27,569][187912] Fps is (10 sec: 409.4, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7249920. Throughput: 0: 358.1. Samples: 7252176. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 23:12:27,569][187912] Avg episode reward: [(0, '1053.542')]
[36m[2025-06-29 23:12:32,588][187912] Fps is (10 sec: 409.6, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 7254016. Throughput: 0: 354.4. Samples: 7254080. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 23:12:32,588][187912] Avg episode reward: [(0, '1081.435')]
[36m[2025-06-29 23:12:37,606][187912] Fps is (10 sec: 408.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7254016. Throughput: 0: 353.1. Samples: 7256368. Policy #0 lag: (min: 1.0, avg: 1.0, max: 17.0)
[36m[2025-06-29 23:12:37,606][187912] Avg episode reward: [(0, '1050.928')]
[36m[2025-06-29 23:12:42,933][187912] Fps is (10 sec: 395.9, 60 sec: 407.5, 300 sec: 360.6). Total num frames: 7258112. Throughput: 0: 347.4. Samples: 7257392. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 23:12:42,933][187912] Avg episode reward: [(0, '962.547')]
[36m[2025-06-29 23:12:47,570][187912] Fps is (10 sec: 411.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7258112. Throughput: 0: 350.7. Samples: 7259360. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 23:12:47,570][187912] Avg episode reward: [(0, '848.078')]
[36m[2025-06-29 23:12:52,589][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 7258112. Throughput: 0: 353.7. Samples: 7261664. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 23:12:52,589][187912] Avg episode reward: [(0, '911.744')]
[36m[2025-06-29 23:12:57,580][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7262208. Throughput: 0: 350.5. Samples: 7262576. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 23:12:57,581][187912] Avg episode reward: [(0, '857.694')]
[36m[2025-06-29 23:13:02,582][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 347.3). Total num frames: 7262208. Throughput: 0: 354.7. Samples: 7264928. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 23:13:02,583][187912] Avg episode reward: [(0, '872.339')]
[36m[2025-06-29 23:13:07,590][187912] Fps is (10 sec: 409.2, 60 sec: 342.9, 300 sec: 361.0). Total num frames: 7266304. Throughput: 0: 356.6. Samples: 7267024. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:13:07,591][187912] Avg episode reward: [(0, '935.685')]
[36m[2025-06-29 23:13:12,572][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7266304. Throughput: 0: 355.2. Samples: 7268160. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:13:12,572][187912] Avg episode reward: [(0, '957.640')]
[36m[2025-06-29 23:13:17,562][187912] Fps is (10 sec: 410.8, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7270400. Throughput: 0: 363.2. Samples: 7270416. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:13:17,562][187912] Avg episode reward: [(0, '965.421')]
[36m[2025-06-29 23:13:22,563][187912] Fps is (10 sec: 410.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7270400. Throughput: 0: 354.8. Samples: 7272320. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:13:22,563][187912] Avg episode reward: [(0, '1009.216')]
[36m[2025-06-29 23:13:27,576][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 7270400. Throughput: 0: 360.2. Samples: 7273472. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:13:27,576][187912] Avg episode reward: [(0, '1010.197')]
[36m[2025-06-29 23:13:32,570][187912] Fps is (10 sec: 409.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7274496. Throughput: 0: 359.8. Samples: 7275552. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 23:13:32,570][187912] Avg episode reward: [(0, '948.216')]
[36m[2025-06-29 23:13:37,584][187912] Fps is (10 sec: 409.3, 60 sec: 341.5, 300 sec: 347.2). Total num frames: 7274496. Throughput: 0: 361.6. Samples: 7277936. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 23:13:37,584][187912] Avg episode reward: [(0, '973.683')]
[31m[20375627 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20375627 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[20375627 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:13:42,590][187912] Fps is (10 sec: 408.8, 60 sec: 343.3, 300 sec: 361.0). Total num frames: 7278592. Throughput: 0: 359.4. Samples: 7278752. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 23:13:42,591][187912] Avg episode reward: [(0, '1006.280')]
[31m[20377525 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20377525 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[20377525 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:13:47,574][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7278592. Throughput: 0: 359.2. Samples: 7281088. Policy #0 lag: (min: 0.0, avg: 0.0, max: 16.0)
[36m[2025-06-29 23:13:47,574][187912] Avg episode reward: [(0, '974.791')]
[36m[2025-06-29 23:13:52,587][187912] Fps is (10 sec: 409.8, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7282688. Throughput: 0: 359.5. Samples: 7283200. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:13:52,587][187912] Avg episode reward: [(0, '959.455')]
[36m[2025-06-29 23:13:57,589][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7282688. Throughput: 0: 361.5. Samples: 7284432. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:13:57,589][187912] Avg episode reward: [(0, '993.993')]
[36m[2025-06-29 23:14:02,569][187912] Fps is (10 sec: 410.3, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 7286784. Throughput: 0: 363.3. Samples: 7286768. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:14:02,569][187912] Avg episode reward: [(0, '1019.413')]
[31m[20396745 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20396745 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[20396745 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:14:07,591][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7286784. Throughput: 0: 367.8. Samples: 7288880. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:14:07,591][187912] Avg episode reward: [(0, '959.198')]
[36m[2025-06-29 23:14:13,248][187912] Fps is (10 sec: 383.6, 60 sec: 405.0, 300 sec: 360.2). Total num frames: 7290880. Throughput: 0: 362.2. Samples: 7290016. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:14:13,248][187912] Avg episode reward: [(0, '1073.097')]
[37m[1m[2025-06-29 23:14:13,290][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000028480_7290880.pth...
[36m[2025-06-29 23:14:13,352][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000028128_7200768.pth
[36m[2025-06-29 23:14:17,569][187912] Fps is (10 sec: 410.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7290880. Throughput: 0: 364.5. Samples: 7291952. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:14:17,569][187912] Avg episode reward: [(0, '1029.454')]
[36m[2025-06-29 23:14:22,575][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.7). Total num frames: 7290880. Throughput: 0: 357.8. Samples: 7294032. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:14:22,575][187912] Avg episode reward: [(0, '1048.650')]
[36m[2025-06-29 23:14:27,566][187912] Fps is (10 sec: 409.7, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 7294976. Throughput: 0: 361.4. Samples: 7295008. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:14:27,567][187912] Avg episode reward: [(0, '970.673')]
[36m[2025-06-29 23:14:32,621][187912] Fps is (10 sec: 407.7, 60 sec: 341.0, 300 sec: 360.9). Total num frames: 7294976. Throughput: 0: 357.3. Samples: 7297184. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:14:32,621][187912] Avg episode reward: [(0, '915.586')]
[36m[2025-06-29 23:14:37,574][187912] Fps is (10 sec: 409.3, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 7299072. Throughput: 0: 354.9. Samples: 7299168. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:14:37,574][187912] Avg episode reward: [(0, '928.531')]
[36m[2025-06-29 23:14:42,586][187912] Fps is (10 sec: 411.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7299072. Throughput: 0: 352.0. Samples: 7300272. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:14:42,587][187912] Avg episode reward: [(0, '919.171')]
[36m[2025-06-29 23:14:48,178][187912] Fps is (10 sec: 386.3, 60 sec: 405.5, 300 sec: 360.3). Total num frames: 7303168. Throughput: 0: 348.7. Samples: 7302672. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:14:48,178][187912] Avg episode reward: [(0, '873.474')]
[36m[2025-06-29 23:14:52,574][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7303168. Throughput: 0: 355.0. Samples: 7304848. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:14:52,574][187912] Avg episode reward: [(0, '907.360')]
[36m[2025-06-29 23:14:57,611][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 7303168. Throughput: 0: 357.4. Samples: 7305872. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:14:57,611][187912] Avg episode reward: [(0, '954.349')]
[36m[2025-06-29 23:15:02,603][187912] Fps is (10 sec: 408.4, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 7307264. Throughput: 0: 352.1. Samples: 7307808. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 23:15:02,603][187912] Avg episode reward: [(0, '950.930')]
[36m[2025-06-29 23:15:07,599][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7307264. Throughput: 0: 355.4. Samples: 7310032. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 23:15:07,600][187912] Avg episode reward: [(0, '942.949')]
[36m[2025-06-29 23:15:12,602][187912] Fps is (10 sec: 409.6, 60 sec: 345.0, 300 sec: 361.0). Total num frames: 7311360. Throughput: 0: 359.5. Samples: 7311200. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 23:15:12,602][187912] Avg episode reward: [(0, '911.165')]
[36m[2025-06-29 23:15:17,580][187912] Fps is (10 sec: 410.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7311360. Throughput: 0: 355.9. Samples: 7313184. Policy #0 lag: (min: 3.0, avg: 3.1, max: 19.0)
[36m[2025-06-29 23:15:17,580][187912] Avg episode reward: [(0, '974.064')]
[36m[2025-06-29 23:15:22,760][187912] Fps is (10 sec: 403.2, 60 sec: 408.3, 300 sec: 360.8). Total num frames: 7315456. Throughput: 0: 335.7. Samples: 7314336. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 23:15:22,760][187912] Avg episode reward: [(0, '935.941')]
[36m[2025-06-29 23:15:27,597][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7315456. Throughput: 0: 358.3. Samples: 7316400. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 23:15:27,597][187912] Avg episode reward: [(0, '882.972')]
[36m[2025-06-29 23:15:32,562][187912] Fps is (10 sec: 0.0, 60 sec: 341.7, 300 sec: 347.1). Total num frames: 7315456. Throughput: 0: 360.1. Samples: 7318656. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 23:15:32,562][187912] Avg episode reward: [(0, '833.907')]
[36m[2025-06-29 23:15:37,590][187912] Fps is (10 sec: 409.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7319552. Throughput: 0: 353.3. Samples: 7320752. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 23:15:37,590][187912] Avg episode reward: [(0, '895.365')]
[36m[2025-06-29 23:15:42,565][187912] Fps is (10 sec: 409.5, 60 sec: 341.5, 300 sec: 347.5). Total num frames: 7319552. Throughput: 0: 355.2. Samples: 7321840. Policy #0 lag: (min: 4.0, avg: 4.0, max: 20.0)
[36m[2025-06-29 23:15:42,565][187912] Avg episode reward: [(0, '832.631')]
[36m[2025-06-29 23:15:47,568][187912] Fps is (10 sec: 410.5, 60 sec: 344.8, 300 sec: 361.0). Total num frames: 7323648. Throughput: 0: 353.0. Samples: 7323680. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 23:15:47,569][187912] Avg episode reward: [(0, '877.429')]
[36m[2025-06-29 23:15:52,586][187912] Fps is (10 sec: 408.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7323648. Throughput: 0: 354.6. Samples: 7325984. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 23:15:52,586][187912] Avg episode reward: [(0, '884.190')]
[36m[2025-06-29 23:15:57,589][187912] Fps is (10 sec: 408.8, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 7327744. Throughput: 0: 353.5. Samples: 7327104. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 23:15:57,589][187912] Avg episode reward: [(0, '917.210')]
[36m[2025-06-29 23:16:02,564][187912] Fps is (10 sec: 410.5, 60 sec: 341.6, 300 sec: 361.0). Total num frames: 7327744. Throughput: 0: 357.5. Samples: 7329264. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 23:16:02,564][187912] Avg episode reward: [(0, '934.043')]
[36m[2025-06-29 23:16:07,564][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 7327744. Throughput: 0: 381.4. Samples: 7331424. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 23:16:07,564][187912] Avg episode reward: [(0, '1000.078')]
[36m[2025-06-29 23:16:12,584][187912] Fps is (10 sec: 408.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7331840. Throughput: 0: 352.8. Samples: 7332272. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:16:12,584][187912] Avg episode reward: [(0, '961.905')]
[37m[1m[2025-06-29 23:16:12,624][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000028640_7331840.pth...
[36m[2025-06-29 23:16:12,680][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000028304_7245824.pth
[36m[2025-06-29 23:16:17,563][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 7331840. Throughput: 0: 352.0. Samples: 7334496. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:16:17,563][187912] Avg episode reward: [(0, '994.625')]
[31m[20534572 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20534572 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[20534573 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:16:22,583][187912] Fps is (10 sec: 409.6, 60 sec: 342.3, 300 sec: 361.0). Total num frames: 7335936. Throughput: 0: 349.9. Samples: 7336496. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:16:22,584][187912] Avg episode reward: [(0, '1017.786')]
[36m[2025-06-29 23:16:27,577][187912] Fps is (10 sec: 409.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7335936. Throughput: 0: 351.9. Samples: 7337680. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:16:27,577][187912] Avg episode reward: [(0, '1039.311')]
[36m[2025-06-29 23:16:32,573][187912] Fps is (10 sec: 410.0, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 7340032. Throughput: 0: 362.3. Samples: 7339984. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:16:32,573][187912] Avg episode reward: [(0, '994.634')]
[36m[2025-06-29 23:16:37,592][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7340032. Throughput: 0: 355.2. Samples: 7341968. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:16:37,592][187912] Avg episode reward: [(0, '986.221')]
[36m[2025-06-29 23:16:42,563][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 7340032. Throughput: 0: 351.1. Samples: 7342896. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:16:42,564][187912] Avg episode reward: [(0, '1005.125')]
[31m[20558192 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20558193 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[20558193 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:16:47,573][187912] Fps is (10 sec: 410.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7344128. Throughput: 0: 344.5. Samples: 7344768. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:16:47,573][187912] Avg episode reward: [(0, '976.477')]
[36m[2025-06-29 23:16:52,568][187912] Fps is (10 sec: 409.4, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 7344128. Throughput: 0: 351.3. Samples: 7347232. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:16:52,568][187912] Avg episode reward: [(0, '886.487')]
[36m[2025-06-29 23:16:57,578][187912] Fps is (10 sec: 409.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7348224. Throughput: 0: 355.3. Samples: 7348256. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:16:57,578][187912] Avg episode reward: [(0, '922.013')]
[36m[2025-06-29 23:17:02,561][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 347.5). Total num frames: 7348224. Throughput: 0: 351.7. Samples: 7350320. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:17:02,561][187912] Avg episode reward: [(0, '920.823')]
[36m[2025-06-29 23:17:07,570][187912] Fps is (10 sec: 409.9, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7352320. Throughput: 0: 355.3. Samples: 7352480. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:17:07,570][187912] Avg episode reward: [(0, '918.149')]
[36m[2025-06-29 23:17:12,588][187912] Fps is (10 sec: 408.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7352320. Throughput: 0: 355.5. Samples: 7353680. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:17:12,588][187912] Avg episode reward: [(0, '933.773')]
[36m[2025-06-29 23:17:17,598][187912] Fps is (10 sec: 408.5, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 7356416. Throughput: 0: 358.9. Samples: 7356144. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:17:17,598][187912] Avg episode reward: [(0, '1016.153')]
[36m[2025-06-29 23:17:22,561][187912] Fps is (10 sec: 410.7, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7356416. Throughput: 0: 360.1. Samples: 7358160. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:17:22,561][187912] Avg episode reward: [(0, '1024.083')]
[36m[2025-06-29 23:17:27,564][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 7356416. Throughput: 0: 364.8. Samples: 7359312. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:17:27,564][187912] Avg episode reward: [(0, '1018.424')]
[36m[2025-06-29 23:17:32,564][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.1). Total num frames: 7360512. Throughput: 0: 369.1. Samples: 7361376. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:17:32,564][187912] Avg episode reward: [(0, '1041.721')]
[36m[2025-06-29 23:17:37,586][187912] Fps is (10 sec: 408.7, 60 sec: 341.4, 300 sec: 347.5). Total num frames: 7360512. Throughput: 0: 367.1. Samples: 7363760. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:17:37,587][187912] Avg episode reward: [(0, '1020.872')]
[36m[2025-06-29 23:17:42,593][187912] Fps is (10 sec: 408.4, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 7364608. Throughput: 0: 365.0. Samples: 7364688. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:17:42,593][187912] Avg episode reward: [(0, '1021.496')]
[36m[2025-06-29 23:17:47,577][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7364608. Throughput: 0: 372.5. Samples: 7367088. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:17:47,577][187912] Avg episode reward: [(0, '968.671')]
[36m[2025-06-29 23:17:52,602][187912] Fps is (10 sec: 409.2, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 7368704. Throughput: 0: 367.4. Samples: 7369024. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:17:52,602][187912] Avg episode reward: [(0, '941.010')]
[36m[2025-06-29 23:17:57,573][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7368704. Throughput: 0: 366.0. Samples: 7370144. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:17:57,573][187912] Avg episode reward: [(0, '892.393')]
[31m[20631703 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20631703 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[20631703 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:18:03,220][187912] Fps is (10 sec: 385.7, 60 sec: 405.1, 300 sec: 360.2). Total num frames: 7372800. Throughput: 0: 355.3. Samples: 7372352. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 23:18:03,221][187912] Avg episode reward: [(0, '845.517')]
[31m[20639701 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20639701 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[20639701 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:18:07,580][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7372800. Throughput: 0: 359.0. Samples: 7374320. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 23:18:07,580][187912] Avg episode reward: [(0, '809.604')]
[36m[2025-06-29 23:18:12,590][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 7372800. Throughput: 0: 357.5. Samples: 7375408. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 23:18:12,590][187912] Avg episode reward: [(0, '781.109')]
[37m[1m[2025-06-29 23:18:12,633][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000028800_7372800.pth...
[36m[2025-06-29 23:18:12,696][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000028480_7290880.pth
[36m[2025-06-29 23:18:17,581][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7376896. Throughput: 0: 358.3. Samples: 7377504. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 23:18:17,581][187912] Avg episode reward: [(0, '848.009')]
[36m[2025-06-29 23:18:22,590][187912] Fps is (10 sec: 409.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7376896. Throughput: 0: 358.0. Samples: 7379872. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 23:18:22,590][187912] Avg episode reward: [(0, '894.007')]
[33m[20657130 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[20657130 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.7589067816734314
[33mCrash Rate: 0.1849682778120041
[33mTimeout Rate: 0.056124940514564514 (navigation_task.py:265)
[33m[20657130 ms][navigation_task] - WARNING : 
[33mSuccesses: 1555
[33mCrashes : 379
[33mTimeouts: 115 (navigation_task.py:268)
[36m[2025-06-29 23:18:27,572][187912] Fps is (10 sec: 410.0, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 7380992. Throughput: 0: 362.8. Samples: 7381008. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:18:27,572][187912] Avg episode reward: [(0, '950.778')]
[36m[2025-06-29 23:18:32,571][187912] Fps is (10 sec: 410.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7380992. Throughput: 0: 357.4. Samples: 7383168. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:18:32,571][187912] Avg episode reward: [(0, '883.340')]
[36m[2025-06-29 23:18:37,577][187912] Fps is (10 sec: 409.4, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 7385088. Throughput: 0: 360.7. Samples: 7385248. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:18:37,577][187912] Avg episode reward: [(0, '1007.350')]
[31m[20672446 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20672446 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[20672447 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:18:42,598][187912] Fps is (10 sec: 408.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7385088. Throughput: 0: 363.5. Samples: 7386512. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:18:42,599][187912] Avg episode reward: [(0, '952.431')]
[36m[2025-06-29 23:18:47,704][187912] Fps is (10 sec: 404.5, 60 sec: 408.7, 300 sec: 360.9). Total num frames: 7389184. Throughput: 0: 371.6. Samples: 7388880. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:18:47,704][187912] Avg episode reward: [(0, '914.059')]
[36m[2025-06-29 23:18:52,589][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7389184. Throughput: 0: 366.9. Samples: 7390832. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:18:52,589][187912] Avg episode reward: [(0, '882.235')]
[36m[2025-06-29 23:18:57,594][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 7389184. Throughput: 0: 364.8. Samples: 7391824. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:18:57,594][187912] Avg episode reward: [(0, '894.892')]
[36m[2025-06-29 23:19:02,584][187912] Fps is (10 sec: 409.8, 60 sec: 345.0, 300 sec: 361.0). Total num frames: 7393280. Throughput: 0: 359.4. Samples: 7393680. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:19:02,584][187912] Avg episode reward: [(0, '951.136')]
[36m[2025-06-29 23:19:07,582][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 347.9). Total num frames: 7393280. Throughput: 0: 357.0. Samples: 7395936. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:19:07,583][187912] Avg episode reward: [(0, '976.617')]
[31m[20702077 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20702078 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[20702078 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:19:12,571][187912] Fps is (10 sec: 410.1, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 7397376. Throughput: 0: 357.7. Samples: 7397104. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 23:19:12,571][187912] Avg episode reward: [(0, '1005.276')]
[36m[2025-06-29 23:19:17,572][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7397376. Throughput: 0: 357.0. Samples: 7399232. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 23:19:17,572][187912] Avg episode reward: [(0, '1041.237')]
[36m[2025-06-29 23:19:22,558][187912] Fps is (10 sec: 410.1, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 7401472. Throughput: 0: 361.4. Samples: 7401504. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 23:19:22,558][187912] Avg episode reward: [(0, '1098.977')]
[36m[2025-06-29 23:19:27,565][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 361.1). Total num frames: 7401472. Throughput: 0: 357.6. Samples: 7402592. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 23:19:27,565][187912] Avg episode reward: [(0, '1020.900')]
[36m[2025-06-29 23:19:32,581][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 7401472. Throughput: 0: 358.3. Samples: 7404960. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 23:19:32,581][187912] Avg episode reward: [(0, '984.155')]
[36m[2025-06-29 23:19:37,560][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7405568. Throughput: 0: 362.2. Samples: 7407120. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:19:37,561][187912] Avg episode reward: [(0, '958.935')]
[36m[2025-06-29 23:19:42,569][187912] Fps is (10 sec: 410.1, 60 sec: 341.5, 300 sec: 347.8). Total num frames: 7405568. Throughput: 0: 364.6. Samples: 7408224. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:19:42,569][187912] Avg episode reward: [(0, '953.524')]
[36m[2025-06-29 23:19:47,590][187912] Fps is (10 sec: 408.4, 60 sec: 342.0, 300 sec: 361.0). Total num frames: 7409664. Throughput: 0: 368.3. Samples: 7410256. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:19:47,590][187912] Avg episode reward: [(0, '932.252')]
[36m[2025-06-29 23:19:52,589][187912] Fps is (10 sec: 408.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7409664. Throughput: 0: 370.4. Samples: 7412608. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:19:52,589][187912] Avg episode reward: [(0, '972.411')]
[36m[2025-06-29 23:19:57,599][187912] Fps is (10 sec: 409.2, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7413760. Throughput: 0: 367.4. Samples: 7413648. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 23:19:57,599][187912] Avg episode reward: [(0, '980.077')]
[36m[2025-06-29 23:20:02,568][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7413760. Throughput: 0: 362.3. Samples: 7415536. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 23:20:02,568][187912] Avg episode reward: [(0, '1027.895')]
[31m[20756036 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20756036 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[20756036 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:20:08,126][187912] Fps is (10 sec: 389.1, 60 sec: 405.9, 300 sec: 360.4). Total num frames: 7417856. Throughput: 0: 356.7. Samples: 7417760. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 23:20:08,126][187912] Avg episode reward: [(0, '985.306')]
[36m[2025-06-29 23:20:12,563][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7417856. Throughput: 0: 357.7. Samples: 7418688. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 23:20:12,563][187912] Avg episode reward: [(0, '1048.074')]
[37m[1m[2025-06-29 23:20:12,618][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000028976_7417856.pth...
[36m[2025-06-29 23:20:12,693][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000028640_7331840.pth
[36m[2025-06-29 23:20:17,565][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.3). Total num frames: 7417856. Throughput: 0: 355.0. Samples: 7420928. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 23:20:17,565][187912] Avg episode reward: [(0, '933.620')]
[36m[2025-06-29 23:20:22,583][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7421952. Throughput: 0: 351.1. Samples: 7422928. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 23:20:22,583][187912] Avg episode reward: [(0, '1000.536')]
[36m[2025-06-29 23:20:27,590][187912] Fps is (10 sec: 408.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7421952. Throughput: 0: 351.5. Samples: 7424048. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 23:20:27,590][187912] Avg episode reward: [(0, '972.750')]
[36m[2025-06-29 23:20:32,578][187912] Fps is (10 sec: 409.8, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7426048. Throughput: 0: 351.7. Samples: 7426080. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 23:20:32,578][187912] Avg episode reward: [(0, '965.053')]
[36m[2025-06-29 23:20:37,562][187912] Fps is (10 sec: 410.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7426048. Throughput: 0: 349.0. Samples: 7428304. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 23:20:37,562][187912] Avg episode reward: [(0, '934.666')]
[31m[20794375 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20794375 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[20794375 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:20:42,721][187912] Fps is (10 sec: 403.8, 60 sec: 408.6, 300 sec: 360.8). Total num frames: 7430144. Throughput: 0: 349.6. Samples: 7429424. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:20:42,721][187912] Avg episode reward: [(0, '957.800')]
[36m[2025-06-29 23:20:47,584][187912] Fps is (10 sec: 408.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7430144. Throughput: 0: 353.6. Samples: 7431456. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:20:47,584][187912] Avg episode reward: [(0, '890.078')]
[36m[2025-06-29 23:20:52,614][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 7430144. Throughput: 0: 359.3. Samples: 7433744. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:20:52,615][187912] Avg episode reward: [(0, '888.017')]
[36m[2025-06-29 23:20:57,580][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7434240. Throughput: 0: 353.3. Samples: 7434592. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:20:57,580][187912] Avg episode reward: [(0, '980.164')]
[36m[2025-06-29 23:21:02,587][187912] Fps is (10 sec: 410.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7434240. Throughput: 0: 356.4. Samples: 7436976. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:21:02,588][187912] Avg episode reward: [(0, '985.807')]
[36m[2025-06-29 23:21:07,577][187912] Fps is (10 sec: 409.7, 60 sec: 344.5, 300 sec: 361.0). Total num frames: 7438336. Throughput: 0: 355.6. Samples: 7438928. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:21:07,577][187912] Avg episode reward: [(0, '1016.677')]
[36m[2025-06-29 23:21:12,568][187912] Fps is (10 sec: 410.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7438336. Throughput: 0: 357.9. Samples: 7440144. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:21:12,568][187912] Avg episode reward: [(0, '1081.549')]
[36m[2025-06-29 23:21:17,569][187912] Fps is (10 sec: 409.9, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7442432. Throughput: 0: 361.7. Samples: 7442352. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:21:17,569][187912] Avg episode reward: [(0, '1064.040')]
[36m[2025-06-29 23:21:22,578][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7442432. Throughput: 0: 357.9. Samples: 7444416. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:21:22,579][187912] Avg episode reward: [(0, '960.651')]
[36m[2025-06-29 23:21:27,589][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 7442432. Throughput: 0: 359.5. Samples: 7445552. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:21:27,589][187912] Avg episode reward: [(0, '974.886')]
[36m[2025-06-29 23:21:32,597][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7446528. Throughput: 0: 357.6. Samples: 7447552. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:21:32,597][187912] Avg episode reward: [(0, '997.490')]
[36m[2025-06-29 23:21:37,569][187912] Fps is (10 sec: 410.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7446528. Throughput: 0: 356.3. Samples: 7449760. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:21:37,570][187912] Avg episode reward: [(0, '980.572')]
[36m[2025-06-29 23:21:42,572][187912] Fps is (10 sec: 410.6, 60 sec: 342.2, 300 sec: 361.0). Total num frames: 7450624. Throughput: 0: 357.0. Samples: 7450656. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:21:42,572][187912] Avg episode reward: [(0, '989.213')]
[36m[2025-06-29 23:21:47,606][187912] Fps is (10 sec: 408.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7450624. Throughput: 0: 357.5. Samples: 7453072. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:21:47,606][187912] Avg episode reward: [(0, '1051.370')]
[36m[2025-06-29 23:21:52,573][187912] Fps is (10 sec: 409.6, 60 sec: 409.9, 300 sec: 361.0). Total num frames: 7454720. Throughput: 0: 357.7. Samples: 7455024. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:21:52,573][187912] Avg episode reward: [(0, '1020.525')]
[36m[2025-06-29 23:21:57,571][187912] Fps is (10 sec: 411.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7454720. Throughput: 0: 355.2. Samples: 7456128. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:21:57,571][187912] Avg episode reward: [(0, '981.622')]
[36m[2025-06-29 23:22:03,014][187912] Fps is (10 sec: 392.3, 60 sec: 406.7, 300 sec: 360.5). Total num frames: 7458816. Throughput: 0: 353.1. Samples: 7458400. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:22:03,015][187912] Avg episode reward: [(0, '932.864')]
[36m[2025-06-29 23:22:07,589][187912] Fps is (10 sec: 408.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7458816. Throughput: 0: 351.9. Samples: 7460256. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:22:07,590][187912] Avg episode reward: [(0, '943.825')]
[36m[2025-06-29 23:22:12,560][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.2). Total num frames: 7458816. Throughput: 0: 351.5. Samples: 7461360. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:22:12,560][187912] Avg episode reward: [(0, '996.796')]
[37m[1m[2025-06-29 23:22:12,620][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000029136_7458816.pth...
[36m[2025-06-29 23:22:12,693][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000028800_7372800.pth
[36m[2025-06-29 23:22:17,567][187912] Fps is (10 sec: 410.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7462912. Throughput: 0: 350.8. Samples: 7463328. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:22:17,567][187912] Avg episode reward: [(0, '925.593')]
[36m[2025-06-29 23:22:22,574][187912] Fps is (10 sec: 409.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7462912. Throughput: 0: 350.5. Samples: 7465536. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:22:22,574][187912] Avg episode reward: [(0, '931.051')]
[36m[2025-06-29 23:22:27,559][187912] Fps is (10 sec: 409.9, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 7467008. Throughput: 0: 357.8. Samples: 7466752. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:22:27,559][187912] Avg episode reward: [(0, '991.120')]
[31m[20900865 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[20900866 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0], device='cuda:0') (navigation_task.py:196)
[31m[20900866 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:22:32,576][187912] Fps is (10 sec: 409.5, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7467008. Throughput: 0: 348.7. Samples: 7468752. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:22:32,576][187912] Avg episode reward: [(0, '932.089')]
[36m[2025-06-29 23:22:37,989][187912] Fps is (10 sec: 392.7, 60 sec: 406.8, 300 sec: 360.5). Total num frames: 7471104. Throughput: 0: 353.4. Samples: 7471072. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 23:22:37,990][187912] Avg episode reward: [(0, '939.806')]
[36m[2025-06-29 23:22:42,558][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7471104. Throughput: 0: 352.8. Samples: 7472000. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 23:22:42,558][187912] Avg episode reward: [(0, '970.664')]
[36m[2025-06-29 23:22:47,575][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.2). Total num frames: 7471104. Throughput: 0: 358.3. Samples: 7474368. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 23:22:47,575][187912] Avg episode reward: [(0, '920.763')]
[36m[2025-06-29 23:22:52,583][187912] Fps is (10 sec: 408.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7475200. Throughput: 0: 355.6. Samples: 7476256. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 23:22:52,584][187912] Avg episode reward: [(0, '887.173')]
[36m[2025-06-29 23:22:57,558][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 347.9). Total num frames: 7475200. Throughput: 0: 355.2. Samples: 7477344. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 23:22:57,559][187912] Avg episode reward: [(0, '965.294')]
[36m[2025-06-29 23:23:02,591][187912] Fps is (10 sec: 409.3, 60 sec: 343.8, 300 sec: 361.0). Total num frames: 7479296. Throughput: 0: 356.8. Samples: 7479392. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:23:02,591][187912] Avg episode reward: [(0, '935.119')]
[36m[2025-06-29 23:23:07,561][187912] Fps is (10 sec: 409.5, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7479296. Throughput: 0: 359.6. Samples: 7481712. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:23:07,561][187912] Avg episode reward: [(0, '947.556')]
[36m[2025-06-29 23:23:12,607][187912] Fps is (10 sec: 408.9, 60 sec: 409.3, 300 sec: 361.0). Total num frames: 7483392. Throughput: 0: 357.7. Samples: 7482864. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:23:12,607][187912] Avg episode reward: [(0, '963.553')]
[36m[2025-06-29 23:23:17,590][187912] Fps is (10 sec: 408.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7483392. Throughput: 0: 359.7. Samples: 7484944. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:23:17,590][187912] Avg episode reward: [(0, '947.972')]
[36m[2025-06-29 23:23:22,610][187912] Fps is (10 sec: 0.0, 60 sec: 341.1, 300 sec: 347.1). Total num frames: 7483392. Throughput: 0: 362.2. Samples: 7487232. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:23:22,611][187912] Avg episode reward: [(0, '940.833')]
[36m[2025-06-29 23:23:27,582][187912] Fps is (10 sec: 409.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7487488. Throughput: 0: 356.8. Samples: 7488064. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:23:27,582][187912] Avg episode reward: [(0, '968.361')]
[36m[2025-06-29 23:23:32,589][187912] Fps is (10 sec: 410.5, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 7487488. Throughput: 0: 356.2. Samples: 7490400. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:23:32,589][187912] Avg episode reward: [(0, '1011.064')]
[36m[2025-06-29 23:23:37,570][187912] Fps is (10 sec: 410.1, 60 sec: 343.7, 300 sec: 361.0). Total num frames: 7491584. Throughput: 0: 361.0. Samples: 7492496. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:23:37,570][187912] Avg episode reward: [(0, '989.157')]
[36m[2025-06-29 23:23:42,588][187912] Fps is (10 sec: 409.7, 60 sec: 341.2, 300 sec: 347.3). Total num frames: 7491584. Throughput: 0: 359.9. Samples: 7493552. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:23:42,588][187912] Avg episode reward: [(0, '1032.557')]
[36m[2025-06-29 23:23:47,580][187912] Fps is (10 sec: 409.2, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7495680. Throughput: 0: 362.7. Samples: 7495712. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:23:47,581][187912] Avg episode reward: [(0, '1043.969')]
[36m[2025-06-29 23:23:52,560][187912] Fps is (10 sec: 410.7, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7495680. Throughput: 0: 358.8. Samples: 7497856. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:23:52,560][187912] Avg episode reward: [(0, '1006.061')]
[36m[2025-06-29 23:23:57,880][187912] Fps is (10 sec: 397.7, 60 sec: 407.4, 300 sec: 360.6). Total num frames: 7499776. Throughput: 0: 358.4. Samples: 7499088. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:23:57,880][187912] Avg episode reward: [(0, '982.901')]
[36m[2025-06-29 23:24:02,579][187912] Fps is (10 sec: 408.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7499776. Throughput: 0: 358.1. Samples: 7501056. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:24:02,579][187912] Avg episode reward: [(0, '1004.802')]
[36m[2025-06-29 23:24:07,576][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 7499776. Throughput: 0: 360.1. Samples: 7503424. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:24:07,577][187912] Avg episode reward: [(0, '1030.309')]
[36m[2025-06-29 23:24:12,600][187912] Fps is (10 sec: 408.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7503872. Throughput: 0: 359.7. Samples: 7504256. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 23:24:12,600][187912] Avg episode reward: [(0, '1044.450')]
[37m[1m[2025-06-29 23:24:12,643][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000029312_7503872.pth...
[36m[2025-06-29 23:24:12,713][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000028976_7417856.pth
[36m[2025-06-29 23:24:17,573][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 7503872. Throughput: 0: 354.6. Samples: 7506352. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 23:24:17,574][187912] Avg episode reward: [(0, '1028.590')]
[36m[2025-06-29 23:24:22,588][187912] Fps is (10 sec: 410.1, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 7507968. Throughput: 0: 349.7. Samples: 7508240. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 23:24:22,588][187912] Avg episode reward: [(0, '1062.962')]
[36m[2025-06-29 23:24:27,590][187912] Fps is (10 sec: 408.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7507968. Throughput: 0: 352.7. Samples: 7509424. Policy #0 lag: (min: 9.0, avg: 9.0, max: 25.0)
[36m[2025-06-29 23:24:27,590][187912] Avg episode reward: [(0, '1028.327')]
[36m[2025-06-29 23:24:32,566][187912] Fps is (10 sec: 410.5, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 7512064. Throughput: 0: 359.2. Samples: 7511872. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:24:32,566][187912] Avg episode reward: [(0, '1032.126')]
[36m[2025-06-29 23:24:37,569][187912] Fps is (10 sec: 410.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7512064. Throughput: 0: 355.8. Samples: 7513872. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:24:37,569][187912] Avg episode reward: [(0, '1039.286')]
[36m[2025-06-29 23:24:42,562][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.2). Total num frames: 7512064. Throughput: 0: 352.7. Samples: 7514848. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:24:42,562][187912] Avg episode reward: [(0, '1045.845')]
[36m[2025-06-29 23:24:47,559][187912] Fps is (10 sec: 410.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7516160. Throughput: 0: 349.7. Samples: 7516784. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:24:47,559][187912] Avg episode reward: [(0, '1049.447')]
[31m[21041598 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21041598 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[21041598 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:24:52,583][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 7516160. Throughput: 0: 351.6. Samples: 7519248. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:24:52,583][187912] Avg episode reward: [(0, '1028.025')]
[36m[2025-06-29 23:24:57,583][187912] Fps is (10 sec: 408.6, 60 sec: 343.0, 300 sec: 361.0). Total num frames: 7520256. Throughput: 0: 356.4. Samples: 7520288. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:24:57,583][187912] Avg episode reward: [(0, '985.508')]
[36m[2025-06-29 23:25:02,589][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 347.8). Total num frames: 7520256. Throughput: 0: 356.5. Samples: 7522400. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:25:02,590][187912] Avg episode reward: [(0, '982.937')]
[36m[2025-06-29 23:25:07,584][187912] Fps is (10 sec: 409.6, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 7524352. Throughput: 0: 362.7. Samples: 7524560. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:25:07,584][187912] Avg episode reward: [(0, '1010.006')]
[36m[2025-06-29 23:25:12,601][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7524352. Throughput: 0: 362.2. Samples: 7525728. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:25:12,601][187912] Avg episode reward: [(0, '1025.509')]
[36m[2025-06-29 23:25:18,295][187912] Fps is (10 sec: 382.4, 60 sec: 404.7, 300 sec: 360.1). Total num frames: 7528448. Throughput: 0: 349.9. Samples: 7527872. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:25:18,295][187912] Avg episode reward: [(0, '1061.583')]
[36m[2025-06-29 23:25:22,564][187912] Fps is (10 sec: 411.1, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7528448. Throughput: 0: 357.0. Samples: 7529936. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:25:22,564][187912] Avg episode reward: [(0, '1089.706')]
[36m[2025-06-29 23:25:27,597][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 7528448. Throughput: 0: 360.3. Samples: 7531072. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:25:27,597][187912] Avg episode reward: [(0, '1066.220')]
[36m[2025-06-29 23:25:32,571][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7532544. Throughput: 0: 358.7. Samples: 7532928. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:25:32,571][187912] Avg episode reward: [(0, '994.554')]
[31m[21086630 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21086630 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[21086630 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:25:37,558][187912] Fps is (10 sec: 411.2, 60 sec: 341.4, 300 sec: 347.3). Total num frames: 7532544. Throughput: 0: 357.9. Samples: 7535344. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:25:37,558][187912] Avg episode reward: [(0, '927.914')]
[31m[21091471 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21091471 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[21091471 ms][navigation_task] - CRITICAL : Time at crash: tensor([2], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:25:42,581][187912] Fps is (10 sec: 409.2, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 7536640. Throughput: 0: 359.8. Samples: 7536480. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:25:42,582][187912] Avg episode reward: [(0, '932.633')]
[31m[21100834 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21100835 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[21100835 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:25:47,569][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.1). Total num frames: 7536640. Throughput: 0: 358.2. Samples: 7538512. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:25:47,570][187912] Avg episode reward: [(0, '833.975')]
[36m[2025-06-29 23:25:52,913][187912] Fps is (10 sec: 396.5, 60 sec: 407.4, 300 sec: 360.6). Total num frames: 7540736. Throughput: 0: 356.9. Samples: 7540736. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:25:52,913][187912] Avg episode reward: [(0, '825.777')]
[36m[2025-06-29 23:25:57,588][187912] Fps is (10 sec: 408.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7540736. Throughput: 0: 353.5. Samples: 7541632. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:25:57,588][187912] Avg episode reward: [(0, '828.426')]
[31m[21113217 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21113217 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[21113217 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:26:02,577][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 7540736. Throughput: 0: 365.3. Samples: 7544048. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:26:02,578][187912] Avg episode reward: [(0, '787.042')]
[36m[2025-06-29 23:26:07,579][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7544832. Throughput: 0: 359.7. Samples: 7546128. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:26:07,580][187912] Avg episode reward: [(0, '809.052')]
[36m[2025-06-29 23:26:12,585][187912] Fps is (10 sec: 409.3, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 7544832. Throughput: 0: 362.1. Samples: 7547360. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:26:12,585][187912] Avg episode reward: [(0, '817.853')]
[37m[1m[2025-06-29 23:26:12,625][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000029472_7544832.pth...
[36m[2025-06-29 23:26:12,696][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000029136_7458816.pth
[36m[2025-06-29 23:26:17,574][187912] Fps is (10 sec: 409.8, 60 sec: 345.5, 300 sec: 361.0). Total num frames: 7548928. Throughput: 0: 366.2. Samples: 7549408. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:26:17,575][187912] Avg episode reward: [(0, '913.474')]
[36m[2025-06-29 23:26:22,578][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7548928. Throughput: 0: 363.9. Samples: 7551728. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:26:22,578][187912] Avg episode reward: [(0, '909.210')]
[36m[2025-06-29 23:26:27,562][187912] Fps is (10 sec: 410.1, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 7553024. Throughput: 0: 363.2. Samples: 7552816. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 23:26:27,562][187912] Avg episode reward: [(0, '949.154')]
[36m[2025-06-29 23:26:32,592][187912] Fps is (10 sec: 409.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7553024. Throughput: 0: 363.2. Samples: 7554864. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 23:26:32,592][187912] Avg episode reward: [(0, '903.888')]
[36m[2025-06-29 23:26:37,922][187912] Fps is (10 sec: 395.3, 60 sec: 407.1, 300 sec: 360.6). Total num frames: 7557120. Throughput: 0: 364.0. Samples: 7557120. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 23:26:37,923][187912] Avg episode reward: [(0, '992.469')]
[36m[2025-06-29 23:26:42,594][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7557120. Throughput: 0: 364.4. Samples: 7558032. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 23:26:42,594][187912] Avg episode reward: [(0, '908.061')]
[36m[2025-06-29 23:26:47,592][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 347.1). Total num frames: 7557120. Throughput: 0: 361.1. Samples: 7560304. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 23:26:47,592][187912] Avg episode reward: [(0, '972.526')]
[36m[2025-06-29 23:26:52,593][187912] Fps is (10 sec: 409.6, 60 sec: 343.2, 300 sec: 361.0). Total num frames: 7561216. Throughput: 0: 357.9. Samples: 7562240. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:26:52,593][187912] Avg episode reward: [(0, '930.468')]
[36m[2025-06-29 23:26:57,590][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 347.6). Total num frames: 7561216. Throughput: 0: 357.6. Samples: 7563456. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:26:57,591][187912] Avg episode reward: [(0, '940.795')]
[31m[21174427 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21174428 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[21174428 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:27:02,568][187912] Fps is (10 sec: 410.6, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 7565312. Throughput: 0: 359.5. Samples: 7565584. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:27:02,568][187912] Avg episode reward: [(0, '931.272')]
[36m[2025-06-29 23:27:07,563][187912] Fps is (10 sec: 410.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7565312. Throughput: 0: 359.2. Samples: 7567888. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:27:07,564][187912] Avg episode reward: [(0, '916.896')]
[33m[21183577 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[21183577 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.77294921875
[33mCrash Rate: 0.166015625
[33mTimeout Rate: 0.06103515625 (navigation_task.py:265)
[33m[21183578 ms][navigation_task] - WARNING : 
[33mSuccesses: 1583
[33mCrashes : 340
[33mTimeouts: 125 (navigation_task.py:268)
[36m[2025-06-29 23:27:12,565][187912] Fps is (10 sec: 409.7, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 7569408. Throughput: 0: 359.1. Samples: 7568976. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:27:12,566][187912] Avg episode reward: [(0, '949.478')]
[36m[2025-06-29 23:27:17,597][187912] Fps is (10 sec: 408.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7569408. Throughput: 0: 362.6. Samples: 7571184. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:27:17,597][187912] Avg episode reward: [(0, '1003.994')]
[36m[2025-06-29 23:27:23,147][187912] Fps is (10 sec: 387.1, 60 sec: 405.8, 300 sec: 360.3). Total num frames: 7573504. Throughput: 0: 360.5. Samples: 7573424. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:27:23,147][187912] Avg episode reward: [(0, '991.153')]
[36m[2025-06-29 23:27:27,584][187912] Fps is (10 sec: 410.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7573504. Throughput: 0: 362.0. Samples: 7574320. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:27:27,584][187912] Avg episode reward: [(0, '1051.991')]
[36m[2025-06-29 23:27:32,574][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.6). Total num frames: 7573504. Throughput: 0: 362.5. Samples: 7576608. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:27:32,574][187912] Avg episode reward: [(0, '1019.459')]
[36m[2025-06-29 23:27:37,569][187912] Fps is (10 sec: 410.2, 60 sec: 343.4, 300 sec: 361.0). Total num frames: 7577600. Throughput: 0: 365.3. Samples: 7578672. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:27:37,569][187912] Avg episode reward: [(0, '964.369')]
[36m[2025-06-29 23:27:42,580][187912] Fps is (10 sec: 409.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7577600. Throughput: 0: 364.2. Samples: 7579840. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:27:42,580][187912] Avg episode reward: [(0, '942.426')]
[36m[2025-06-29 23:27:47,578][187912] Fps is (10 sec: 409.2, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 7581696. Throughput: 0: 360.1. Samples: 7581792. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:27:47,579][187912] Avg episode reward: [(0, '983.996')]
[36m[2025-06-29 23:27:52,574][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7581696. Throughput: 0: 360.4. Samples: 7584112. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:27:52,574][187912] Avg episode reward: [(0, '878.130')]
[36m[2025-06-29 23:27:57,586][187912] Fps is (10 sec: 409.3, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7585792. Throughput: 0: 363.6. Samples: 7585344. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 23:27:57,586][187912] Avg episode reward: [(0, '931.184')]
[36m[2025-06-29 23:28:02,572][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7585792. Throughput: 0: 363.6. Samples: 7587536. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 23:28:02,572][187912] Avg episode reward: [(0, '950.829')]
[36m[2025-06-29 23:28:08,068][187912] Fps is (10 sec: 390.8, 60 sec: 406.2, 300 sec: 360.4). Total num frames: 7589888. Throughput: 0: 365.4. Samples: 7589840. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 23:28:08,068][187912] Avg episode reward: [(0, '990.210')]
[36m[2025-06-29 23:28:12,572][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7589888. Throughput: 0: 364.9. Samples: 7590736. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 23:28:12,572][187912] Avg episode reward: [(0, '989.518')]
[37m[1m[2025-06-29 23:28:12,621][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000029648_7589888.pth...
[36m[2025-06-29 23:28:12,691][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000029312_7503872.pth
[36m[2025-06-29 23:28:17,597][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7589888. Throughput: 0: 364.6. Samples: 7593024. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 23:28:17,597][187912] Avg episode reward: [(0, '1027.347')]
[36m[2025-06-29 23:28:22,593][187912] Fps is (10 sec: 408.7, 60 sec: 344.5, 300 sec: 361.0). Total num frames: 7593984. Throughput: 0: 363.9. Samples: 7595056. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 23:28:22,594][187912] Avg episode reward: [(0, '1053.902')]
[36m[2025-06-29 23:28:27,592][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7593984. Throughput: 0: 364.0. Samples: 7596224. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 23:28:27,592][187912] Avg episode reward: [(0, '1028.940')]
[36m[2025-06-29 23:28:32,568][187912] Fps is (10 sec: 410.6, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7598080. Throughput: 0: 363.5. Samples: 7598144. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 23:28:32,568][187912] Avg episode reward: [(0, '1040.120')]
[36m[2025-06-29 23:28:37,565][187912] Fps is (10 sec: 410.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7598080. Throughput: 0: 363.8. Samples: 7600480. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 23:28:37,565][187912] Avg episode reward: [(0, '1040.398')]
[36m[2025-06-29 23:28:42,569][187912] Fps is (10 sec: 409.5, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 7602176. Throughput: 0: 362.4. Samples: 7601648. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:28:42,570][187912] Avg episode reward: [(0, '1046.834')]
[36m[2025-06-29 23:28:47,579][187912] Fps is (10 sec: 409.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7602176. Throughput: 0: 360.8. Samples: 7603776. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:28:47,579][187912] Avg episode reward: [(0, '916.213')]
[36m[2025-06-29 23:28:52,575][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.5). Total num frames: 7602176. Throughput: 0: 364.9. Samples: 7606080. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:28:52,576][187912] Avg episode reward: [(0, '875.277')]
[36m[2025-06-29 23:28:57,579][187912] Fps is (10 sec: 409.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7606272. Throughput: 0: 361.9. Samples: 7607024. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:28:57,579][187912] Avg episode reward: [(0, '878.333')]
[36m[2025-06-29 23:29:02,590][187912] Fps is (10 sec: 409.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7606272. Throughput: 0: 361.7. Samples: 7609296. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:29:02,590][187912] Avg episode reward: [(0, '891.314')]
[36m[2025-06-29 23:29:07,584][187912] Fps is (10 sec: 409.4, 60 sec: 344.1, 300 sec: 361.0). Total num frames: 7610368. Throughput: 0: 362.7. Samples: 7611376. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 23:29:07,585][187912] Avg episode reward: [(0, '797.358')]
[36m[2025-06-29 23:29:12,581][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7610368. Throughput: 0: 359.6. Samples: 7612400. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 23:29:12,581][187912] Avg episode reward: [(0, '923.066')]
[36m[2025-06-29 23:29:17,564][187912] Fps is (10 sec: 410.4, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 7614464. Throughput: 0: 363.4. Samples: 7614496. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 23:29:17,565][187912] Avg episode reward: [(0, '967.284')]
[36m[2025-06-29 23:29:22,564][187912] Fps is (10 sec: 410.3, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7614464. Throughput: 0: 360.2. Samples: 7616688. Policy #0 lag: (min: 10.0, avg: 10.0, max: 26.0)
[36m[2025-06-29 23:29:22,564][187912] Avg episode reward: [(0, '945.894')]
[36m[2025-06-29 23:29:28,003][187912] Fps is (10 sec: 392.4, 60 sec: 406.8, 300 sec: 360.5). Total num frames: 7618560. Throughput: 0: 355.0. Samples: 7617776. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:29:28,003][187912] Avg episode reward: [(0, '945.424')]
[36m[2025-06-29 23:29:32,569][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7618560. Throughput: 0: 354.9. Samples: 7619744. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:29:32,569][187912] Avg episode reward: [(0, '919.670')]
[36m[2025-06-29 23:29:37,579][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7618560. Throughput: 0: 355.9. Samples: 7622096. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:29:37,579][187912] Avg episode reward: [(0, '960.075')]
[36m[2025-06-29 23:29:42,560][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7622656. Throughput: 0: 355.3. Samples: 7623008. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:29:42,561][187912] Avg episode reward: [(0, '874.056')]
[36m[2025-06-29 23:29:47,570][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7622656. Throughput: 0: 355.7. Samples: 7625296. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:29:47,571][187912] Avg episode reward: [(0, '922.744')]
[31m[21345195 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21345195 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[21345195 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:29:52,565][187912] Fps is (10 sec: 409.4, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 7626752. Throughput: 0: 354.6. Samples: 7627328. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:29:52,565][187912] Avg episode reward: [(0, '924.396')]
[36m[2025-06-29 23:29:57,575][187912] Fps is (10 sec: 409.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7626752. Throughput: 0: 359.2. Samples: 7628560. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:29:57,575][187912] Avg episode reward: [(0, '946.759')]
[36m[2025-06-29 23:30:02,587][187912] Fps is (10 sec: 408.7, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7630848. Throughput: 0: 362.1. Samples: 7630800. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:30:02,587][187912] Avg episode reward: [(0, '886.353')]
[36m[2025-06-29 23:30:07,570][187912] Fps is (10 sec: 409.8, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7630848. Throughput: 0: 355.9. Samples: 7632704. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:30:07,571][187912] Avg episode reward: [(0, '891.516')]
[36m[2025-06-29 23:30:12,586][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 348.0). Total num frames: 7630848. Throughput: 0: 357.1. Samples: 7633696. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:30:12,586][187912] Avg episode reward: [(0, '875.030')]
[37m[1m[2025-06-29 23:30:12,637][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000029808_7630848.pth...
[36m[2025-06-29 23:30:12,700][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000029472_7544832.pth
[36m[2025-06-29 23:30:17,565][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7634944. Throughput: 0: 354.2. Samples: 7635680. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:30:17,566][187912] Avg episode reward: [(0, '898.718')]
[36m[2025-06-29 23:30:22,586][187912] Fps is (10 sec: 409.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7634944. Throughput: 0: 353.7. Samples: 7638016. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:30:22,587][187912] Avg episode reward: [(0, '917.901')]
[36m[2025-06-29 23:30:27,564][187912] Fps is (10 sec: 409.6, 60 sec: 343.8, 300 sec: 361.0). Total num frames: 7639040. Throughput: 0: 356.9. Samples: 7639072. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:30:27,565][187912] Avg episode reward: [(0, '949.554')]
[36m[2025-06-29 23:30:32,566][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7639040. Throughput: 0: 354.5. Samples: 7641248. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:30:32,566][187912] Avg episode reward: [(0, '970.528')]
[36m[2025-06-29 23:30:37,584][187912] Fps is (10 sec: 408.8, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7643136. Throughput: 0: 355.8. Samples: 7643344. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:30:37,584][187912] Avg episode reward: [(0, '1003.916')]
[36m[2025-06-29 23:30:42,587][187912] Fps is (10 sec: 408.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7643136. Throughput: 0: 355.5. Samples: 7644560. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:30:42,587][187912] Avg episode reward: [(0, '1047.353')]
[36m[2025-06-29 23:30:48,046][187912] Fps is (10 sec: 391.5, 60 sec: 406.4, 300 sec: 360.8). Total num frames: 7647232. Throughput: 0: 353.0. Samples: 7646848. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:30:48,047][187912] Avg episode reward: [(0, '1045.232')]
[36m[2025-06-29 23:30:52,580][187912] Fps is (10 sec: 409.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7647232. Throughput: 0: 358.7. Samples: 7648848. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:30:52,580][187912] Avg episode reward: [(0, '1027.874')]
[36m[2025-06-29 23:30:57,589][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7647232. Throughput: 0: 363.0. Samples: 7650032. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:30:57,589][187912] Avg episode reward: [(0, '1032.735')]
[36m[2025-06-29 23:31:02,585][187912] Fps is (10 sec: 409.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7651328. Throughput: 0: 365.7. Samples: 7652144. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 23:31:02,585][187912] Avg episode reward: [(0, '982.526')]
[36m[2025-06-29 23:31:07,578][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7651328. Throughput: 0: 364.5. Samples: 7654416. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 23:31:07,578][187912] Avg episode reward: [(0, '980.808')]
[36m[2025-06-29 23:31:12,608][187912] Fps is (10 sec: 408.7, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 7655424. Throughput: 0: 363.7. Samples: 7655456. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 23:31:12,608][187912] Avg episode reward: [(0, '950.415')]
[36m[2025-06-29 23:31:17,574][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7655424. Throughput: 0: 364.0. Samples: 7657632. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-06-29 23:31:17,574][187912] Avg episode reward: [(0, '901.275')]
[36m[2025-06-29 23:31:22,574][187912] Fps is (10 sec: 411.0, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 7659520. Throughput: 0: 362.0. Samples: 7659632. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 23:31:22,574][187912] Avg episode reward: [(0, '886.364')]
[36m[2025-06-29 23:31:27,587][187912] Fps is (10 sec: 409.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7659520. Throughput: 0: 361.2. Samples: 7660816. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 23:31:27,587][187912] Avg episode reward: [(0, '879.666')]
[36m[2025-06-29 23:31:33,151][187912] Fps is (10 sec: 387.3, 60 sec: 405.6, 300 sec: 360.7). Total num frames: 7663616. Throughput: 0: 361.1. Samples: 7663136. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 23:31:33,151][187912] Avg episode reward: [(0, '873.484')]
[36m[2025-06-29 23:31:37,579][187912] Fps is (10 sec: 409.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7663616. Throughput: 0: 363.0. Samples: 7665184. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 23:31:37,580][187912] Avg episode reward: [(0, '822.822')]
[36m[2025-06-29 23:31:42,590][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7663616. Throughput: 0: 360.2. Samples: 7666240. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 23:31:42,590][187912] Avg episode reward: [(0, '861.850')]
[36m[2025-06-29 23:31:47,591][187912] Fps is (10 sec: 409.1, 60 sec: 343.9, 300 sec: 361.0). Total num frames: 7667712. Throughput: 0: 360.1. Samples: 7668352. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 23:31:47,591][187912] Avg episode reward: [(0, '944.790')]
[36m[2025-06-29 23:31:52,578][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7667712. Throughput: 0: 359.5. Samples: 7670592. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 23:31:52,578][187912] Avg episode reward: [(0, '1027.192')]
[36m[2025-06-29 23:31:57,584][187912] Fps is (10 sec: 409.9, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7671808. Throughput: 0: 362.1. Samples: 7671744. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 23:31:57,585][187912] Avg episode reward: [(0, '1012.456')]
[36m[2025-06-29 23:32:02,609][187912] Fps is (10 sec: 408.3, 60 sec: 341.2, 300 sec: 360.9). Total num frames: 7671808. Throughput: 0: 361.0. Samples: 7673888. Policy #0 lag: (min: 10.0, avg: 10.1, max: 26.0)
[36m[2025-06-29 23:32:02,609][187912] Avg episode reward: [(0, '1071.318')]
[36m[2025-06-29 23:32:07,575][187912] Fps is (10 sec: 410.0, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7675904. Throughput: 0: 363.0. Samples: 7675968. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 23:32:07,576][187912] Avg episode reward: [(0, '1066.492')]
[36m[2025-06-29 23:32:12,610][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7675904. Throughput: 0: 362.1. Samples: 7677120. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 23:32:12,610][187912] Avg episode reward: [(0, '1029.034')]
[37m[1m[2025-06-29 23:32:12,659][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000029984_7675904.pth...
[36m[2025-06-29 23:32:12,723][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000029648_7589888.pth
[36m[2025-06-29 23:32:17,563][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.8). Total num frames: 7675904. Throughput: 0: 364.2. Samples: 7679312. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 23:32:17,564][187912] Avg episode reward: [(0, '937.975')]
[36m[2025-06-29 23:32:22,566][187912] Fps is (10 sec: 411.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7680000. Throughput: 0: 356.0. Samples: 7681200. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 23:32:22,566][187912] Avg episode reward: [(0, '895.814')]
[31m[21497594 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21497594 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[21497594 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:32:27,588][187912] Fps is (10 sec: 408.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7680000. Throughput: 0: 358.1. Samples: 7682352. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 23:32:27,588][187912] Avg episode reward: [(0, '904.176')]
[36m[2025-06-29 23:32:32,561][187912] Fps is (10 sec: 409.8, 60 sec: 344.7, 300 sec: 361.0). Total num frames: 7684096. Throughput: 0: 355.4. Samples: 7684336. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 23:32:32,561][187912] Avg episode reward: [(0, '911.610')]
[36m[2025-06-29 23:32:37,558][187912] Fps is (10 sec: 410.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7684096. Throughput: 0: 355.7. Samples: 7686592. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 23:32:37,558][187912] Avg episode reward: [(0, '947.973')]
[36m[2025-06-29 23:32:42,569][187912] Fps is (10 sec: 409.3, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 7688192. Throughput: 0: 353.5. Samples: 7687648. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 23:32:42,569][187912] Avg episode reward: [(0, '982.974')]
[31m[21518241 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21518241 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[21518242 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:32:47,589][187912] Fps is (10 sec: 408.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7688192. Throughput: 0: 352.9. Samples: 7689760. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 23:32:47,590][187912] Avg episode reward: [(0, '998.928')]
[36m[2025-06-29 23:32:52,591][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 7688192. Throughput: 0: 355.4. Samples: 7691968. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 23:32:52,592][187912] Avg episode reward: [(0, '1016.492')]
[36m[2025-06-29 23:32:57,565][187912] Fps is (10 sec: 410.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7692288. Throughput: 0: 349.5. Samples: 7692832. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:32:57,565][187912] Avg episode reward: [(0, '967.559')]
[31m[21531740 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21531741 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[21531741 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:33:02,570][187912] Fps is (10 sec: 410.5, 60 sec: 341.6, 300 sec: 347.7). Total num frames: 7692288. Throughput: 0: 353.4. Samples: 7695216. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:33:02,570][187912] Avg episode reward: [(0, '823.845')]
[36m[2025-06-29 23:33:07,562][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7696384. Throughput: 0: 355.9. Samples: 7697216. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:33:07,562][187912] Avg episode reward: [(0, '804.692')]
[36m[2025-06-29 23:33:12,593][187912] Fps is (10 sec: 408.6, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7696384. Throughput: 0: 356.6. Samples: 7698400. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:33:12,593][187912] Avg episode reward: [(0, '813.717')]
[36m[2025-06-29 23:33:17,562][187912] Fps is (10 sec: 409.6, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7700480. Throughput: 0: 359.5. Samples: 7700512. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:33:17,562][187912] Avg episode reward: [(0, '882.352')]
[36m[2025-06-29 23:33:22,606][187912] Fps is (10 sec: 409.1, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 7700480. Throughput: 0: 356.2. Samples: 7702640. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:33:22,606][187912] Avg episode reward: [(0, '890.016')]
[36m[2025-06-29 23:33:27,853][187912] Fps is (10 sec: 398.0, 60 sec: 407.8, 300 sec: 360.7). Total num frames: 7704576. Throughput: 0: 356.5. Samples: 7703792. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:33:27,853][187912] Avg episode reward: [(0, '1006.229')]
[36m[2025-06-29 23:33:32,594][187912] Fps is (10 sec: 410.1, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 7704576. Throughput: 0: 359.1. Samples: 7705920. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:33:32,595][187912] Avg episode reward: [(0, '1021.626')]
[36m[2025-06-29 23:33:37,594][187912] Fps is (10 sec: 0.0, 60 sec: 341.1, 300 sec: 347.1). Total num frames: 7704576. Throughput: 0: 359.8. Samples: 7708160. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:33:37,595][187912] Avg episode reward: [(0, '964.728')]
[31m[21574049 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21574049 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[21574050 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:33:42,592][187912] Fps is (10 sec: 409.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7708672. Throughput: 0: 360.3. Samples: 7709056. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:33:42,592][187912] Avg episode reward: [(0, '859.255')]
[36m[2025-06-29 23:33:47,585][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7708672. Throughput: 0: 358.3. Samples: 7711344. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:33:47,585][187912] Avg episode reward: [(0, '903.550')]
[36m[2025-06-29 23:33:52,595][187912] Fps is (10 sec: 409.5, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7712768. Throughput: 0: 357.8. Samples: 7713328. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:33:52,595][187912] Avg episode reward: [(0, '913.064')]
[36m[2025-06-29 23:33:57,586][187912] Fps is (10 sec: 409.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7712768. Throughput: 0: 356.3. Samples: 7714432. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:33:57,586][187912] Avg episode reward: [(0, '850.182')]
[36m[2025-06-29 23:34:02,561][187912] Fps is (10 sec: 411.0, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 7716864. Throughput: 0: 359.5. Samples: 7716688. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:34:02,562][187912] Avg episode reward: [(0, '877.034')]
[36m[2025-06-29 23:34:07,568][187912] Fps is (10 sec: 410.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7716864. Throughput: 0: 355.5. Samples: 7718624. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:34:07,568][187912] Avg episode reward: [(0, '858.854')]
[36m[2025-06-29 23:34:12,563][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 7716864. Throughput: 0: 357.1. Samples: 7719760. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:34:12,563][187912] Avg episode reward: [(0, '853.564')]
[37m[1m[2025-06-29 23:34:12,605][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000030144_7716864.pth...
[36m[2025-06-29 23:34:12,675][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000029808_7630848.pth
[36m[2025-06-29 23:34:17,561][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7720960. Throughput: 0: 353.3. Samples: 7721808. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:34:17,561][187912] Avg episode reward: [(0, '804.818')]
[36m[2025-06-29 23:34:22,559][187912] Fps is (10 sec: 409.8, 60 sec: 341.6, 300 sec: 347.6). Total num frames: 7720960. Throughput: 0: 353.0. Samples: 7724032. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:34:22,559][187912] Avg episode reward: [(0, '856.672')]
[36m[2025-06-29 23:34:27,562][187912] Fps is (10 sec: 409.6, 60 sec: 343.0, 300 sec: 361.0). Total num frames: 7725056. Throughput: 0: 356.5. Samples: 7725088. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 23:34:27,562][187912] Avg episode reward: [(0, '889.701')]
[36m[2025-06-29 23:34:32,581][187912] Fps is (10 sec: 408.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7725056. Throughput: 0: 354.2. Samples: 7727280. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 23:34:32,582][187912] Avg episode reward: [(0, '995.510')]
[36m[2025-06-29 23:34:37,584][187912] Fps is (10 sec: 408.7, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 7729152. Throughput: 0: 355.3. Samples: 7729312. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 23:34:37,585][187912] Avg episode reward: [(0, '1024.269')]
[36m[2025-06-29 23:34:42,565][187912] Fps is (10 sec: 410.3, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7729152. Throughput: 0: 355.7. Samples: 7730432. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 23:34:42,565][187912] Avg episode reward: [(0, '1031.131')]
[36m[2025-06-29 23:34:47,565][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 7729152. Throughput: 0: 354.5. Samples: 7732640. Policy #0 lag: (min: 13.0, avg: 13.1, max: 29.0)
[36m[2025-06-29 23:34:47,565][187912] Avg episode reward: [(0, '975.242')]
[36m[2025-06-29 23:34:52,559][187912] Fps is (10 sec: 409.8, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7733248. Throughput: 0: 356.3. Samples: 7734656. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 23:34:52,560][187912] Avg episode reward: [(0, '1005.515')]
[36m[2025-06-29 23:34:57,580][187912] Fps is (10 sec: 409.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 7733248. Throughput: 0: 357.9. Samples: 7735872. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 23:34:57,580][187912] Avg episode reward: [(0, '972.433')]
[36m[2025-06-29 23:35:02,563][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7737344. Throughput: 0: 356.3. Samples: 7737840. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 23:35:02,563][187912] Avg episode reward: [(0, '968.280')]
[36m[2025-06-29 23:35:07,562][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7737344. Throughput: 0: 360.1. Samples: 7740240. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 23:35:07,563][187912] Avg episode reward: [(0, '971.846')]
[36m[2025-06-29 23:35:12,580][187912] Fps is (10 sec: 408.9, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 7741440. Throughput: 0: 363.9. Samples: 7741472. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 23:35:12,580][187912] Avg episode reward: [(0, '1017.051')]
[36m[2025-06-29 23:35:17,581][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7741440. Throughput: 0: 360.2. Samples: 7743488. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 23:35:17,582][187912] Avg episode reward: [(0, '1009.009')]
[36m[2025-06-29 23:35:22,581][187912] Fps is (10 sec: 409.5, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 7745536. Throughput: 0: 361.3. Samples: 7745568. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 23:35:22,581][187912] Avg episode reward: [(0, '885.571')]
[36m[2025-06-29 23:35:27,574][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7745536. Throughput: 0: 359.4. Samples: 7746608. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 23:35:27,574][187912] Avg episode reward: [(0, '863.020')]
[36m[2025-06-29 23:35:32,585][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 7745536. Throughput: 0: 358.2. Samples: 7748768. Policy #0 lag: (min: 5.0, avg: 5.1, max: 21.0)
[36m[2025-06-29 23:35:32,585][187912] Avg episode reward: [(0, '902.346')]
[36m[2025-06-29 23:35:37,590][187912] Fps is (10 sec: 408.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7749632. Throughput: 0: 357.8. Samples: 7750768. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:35:37,590][187912] Avg episode reward: [(0, '949.317')]
[36m[2025-06-29 23:35:42,573][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 347.7). Total num frames: 7749632. Throughput: 0: 357.0. Samples: 7751936. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:35:42,573][187912] Avg episode reward: [(0, '914.623')]
[36m[2025-06-29 23:35:47,558][187912] Fps is (10 sec: 410.9, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7753728. Throughput: 0: 360.9. Samples: 7754080. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:35:47,558][187912] Avg episode reward: [(0, '973.975')]
[36m[2025-06-29 23:35:52,565][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7753728. Throughput: 0: 359.4. Samples: 7756416. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:35:52,565][187912] Avg episode reward: [(0, '919.221')]
[33m[21706158 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[21706158 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.75
[33mCrash Rate: 0.173828125
[33mTimeout Rate: 0.076171875 (navigation_task.py:265)
[33m[21706158 ms][navigation_task] - WARNING : 
[33mSuccesses: 1536
[33mCrashes : 356
[33mTimeouts: 156 (navigation_task.py:268)
[36m[2025-06-29 23:35:57,566][187912] Fps is (10 sec: 409.3, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 7757824. Throughput: 0: 357.4. Samples: 7757552. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 23:35:57,566][187912] Avg episode reward: [(0, '895.419')]
[36m[2025-06-29 23:36:02,581][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7757824. Throughput: 0: 357.7. Samples: 7759584. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 23:36:02,581][187912] Avg episode reward: [(0, '859.192')]
[36m[2025-06-29 23:36:07,733][187912] Fps is (10 sec: 402.9, 60 sec: 408.4, 300 sec: 360.9). Total num frames: 7761920. Throughput: 0: 337.0. Samples: 7760784. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 23:36:07,733][187912] Avg episode reward: [(0, '915.903')]
[36m[2025-06-29 23:36:12,608][187912] Fps is (10 sec: 408.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7761920. Throughput: 0: 362.0. Samples: 7762912. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 23:36:12,608][187912] Avg episode reward: [(0, '933.065')]
[37m[1m[2025-06-29 23:36:12,653][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000030320_7761920.pth...
[36m[2025-06-29 23:36:12,716][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000029984_7675904.pth
[36m[2025-06-29 23:36:17,574][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 7761920. Throughput: 0: 363.8. Samples: 7765136. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 23:36:17,574][187912] Avg episode reward: [(0, '956.927')]
[36m[2025-06-29 23:36:22,571][187912] Fps is (10 sec: 411.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7766016. Throughput: 0: 363.5. Samples: 7767120. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:36:22,571][187912] Avg episode reward: [(0, '893.326')]
[36m[2025-06-29 23:36:27,583][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 347.8). Total num frames: 7766016. Throughput: 0: 363.7. Samples: 7768304. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:36:27,583][187912] Avg episode reward: [(0, '887.053')]
[36m[2025-06-29 23:36:32,584][187912] Fps is (10 sec: 409.1, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7770112. Throughput: 0: 364.2. Samples: 7770480. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:36:32,584][187912] Avg episode reward: [(0, '887.996')]
[36m[2025-06-29 23:36:37,567][187912] Fps is (10 sec: 410.3, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7770112. Throughput: 0: 365.9. Samples: 7772880. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:36:37,567][187912] Avg episode reward: [(0, '975.297')]
[36m[2025-06-29 23:36:42,583][187912] Fps is (10 sec: 409.6, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 7774208. Throughput: 0: 367.2. Samples: 7774080. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 23:36:42,583][187912] Avg episode reward: [(0, '965.922')]
[36m[2025-06-29 23:36:47,585][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7774208. Throughput: 0: 366.9. Samples: 7776096. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 23:36:47,585][187912] Avg episode reward: [(0, '1014.240')]
[36m[2025-06-29 23:36:52,602][187912] Fps is (10 sec: 408.8, 60 sec: 409.3, 300 sec: 361.0). Total num frames: 7778304. Throughput: 0: 368.4. Samples: 7777312. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 23:36:52,602][187912] Avg episode reward: [(0, '1008.109')]
[36m[2025-06-29 23:36:57,580][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7778304. Throughput: 0: 365.7. Samples: 7779360. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 23:36:57,581][187912] Avg episode reward: [(0, '995.768')]
[31m[21771524 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21771524 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[21771524 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[21771566 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21771566 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[21771566 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:37:02,583][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 7778304. Throughput: 0: 365.1. Samples: 7781568. Policy #0 lag: (min: 14.0, avg: 14.0, max: 30.0)
[36m[2025-06-29 23:37:02,583][187912] Avg episode reward: [(0, '848.220')]
[36m[2025-06-29 23:37:07,593][187912] Fps is (10 sec: 409.1, 60 sec: 342.1, 300 sec: 361.0). Total num frames: 7782400. Throughput: 0: 366.0. Samples: 7783600. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:37:07,593][187912] Avg episode reward: [(0, '820.249')]
[36m[2025-06-29 23:37:12,561][187912] Fps is (10 sec: 410.5, 60 sec: 341.6, 300 sec: 361.0). Total num frames: 7782400. Throughput: 0: 366.8. Samples: 7784800. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:37:12,561][187912] Avg episode reward: [(0, '758.605')]
[36m[2025-06-29 23:37:17,577][187912] Fps is (10 sec: 410.3, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7786496. Throughput: 0: 362.4. Samples: 7786784. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:37:17,577][187912] Avg episode reward: [(0, '819.098')]
[31m[21794863 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21794863 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[21794863 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:37:22,574][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7786496. Throughput: 0: 362.6. Samples: 7789200. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:37:22,574][187912] Avg episode reward: [(0, '830.093')]
[36m[2025-06-29 23:37:27,594][187912] Fps is (10 sec: 408.9, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 7790592. Throughput: 0: 360.8. Samples: 7790320. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 23:37:27,594][187912] Avg episode reward: [(0, '877.124')]
[36m[2025-06-29 23:37:32,584][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7790592. Throughput: 0: 358.4. Samples: 7792224. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 23:37:32,584][187912] Avg episode reward: [(0, '915.587')]
[31m[21807077 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21807077 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([3], device='cuda:0') (navigation_task.py:196)
[31m[21807077 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:37:38,070][187912] Fps is (10 sec: 391.0, 60 sec: 406.2, 300 sec: 360.4). Total num frames: 7794688. Throughput: 0: 380.4. Samples: 7794608. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 23:37:38,071][187912] Avg episode reward: [(0, '991.807')]
[36m[2025-06-29 23:37:42,568][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7794688. Throughput: 0: 359.6. Samples: 7795536. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 23:37:42,568][187912] Avg episode reward: [(0, '969.096')]
[36m[2025-06-29 23:37:47,572][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7794688. Throughput: 0: 360.3. Samples: 7797776. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 23:37:47,573][187912] Avg episode reward: [(0, '949.980')]
[36m[2025-06-29 23:37:52,590][187912] Fps is (10 sec: 408.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7798784. Throughput: 0: 360.6. Samples: 7799824. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:37:52,590][187912] Avg episode reward: [(0, '944.114')]
[36m[2025-06-29 23:37:57,583][187912] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7798784. Throughput: 0: 361.1. Samples: 7801056. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:37:57,583][187912] Avg episode reward: [(0, '923.577')]
[36m[2025-06-29 23:38:02,579][187912] Fps is (10 sec: 410.1, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7802880. Throughput: 0: 363.7. Samples: 7803152. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:38:02,579][187912] Avg episode reward: [(0, '887.936')]
[36m[2025-06-29 23:38:07,590][187912] Fps is (10 sec: 409.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7802880. Throughput: 0: 358.3. Samples: 7805328. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:38:07,590][187912] Avg episode reward: [(0, '860.188')]
[36m[2025-06-29 23:38:12,570][187912] Fps is (10 sec: 410.0, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 7806976. Throughput: 0: 360.4. Samples: 7806528. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 23:38:12,570][187912] Avg episode reward: [(0, '842.471')]
[37m[1m[2025-06-29 23:38:12,609][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000030496_7806976.pth...
[36m[2025-06-29 23:38:12,664][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000030144_7716864.pth
[36m[2025-06-29 23:38:17,595][187912] Fps is (10 sec: 409.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7806976. Throughput: 0: 361.9. Samples: 7808512. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 23:38:17,595][187912] Avg episode reward: [(0, '789.997')]
[36m[2025-06-29 23:38:22,558][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.5). Total num frames: 7806976. Throughput: 0: 360.4. Samples: 7810640. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 23:38:22,558][187912] Avg episode reward: [(0, '805.259')]
[36m[2025-06-29 23:38:27,587][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7811072. Throughput: 0: 354.3. Samples: 7811488. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 23:38:27,587][187912] Avg episode reward: [(0, '828.178')]
[36m[2025-06-29 23:38:32,589][187912] Fps is (10 sec: 408.4, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7811072. Throughput: 0: 356.5. Samples: 7813824. Policy #0 lag: (min: 4.0, avg: 4.1, max: 20.0)
[36m[2025-06-29 23:38:32,589][187912] Avg episode reward: [(0, '835.714')]
[36m[2025-06-29 23:38:37,559][187912] Fps is (10 sec: 410.7, 60 sec: 344.3, 300 sec: 361.0). Total num frames: 7815168. Throughput: 0: 359.0. Samples: 7815968. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:38:37,559][187912] Avg episode reward: [(0, '823.321')]
[36m[2025-06-29 23:38:42,592][187912] Fps is (10 sec: 409.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7815168. Throughput: 0: 358.3. Samples: 7817184. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:38:42,592][187912] Avg episode reward: [(0, '847.981')]
[36m[2025-06-29 23:38:47,588][187912] Fps is (10 sec: 408.4, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 7819264. Throughput: 0: 358.7. Samples: 7819296. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:38:47,589][187912] Avg episode reward: [(0, '814.427')]
[36m[2025-06-29 23:38:52,593][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7819264. Throughput: 0: 357.0. Samples: 7821392. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:38:52,593][187912] Avg episode reward: [(0, '833.390')]
[36m[2025-06-29 23:38:57,917][187912] Fps is (10 sec: 396.6, 60 sec: 407.3, 300 sec: 360.6). Total num frames: 7823360. Throughput: 0: 354.2. Samples: 7822592. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 23:38:57,917][187912] Avg episode reward: [(0, '845.658')]
[36m[2025-06-29 23:39:02,571][187912] Fps is (10 sec: 410.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7823360. Throughput: 0: 357.5. Samples: 7824592. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 23:39:02,572][187912] Avg episode reward: [(0, '900.753')]
[31m[21897517 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21897517 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[21897517 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:39:07,588][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7823360. Throughput: 0: 359.9. Samples: 7826848. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 23:39:07,588][187912] Avg episode reward: [(0, '950.156')]
[36m[2025-06-29 23:39:12,569][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7827456. Throughput: 0: 360.7. Samples: 7827712. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 23:39:12,569][187912] Avg episode reward: [(0, '932.230')]
[36m[2025-06-29 23:39:17,578][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7827456. Throughput: 0: 360.3. Samples: 7830032. Policy #0 lag: (min: 5.0, avg: 5.0, max: 21.0)
[36m[2025-06-29 23:39:17,578][187912] Avg episode reward: [(0, '978.400')]
[36m[2025-06-29 23:39:22,564][187912] Fps is (10 sec: 409.8, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7831552. Throughput: 0: 358.0. Samples: 7832080. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 23:39:22,564][187912] Avg episode reward: [(0, '978.995')]
[36m[2025-06-29 23:39:27,565][187912] Fps is (10 sec: 410.1, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7831552. Throughput: 0: 357.6. Samples: 7833264. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 23:39:27,565][187912] Avg episode reward: [(0, '948.060')]
[36m[2025-06-29 23:39:32,586][187912] Fps is (10 sec: 408.7, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 7835648. Throughput: 0: 360.2. Samples: 7835504. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 23:39:32,586][187912] Avg episode reward: [(0, '945.229')]
[36m[2025-06-29 23:39:37,558][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7835648. Throughput: 0: 362.2. Samples: 7837680. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 23:39:37,558][187912] Avg episode reward: [(0, '920.711')]
[36m[2025-06-29 23:39:42,562][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7835648. Throughput: 0: 362.7. Samples: 7838784. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 23:39:42,562][187912] Avg episode reward: [(0, '847.332')]
[36m[2025-06-29 23:39:47,593][187912] Fps is (10 sec: 408.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7839744. Throughput: 0: 359.7. Samples: 7840784. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 23:39:47,593][187912] Avg episode reward: [(0, '837.242')]
[36m[2025-06-29 23:39:52,593][187912] Fps is (10 sec: 408.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7839744. Throughput: 0: 363.0. Samples: 7843184. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 23:39:52,593][187912] Avg episode reward: [(0, '798.791')]
[36m[2025-06-29 23:39:57,574][187912] Fps is (10 sec: 410.4, 60 sec: 343.3, 300 sec: 361.0). Total num frames: 7843840. Throughput: 0: 364.8. Samples: 7844128. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 23:39:57,574][187912] Avg episode reward: [(0, '800.352')]
[31m[21951038 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21951038 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[21951038 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:40:02,558][187912] Fps is (10 sec: 411.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7843840. Throughput: 0: 367.1. Samples: 7846544. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 23:40:02,558][187912] Avg episode reward: [(0, '786.076')]
[36m[2025-06-29 23:40:07,564][187912] Fps is (10 sec: 410.0, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 7847936. Throughput: 0: 366.9. Samples: 7848592. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:40:07,564][187912] Avg episode reward: [(0, '861.945')]
[36m[2025-06-29 23:40:12,584][187912] Fps is (10 sec: 408.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7847936. Throughput: 0: 366.8. Samples: 7849776. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:40:12,584][187912] Avg episode reward: [(0, '844.477')]
[37m[1m[2025-06-29 23:40:12,637][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000030656_7847936.pth...
[36m[2025-06-29 23:40:12,694][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000030320_7761920.pth
[36m[2025-06-29 23:40:17,602][187912] Fps is (10 sec: 408.0, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 7852032. Throughput: 0: 367.9. Samples: 7852064. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:40:17,603][187912] Avg episode reward: [(0, '925.619')]
[31m[21972723 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21972723 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[21972723 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[21974159 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[21974159 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([1], device='cuda:0') (navigation_task.py:196)
[31m[21974160 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:40:22,590][187912] Fps is (10 sec: 409.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7852032. Throughput: 0: 362.1. Samples: 7853984. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:40:22,590][187912] Avg episode reward: [(0, '921.029')]
[36m[2025-06-29 23:40:27,595][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7852032. Throughput: 0: 363.1. Samples: 7855136. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:40:27,596][187912] Avg episode reward: [(0, '968.690')]
[36m[2025-06-29 23:40:32,573][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7856128. Throughput: 0: 367.5. Samples: 7857312. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:40:32,573][187912] Avg episode reward: [(0, '960.431')]
[36m[2025-06-29 23:40:37,583][187912] Fps is (10 sec: 410.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7856128. Throughput: 0: 368.1. Samples: 7859744. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:40:37,583][187912] Avg episode reward: [(0, '1034.157')]
[36m[2025-06-29 23:40:42,591][187912] Fps is (10 sec: 408.8, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 7860224. Throughput: 0: 367.9. Samples: 7860688. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:40:42,592][187912] Avg episode reward: [(0, '1010.322')]
[36m[2025-06-29 23:40:47,585][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7860224. Throughput: 0: 366.4. Samples: 7863040. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:40:47,586][187912] Avg episode reward: [(0, '1081.491')]
[36m[2025-06-29 23:40:52,584][187912] Fps is (10 sec: 409.9, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 7864320. Throughput: 0: 366.4. Samples: 7865088. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:40:52,584][187912] Avg episode reward: [(0, '1028.157')]
[36m[2025-06-29 23:40:57,565][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7864320. Throughput: 0: 366.7. Samples: 7866272. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:40:57,566][187912] Avg episode reward: [(0, '954.549')]
[31m[22012516 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[22012516 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([9], device='cuda:0') (navigation_task.py:196)
[31m[22012516 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:41:02,559][187912] Fps is (10 sec: 410.6, 60 sec: 409.6, 300 sec: 361.2). Total num frames: 7868416. Throughput: 0: 364.4. Samples: 7868448. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:41:02,559][187912] Avg episode reward: [(0, '950.524')]
[36m[2025-06-29 23:41:07,567][187912] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 361.1). Total num frames: 7868416. Throughput: 0: 368.9. Samples: 7870576. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:41:07,567][187912] Avg episode reward: [(0, '916.263')]
[36m[2025-06-29 23:41:12,816][187912] Fps is (10 sec: 399.4, 60 sec: 408.0, 300 sec: 374.6). Total num frames: 7872512. Throughput: 0: 366.9. Samples: 7871728. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:41:12,816][187912] Avg episode reward: [(0, '910.784')]
[36m[2025-06-29 23:41:17,566][187912] Fps is (10 sec: 409.6, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7872512. Throughput: 0: 369.1. Samples: 7873920. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:41:17,566][187912] Avg episode reward: [(0, '1005.491')]
[36m[2025-06-29 23:41:22,566][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7872512. Throughput: 0: 368.5. Samples: 7876320. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:41:22,566][187912] Avg episode reward: [(0, '1092.160')]
[36m[2025-06-29 23:41:27,574][187912] Fps is (10 sec: 409.3, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 7876608. Throughput: 0: 366.4. Samples: 7877168. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:41:27,575][187912] Avg episode reward: [(0, '1081.233')]
[36m[2025-06-29 23:41:32,585][187912] Fps is (10 sec: 408.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7876608. Throughput: 0: 361.6. Samples: 7879312. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:41:32,585][187912] Avg episode reward: [(0, '1141.880')]
[31m[22050194 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[22050194 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[22050194 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:41:37,566][187912] Fps is (10 sec: 409.9, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 7880704. Throughput: 0: 360.3. Samples: 7881296. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 23:41:37,567][187912] Avg episode reward: [(0, '1065.098')]
[36m[2025-06-29 23:41:42,572][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7880704. Throughput: 0: 359.4. Samples: 7882448. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 23:41:42,572][187912] Avg episode reward: [(0, '967.218')]
[36m[2025-06-29 23:41:47,562][187912] Fps is (10 sec: 409.8, 60 sec: 409.8, 300 sec: 361.1). Total num frames: 7884800. Throughput: 0: 364.1. Samples: 7884832. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 23:41:47,562][187912] Avg episode reward: [(0, '995.970')]
[36m[2025-06-29 23:41:52,558][187912] Fps is (10 sec: 410.2, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7884800. Throughput: 0: 365.6. Samples: 7887024. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 23:41:52,558][187912] Avg episode reward: [(0, '973.226')]
[36m[2025-06-29 23:41:57,582][187912] Fps is (10 sec: 408.8, 60 sec: 409.5, 300 sec: 374.9). Total num frames: 7888896. Throughput: 0: 369.2. Samples: 7888256. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 23:41:57,582][187912] Avg episode reward: [(0, '964.489')]
[36m[2025-06-29 23:42:02,597][187912] Fps is (10 sec: 408.0, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 7888896. Throughput: 0: 365.6. Samples: 7890384. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 23:42:02,597][187912] Avg episode reward: [(0, '985.754')]
[36m[2025-06-29 23:42:07,571][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7888896. Throughput: 0: 365.5. Samples: 7892768. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 23:42:07,571][187912] Avg episode reward: [(0, '1034.228')]
[36m[2025-06-29 23:42:12,573][187912] Fps is (10 sec: 410.6, 60 sec: 342.7, 300 sec: 361.0). Total num frames: 7892992. Throughput: 0: 367.3. Samples: 7893696. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 23:42:12,573][187912] Avg episode reward: [(0, '961.951')]
[37m[1m[2025-06-29 23:42:12,624][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000030832_7892992.pth...
[36m[2025-06-29 23:42:12,687][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000030496_7806976.pth
[36m[2025-06-29 23:42:17,591][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7892992. Throughput: 0: 368.3. Samples: 7895888. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 23:42:17,591][187912] Avg episode reward: [(0, '927.987')]
[36m[2025-06-29 23:42:22,589][187912] Fps is (10 sec: 408.9, 60 sec: 409.4, 300 sec: 361.0). Total num frames: 7897088. Throughput: 0: 369.2. Samples: 7897920. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:42:22,590][187912] Avg episode reward: [(0, '929.721')]
[36m[2025-06-29 23:42:27,595][187912] Fps is (10 sec: 409.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7897088. Throughput: 0: 370.6. Samples: 7899136. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:42:27,595][187912] Avg episode reward: [(0, '932.619')]
[36m[2025-06-29 23:42:32,587][187912] Fps is (10 sec: 409.7, 60 sec: 409.6, 300 sec: 361.6). Total num frames: 7901184. Throughput: 0: 363.9. Samples: 7901216. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:42:32,588][187912] Avg episode reward: [(0, '956.060')]
[36m[2025-06-29 23:42:37,561][187912] Fps is (10 sec: 411.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7901184. Throughput: 0: 367.6. Samples: 7903568. Policy #0 lag: (min: 1.0, avg: 1.1, max: 17.0)
[36m[2025-06-29 23:42:37,561][187912] Avg episode reward: [(0, '1003.400')]
[36m[2025-06-29 23:42:42,566][187912] Fps is (10 sec: 410.5, 60 sec: 409.6, 300 sec: 374.9). Total num frames: 7905280. Throughput: 0: 366.0. Samples: 7904720. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:42:42,566][187912] Avg episode reward: [(0, '1024.702')]
[31m[22117644 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[22117644 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[22117644 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:42:47,572][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7905280. Throughput: 0: 363.9. Samples: 7906752. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:42:47,573][187912] Avg episode reward: [(0, '1006.455')]
[36m[2025-06-29 23:42:52,577][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7905280. Throughput: 0: 364.4. Samples: 7909168. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:42:52,577][187912] Avg episode reward: [(0, '980.924')]
[36m[2025-06-29 23:42:57,558][187912] Fps is (10 sec: 410.2, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7909376. Throughput: 0: 362.8. Samples: 7910016. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:42:57,558][187912] Avg episode reward: [(0, '949.748')]
[36m[2025-06-29 23:43:02,593][187912] Fps is (10 sec: 408.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7909376. Throughput: 0: 366.9. Samples: 7912400. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:43:02,594][187912] Avg episode reward: [(0, '862.279')]
[36m[2025-06-29 23:43:07,588][187912] Fps is (10 sec: 408.4, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 7913472. Throughput: 0: 367.3. Samples: 7914448. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 23:43:07,588][187912] Avg episode reward: [(0, '795.102')]
[36m[2025-06-29 23:43:12,582][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7913472. Throughput: 0: 364.9. Samples: 7915552. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 23:43:12,583][187912] Avg episode reward: [(0, '797.076')]
[36m[2025-06-29 23:43:17,565][187912] Fps is (10 sec: 410.6, 60 sec: 409.8, 300 sec: 374.9). Total num frames: 7917568. Throughput: 0: 366.0. Samples: 7917680. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 23:43:17,565][187912] Avg episode reward: [(0, '854.500')]
[36m[2025-06-29 23:43:22,560][187912] Fps is (10 sec: 410.5, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7917568. Throughput: 0: 365.5. Samples: 7920016. Policy #0 lag: (min: 12.0, avg: 12.0, max: 28.0)
[36m[2025-06-29 23:43:22,560][187912] Avg episode reward: [(0, '810.259')]
[36m[2025-06-29 23:43:27,586][187912] Fps is (10 sec: 408.8, 60 sec: 409.7, 300 sec: 374.9). Total num frames: 7921664. Throughput: 0: 363.2. Samples: 7921072. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:43:27,586][187912] Avg episode reward: [(0, '876.618')]
[36m[2025-06-29 23:43:32,593][187912] Fps is (10 sec: 408.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7921664. Throughput: 0: 364.3. Samples: 7923152. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:43:32,593][187912] Avg episode reward: [(0, '888.357')]
[36m[2025-06-29 23:43:37,563][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7921664. Throughput: 0: 361.4. Samples: 7925424. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:43:37,564][187912] Avg episode reward: [(0, '923.284')]
[36m[2025-06-29 23:43:42,567][187912] Fps is (10 sec: 410.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7925760. Throughput: 0: 362.6. Samples: 7926336. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:43:42,568][187912] Avg episode reward: [(0, '834.233')]
[36m[2025-06-29 23:43:47,583][187912] Fps is (10 sec: 408.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7925760. Throughput: 0: 363.5. Samples: 7928752. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:43:47,583][187912] Avg episode reward: [(0, '912.690')]
[36m[2025-06-29 23:43:52,564][187912] Fps is (10 sec: 409.7, 60 sec: 409.7, 300 sec: 361.4). Total num frames: 7929856. Throughput: 0: 361.1. Samples: 7930688. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 23:43:52,564][187912] Avg episode reward: [(0, '910.224')]
[36m[2025-06-29 23:43:57,565][187912] Fps is (10 sec: 410.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7929856. Throughput: 0: 362.4. Samples: 7931856. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 23:43:57,566][187912] Avg episode reward: [(0, '938.082')]
[36m[2025-06-29 23:44:02,573][187912] Fps is (10 sec: 409.2, 60 sec: 409.7, 300 sec: 374.9). Total num frames: 7933952. Throughput: 0: 362.2. Samples: 7933984. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 23:44:02,573][187912] Avg episode reward: [(0, '900.883')]
[36m[2025-06-29 23:44:07,579][187912] Fps is (10 sec: 409.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7933952. Throughput: 0: 358.2. Samples: 7936144. Policy #0 lag: (min: 2.0, avg: 2.1, max: 18.0)
[36m[2025-06-29 23:44:07,580][187912] Avg episode reward: [(0, '934.617')]
[36m[2025-06-29 23:44:12,691][187912] Fps is (10 sec: 404.8, 60 sec: 408.9, 300 sec: 374.7). Total num frames: 7938048. Throughput: 0: 360.4. Samples: 7937328. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 23:44:12,691][187912] Avg episode reward: [(0, '918.727')]
[37m[1m[2025-06-29 23:44:12,734][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000031008_7938048.pth...
[36m[2025-06-29 23:44:12,805][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000030656_7847936.pth
[36m[2025-06-29 23:44:17,571][187912] Fps is (10 sec: 410.0, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7938048. Throughput: 0: 362.1. Samples: 7939440. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 23:44:17,571][187912] Avg episode reward: [(0, '927.660')]
[36m[2025-06-29 23:44:22,576][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7938048. Throughput: 0: 363.3. Samples: 7941776. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 23:44:22,576][187912] Avg episode reward: [(0, '875.143')]
[33m[22220540 ms][navigation_task] - WARNING : Curriculum Level: 50, Curriculum progress fraction: 1.0 (navigation_task.py:262)
[33m[22220541 ms][navigation_task] - WARNING : 
[33mSuccess Rate: 0.75244140625
[33mCrash Rate: 0.17626953125
[33mTimeout Rate: 0.0712890625 (navigation_task.py:265)
[33m[22220541 ms][navigation_task] - WARNING : 
[33mSuccesses: 1541
[33mCrashes : 361
[33mTimeouts: 146 (navigation_task.py:268)
[36m[2025-06-29 23:44:27,580][187912] Fps is (10 sec: 409.2, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7942144. Throughput: 0: 362.9. Samples: 7942672. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 23:44:27,581][187912] Avg episode reward: [(0, '907.070')]
[36m[2025-06-29 23:44:32,569][187912] Fps is (10 sec: 409.9, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7942144. Throughput: 0: 359.2. Samples: 7944912. Policy #0 lag: (min: 7.0, avg: 7.1, max: 23.0)
[36m[2025-06-29 23:44:32,569][187912] Avg episode reward: [(0, '861.231')]
[36m[2025-06-29 23:44:37,564][187912] Fps is (10 sec: 410.3, 60 sec: 409.6, 300 sec: 374.9). Total num frames: 7946240. Throughput: 0: 360.9. Samples: 7946928. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:44:37,564][187912] Avg episode reward: [(0, '942.762')]
[36m[2025-06-29 23:44:42,569][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7946240. Throughput: 0: 359.4. Samples: 7948032. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:44:42,569][187912] Avg episode reward: [(0, '910.972')]
[36m[2025-06-29 23:44:47,678][187912] Fps is (10 sec: 405.0, 60 sec: 409.0, 300 sec: 374.8). Total num frames: 7950336. Throughput: 0: 356.1. Samples: 7950048. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:44:47,678][187912] Avg episode reward: [(0, '933.533')]
[36m[2025-06-29 23:44:52,577][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7950336. Throughput: 0: 354.5. Samples: 7952096. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:44:52,577][187912] Avg episode reward: [(0, '934.056')]
[36m[2025-06-29 23:44:57,587][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7950336. Throughput: 0: 355.3. Samples: 7953280. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:44:57,587][187912] Avg episode reward: [(0, '977.309')]
[36m[2025-06-29 23:45:02,586][187912] Fps is (10 sec: 409.3, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7954432. Throughput: 0: 354.7. Samples: 7955408. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 23:45:02,586][187912] Avg episode reward: [(0, '960.368')]
[36m[2025-06-29 23:45:07,575][187912] Fps is (10 sec: 410.1, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7954432. Throughput: 0: 353.4. Samples: 7957680. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 23:45:07,576][187912] Avg episode reward: [(0, '893.436')]
[36m[2025-06-29 23:45:12,591][187912] Fps is (10 sec: 409.4, 60 sec: 341.9, 300 sec: 361.0). Total num frames: 7958528. Throughput: 0: 354.0. Samples: 7958608. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 23:45:12,591][187912] Avg episode reward: [(0, '967.834')]
[36m[2025-06-29 23:45:17,593][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7958528. Throughput: 0: 355.0. Samples: 7960896. Policy #0 lag: (min: 2.0, avg: 2.0, max: 18.0)
[36m[2025-06-29 23:45:17,593][187912] Avg episode reward: [(0, '953.512')]
[36m[2025-06-29 23:45:22,559][187912] Fps is (10 sec: 410.9, 60 sec: 409.7, 300 sec: 374.9). Total num frames: 7962624. Throughput: 0: 353.5. Samples: 7962832. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 23:45:22,559][187912] Avg episode reward: [(0, '881.774')]
[36m[2025-06-29 23:45:27,584][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7962624. Throughput: 0: 351.5. Samples: 7963856. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 23:45:27,585][187912] Avg episode reward: [(0, '881.106')]
[36m[2025-06-29 23:45:32,590][187912] Fps is (10 sec: 0.0, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7962624. Throughput: 0: 357.7. Samples: 7966112. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 23:45:32,590][187912] Avg episode reward: [(0, '915.056')]
[36m[2025-06-29 23:45:37,583][187912] Fps is (10 sec: 409.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7966720. Throughput: 0: 355.2. Samples: 7968080. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 23:45:37,583][187912] Avg episode reward: [(0, '868.368')]
[36m[2025-06-29 23:45:42,586][187912] Fps is (10 sec: 409.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7966720. Throughput: 0: 351.7. Samples: 7969104. Policy #0 lag: (min: 7.0, avg: 7.0, max: 23.0)
[36m[2025-06-29 23:45:42,586][187912] Avg episode reward: [(0, '842.045')]
[36m[2025-06-29 23:45:47,562][187912] Fps is (10 sec: 410.4, 60 sec: 342.0, 300 sec: 361.0). Total num frames: 7970816. Throughput: 0: 346.1. Samples: 7970976. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:45:47,563][187912] Avg episode reward: [(0, '858.503')]
[36m[2025-06-29 23:45:52,594][187912] Fps is (10 sec: 409.3, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7970816. Throughput: 0: 349.0. Samples: 7973392. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:45:52,594][187912] Avg episode reward: [(0, '854.049')]
[36m[2025-06-29 23:45:57,579][187912] Fps is (10 sec: 408.9, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 7974912. Throughput: 0: 356.0. Samples: 7974624. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:45:57,579][187912] Avg episode reward: [(0, '851.225')]
[36m[2025-06-29 23:46:02,595][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7974912. Throughput: 0: 353.1. Samples: 7976784. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:46:02,595][187912] Avg episode reward: [(0, '828.469')]
[36m[2025-06-29 23:46:07,859][187912] Fps is (10 sec: 398.4, 60 sec: 407.7, 300 sec: 361.0). Total num frames: 7979008. Throughput: 0: 332.0. Samples: 7977872. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:46:07,859][187912] Avg episode reward: [(0, '816.892')]
[36m[2025-06-29 23:46:12,576][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7979008. Throughput: 0: 356.3. Samples: 7979888. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:46:12,576][187912] Avg episode reward: [(0, '746.788')]
[37m[1m[2025-06-29 23:46:12,621][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000031168_7979008.pth...
[36m[2025-06-29 23:46:12,689][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000030832_7892992.pth
[36m[2025-06-29 23:46:17,583][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7979008. Throughput: 0: 354.5. Samples: 7982064. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:46:17,583][187912] Avg episode reward: [(0, '806.325')]
[36m[2025-06-29 23:46:22,594][187912] Fps is (10 sec: 408.9, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 7983104. Throughput: 0: 356.9. Samples: 7984144. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:46:22,594][187912] Avg episode reward: [(0, '813.065')]
[36m[2025-06-29 23:46:27,559][187912] Fps is (10 sec: 410.6, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 7983104. Throughput: 0: 357.9. Samples: 7985200. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:46:27,559][187912] Avg episode reward: [(0, '874.155')]
[31m[22341905 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[22341905 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[22341906 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:46:32,577][187912] Fps is (10 sec: 410.3, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 7987200. Throughput: 0: 361.1. Samples: 7987232. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 23:46:32,578][187912] Avg episode reward: [(0, '820.373')]
[36m[2025-06-29 23:46:37,581][187912] Fps is (10 sec: 408.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7987200. Throughput: 0: 359.2. Samples: 7989552. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 23:46:37,581][187912] Avg episode reward: [(0, '962.555')]
[36m[2025-06-29 23:46:42,604][187912] Fps is (10 sec: 408.5, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 7991296. Throughput: 0: 356.4. Samples: 7990672. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 23:46:42,604][187912] Avg episode reward: [(0, '931.833')]
[36m[2025-06-29 23:46:47,568][187912] Fps is (10 sec: 410.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7991296. Throughput: 0: 352.2. Samples: 7992624. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 23:46:47,568][187912] Avg episode reward: [(0, '912.165')]
[36m[2025-06-29 23:46:52,594][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 7991296. Throughput: 0: 378.8. Samples: 7994816. Policy #0 lag: (min: 14.0, avg: 14.1, max: 30.0)
[36m[2025-06-29 23:46:52,594][187912] Avg episode reward: [(0, '921.781')]
[36m[2025-06-29 23:46:57,579][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 7995392. Throughput: 0: 349.8. Samples: 7995632. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 23:46:57,580][187912] Avg episode reward: [(0, '1001.154')]
[36m[2025-06-29 23:47:02,578][187912] Fps is (10 sec: 410.3, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 7995392. Throughput: 0: 351.0. Samples: 7997856. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 23:47:02,578][187912] Avg episode reward: [(0, '935.770')]
[36m[2025-06-29 23:47:07,574][187912] Fps is (10 sec: 409.8, 60 sec: 343.0, 300 sec: 361.0). Total num frames: 7999488. Throughput: 0: 348.2. Samples: 7999808. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 23:47:07,575][187912] Avg episode reward: [(0, '980.296')]
[36m[2025-06-29 23:47:12,596][187912] Fps is (10 sec: 408.9, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 7999488. Throughput: 0: 351.0. Samples: 8001008. Policy #0 lag: (min: 6.0, avg: 6.1, max: 22.0)
[36m[2025-06-29 23:47:12,596][187912] Avg episode reward: [(0, '1002.446')]
[36m[2025-06-29 23:47:17,583][187912] Fps is (10 sec: 409.2, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 8003584. Throughput: 0: 357.3. Samples: 8003312. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:47:17,583][187912] Avg episode reward: [(0, '1007.923')]
[36m[2025-06-29 23:47:22,593][187912] Fps is (10 sec: 409.7, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 8003584. Throughput: 0: 350.8. Samples: 8005344. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:47:22,593][187912] Avg episode reward: [(0, '908.929')]
[36m[2025-06-29 23:47:27,601][187912] Fps is (10 sec: 0.0, 60 sec: 341.1, 300 sec: 347.1). Total num frames: 8003584. Throughput: 0: 351.0. Samples: 8006464. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:47:27,601][187912] Avg episode reward: [(0, '924.172')]
[36m[2025-06-29 23:47:32,594][187912] Fps is (10 sec: 409.6, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 8007680. Throughput: 0: 351.1. Samples: 8008432. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:47:32,595][187912] Avg episode reward: [(0, '883.601')]
[36m[2025-06-29 23:47:37,593][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 8007680. Throughput: 0: 353.4. Samples: 8010720. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:47:37,594][187912] Avg episode reward: [(0, '891.238')]
[36m[2025-06-29 23:47:42,579][187912] Fps is (10 sec: 410.2, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 8011776. Throughput: 0: 359.5. Samples: 8011808. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:47:42,579][187912] Avg episode reward: [(0, '867.198')]
[36m[2025-06-29 23:47:47,571][187912] Fps is (10 sec: 410.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 8011776. Throughput: 0: 359.2. Samples: 8014016. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:47:47,571][187912] Avg episode reward: [(0, '904.314')]
[36m[2025-06-29 23:47:52,570][187912] Fps is (10 sec: 410.0, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 8015872. Throughput: 0: 359.5. Samples: 8015984. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:47:52,570][187912] Avg episode reward: [(0, '956.985')]
[31m[22426828 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[22426828 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([11], device='cuda:0') (navigation_task.py:196)
[31m[22426829 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[31m[22427872 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[22427872 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([12], device='cuda:0') (navigation_task.py:196)
[31m[22427872 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:47:57,569][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 8015872. Throughput: 0: 359.7. Samples: 8017184. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:47:57,570][187912] Avg episode reward: [(0, '760.308')]
[36m[2025-06-29 23:48:03,011][187912] Fps is (10 sec: 392.3, 60 sec: 406.7, 300 sec: 360.5). Total num frames: 8019968. Throughput: 0: 357.1. Samples: 8019536. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 23:48:03,012][187912] Avg episode reward: [(0, '816.785')]
[36m[2025-06-29 23:48:07,589][187912] Fps is (10 sec: 408.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 8019968. Throughput: 0: 360.6. Samples: 8021568. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 23:48:07,590][187912] Avg episode reward: [(0, '795.854')]
[36m[2025-06-29 23:48:12,571][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.1). Total num frames: 8019968. Throughput: 0: 362.5. Samples: 8022768. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 23:48:12,572][187912] Avg episode reward: [(0, '780.829')]
[37m[1m[2025-06-29 23:48:12,612][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000031328_8019968.pth...
[36m[2025-06-29 23:48:12,673][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000031008_7938048.pth
[36m[2025-06-29 23:48:17,562][187912] Fps is (10 sec: 410.7, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 8024064. Throughput: 0: 366.5. Samples: 8024912. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 23:48:17,563][187912] Avg episode reward: [(0, '799.371')]
[36m[2025-06-29 23:48:22,603][187912] Fps is (10 sec: 408.3, 60 sec: 341.3, 300 sec: 347.1). Total num frames: 8024064. Throughput: 0: 368.3. Samples: 8027296. Policy #0 lag: (min: 11.0, avg: 11.0, max: 27.0)
[36m[2025-06-29 23:48:22,603][187912] Avg episode reward: [(0, '925.199')]
[36m[2025-06-29 23:48:27,583][187912] Fps is (10 sec: 408.8, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 8028160. Throughput: 0: 364.1. Samples: 8028192. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:48:27,584][187912] Avg episode reward: [(0, '891.399')]
[36m[2025-06-29 23:48:32,595][187912] Fps is (10 sec: 409.9, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 8028160. Throughput: 0: 366.7. Samples: 8030528. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:48:32,595][187912] Avg episode reward: [(0, '884.134')]
[36m[2025-06-29 23:48:37,588][187912] Fps is (10 sec: 409.4, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 8032256. Throughput: 0: 368.2. Samples: 8032560. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:48:37,588][187912] Avg episode reward: [(0, '978.129')]
[36m[2025-06-29 23:48:42,591][187912] Fps is (10 sec: 409.8, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 8032256. Throughput: 0: 368.9. Samples: 8033792. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:48:42,591][187912] Avg episode reward: [(0, '910.999')]
[36m[2025-06-29 23:48:47,593][187912] Fps is (10 sec: 409.4, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 8036352. Throughput: 0: 371.5. Samples: 8036096. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:48:47,593][187912] Avg episode reward: [(0, '846.638')]
[36m[2025-06-29 23:48:52,588][187912] Fps is (10 sec: 409.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 8036352. Throughput: 0: 368.4. Samples: 8038144. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:48:52,588][187912] Avg episode reward: [(0, '899.882')]
[36m[2025-06-29 23:48:57,566][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 8036352. Throughput: 0: 366.6. Samples: 8039264. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:48:57,566][187912] Avg episode reward: [(0, '983.980')]
[36m[2025-06-29 23:49:02,575][187912] Fps is (10 sec: 410.1, 60 sec: 343.8, 300 sec: 361.0). Total num frames: 8040448. Throughput: 0: 364.0. Samples: 8041296. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:49:02,575][187912] Avg episode reward: [(0, '898.403')]
[36m[2025-06-29 23:49:07,561][187912] Fps is (10 sec: 409.8, 60 sec: 341.5, 300 sec: 347.3). Total num frames: 8040448. Throughput: 0: 362.3. Samples: 8043584. Policy #0 lag: (min: 15.0, avg: 15.0, max: 31.0)
[36m[2025-06-29 23:49:07,562][187912] Avg episode reward: [(0, '933.805')]
[36m[2025-06-29 23:49:12,574][187912] Fps is (10 sec: 409.6, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 8044544. Throughput: 0: 364.2. Samples: 8044576. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:49:12,574][187912] Avg episode reward: [(0, '981.629')]
[36m[2025-06-29 23:49:17,574][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 8044544. Throughput: 0: 359.3. Samples: 8046688. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:49:17,574][187912] Avg episode reward: [(0, '947.042')]
[36m[2025-06-29 23:49:22,592][187912] Fps is (10 sec: 408.9, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 8048640. Throughput: 0: 361.6. Samples: 8048832. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:49:22,592][187912] Avg episode reward: [(0, '914.807')]
[36m[2025-06-29 23:49:27,572][187912] Fps is (10 sec: 409.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 8048640. Throughput: 0: 360.7. Samples: 8050016. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:49:27,572][187912] Avg episode reward: [(0, '918.524')]
[36m[2025-06-29 23:49:33,285][187912] Fps is (10 sec: 383.1, 60 sec: 404.9, 300 sec: 360.1). Total num frames: 8052736. Throughput: 0: 353.0. Samples: 8052224. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 23:49:33,285][187912] Avg episode reward: [(0, '906.168')]
[36m[2025-06-29 23:49:37,560][187912] Fps is (10 sec: 410.1, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 8052736. Throughput: 0: 358.3. Samples: 8054256. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 23:49:37,560][187912] Avg episode reward: [(0, '855.564')]
[36m[2025-06-29 23:49:42,559][187912] Fps is (10 sec: 0.0, 60 sec: 341.5, 300 sec: 347.3). Total num frames: 8052736. Throughput: 0: 358.5. Samples: 8055392. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 23:49:42,560][187912] Avg episode reward: [(0, '884.353')]
[36m[2025-06-29 23:49:47,577][187912] Fps is (10 sec: 408.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 8056832. Throughput: 0: 358.7. Samples: 8057440. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 23:49:47,577][187912] Avg episode reward: [(0, '843.215')]
[36m[2025-06-29 23:49:52,611][187912] Fps is (10 sec: 407.5, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 8056832. Throughput: 0: 361.6. Samples: 8059872. Policy #0 lag: (min: 8.0, avg: 8.1, max: 24.0)
[36m[2025-06-29 23:49:52,611][187912] Avg episode reward: [(0, '783.351')]
[36m[2025-06-29 23:49:57,574][187912] Fps is (10 sec: 409.7, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 8060928. Throughput: 0: 364.1. Samples: 8060960. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:49:57,575][187912] Avg episode reward: [(0, '814.058')]
[36m[2025-06-29 23:50:02,592][187912] Fps is (10 sec: 410.4, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 8060928. Throughput: 0: 364.3. Samples: 8063088. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:50:02,592][187912] Avg episode reward: [(0, '861.278')]
[36m[2025-06-29 23:50:07,569][187912] Fps is (10 sec: 409.8, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 8065024. Throughput: 0: 360.7. Samples: 8065056. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:50:07,569][187912] Avg episode reward: [(0, '820.724')]
[36m[2025-06-29 23:50:12,565][187912] Fps is (10 sec: 410.7, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 8065024. Throughput: 0: 360.6. Samples: 8066240. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:50:12,565][187912] Avg episode reward: [(0, '814.469')]
[37m[1m[2025-06-29 23:50:12,605][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000031504_8065024.pth...
[36m[2025-06-29 23:50:12,668][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000031168_7979008.pth
[36m[2025-06-29 23:50:17,561][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 8065024. Throughput: 0: 367.5. Samples: 8068496. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:50:17,562][187912] Avg episode reward: [(0, '823.038')]
[36m[2025-06-29 23:50:22,593][187912] Fps is (10 sec: 408.5, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 8069120. Throughput: 0: 363.1. Samples: 8070608. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:50:22,593][187912] Avg episode reward: [(0, '801.464')]
[36m[2025-06-29 23:50:27,574][187912] Fps is (10 sec: 409.1, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 8069120. Throughput: 0: 362.2. Samples: 8071696. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:50:27,574][187912] Avg episode reward: [(0, '814.191')]
[36m[2025-06-29 23:50:32,581][187912] Fps is (10 sec: 410.1, 60 sec: 345.4, 300 sec: 361.0). Total num frames: 8073216. Throughput: 0: 360.9. Samples: 8073680. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:50:32,581][187912] Avg episode reward: [(0, '824.351')]
[36m[2025-06-29 23:50:37,587][187912] Fps is (10 sec: 409.1, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 8073216. Throughput: 0: 359.3. Samples: 8076032. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:50:37,587][187912] Avg episode reward: [(0, '811.766')]
[36m[2025-06-29 23:50:42,558][187912] Fps is (10 sec: 410.6, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 8077312. Throughput: 0: 361.0. Samples: 8077200. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:50:42,558][187912] Avg episode reward: [(0, '805.526')]
[36m[2025-06-29 23:50:47,588][187912] Fps is (10 sec: 409.6, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 8077312. Throughput: 0: 359.1. Samples: 8079248. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:50:47,588][187912] Avg episode reward: [(0, '830.252')]
[36m[2025-06-29 23:50:52,576][187912] Fps is (10 sec: 408.8, 60 sec: 409.8, 300 sec: 361.0). Total num frames: 8081408. Throughput: 0: 364.0. Samples: 8081440. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:50:52,576][187912] Avg episode reward: [(0, '792.659')]
[36m[2025-06-29 23:50:57,575][187912] Fps is (10 sec: 410.2, 60 sec: 341.3, 300 sec: 361.0). Total num frames: 8081408. Throughput: 0: 361.2. Samples: 8082496. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:50:57,575][187912] Avg episode reward: [(0, '814.012')]
[36m[2025-06-29 23:51:02,593][187912] Fps is (10 sec: 0.0, 60 sec: 341.3, 300 sec: 347.4). Total num frames: 8081408. Throughput: 0: 362.1. Samples: 8084800. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:51:02,594][187912] Avg episode reward: [(0, '830.982')]
[36m[2025-06-29 23:51:07,597][187912] Fps is (10 sec: 408.7, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 8085504. Throughput: 0: 358.4. Samples: 8086736. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 23:51:07,597][187912] Avg episode reward: [(0, '883.651')]
[36m[2025-06-29 23:51:12,589][187912] Fps is (10 sec: 409.8, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 8085504. Throughput: 0: 357.9. Samples: 8087808. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 23:51:12,589][187912] Avg episode reward: [(0, '832.309')]
[36m[2025-06-29 23:51:17,576][187912] Fps is (10 sec: 410.5, 60 sec: 409.5, 300 sec: 361.0). Total num frames: 8089600. Throughput: 0: 357.0. Samples: 8089744. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 23:51:17,576][187912] Avg episode reward: [(0, '873.073')]
[36m[2025-06-29 23:51:22,562][187912] Fps is (10 sec: 410.7, 60 sec: 341.5, 300 sec: 361.0). Total num frames: 8089600. Throughput: 0: 357.5. Samples: 8092112. Policy #0 lag: (min: 9.0, avg: 9.1, max: 25.0)
[36m[2025-06-29 23:51:22,563][187912] Avg episode reward: [(0, '793.248')]
[36m[2025-06-29 23:51:27,572][187912] Fps is (10 sec: 409.8, 60 sec: 409.6, 300 sec: 361.0). Total num frames: 8093696. Throughput: 0: 354.4. Samples: 8093152. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 23:51:27,572][187912] Avg episode reward: [(0, '808.970')]
[36m[2025-06-29 23:51:32,617][187912] Fps is (10 sec: 407.4, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 8093696. Throughput: 0: 352.8. Samples: 8095136. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 23:51:32,617][187912] Avg episode reward: [(0, '833.450')]
[36m[2025-06-29 23:51:37,579][187912] Fps is (10 sec: 0.0, 60 sec: 341.4, 300 sec: 347.1). Total num frames: 8093696. Throughput: 0: 353.4. Samples: 8097344. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 23:51:37,580][187912] Avg episode reward: [(0, '847.650')]
[36m[2025-06-29 23:51:42,590][187912] Fps is (10 sec: 410.7, 60 sec: 341.1, 300 sec: 361.0). Total num frames: 8097792. Throughput: 0: 350.5. Samples: 8098272. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 23:51:42,590][187912] Avg episode reward: [(0, '842.319')]
[36m[2025-06-29 23:51:47,583][187912] Fps is (10 sec: 409.5, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 8097792. Throughput: 0: 352.8. Samples: 8100672. Policy #0 lag: (min: 11.0, avg: 11.1, max: 27.0)
[36m[2025-06-29 23:51:47,583][187912] Avg episode reward: [(0, '831.867')]
[36m[2025-06-29 23:51:52,558][187912] Fps is (10 sec: 410.9, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 8101888. Throughput: 0: 355.5. Samples: 8102720. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 23:51:52,558][187912] Avg episode reward: [(0, '803.988')]
[36m[2025-06-29 23:51:57,593][187912] Fps is (10 sec: 409.2, 60 sec: 341.2, 300 sec: 361.0). Total num frames: 8101888. Throughput: 0: 356.6. Samples: 8103856. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 23:51:57,594][187912] Avg episode reward: [(0, '862.502')]
[31m[22672893 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[22672894 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([5], device='cuda:0') (navigation_task.py:196)
[31m[22672894 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:52:02,574][187912] Fps is (10 sec: 409.0, 60 sec: 409.7, 300 sec: 361.0). Total num frames: 8105984. Throughput: 0: 361.6. Samples: 8106016. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 23:52:02,574][187912] Avg episode reward: [(0, '862.366')]
[36m[2025-06-29 23:52:07,584][187912] Fps is (10 sec: 410.0, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 8105984. Throughput: 0: 359.3. Samples: 8108288. Policy #0 lag: (min: 0.0, avg: 0.1, max: 16.0)
[36m[2025-06-29 23:52:07,585][187912] Avg episode reward: [(0, '865.737')]
[36m[2025-06-29 23:52:12,920][187912] Fps is (10 sec: 395.9, 60 sec: 407.4, 300 sec: 360.6). Total num frames: 8110080. Throughput: 0: 358.1. Samples: 8109392. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:52:12,920][187912] Avg episode reward: [(0, '897.143')]
[37m[1m[2025-06-29 23:52:12,973][187912] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000031680_8110080.pth...
[36m[2025-06-29 23:52:13,062][187912] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_first_test/checkpoint_p0/checkpoint_000031328_8019968.pth
[31m[22689666 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[22689667 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[22689667 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-06-29 23:52:17,566][187912] Fps is (10 sec: 410.4, 60 sec: 341.4, 300 sec: 361.0). Total num frames: 8110080. Throughput: 0: 355.6. Samples: 8111120. Policy #0 lag: (min: 15.0, avg: 15.1, max: 31.0)
[36m[2025-06-29 23:52:17,566][187912] Avg episode reward: [(0, '902.692')]
[37m[1m[2025-06-29 23:52:21,698][187912] Keyboard interrupt detected in the event loop EvtLoop [Runner_EvtLoop, process=main process 187912], exiting...
[37m[1m[2025-06-29 23:52:21,698][187912] Runner profile tree view:
[37m[1mmain_loop: 22683.1899
[37m[1m[2025-06-29 23:52:21,698][187912] Collected {0: 8110080}, FPS: 357.5