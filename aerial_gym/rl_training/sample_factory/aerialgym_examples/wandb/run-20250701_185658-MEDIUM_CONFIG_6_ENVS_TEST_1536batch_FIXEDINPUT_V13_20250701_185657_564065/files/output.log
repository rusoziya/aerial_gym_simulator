Importing module 'gym_38' (/home/ziyar/aerialgym/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_38.so)
Setting GYM_USD_PLUG_INFO_PATH to /home/ziyar/aerialgym/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json
[36m[2025-07-01 18:57:01,161][132155] Queried available GPUs: 0
[37m[1m[2025-07-01 18:57:01,161][132155] Environment var CUDA_VISIBLE_DEVICES is 0
PyTorch version 1.13.1
Device count 1
/home/ziyar/aerialgym/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/_bindings/src/gymtorch
ninja: no work to do.
Warp 1.0.0-beta.5 initialized:
   CUDA Toolkit: 11.5, Driver: 12.4
   Devices:
     "cpu"    | x86_64
     "cuda:0" | NVIDIA GeForce RTX 4080 Laptop GPU (sm_89)
   Kernel cache: /home/ziyar/.cache/warp/1.0.0-beta.5
[SUBPROCESS] DCE task action_space_dim: 3
[SUBPROCESS] Target Sample Factory action space: 6D
[SUBPROCESS] Setting num_envs to 6 based on env_agents=6
[SUBPROCESS] Set SF_ENV_AGENTS=6 environment variable
[SUBPROCESS] DCE config batch_size: 1536
[SUBPROCESS] Using MODIFIED ARCHITECTURE (6 environments - inference compatible)
Registered quad_with_obstacles and dce_navigation_task in subprocess
[isaacgym:gymutil.py] Unknown args:  ['--env=quad_with_obstacles', '--train_for_env_steps=100000000', '--experiment=MEDIUM_CONFIG_6_ENVS_TEST_1536batch_FIXEDINPUT_V13', '--async_rl=True', '--use_env_info_cache=False', '--normalize_input=True']
Not connected to PVD
+++ Using GPU PhysX
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/torch/utils/cpp_extension.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
  from pkg_resources import packaging  # type: ignore[attr-defined]
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
Using /home/ziyar/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Emitting ninja build file /home/ziyar/.cache/torch_extensions/py38_cu117/gymtorch/build.ninja...
Building extension module gymtorch...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module gymtorch...
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/networkx/classes/graph.py:23: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  from collections import Mapping
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/networkx/classes/reportviews.py:95: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  from collections import Mapping, Set, Iterable
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/networkx/readwrite/graphml.py:346: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.int, "int"), (np.int8, "int"),
/home/ziyar/aerialgym/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/torch_utils.py:135: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  def get_axis_params(value, axis_idx, x_value=0., dtype=np.float, n_dims=3):
[37m[1832 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task] - INFO : Found SF_ENV_AGENTS environment variable: 6 (dce_navigation_task.py:23)
[37m[1832 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task] - INFO : Detected env_agents=6 from environment - setting environment count. (dce_navigation_task.py:29)
[37m[1832 ms][base_task] - INFO : Setting seed: 3199937090 (base_task.py:38)
[37m[1833 ms][navigation_task] - INFO : Building environment for navigation task. (navigation_task.py:44)
[37m[1833 ms][navigation_task] - INFO : Sim Name: base_sim, Env Name: env_with_obstacles, Robot Name: lmf2, Controller Name: lmf2_velocity_control (navigation_task.py:45)
[37m[1833 ms][env_manager] - INFO : Populating environments. (env_manager.py:73)
[37m[1833 ms][env_manager] - INFO : Creating simulation instance. (env_manager.py:87)
[37m[1833 ms][env_manager] - INFO : Instantiating IGE object. (env_manager.py:88)
[37m[1833 ms][IsaacGymEnvManager] - INFO : Creating Isaac Gym Environment (IGE_env_manager.py:41)
[37m[1833 ms][IsaacGymEnvManager] - INFO : Acquiring gym object (IGE_env_manager.py:73)
[37m[1833 ms][IsaacGymEnvManager] - INFO : Acquired gym object (IGE_env_manager.py:75)
[37m[1834 ms][IsaacGymEnvManager] - INFO : Fixing devices (IGE_env_manager.py:89)
[37m[1834 ms][IsaacGymEnvManager] - INFO : Using GPU pipeline for simulation. (IGE_env_manager.py:102)
[37m[1834 ms][IsaacGymEnvManager] - INFO : Sim Device type: cuda, Sim Device ID: 0 (IGE_env_manager.py:105)
[31m[1834 ms][IsaacGymEnvManager] - CRITICAL : 
[31m Setting graphics device to -1.
[31m This is done because the simulation is run in headless mode and no Isaac Gym cameras are used.
[31m No need to worry. The simulation and warp rendering will work as expected. (IGE_env_manager.py:112)
[37m[1834 ms][IsaacGymEnvManager] - INFO : Graphics Device ID: -1 (IGE_env_manager.py:119)
[37m[1834 ms][IsaacGymEnvManager] - INFO : Creating Isaac Gym Simulation Object (IGE_env_manager.py:120)
[33m[1834 ms][IsaacGymEnvManager] - WARNING : If you have set the CUDA_VISIBLE_DEVICES environment variable, please ensure that you set it
[33mto a particular one that works for your system to use the viewer or Isaac Gym cameras.
[33mIf you want to run parallel simulations on multiple GPUs with camera sensors,
[33mplease disable Isaac Gym and use warp (by setting use_warp=True), set the viewer to headless. (IGE_env_manager.py:127)
[33m[1834 ms][IsaacGymEnvManager] - WARNING : If you see a segfault in the next lines, it is because of the discrepancy between the CUDA device and the graphics device.
[33mPlease ensure that the CUDA device and the graphics device are the same. (IGE_env_manager.py:132)
Physics Engine: PhysX
Physics Device: cuda:0
GPU Pipeline: enabled
*** Can't create empty tensor
WARNING: allocation matrix is not full rank. Rank: 4
creating render graph
Module warp.utils load on device 'cuda:0' took 1.39 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_camera_kernels load on device 'cuda:0' took 7.74 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_stereo_camera_kernels load on device 'cuda:0' took 12.42 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_lidar_kernels load on device 'cuda:0' took 5.70 ms
finishing capture of render graph
Encoder network initialized.
Defined encoder.
[ImgDecoder] Starting create_model
[ImgDecoder] Done with create_model
Defined decoder.
Loading weights from file:  /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/utils/vae/weights/ICRA_test_set_more_sim_data_kld_beta_3_LD_64_epoch_49.pth
[AerialGymVecEnv] Forced action space shape: (6,)
[AerialGymVecEnv] is_multiagent: True, num_agents: 6
[make_aerialgym_env] Final action space shape: (6,)
[make_aerialgym_env] Action space: Box(-1.0, 1.0, (6,), float32)
[37m[2659 ms][IsaacGymEnvManager] - INFO : Created Isaac Gym Simulation Object (IGE_env_manager.py:136)
[37m[2659 ms][IsaacGymEnvManager] - INFO : Created Isaac Gym Environment (IGE_env_manager.py:43)
[37m[2864 ms][env_manager] - INFO : IGE object instantiated. (env_manager.py:109)
[37m[2864 ms][env_manager] - INFO : Creating warp environment. (env_manager.py:112)
[37m[2864 ms][env_manager] - INFO : Warp environment created. (env_manager.py:114)
[37m[2864 ms][env_manager] - INFO : Creating robot manager. (env_manager.py:118)
[37m[2864 ms][BaseRobot] - INFO : [DONE] Initializing controller (base_robot.py:26)
[37m[2864 ms][BaseRobot] - INFO : Initializing controller lmf2_velocity_control (base_robot.py:29)
[33m[2864 ms][base_multirotor] - WARNING : Creating 6 multirotors. (base_multirotor.py:32)
[37m[2865 ms][env_manager] - INFO : [DONE] Creating robot manager. (env_manager.py:123)
[37m[2865 ms][env_manager] - INFO : [DONE] Creating simulation instance. (env_manager.py:125)
[37m[2865 ms][asset_loader] - INFO : Loading asset: model.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2865 ms][asset_loader] - INFO : Loading asset: panel.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2867 ms][asset_loader] - INFO : Loading asset: 0_5_x_0_5_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2868 ms][asset_loader] - INFO : Loading asset: 1_x_1_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2869 ms][asset_loader] - INFO : Loading asset: left_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2870 ms][asset_loader] - INFO : Loading asset: right_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2871 ms][asset_loader] - INFO : Loading asset: back_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2872 ms][asset_loader] - INFO : Loading asset: front_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2873 ms][asset_loader] - INFO : Loading asset: bottom_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2874 ms][asset_loader] - INFO : Loading asset: top_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2875 ms][asset_loader] - INFO : Loading asset: small_cube.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2876 ms][asset_loader] - INFO : Loading asset: cuboidal_rod.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2877 ms][asset_loader] - INFO : Loading asset: gate.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2879 ms][env_manager] - INFO : Populating environment 0 (env_manager.py:179)
[33m[3292 ms][robot_manager] - WARNING : 
[33mRobot mass: 1.2400000467896461,
[33mInertia: tensor([[0.0134, 0.0000, 0.0000],
[33m        [0.0000, 0.0144, 0.0000],
[33m        [0.0000, 0.0000, 0.0138]], device='cuda:0'),
[33mRobot COM: tensor([[0., 0., 0., 1.]], device='cuda:0') (robot_manager.py:437)
[33m[3292 ms][robot_manager] - WARNING : Calculated robot mass and inertia for this robot. This code assumes that your robot is the same across environments. (robot_manager.py:440)
[31m[3292 ms][robot_manager] - CRITICAL : If your robot differs across environments you need to perform this computation for each different robot here. (robot_manager.py:443)
[37m[3302 ms][env_manager] - INFO : [DONE] Populating environments. (env_manager.py:75)
[33m[3308 ms][IsaacGymEnvManager] - WARNING : Headless: True (IGE_env_manager.py:424)
[37m[3308 ms][IsaacGymEnvManager] - INFO : Headless mode. Viewer not created. (IGE_env_manager.py:434)
[33m[3354 ms][asset_manager] - WARNING : Number of obstacles to be kept in the environment: 9 (asset_manager.py:32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/control/motor_model.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(self.min_thrust, device=self.device, dtype=torch.float32).expand(
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/control/motor_model.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(self.max_thrust, device=self.device, dtype=torch.float32).expand(
[33m[3547 ms][control_allocation] - WARNING : Control allocation does not account for actuator limits. This leads to suboptimal allocation (control_allocation.py:48)
[37m[3547 ms][WarpSensor] - INFO : Camera sensor initialized (warp_sensor.py:50)
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.
  logger.warn(
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.
  logger.warn(
[36m[2025-07-01 18:57:05,479][132268] Env info: EnvInfo(obs_space=Dict('obs': Box(-inf, inf, (81,), float32)), action_space=Box(-1.0, 1.0, (6,), float32), num_agents=6, gpu_actions=True, gpu_observations=True, action_splits=None, all_discrete=None, frameskip=1, reward_shaping_scheme=None, env_info_protocol_version=1)
[isaacgym:gymutil.py] Unknown args:  ['--env=quad_with_obstacles', '--train_for_env_steps=100000000', '--experiment=MEDIUM_CONFIG_6_ENVS_TEST_1536batch_FIXEDINPUT_V13', '--async_rl=True', '--use_env_info_cache=False', '--normalize_input=True']
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:0
GPU Pipeline: enabled
*** Can't create empty tensor
WARNING: allocation matrix is not full rank. Rank: 4
[33m[2025-07-01 18:57:06,200][132155] In serial mode all components run on the same process. Only use async_rl and serial mode together for debugging.
[36m[2025-07-01 18:57:06,201][132155] Starting experiment with the following configuration:
[36mhelp=False
[36malgo=APPO
[36menv=quad_with_obstacles
[36mexperiment=MEDIUM_CONFIG_6_ENVS_TEST_1536batch_FIXEDINPUT_V13
[36mtrain_dir=/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir
[36mrestart_behavior=resume
[36mdevice=gpu
[36mseed=None
[36mnum_policies=1
[36masync_rl=True
[36mserial_mode=True
[36mbatched_sampling=True
[36mnum_batches_to_accumulate=2
[36mworker_num_splits=1
[36mpolicy_workers_per_policy=1
[36mmax_policy_lag=1000
[36mnum_workers=1
[36mnum_envs_per_worker=1
[36mbatch_size=1536
[36mnum_batches_per_epoch=4
[36mnum_epochs=4
[36mrollout=32
[36mrecurrence=32
[36mshuffle_minibatches=False
[36mgamma=0.98
[36mreward_scale=0.1
[36mreward_clip=1000.0
[36mvalue_bootstrap=True
[36mnormalize_returns=True
[36mexploration_loss_coeff=0.001
[36mvalue_loss_coeff=2.0
[36mkl_loss_coeff=0.1
[36mexploration_loss=entropy
[36mgae_lambda=0.95
[36mppo_clip_ratio=0.2
[36mppo_clip_value=1.0
[36mwith_vtrace=False
[36mvtrace_rho=1.0
[36mvtrace_c=1.0
[36moptimizer=adam
[36madam_eps=1e-06
[36madam_beta1=0.9
[36madam_beta2=0.999
[36mmax_grad_norm=1.0
[36mlearning_rate=0.0003
[36mlr_schedule=kl_adaptive_epoch
[36mlr_schedule_kl_threshold=0.016
[36mlr_adaptive_min=1e-06
[36mlr_adaptive_max=0.01
[36mobs_subtract_mean=0.0
[36mobs_scale=1.0
[36mnormalize_input=True
[36mnormalize_input_keys=None
[36mdecorrelate_experience_max_seconds=0
[36mdecorrelate_envs_on_one_worker=True
[36mactor_worker_gpus=[0]
[36mset_workers_cpu_affinity=True
[36mforce_envs_single_thread=False
[36mdefault_niceness=0
[36mlog_to_file=True
[36mexperiment_summaries_interval=10
[36mflush_summaries_interval=30
[36mstats_avg=100
[36msummaries_use_frameskip=True
[36mheartbeat_interval=20
[36mheartbeat_reporting_interval=180
[36mtrain_for_env_steps=100000000
[36mtrain_for_seconds=10000000000
[36msave_every_sec=120
[36mkeep_checkpoints=2
[36mload_checkpoint_kind=latest
[36msave_milestones_sec=-1
[36msave_best_every_sec=5
[36msave_best_metric=reward
[36msave_best_after=5000000
[36mbenchmark=False
[36mencoder_mlp_layers=[512, 256, 64]
[36mencoder_conv_architecture=convnet_simple
[36mencoder_conv_mlp_layers=[]
[36muse_rnn=True
[36mrnn_size=64
[36mrnn_type=gru
[36mrnn_num_layers=1
[36mdecoder_mlp_layers=[]
[36mnonlinearity=elu
[36mpolicy_initialization=torch_default
[36mpolicy_init_gain=1.0
[36mactor_critic_share_weights=True
[36madaptive_stddev=True
[36mcontinuous_tanh_scale=0.0
[36minitial_stddev=1.0
[36muse_env_info_cache=False
[36menv_gpu_actions=True
[36menv_gpu_observations=True
[36menv_frameskip=1
[36menv_framestack=1
[36mpixel_format=CHW
[36muse_record_episode_statistics=False
[36mwith_wandb=True
[36mwandb_user=ziya-ruso-ucl
[36mwandb_project=vae_rl_navigation
[36mwandb_group=dce_navigation_training
[36mwandb_job_type=SF
[36mwandb_tags=['aerial_gym', 'dce', 'navigation', 'sample_factory']
[36mwith_pbt=False
[36mpbt_mix_policies_in_one_env=True
[36mpbt_period_env_steps=5000000
[36mpbt_start_mutation=20000000
[36mpbt_replace_fraction=0.3
[36mpbt_mutation_rate=0.15
[36mpbt_replace_reward_gap=0.1
[36mpbt_replace_reward_gap_absolute=1e-06
[36mpbt_optimize_gamma=False
[36mpbt_target_objective=true_objective
[36mpbt_perturb_min=1.1
[36mpbt_perturb_max=1.5
[36menv_agents=6
[36mobs_key=obs
[36msubtask=None
[36mige_api_version=preview4
[36meval_stats=False
[36maction_space_dim=6
[36mcommand_line=--env=quad_with_obstacles --train_for_env_steps=100000000 --experiment=MEDIUM_CONFIG_6_ENVS_TEST_1536batch_FIXEDINPUT_V13 --async_rl=True --use_env_info_cache=False --normalize_input=True
[36mcli_args={'env': 'quad_with_obstacles', 'experiment': 'MEDIUM_CONFIG_6_ENVS_TEST_1536batch_FIXEDINPUT_V13', 'async_rl': True, 'normalize_input': True, 'train_for_env_steps': 100000000, 'use_env_info_cache': False}
[36mgit_hash=7f35eed17f2afcde33e3a7aec669b48e9e8e34cd
[36mgit_repo_name=https://github.com/ntnu-arl/aerial_gym_simulator.git
[36mwandb_unique_id=MEDIUM_CONFIG_6_ENVS_TEST_1536batch_FIXEDINPUT_V13_20250701_185657_564065
[36m[2025-07-01 18:57:06,202][132155] Saving configuration to /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/MEDIUM_CONFIG_6_ENVS_TEST_1536batch_FIXEDINPUT_V13/config.json...
[36m[2025-07-01 18:57:06,398][132155] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[36m[2025-07-01 18:57:06,399][132155] Rollout worker 0 uses device cuda:0
[36m[2025-07-01 18:57:06,408][132155] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[37m[1m[2025-07-01 18:57:06,408][132155] InferenceWorker_p0-w0: min num requests: 1
[36m[2025-07-01 18:57:06,408][132155] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[37m[1m[2025-07-01 18:57:06,409][132155] Starting seed is not provided
[36m[2025-07-01 18:57:06,409][132155] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[36m[2025-07-01 18:57:06,409][132155] Initializing actor-critic model on device cuda:0
[36m[2025-07-01 18:57:06,409][132155] RunningMeanStd input shape: (81,)
[36m[2025-07-01 18:57:06,410][132155] RunningMeanStd input shape: (1,)
[36m[2025-07-01 18:57:06,435][132155] Created Actor Critic model with architecture:
[36m[2025-07-01 18:57:06,436][132155] ActorCriticSharedWeights(
[36m  (obs_normalizer): ObservationNormalizer(
[36m    (running_mean_std): RunningMeanStdDictInPlace(
[36m      (running_mean_std): ModuleDict(
[36m        (obs): RunningMeanStdInPlace()
[36m      )
[36m    )
[36m  )
[36m  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)
[36m  (encoder): MultiInputEncoder(
[36m    (encoders): ModuleDict(
[36m      (obs): MlpEncoder(
[36m        (mlp_head): RecursiveScriptModule(
[36m          original_name=Sequential
[36m          (0): RecursiveScriptModule(original_name=Linear)
[36m          (1): RecursiveScriptModule(original_name=ELU)
[36m          (2): RecursiveScriptModule(original_name=Linear)
[36m          (3): RecursiveScriptModule(original_name=ELU)
[36m          (4): RecursiveScriptModule(original_name=Linear)
[36m          (5): RecursiveScriptModule(original_name=ELU)
[36m        )
[36m      )
[36m    )
[36m  )
[36m  (core): ModelCoreRNN(
[36m    (core): GRU(64, 64)
[36m  )
[36m  (decoder): MlpDecoder(
[36m    (mlp): Identity()
[36m  )
[36m  (critic_linear): Linear(in_features=64, out_features=1, bias=True)
[36m  (action_parameterization): ActionParameterizationDefault(
[36m    (distribution_linear): Linear(in_features=64, out_features=12, bias=True)
[36m  )
[36m)
[36m[2025-07-01 18:57:06,868][132155] Using optimizer <class 'torch.optim.adam.Adam'>
[33m[2025-07-01 18:57:06,868][132155] No checkpoints found
[36m[2025-07-01 18:57:06,868][132155] Did not load from checkpoint, starting from scratch!
[36m[2025-07-01 18:57:06,868][132155] Initialized policy 0 weights for model version 0
[36m[2025-07-01 18:57:06,868][132155] LearnerWorker_p0 finished initialization!
[36m[2025-07-01 18:57:06,869][132155] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[37m[1m[2025-07-01 18:57:06,874][132155] Inference worker 0-0 is ready!
[37m[1m[2025-07-01 18:57:06,874][132155] All inference workers are ready! Signal rollout workers to start!
[36m[2025-07-01 18:57:06,875][132155] EnvRunner 0-0 uses policy 0
[37m[11663 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task] - INFO : Found SF_ENV_AGENTS environment variable: 6 (dce_navigation_task.py:23)
[37m[11663 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task] - INFO : Detected env_agents=6 from environment - setting environment count. (dce_navigation_task.py:29)
[37m[11663 ms][base_task] - INFO : Setting seed: 2602836869 (base_task.py:38)
[37m[11663 ms][navigation_task] - INFO : Building environment for navigation task. (navigation_task.py:44)
[37m[11663 ms][navigation_task] - INFO : Sim Name: base_sim, Env Name: env_with_obstacles, Robot Name: lmf2, Controller Name: lmf2_velocity_control (navigation_task.py:45)
[37m[11664 ms][env_manager] - INFO : Populating environments. (env_manager.py:73)
[37m[11664 ms][env_manager] - INFO : Creating simulation instance. (env_manager.py:87)
[37m[11664 ms][env_manager] - INFO : Instantiating IGE object. (env_manager.py:88)
[37m[11664 ms][IsaacGymEnvManager] - INFO : Creating Isaac Gym Environment (IGE_env_manager.py:41)
[37m[11664 ms][IsaacGymEnvManager] - INFO : Acquiring gym object (IGE_env_manager.py:73)
[37m[11664 ms][IsaacGymEnvManager] - INFO : Acquired gym object (IGE_env_manager.py:75)
[37m[11665 ms][IsaacGymEnvManager] - INFO : Fixing devices (IGE_env_manager.py:89)
[37m[11665 ms][IsaacGymEnvManager] - INFO : Using GPU pipeline for simulation. (IGE_env_manager.py:102)
[37m[11665 ms][IsaacGymEnvManager] - INFO : Sim Device type: cuda, Sim Device ID: 0 (IGE_env_manager.py:105)
[31m[11665 ms][IsaacGymEnvManager] - CRITICAL : 
[31m Setting graphics device to -1.
[31m This is done because the simulation is run in headless mode and no Isaac Gym cameras are used.
[31m No need to worry. The simulation and warp rendering will work as expected. (IGE_env_manager.py:112)
[37m[11665 ms][IsaacGymEnvManager] - INFO : Graphics Device ID: -1 (IGE_env_manager.py:119)
[37m[11665 ms][IsaacGymEnvManager] - INFO : Creating Isaac Gym Simulation Object (IGE_env_manager.py:120)
[33m[11665 ms][IsaacGymEnvManager] - WARNING : If you have set the CUDA_VISIBLE_DEVICES environment variable, please ensure that you set it
[33mto a particular one that works for your system to use the viewer or Isaac Gym cameras.
[33mIf you want to run parallel simulations on multiple GPUs with camera sensors,
[33mplease disable Isaac Gym and use warp (by setting use_warp=True), set the viewer to headless. (IGE_env_manager.py:127)
[33m[11665 ms][IsaacGymEnvManager] - WARNING : If you see a segfault in the next lines, it is because of the discrepancy between the CUDA device and the graphics device.
[33mPlease ensure that the CUDA device and the graphics device are the same. (IGE_env_manager.py:132)
[37m[12494 ms][IsaacGymEnvManager] - INFO : Created Isaac Gym Simulation Object (IGE_env_manager.py:136)
[37m[12494 ms][IsaacGymEnvManager] - INFO : Created Isaac Gym Environment (IGE_env_manager.py:43)
[37m[12699 ms][env_manager] - INFO : IGE object instantiated. (env_manager.py:109)
[37m[12699 ms][env_manager] - INFO : Creating warp environment. (env_manager.py:112)
[37m[12699 ms][env_manager] - INFO : Warp environment created. (env_manager.py:114)
[37m[12699 ms][env_manager] - INFO : Creating robot manager. (env_manager.py:118)
[37m[12699 ms][BaseRobot] - INFO : [DONE] Initializing controller (base_robot.py:26)
[37m[12699 ms][BaseRobot] - INFO : Initializing controller lmf2_velocity_control (base_robot.py:29)
[33m[12699 ms][base_multirotor] - WARNING : Creating 6 multirotors. (base_multirotor.py:32)
[37m[12699 ms][env_manager] - INFO : [DONE] Creating robot manager. (env_manager.py:123)
[37m[12699 ms][env_manager] - INFO : [DONE] Creating simulation instance. (env_manager.py:125)
[37m[12699 ms][asset_loader] - INFO : Loading asset: model.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[12700 ms][asset_loader] - INFO : Loading asset: panel.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[12703 ms][asset_loader] - INFO : Loading asset: 0_5_x_0_5_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[12704 ms][asset_loader] - INFO : Loading asset: cuboidal_rod.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[12705 ms][asset_loader] - INFO : Loading asset: small_cube.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[12706 ms][asset_loader] - INFO : Loading asset: left_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[12707 ms][asset_loader] - INFO : Loading asset: right_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[12708 ms][asset_loader] - INFO : Loading asset: back_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[12709 ms][asset_loader] - INFO : Loading asset: front_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[12710 ms][asset_loader] - INFO : Loading asset: bottom_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[12711 ms][asset_loader] - INFO : Loading asset: top_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[12712 ms][asset_loader] - INFO : Loading asset: 1_x_1_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[12713 ms][asset_loader] - INFO : Loading asset: gate.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[12716 ms][env_manager] - INFO : Populating environment 0 (env_manager.py:179)
[33m[12733 ms][robot_manager] - WARNING : 
[33mRobot mass: 1.2400000467896461,
[33mInertia: tensor([[0.0134, 0.0000, 0.0000],
[33m        [0.0000, 0.0144, 0.0000],
[33m        [0.0000, 0.0000, 0.0138]], device='cuda:0'),
[33mRobot COM: tensor([[0., 0., 0., 1.]], device='cuda:0') (robot_manager.py:437)
[33m[12733 ms][robot_manager] - WARNING : Calculated robot mass and inertia for this robot. This code assumes that your robot is the same across environments. (robot_manager.py:440)
[31m[12733 ms][robot_manager] - CRITICAL : If your robot differs across environments you need to perform this computation for each different robot here. (robot_manager.py:443)
[37m[12743 ms][env_manager] - INFO : [DONE] Populating environments. (env_manager.py:75)
[33m[12749 ms][IsaacGymEnvManager] - WARNING : Headless: True (IGE_env_manager.py:424)
[37m[12749 ms][IsaacGymEnvManager] - INFO : Headless mode. Viewer not created. (IGE_env_manager.py:434)
[33m[12793 ms][asset_manager] - WARNING : Number of obstacles to be kept in the environment: 9 (asset_manager.py:32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/control/motor_model.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(self.min_thrust, device=self.device, dtype=torch.float32).expand(
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/control/motor_model.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(self.max_thrust, device=self.device, dtype=torch.float32).expand(
[33m[12989 ms][control_allocation] - WARNING : Control allocation does not account for actuator limits. This leads to suboptimal allocation (control_allocation.py:48)
[37m[12990 ms][WarpSensor] - INFO : Camera sensor initialized (warp_sensor.py:50)
creating render graph
Module warp.utils load on device 'cuda:0' took 1.70 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_camera_kernels load on device 'cuda:0' took 8.16 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_stereo_camera_kernels load on device 'cuda:0' took 12.12 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_lidar_kernels load on device 'cuda:0' took 7.66 ms
finishing capture of render graph
Encoder network initialized.
Defined encoder.
[ImgDecoder] Starting create_model
[ImgDecoder] Done with create_model
Defined decoder.
Loading weights from file:  /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/utils/vae/weights/ICRA_test_set_more_sim_data_kld_beta_3_LD_64_epoch_49.pth
[AerialGymVecEnv] Forced action space shape: (6,)
[AerialGymVecEnv] is_multiagent: True, num_agents: 6
[make_aerialgym_env] Final action space shape: (6,)
[make_aerialgym_env] Action space: Box(-1.0, 1.0, (6,), float32)
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.
  logger.warn(
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.
  logger.warn(
[31m[15476 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[15476 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([0, 1, 2, 3, 4, 5], device='cuda:0') (navigation_task.py:196)
[31m[15477 ms][navigation_task] - CRITICAL : Time at crash: tensor([1, 1, 1, 1, 1, 1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 18:57:11,632][132155] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 0. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 18:57:11,633][132155] Avg episode reward: [(0, '-100.000')]
[36m[2025-07-01 18:57:15,475][132155] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 26.5. Samples: 102. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 18:57:15,476][132155] Avg episode reward: [(0, '-84.960')]
[36m[2025-07-01 18:57:20,473][132155] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 74.7. Samples: 660. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 18:57:20,474][132155] Avg episode reward: [(0, '-91.673')]
[36m[2025-07-01 18:57:25,554][132155] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 98.7. Samples: 1374. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 18:57:25,555][132155] Avg episode reward: [(0, '-93.995')]
[37m[1m[2025-07-01 18:57:26,541][132155] Heartbeat connected on Batcher_0
[37m[1m[2025-07-01 18:57:26,541][132155] Heartbeat connected on LearnerWorker_p0
[37m[1m[2025-07-01 18:57:26,541][132155] Heartbeat connected on InferenceWorker_p0-w0
[37m[1m[2025-07-01 18:57:26,541][132155] Heartbeat connected on RolloutWorker_w0
[36m[2025-07-01 18:57:30,497][132155] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 86.2. Samples: 1626. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 18:57:30,497][132155] Avg episode reward: [(0, '-97.686')]
[36m[2025-07-01 18:57:35,487][132155] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 97.3. Samples: 2322. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 18:57:35,487][132155] Avg episode reward: [(0, '-98.415')]
[36m[2025-07-01 18:57:40,501][132155] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 105.0. Samples: 3030. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 18:57:40,502][132155] Avg episode reward: [(0, '-97.527')]
[36m[2025-07-01 18:57:45,504][132155] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 101.7. Samples: 3444. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 18:57:45,504][132155] Avg episode reward: [(0, '-101.097')]
[36m[2025-07-01 18:57:50,497][132155] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 107.9. Samples: 4194. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 18:57:50,497][132155] Avg episode reward: [(0, '-99.062')]
[36m[2025-07-01 18:57:55,515][132155] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 112.5. Samples: 4938. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 18:57:55,516][132155] Avg episode reward: [(0, '-98.602')]
[36m[2025-07-01 18:58:00,496][132155] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 114.9. Samples: 5274. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 18:58:00,497][132155] Avg episode reward: [(0, '-100.305')]
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/torch/nn/modules/module.py:1194: UserWarning: operator() profile_node %104 : int[] = prim::profile_ivalue(%102)
 does not have profile information (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
[36m[2025-07-01 18:58:05,973][132155] Fps is (10 sec: 587.5, 60 sec: 113.1, 300 sec: 113.1). Total num frames: 6144. Throughput: 0: 117.1. Samples: 5988. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)
[36m[2025-07-01 18:58:05,974][132155] Avg episode reward: [(0, '-99.525')]
[31m[72095 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[72096 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[72096 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 18:58:10,512][132155] Fps is (10 sec: 613.4, 60 sec: 104.3, 300 sec: 104.3). Total num frames: 6144. Throughput: 0: 116.9. Samples: 6630. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)
[36m[2025-07-01 18:58:10,513][132155] Avg episode reward: [(0, '-98.173')]
[36m[2025-07-01 18:58:15,483][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 96.2). Total num frames: 6144. Throughput: 0: 119.5. Samples: 7002. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)
[36m[2025-07-01 18:58:15,484][132155] Avg episode reward: [(0, '-98.720')]
[36m[2025-07-01 18:58:20,531][132155] Fps is (10 sec: 0.0, 60 sec: 102.3, 300 sec: 89.2). Total num frames: 6144. Throughput: 0: 119.7. Samples: 7716. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)
[36m[2025-07-01 18:58:20,531][132155] Avg episode reward: [(0, '-100.583')]
[36m[2025-07-01 18:58:25,521][132155] Fps is (10 sec: 0.0, 60 sec: 102.5, 300 sec: 83.2). Total num frames: 6144. Throughput: 0: 119.7. Samples: 8418. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)
[36m[2025-07-01 18:58:25,521][132155] Avg episode reward: [(0, '-99.410')]
[36m[2025-07-01 18:58:30,505][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 77.9). Total num frames: 6144. Throughput: 0: 117.9. Samples: 8748. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)
[36m[2025-07-01 18:58:30,505][132155] Avg episode reward: [(0, '-97.519')]
[36m[2025-07-01 18:58:35,480][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 73.3). Total num frames: 6144. Throughput: 0: 118.4. Samples: 9522. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)
[36m[2025-07-01 18:58:35,480][132155] Avg episode reward: [(0, '-100.020')]
[36m[2025-07-01 18:58:40,500][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 69.1). Total num frames: 6144. Throughput: 0: 119.4. Samples: 10308. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)
[36m[2025-07-01 18:58:40,501][132155] Avg episode reward: [(0, '-99.372')]
[36m[2025-07-01 18:58:45,504][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 65.5). Total num frames: 6144. Throughput: 0: 119.3. Samples: 10644. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)
[36m[2025-07-01 18:58:45,505][132155] Avg episode reward: [(0, '-98.980')]
[36m[2025-07-01 18:58:50,498][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 62.1). Total num frames: 6144. Throughput: 0: 121.5. Samples: 11400. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)
[36m[2025-07-01 18:58:50,499][132155] Avg episode reward: [(0, '-98.896')]
[31m[116088 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[116088 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[116088 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 18:58:55,504][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 59.1). Total num frames: 6144. Throughput: 0: 121.8. Samples: 12108. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)
[36m[2025-07-01 18:58:55,505][132155] Avg episode reward: [(0, '-97.833')]
[36m[2025-07-01 18:59:00,527][132155] Fps is (10 sec: 612.7, 60 sec: 204.7, 300 sec: 112.8). Total num frames: 12288. Throughput: 0: 120.4. Samples: 12426. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-07-01 18:59:00,527][132155] Avg episode reward: [(0, '-99.393')]
[37m[1m[2025-07-01 18:59:00,578][132155] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/MEDIUM_CONFIG_6_ENVS_TEST_1536batch_FIXEDINPUT_V13/checkpoint_p0/checkpoint_000000032_12288.pth...
[36m[2025-07-01 18:59:05,510][132155] Fps is (10 sec: 614.0, 60 sec: 103.2, 300 sec: 107.9). Total num frames: 12288. Throughput: 0: 119.9. Samples: 13110. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-07-01 18:59:05,510][132155] Avg episode reward: [(0, '-99.484')]
[36m[2025-07-01 18:59:10,533][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 103.3). Total num frames: 12288. Throughput: 0: 121.7. Samples: 13896. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-07-01 18:59:10,534][132155] Avg episode reward: [(0, '-99.906')]
[36m[2025-07-01 18:59:15,472][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 99.2). Total num frames: 12288. Throughput: 0: 122.0. Samples: 14232. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-07-01 18:59:15,473][132155] Avg episode reward: [(0, '-99.334')]
[36m[2025-07-01 18:59:20,495][132155] Fps is (10 sec: 0.0, 60 sec: 102.5, 300 sec: 95.4). Total num frames: 12288. Throughput: 0: 119.7. Samples: 14910. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-07-01 18:59:20,496][132155] Avg episode reward: [(0, '-100.969')]
[31m[146945 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[146945 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([2], device='cuda:0') (navigation_task.py:196)
[31m[146945 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 18:59:25,513][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 91.8). Total num frames: 12288. Throughput: 0: 117.6. Samples: 15600. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-07-01 18:59:25,514][132155] Avg episode reward: [(0, '-99.594')]
[36m[2025-07-01 18:59:30,533][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 88.5). Total num frames: 12288. Throughput: 0: 117.5. Samples: 15936. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-07-01 18:59:30,533][132155] Avg episode reward: [(0, '-98.386')]
[36m[2025-07-01 18:59:35,495][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 85.4). Total num frames: 12288. Throughput: 0: 116.4. Samples: 16638. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-07-01 18:59:35,496][132155] Avg episode reward: [(0, '-98.861')]
[36m[2025-07-01 18:59:40,490][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 82.5). Total num frames: 12288. Throughput: 0: 115.1. Samples: 17286. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-07-01 18:59:40,491][132155] Avg episode reward: [(0, '-98.147')]
[36m[2025-07-01 18:59:45,482][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 79.9). Total num frames: 12288. Throughput: 0: 115.6. Samples: 17622. Policy #0 lag: (min: 12.0, avg: 12.1, max: 28.0)
[36m[2025-07-01 18:59:45,482][132155] Avg episode reward: [(0, '-100.130')]
[36m[2025-07-01 18:59:50,504][132155] Fps is (10 sec: 613.6, 60 sec: 204.8, 300 sec: 116.0). Total num frames: 18432. Throughput: 0: 116.5. Samples: 18354. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 18:59:50,504][132155] Avg episode reward: [(0, '-99.935')]
[36m[2025-07-01 18:59:55,489][132155] Fps is (10 sec: 614.0, 60 sec: 204.9, 300 sec: 112.5). Total num frames: 18432. Throughput: 0: 115.0. Samples: 19068. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 18:59:55,489][132155] Avg episode reward: [(0, '-99.669')]
[36m[2025-07-01 19:00:00,515][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 109.1). Total num frames: 18432. Throughput: 0: 115.1. Samples: 19416. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 19:00:00,516][132155] Avg episode reward: [(0, '-98.608')]
[36m[2025-07-01 19:00:05,472][132155] Fps is (10 sec: 0.0, 60 sec: 102.5, 300 sec: 106.0). Total num frames: 18432. Throughput: 0: 117.7. Samples: 20202. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 19:00:05,473][132155] Avg episode reward: [(0, '-99.751')]
[36m[2025-07-01 19:00:10,546][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 103.0). Total num frames: 18432. Throughput: 0: 119.6. Samples: 20988. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 19:00:10,546][132155] Avg episode reward: [(0, '-98.677')]
[36m[2025-07-01 19:00:15,512][132155] Fps is (10 sec: 0.0, 60 sec: 102.3, 300 sec: 100.2). Total num frames: 18432. Throughput: 0: 120.6. Samples: 21360. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 19:00:15,512][132155] Avg episode reward: [(0, '-98.944')]
[36m[2025-07-01 19:00:20,529][132155] Fps is (10 sec: 0.0, 60 sec: 102.3, 300 sec: 97.6). Total num frames: 18432. Throughput: 0: 120.6. Samples: 22068. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 19:00:20,530][132155] Avg episode reward: [(0, '-97.149')]
[36m[2025-07-01 19:00:25,506][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 95.1). Total num frames: 18432. Throughput: 0: 123.0. Samples: 22824. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 19:00:25,506][132155] Avg episode reward: [(0, '-98.490')]
[36m[2025-07-01 19:00:30,488][132155] Fps is (10 sec: 0.0, 60 sec: 102.5, 300 sec: 92.7). Total num frames: 18432. Throughput: 0: 123.4. Samples: 23178. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 19:00:30,488][132155] Avg episode reward: [(0, '-100.407')]
[36m[2025-07-01 19:00:35,533][132155] Fps is (10 sec: 0.0, 60 sec: 102.3, 300 sec: 90.4). Total num frames: 18432. Throughput: 0: 121.1. Samples: 23808. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 19:00:35,534][132155] Avg episode reward: [(0, '-100.407')]
[36m[2025-07-01 19:00:40,479][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 88.3). Total num frames: 18432. Throughput: 0: 119.2. Samples: 24432. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 19:00:40,479][132155] Avg episode reward: [(0, '-98.913')]
[36m[2025-07-01 19:00:45,484][132155] Fps is (10 sec: 617.4, 60 sec: 204.8, 300 sec: 114.9). Total num frames: 24576. Throughput: 0: 118.6. Samples: 24750. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 19:00:45,484][132155] Avg episode reward: [(0, '-101.037')]
[36m[2025-07-01 19:00:50,503][132155] Fps is (10 sec: 612.9, 60 sec: 102.4, 300 sec: 112.3). Total num frames: 24576. Throughput: 0: 115.4. Samples: 25398. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 19:00:50,504][132155] Avg episode reward: [(0, '-100.552')]
[36m[2025-07-01 19:00:55,528][132155] Fps is (10 sec: 0.0, 60 sec: 102.3, 300 sec: 109.8). Total num frames: 24576. Throughput: 0: 111.8. Samples: 26016. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 19:00:55,528][132155] Avg episode reward: [(0, '-99.065')]
[36m[2025-07-01 19:01:00,486][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 107.4). Total num frames: 24576. Throughput: 0: 109.9. Samples: 26304. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 19:01:00,487][132155] Avg episode reward: [(0, '-99.402')]
[37m[1m[2025-07-01 19:01:00,538][132155] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/MEDIUM_CONFIG_6_ENVS_TEST_1536batch_FIXEDINPUT_V13/checkpoint_p0/checkpoint_000000064_24576.pth...
[36m[2025-07-01 19:01:05,495][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 105.1). Total num frames: 24576. Throughput: 0: 108.3. Samples: 26940. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 19:01:05,496][132155] Avg episode reward: [(0, '-100.433')]
[36m[2025-07-01 19:01:10,531][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 102.9). Total num frames: 24576. Throughput: 0: 105.7. Samples: 27582. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 19:01:10,532][132155] Avg episode reward: [(0, '-101.072')]
[36m[2025-07-01 19:01:15,528][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 100.8). Total num frames: 24576. Throughput: 0: 104.4. Samples: 27882. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 19:01:15,529][132155] Avg episode reward: [(0, '-102.342')]
[36m[2025-07-01 19:01:20,482][132155] Fps is (10 sec: 0.0, 60 sec: 102.5, 300 sec: 98.8). Total num frames: 24576. Throughput: 0: 103.1. Samples: 28440. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 19:01:20,482][132155] Avg episode reward: [(0, '-100.738')]
[36m[2025-07-01 19:01:25,487][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 96.8). Total num frames: 24576. Throughput: 0: 100.9. Samples: 28974. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 19:01:25,487][132155] Avg episode reward: [(0, '-101.550')]
[31m[270931 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[270931 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[270931 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 19:01:30,482][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 94.9). Total num frames: 24576. Throughput: 0: 101.1. Samples: 29298. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 19:01:30,482][132155] Avg episode reward: [(0, '-100.713')]
[36m[2025-07-01 19:01:35,490][132155] Fps is (10 sec: 0.0, 60 sec: 102.5, 300 sec: 93.1). Total num frames: 24576. Throughput: 0: 100.8. Samples: 29934. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 19:01:35,490][132155] Avg episode reward: [(0, '-99.189')]
[36m[2025-07-01 19:01:40,541][132155] Fps is (10 sec: 0.0, 60 sec: 102.3, 300 sec: 91.4). Total num frames: 24576. Throughput: 0: 101.0. Samples: 30564. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 19:01:40,541][132155] Avg episode reward: [(0, '-97.505')]
[36m[2025-07-01 19:01:45,510][132155] Fps is (10 sec: 613.2, 60 sec: 102.4, 300 sec: 112.2). Total num frames: 30720. Throughput: 0: 101.0. Samples: 30852. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 19:01:45,510][132155] Avg episode reward: [(0, '-97.008')]
[36m[2025-07-01 19:01:50,482][132155] Fps is (10 sec: 618.0, 60 sec: 102.4, 300 sec: 110.2). Total num frames: 30720. Throughput: 0: 100.3. Samples: 31452. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 19:01:50,483][132155] Avg episode reward: [(0, '-99.831')]
[36m[2025-07-01 19:01:55,596][132155] Fps is (10 sec: 0.0, 60 sec: 102.3, 300 sec: 108.2). Total num frames: 30720. Throughput: 0: 96.5. Samples: 31932. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 19:01:55,596][132155] Avg episode reward: [(0, '-98.075')]
[36m[2025-07-01 19:02:00,488][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 106.4). Total num frames: 30720. Throughput: 0: 94.5. Samples: 32130. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 19:02:00,488][132155] Avg episode reward: [(0, '-99.540')]
[36m[2025-07-01 19:02:05,513][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 104.5). Total num frames: 30720. Throughput: 0: 92.7. Samples: 32616. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 19:02:05,513][132155] Avg episode reward: [(0, '-99.942')]
[36m[2025-07-01 19:02:10,479][132155] Fps is (10 sec: 0.0, 60 sec: 102.5, 300 sec: 104.1). Total num frames: 30720. Throughput: 0: 91.8. Samples: 33102. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 19:02:10,479][132155] Avg episode reward: [(0, '-97.857')]
[36m[2025-07-01 19:02:15,488][132155] Fps is (10 sec: 0.0, 60 sec: 102.5, 300 sec: 104.1). Total num frames: 30720. Throughput: 0: 90.0. Samples: 33348. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 19:02:15,489][132155] Avg episode reward: [(0, '-101.245')]
[31m[320491 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[320492 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[320492 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 19:02:20,494][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 104.2). Total num frames: 30720. Throughput: 0: 86.8. Samples: 33840. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 19:02:20,494][132155] Avg episode reward: [(0, '-99.205')]
[36m[2025-07-01 19:02:25,502][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 104.1). Total num frames: 30720. Throughput: 0: 80.9. Samples: 34200. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 19:02:25,503][132155] Avg episode reward: [(0, '-99.303')]
[36m[2025-07-01 19:02:30,477][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 104.1). Total num frames: 30720. Throughput: 0: 80.7. Samples: 34482. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 19:02:30,478][132155] Avg episode reward: [(0, '-102.148')]
[36m[2025-07-01 19:02:35,526][132155] Fps is (10 sec: 0.0, 60 sec: 102.3, 300 sec: 104.1). Total num frames: 30720. Throughput: 0: 80.3. Samples: 35070. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 19:02:35,526][132155] Avg episode reward: [(0, '-101.168')]
[36m[2025-07-01 19:02:40,520][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 104.1). Total num frames: 30720. Throughput: 0: 81.6. Samples: 35598. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 19:02:40,521][132155] Avg episode reward: [(0, '-99.927')]
[36m[2025-07-01 19:02:45,494][132155] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 104.1). Total num frames: 30720. Throughput: 0: 83.5. Samples: 35886. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 19:02:45,494][132155] Avg episode reward: [(0, '-101.010')]
[36m[2025-07-01 19:02:50,476][132155] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 104.1). Total num frames: 30720. Throughput: 0: 86.1. Samples: 36486. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 19:02:50,476][132155] Avg episode reward: [(0, '-101.198')]
[36m[2025-07-01 19:02:55,523][132155] Fps is (10 sec: 612.6, 60 sec: 102.5, 300 sec: 125.0). Total num frames: 36864. Throughput: 0: 87.8. Samples: 37056. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 19:02:55,523][132155] Avg episode reward: [(0, '-101.889')]
[36m[2025-07-01 19:03:00,506][132155] Fps is (10 sec: 612.5, 60 sec: 102.4, 300 sec: 104.3). Total num frames: 36864. Throughput: 0: 90.2. Samples: 37410. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 19:03:00,506][132155] Avg episode reward: [(0, '-102.115')]
[37m[1m[2025-07-01 19:03:00,572][132155] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/MEDIUM_CONFIG_6_ENVS_TEST_1536batch_FIXEDINPUT_V13/checkpoint_p0/checkpoint_000000096_36864.pth...
[36m[2025-07-01 19:03:00,576][132155] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/MEDIUM_CONFIG_6_ENVS_TEST_1536batch_FIXEDINPUT_V13/checkpoint_p0/checkpoint_000000032_12288.pth
[36m[2025-07-01 19:03:05,516][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 104.1). Total num frames: 36864. Throughput: 0: 93.8. Samples: 38064. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 19:03:05,516][132155] Avg episode reward: [(0, '-99.577')]
[36m[2025-07-01 19:03:10,539][132155] Fps is (10 sec: 0.0, 60 sec: 102.3, 300 sec: 104.1). Total num frames: 36864. Throughput: 0: 99.9. Samples: 38700. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 19:03:10,540][132155] Avg episode reward: [(0, '-100.334')]
[36m[2025-07-01 19:03:15,522][132155] Fps is (10 sec: 0.0, 60 sec: 102.3, 300 sec: 104.1). Total num frames: 36864. Throughput: 0: 101.9. Samples: 39072. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 19:03:15,522][132155] Avg episode reward: [(0, '-100.252')]
[31m[384732 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[384732 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[384733 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 19:03:20,514][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 104.1). Total num frames: 36864. Throughput: 0: 102.7. Samples: 39690. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 19:03:20,514][132155] Avg episode reward: [(0, '-99.580')]
[36m[2025-07-01 19:03:25,483][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 104.1). Total num frames: 36864. Throughput: 0: 105.0. Samples: 40320. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 19:03:25,484][132155] Avg episode reward: [(0, '-99.646')]
[36m[2025-07-01 19:03:30,505][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 104.1). Total num frames: 36864. Throughput: 0: 106.0. Samples: 40656. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 19:03:30,505][132155] Avg episode reward: [(0, '-99.852')]
[36m[2025-07-01 19:03:35,504][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 104.1). Total num frames: 36864. Throughput: 0: 106.6. Samples: 41286. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 19:03:35,505][132155] Avg episode reward: [(0, '-100.861')]
[36m[2025-07-01 19:03:40,511][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 104.1). Total num frames: 36864. Throughput: 0: 109.1. Samples: 41964. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 19:03:40,511][132155] Avg episode reward: [(0, '-100.792')]
[36m[2025-07-01 19:03:45,530][132155] Fps is (10 sec: 0.0, 60 sec: 102.3, 300 sec: 104.1). Total num frames: 36864. Throughput: 0: 108.5. Samples: 42294. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 19:03:45,530][132155] Avg episode reward: [(0, '-100.337')]
[36m[2025-07-01 19:03:50,476][132155] Fps is (10 sec: 616.5, 60 sec: 204.8, 300 sec: 125.0). Total num frames: 43008. Throughput: 0: 108.9. Samples: 42960. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 19:03:50,477][132155] Avg episode reward: [(0, '-99.526')]
[36m[2025-07-01 19:03:55,488][132155] Fps is (10 sec: 617.0, 60 sec: 102.5, 300 sec: 104.1). Total num frames: 43008. Throughput: 0: 108.1. Samples: 43560. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 19:03:55,488][132155] Avg episode reward: [(0, '-98.301')]
[36m[2025-07-01 19:04:00,527][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 104.1). Total num frames: 43008. Throughput: 0: 106.5. Samples: 43866. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 19:04:00,528][132155] Avg episode reward: [(0, '-99.867')]
[36m[2025-07-01 19:04:05,516][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 104.1). Total num frames: 43008. Throughput: 0: 105.9. Samples: 44454. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 19:04:05,517][132155] Avg episode reward: [(0, '-100.060')]
[36m[2025-07-01 19:04:10,528][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 104.1). Total num frames: 43008. Throughput: 0: 105.5. Samples: 45072. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 19:04:10,529][132155] Avg episode reward: [(0, '-99.586')]
[36m[2025-07-01 19:04:15,505][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 104.1). Total num frames: 43008. Throughput: 0: 104.8. Samples: 45372. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 19:04:15,505][132155] Avg episode reward: [(0, '-99.729')]
[36m[2025-07-01 19:04:20,486][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 104.1). Total num frames: 43008. Throughput: 0: 104.8. Samples: 46002. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 19:04:20,486][132155] Avg episode reward: [(0, '-99.760')]
[36m[2025-07-01 19:04:25,486][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 104.2). Total num frames: 43008. Throughput: 0: 103.3. Samples: 46608. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 19:04:25,487][132155] Avg episode reward: [(0, '-99.108')]
[36m[2025-07-01 19:04:30,502][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 104.1). Total num frames: 43008. Throughput: 0: 103.1. Samples: 46932. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 19:04:30,502][132155] Avg episode reward: [(0, '-98.925')]
[36m[2025-07-01 19:04:35,485][132155] Fps is (10 sec: 0.0, 60 sec: 102.4, 300 sec: 104.1). Total num frames: 43008. Throughput: 0: 104.1. Samples: 47646. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 19:04:35,485][132155] Avg episode reward: [(0, '-99.954')]
[36m[2025-07-01 19:04:40,559][132155] Fps is (10 sec: 0.0, 60 sec: 102.3, 300 sec: 104.1). Total num frames: 43008. Throughput: 0: 104.1. Samples: 48252. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 19:04:40,559][132155] Avg episode reward: [(0, '-99.680')]
[37m[1m[2025-07-01 19:04:40,642][132155] Keyboard interrupt detected in the event loop EvtLoop [Runner_EvtLoop, process=main process 132155], exiting...
[37m[1m[2025-07-01 19:04:40,643][132155] Runner profile tree view:
[37m[1mmain_loop: 454.2341
[37m[1m[2025-07-01 19:04:40,643][132155] Collected {0: 43008}, FPS: 94.7