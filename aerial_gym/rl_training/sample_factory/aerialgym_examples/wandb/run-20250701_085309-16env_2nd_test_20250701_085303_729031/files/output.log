Importing module 'gym_38' (/home/ziyar/aerialgym/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_38.so)
Setting GYM_USD_PLUG_INFO_PATH to /home/ziyar/aerialgym/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json
[36m[2025-07-01 08:53:24,597][11329] Queried available GPUs: 0
[37m[1m[2025-07-01 08:53:24,597][11329] Environment var CUDA_VISIBLE_DEVICES is 0
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/torch/utils/cpp_extension.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
  from pkg_resources import packaging  # type: ignore[attr-defined]
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
Using /home/ziyar/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Emitting ninja build file /home/ziyar/.cache/torch_extensions/py38_cu117/gymtorch/build.ninja...
Building extension module gymtorch...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module gymtorch...
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/networkx/classes/graph.py:23: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  from collections import Mapping
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/networkx/classes/reportviews.py:95: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  from collections import Mapping, Set, Iterable
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/networkx/readwrite/graphml.py:346: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.int, "int"), (np.int8, "int"),
/home/ziyar/aerialgym/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/torch_utils.py:135: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  def get_axis_params(value, axis_idx, x_value=0., dtype=np.float, n_dims=3):
[31m[1759 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task] - CRITICAL : Hardcoding number of envs to 16 if it is greater than that. (dce_navigation_task.py:15)
[37m[1759 ms][base_task] - INFO : Setting seed: 1722077536 (base_task.py:38)
[37m[1759 ms][navigation_task] - INFO : Building environment for navigation task. (navigation_task.py:44)
[37m[1759 ms][navigation_task] - INFO : Sim Name: base_sim, Env Name: env_with_obstacles, Robot Name: lmf2, Controller Name: lmf2_velocity_control (navigation_task.py:45)
[37m[1759 ms][env_manager] - INFO : Populating environments. (env_manager.py:73)
[37m[1759 ms][env_manager] - INFO : Creating simulation instance. (env_manager.py:87)
[37m[1759 ms][env_manager] - INFO : Instantiating IGE object. (env_manager.py:88)
[37m[1759 ms][IsaacGymEnvManager] - INFO : Creating Isaac Gym Environment (IGE_env_manager.py:41)
[37m[1759 ms][IsaacGymEnvManager] - INFO : Acquiring gym object (IGE_env_manager.py:73)
[37m[1759 ms][IsaacGymEnvManager] - INFO : Acquired gym object (IGE_env_manager.py:75)
[37m[1760 ms][IsaacGymEnvManager] - INFO : Fixing devices (IGE_env_manager.py:89)
[37m[1760 ms][IsaacGymEnvManager] - INFO : Using GPU pipeline for simulation. (IGE_env_manager.py:102)
[37m[1760 ms][IsaacGymEnvManager] - INFO : Sim Device type: cuda, Sim Device ID: 0 (IGE_env_manager.py:105)
[31m[1760 ms][IsaacGymEnvManager] - CRITICAL : 
[31m Setting graphics device to -1.
[31m This is done because the simulation is run in headless mode and no Isaac Gym cameras are used.
[31m No need to worry. The simulation and warp rendering will work as expected. (IGE_env_manager.py:112)
[37m[1760 ms][IsaacGymEnvManager] - INFO : Graphics Device ID: -1 (IGE_env_manager.py:119)
[37m[1760 ms][IsaacGymEnvManager] - INFO : Creating Isaac Gym Simulation Object (IGE_env_manager.py:120)
[33m[1760 ms][IsaacGymEnvManager] - WARNING : If you have set the CUDA_VISIBLE_DEVICES environment variable, please ensure that you set it
[33mto a particular one that works for your system to use the viewer or Isaac Gym cameras.
[33mIf you want to run parallel simulations on multiple GPUs with camera sensors,
[33mplease disable Isaac Gym and use warp (by setting use_warp=True), set the viewer to headless. (IGE_env_manager.py:127)
[33m[1760 ms][IsaacGymEnvManager] - WARNING : If you see a segfault in the next lines, it is because of the discrepancy between the CUDA device and the graphics device.
[33mPlease ensure that the CUDA device and the graphics device are the same. (IGE_env_manager.py:132)
[37m[2558 ms][IsaacGymEnvManager] - INFO : Created Isaac Gym Simulation Object (IGE_env_manager.py:136)
[37m[2558 ms][IsaacGymEnvManager] - INFO : Created Isaac Gym Environment (IGE_env_manager.py:43)
PyTorch version 1.13.1
Device count 1
/home/ziyar/aerialgym/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/_bindings/src/gymtorch
ninja: no work to do.
Warp 1.0.0-beta.5 initialized:
   CUDA Toolkit: 11.5, Driver: 12.4
   Devices:
     "cpu"    | x86_64
     "cuda:0" | NVIDIA GeForce RTX 4080 Laptop GPU (sm_89)
   Kernel cache: /home/ziyar/.cache/warp/1.0.0-beta.5
Registered quad_with_obstacles and dce_navigation_task in subprocess
[isaacgym:gymutil.py] Unknown args:  ['--env=quad_with_obstacles', '--train_for_env_steps=100000000', '--experiment=16env_2nd_test', '--async_rl=True', '--use_env_info_cache=False', '--normalize_input=True']
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:0
GPU Pipeline: enabled
[37m[2762 ms][env_manager] - INFO : IGE object instantiated. (env_manager.py:109)
[37m[2762 ms][env_manager] - INFO : Creating warp environment. (env_manager.py:112)
[37m[2762 ms][env_manager] - INFO : Warp environment created. (env_manager.py:114)
[37m[2762 ms][env_manager] - INFO : Creating robot manager. (env_manager.py:118)
[37m[2762 ms][BaseRobot] - INFO : [DONE] Initializing controller (base_robot.py:26)
[37m[2762 ms][BaseRobot] - INFO : Initializing controller lmf2_velocity_control (base_robot.py:29)
[33m[2762 ms][base_multirotor] - WARNING : Creating 16 multirotors. (base_multirotor.py:32)
[37m[2762 ms][env_manager] - INFO : [DONE] Creating robot manager. (env_manager.py:123)
[37m[2762 ms][env_manager] - INFO : [DONE] Creating simulation instance. (env_manager.py:125)
[37m[2762 ms][asset_loader] - INFO : Loading asset: model.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2762 ms][asset_loader] - INFO : Loading asset: panel.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2764 ms][asset_loader] - INFO : Loading asset: small_cube.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2765 ms][asset_loader] - INFO : Loading asset: 1_x_1_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2766 ms][asset_loader] - INFO : Loading asset: 0_5_x_0_5_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2767 ms][asset_loader] - INFO : Loading asset: left_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2768 ms][asset_loader] - INFO : Loading asset: right_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2769 ms][asset_loader] - INFO : Loading asset: back_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2770 ms][asset_loader] - INFO : Loading asset: front_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2771 ms][asset_loader] - INFO : Loading asset: bottom_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2772 ms][asset_loader] - INFO : Loading asset: top_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2773 ms][asset_loader] - INFO : Loading asset: cuboidal_rod.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2774 ms][asset_loader] - INFO : Loading asset: gate.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[2777 ms][env_manager] - INFO : Populating environment 0 (env_manager.py:179)
[33m[3147 ms][robot_manager] - WARNING : 
[33mRobot mass: 1.2400000467896461,
[33mInertia: tensor([[0.0134, 0.0000, 0.0000],
[33m        [0.0000, 0.0144, 0.0000],
[33m        [0.0000, 0.0000, 0.0138]], device='cuda:0'),
[33mRobot COM: tensor([[0., 0., 0., 1.]], device='cuda:0') (robot_manager.py:437)
[33m[3147 ms][robot_manager] - WARNING : Calculated robot mass and inertia for this robot. This code assumes that your robot is the same across environments. (robot_manager.py:440)
[31m[3147 ms][robot_manager] - CRITICAL : If your robot differs across environments you need to perform this computation for each different robot here. (robot_manager.py:443)
[37m[3170 ms][env_manager] - INFO : [DONE] Populating environments. (env_manager.py:75)
[33m[3177 ms][IsaacGymEnvManager] - WARNING : Headless: True (IGE_env_manager.py:424)
[37m[3177 ms][IsaacGymEnvManager] - INFO : Headless mode. Viewer not created. (IGE_env_manager.py:434)
[33m[3233 ms][asset_manager] - WARNING : Number of obstacles to be kept in the environment: 9 (asset_manager.py:32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/control/motor_model.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(self.min_thrust, device=self.device, dtype=torch.float32).expand(
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/control/motor_model.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(self.max_thrust, device=self.device, dtype=torch.float32).expand(
[33m[3425 ms][control_allocation] - WARNING : Control allocation does not account for actuator limits. This leads to suboptimal allocation (control_allocation.py:48)
[37m[3426 ms][WarpSensor] - INFO : Camera sensor initialized (warp_sensor.py:50)
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.
  logger.warn(
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.
  logger.warn(
[36m[2025-07-01 08:53:28,783][11578] Env info: EnvInfo(obs_space=Dict('image_obs': Box(-1.0, 1.0, (1, 135, 240), float32), 'observations': Box(-1.0, 1.0, (81,), float32)), action_space=Box(-1.0, 1.0, (4,), float32), num_agents=16, gpu_actions=True, gpu_observations=True, action_splits=None, all_discrete=None, frameskip=1, reward_shaping_scheme=None, env_info_protocol_version=1)
*** Can't create empty tensor
WARNING: allocation matrix is not full rank. Rank: 4
creating render graph
Module warp.utils load on device 'cuda:0' took 1.40 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_camera_kernels load on device 'cuda:0' took 7.55 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_stereo_camera_kernels load on device 'cuda:0' took 11.88 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_lidar_kernels load on device 'cuda:0' took 5.48 ms
finishing capture of render graph
Encoder network initialized.
Defined encoder.
[ImgDecoder] Starting create_model
[ImgDecoder] Done with create_model
Defined decoder.
Loading weights from file:  /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/utils/vae/weights/ICRA_test_set_more_sim_data_kld_beta_3_LD_64_epoch_49.pth
[33m[2025-07-01 08:53:29,459][11329] In serial mode all components run on the same process. Only use async_rl and serial mode together for debugging.
[36m[2025-07-01 08:53:29,459][11329] Starting experiment with the following configuration:
[36mhelp=False
[36malgo=APPO
[36menv=quad_with_obstacles
[36mexperiment=16env_2nd_test
[36mtrain_dir=/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir
[36mrestart_behavior=resume
[36mdevice=gpu
[36mseed=None
[36mnum_policies=1
[36masync_rl=True
[36mserial_mode=True
[36mbatched_sampling=True
[36mnum_batches_to_accumulate=8
[36mworker_num_splits=1
[36mpolicy_workers_per_policy=1
[36mmax_policy_lag=1000
[36mnum_workers=1
[36mnum_envs_per_worker=1
[36mbatch_size=512
[36mnum_batches_per_epoch=4
[36mnum_epochs=4
[36mrollout=32
[36mrecurrence=32
[36mshuffle_minibatches=False
[36mgamma=0.98
[36mreward_scale=0.1
[36mreward_clip=1000.0
[36mvalue_bootstrap=True
[36mnormalize_returns=True
[36mexploration_loss_coeff=0.001
[36mvalue_loss_coeff=2.0
[36mkl_loss_coeff=0.1
[36mexploration_loss=entropy
[36mgae_lambda=0.95
[36mppo_clip_ratio=0.2
[36mppo_clip_value=1.0
[36mwith_vtrace=False
[36mvtrace_rho=1.0
[36mvtrace_c=1.0
[36moptimizer=adam
[36madam_eps=1e-06
[36madam_beta1=0.9
[36madam_beta2=0.999
[36mmax_grad_norm=1.0
[36mlearning_rate=0.0003
[36mlr_schedule=kl_adaptive_epoch
[36mlr_schedule_kl_threshold=0.016
[36mlr_adaptive_min=1e-06
[36mlr_adaptive_max=0.01
[36mobs_subtract_mean=0.0
[36mobs_scale=1.0
[36mnormalize_input=True
[36mnormalize_input_keys=None
[36mdecorrelate_experience_max_seconds=0
[36mdecorrelate_envs_on_one_worker=True
[36mactor_worker_gpus=[0]
[36mset_workers_cpu_affinity=True
[36mforce_envs_single_thread=False
[36mdefault_niceness=0
[36mlog_to_file=True
[36mexperiment_summaries_interval=10
[36mflush_summaries_interval=30
[36mstats_avg=100
[36msummaries_use_frameskip=True
[36mheartbeat_interval=20
[36mheartbeat_reporting_interval=180
[36mtrain_for_env_steps=100000000
[36mtrain_for_seconds=10000000000
[36msave_every_sec=120
[36mkeep_checkpoints=2
[36mload_checkpoint_kind=latest
[36msave_milestones_sec=-1
[36msave_best_every_sec=5
[36msave_best_metric=reward
[36msave_best_after=5000000
[36mbenchmark=False
[36mencoder_mlp_layers=[512, 256, 64]
[36mencoder_conv_architecture=convnet_simple
[36mencoder_conv_mlp_layers=[512]
[36muse_rnn=True
[36mrnn_size=64
[36mrnn_type=gru
[36mrnn_num_layers=1
[36mdecoder_mlp_layers=[]
[36mnonlinearity=elu
[36mpolicy_initialization=torch_default
[36mpolicy_init_gain=1.0
[36mactor_critic_share_weights=True
[36madaptive_stddev=True
[36mcontinuous_tanh_scale=0.0
[36minitial_stddev=1.0
[36muse_env_info_cache=False
[36menv_gpu_actions=True
[36menv_gpu_observations=True
[36menv_frameskip=1
[36menv_framestack=1
[36mpixel_format=CHW
[36muse_record_episode_statistics=False
[36mwith_wandb=True
[36mwandb_user=ziya-ruso-ucl
[36mwandb_project=vae_rl_navigation
[36mwandb_group=dce_navigation_training
[36mwandb_job_type=SF
[36mwandb_tags=['aerial_gym', 'dce', 'navigation', 'sample_factory']
[36mwith_pbt=False
[36mpbt_mix_policies_in_one_env=True
[36mpbt_period_env_steps=5000000
[36mpbt_start_mutation=20000000
[36mpbt_replace_fraction=0.3
[36mpbt_mutation_rate=0.15
[36mpbt_replace_reward_gap=0.1
[36mpbt_replace_reward_gap_absolute=1e-06
[36mpbt_optimize_gamma=False
[36mpbt_target_objective=true_objective
[36mpbt_perturb_min=1.1
[36mpbt_perturb_max=1.5
[36menv_agents=-1
[36mobs_key=obs
[36msubtask=None
[36mige_api_version=preview4
[36meval_stats=False
[36mcommand_line=--env=quad_with_obstacles --train_for_env_steps=100000000 --experiment=16env_2nd_test --async_rl=True --use_env_info_cache=False --normalize_input=True
[36mcli_args={'env': 'quad_with_obstacles', 'experiment': '16env_2nd_test', 'async_rl': True, 'normalize_input': True, 'train_for_env_steps': 100000000, 'use_env_info_cache': False}
[36mgit_hash=7f35eed17f2afcde33e3a7aec669b48e9e8e34cd
[36mgit_repo_name=https://github.com/ntnu-arl/aerial_gym_simulator.git
[36mwandb_unique_id=16env_2nd_test_20250701_085303_729031
[36m[2025-07-01 08:53:29,460][11329] Saving configuration to /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_2nd_test/config.json...
[36m[2025-07-01 08:53:29,499][11329] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[36m[2025-07-01 08:53:29,500][11329] Rollout worker 0 uses device cuda:0
[36m[2025-07-01 08:53:29,900][11329] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[37m[1m[2025-07-01 08:53:29,900][11329] InferenceWorker_p0-w0: min num requests: 1
[36m[2025-07-01 08:53:29,901][11329] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[37m[1m[2025-07-01 08:53:29,901][11329] Starting seed is not provided
[36m[2025-07-01 08:53:29,901][11329] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[36m[2025-07-01 08:53:29,902][11329] Initializing actor-critic model on device cuda:0
[36m[2025-07-01 08:53:29,902][11329] RunningMeanStd input shape: (1, 135, 240)
[36m[2025-07-01 08:53:29,902][11329] RunningMeanStd input shape: (81,)
[36m[2025-07-01 08:53:29,902][11329] RunningMeanStd input shape: (1,)
[36m[2025-07-01 08:53:29,910][11329] ConvEncoder: input_channels=1
[36m[2025-07-01 08:53:29,989][11329] Conv encoder output size: 512
[36m[2025-07-01 08:53:30,000][11329] Created Actor Critic model with architecture:
[36m[2025-07-01 08:53:30,000][11329] ActorCriticSharedWeights(
[36m  (obs_normalizer): ObservationNormalizer(
[36m    (running_mean_std): RunningMeanStdDictInPlace(
[36m      (running_mean_std): ModuleDict(
[36m        (image_obs): RunningMeanStdInPlace()
[36m        (observations): RunningMeanStdInPlace()
[36m      )
[36m    )
[36m  )
[36m  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)
[36m  (encoder): MultiInputEncoder(
[36m    (encoders): ModuleDict(
[36m      (image_obs): ConvEncoder(
[36m        (enc): RecursiveScriptModule(
[36m          original_name=ConvEncoderImpl
[36m          (conv_head): RecursiveScriptModule(
[36m            original_name=Sequential
[36m            (0): RecursiveScriptModule(original_name=Conv2d)
[36m            (1): RecursiveScriptModule(original_name=ELU)
[36m            (2): RecursiveScriptModule(original_name=Conv2d)
[36m            (3): RecursiveScriptModule(original_name=ELU)
[36m            (4): RecursiveScriptModule(original_name=Conv2d)
[36m            (5): RecursiveScriptModule(original_name=ELU)
[36m          )
[36m          (mlp_layers): RecursiveScriptModule(
[36m            original_name=Sequential
[36m            (0): RecursiveScriptModule(original_name=Linear)
[36m            (1): RecursiveScriptModule(original_name=ELU)
[36m          )
[36m        )
[36m      )
[36m      (observations): MlpEncoder(
[36m        (mlp_head): RecursiveScriptModule(
[36m          original_name=Sequential
[36m          (0): RecursiveScriptModule(original_name=Linear)
[36m          (1): RecursiveScriptModule(original_name=ELU)
[36m          (2): RecursiveScriptModule(original_name=Linear)
[36m          (3): RecursiveScriptModule(original_name=ELU)
[36m          (4): RecursiveScriptModule(original_name=Linear)
[36m          (5): RecursiveScriptModule(original_name=ELU)
[36m        )
[36m      )
[36m    )
[36m  )
[36m  (core): ModelCoreRNN(
[36m    (core): GRU(576, 64)
[36m  )
[36m  (decoder): MlpDecoder(
[36m    (mlp): Identity()
[36m  )
[36m  (critic_linear): Linear(in_features=64, out_features=1, bias=True)
[36m  (action_parameterization): ActionParameterizationDefault(
[36m    (distribution_linear): Linear(in_features=64, out_features=8, bias=True)
[36m  )
[36m)
[36m[2025-07-01 08:53:30,413][11329] Using optimizer <class 'torch.optim.adam.Adam'>
[33m[2025-07-01 08:53:30,414][11329] No checkpoints found
[36m[2025-07-01 08:53:30,414][11329] Did not load from checkpoint, starting from scratch!
[36m[2025-07-01 08:53:30,414][11329] Initialized policy 0 weights for model version 0
[36m[2025-07-01 08:53:30,414][11329] LearnerWorker_p0 finished initialization!
[36m[2025-07-01 08:53:30,415][11329] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[37m[1m[2025-07-01 08:53:30,821][11329] Inference worker 0-0 is ready!
[37m[1m[2025-07-01 08:53:30,821][11329] All inference workers are ready! Signal rollout workers to start!
[36m[2025-07-01 08:53:30,822][11329] EnvRunner 0-0 uses policy 0
[31m[30360 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task] - CRITICAL : Hardcoding number of envs to 16 if it is greater than that. (dce_navigation_task.py:15)
[37m[30360 ms][base_task] - INFO : Setting seed: 1714133340 (base_task.py:38)
[37m[30360 ms][navigation_task] - INFO : Building environment for navigation task. (navigation_task.py:44)
[37m[30360 ms][navigation_task] - INFO : Sim Name: base_sim, Env Name: env_with_obstacles, Robot Name: lmf2, Controller Name: lmf2_velocity_control (navigation_task.py:45)
[37m[30360 ms][env_manager] - INFO : Populating environments. (env_manager.py:73)
[37m[30360 ms][env_manager] - INFO : Creating simulation instance. (env_manager.py:87)
[37m[30360 ms][env_manager] - INFO : Instantiating IGE object. (env_manager.py:88)
[37m[30360 ms][IsaacGymEnvManager] - INFO : Creating Isaac Gym Environment (IGE_env_manager.py:41)
[37m[30360 ms][IsaacGymEnvManager] - INFO : Acquiring gym object (IGE_env_manager.py:73)
[37m[30360 ms][IsaacGymEnvManager] - INFO : Acquired gym object (IGE_env_manager.py:75)
[37m[30361 ms][IsaacGymEnvManager] - INFO : Fixing devices (IGE_env_manager.py:89)
[37m[30361 ms][IsaacGymEnvManager] - INFO : Using GPU pipeline for simulation. (IGE_env_manager.py:102)
[37m[30361 ms][IsaacGymEnvManager] - INFO : Sim Device type: cuda, Sim Device ID: 0 (IGE_env_manager.py:105)
[31m[30361 ms][IsaacGymEnvManager] - CRITICAL : 
[31m Setting graphics device to -1.
[31m This is done because the simulation is run in headless mode and no Isaac Gym cameras are used.
[31m No need to worry. The simulation and warp rendering will work as expected. (IGE_env_manager.py:112)
[37m[30361 ms][IsaacGymEnvManager] - INFO : Graphics Device ID: -1 (IGE_env_manager.py:119)
[37m[30361 ms][IsaacGymEnvManager] - INFO : Creating Isaac Gym Simulation Object (IGE_env_manager.py:120)
[33m[30361 ms][IsaacGymEnvManager] - WARNING : If you have set the CUDA_VISIBLE_DEVICES environment variable, please ensure that you set it
[33mto a particular one that works for your system to use the viewer or Isaac Gym cameras.
[33mIf you want to run parallel simulations on multiple GPUs with camera sensors,
[33mplease disable Isaac Gym and use warp (by setting use_warp=True), set the viewer to headless. (IGE_env_manager.py:127)
[33m[30361 ms][IsaacGymEnvManager] - WARNING : If you see a segfault in the next lines, it is because of the discrepancy between the CUDA device and the graphics device.
[33mPlease ensure that the CUDA device and the graphics device are the same. (IGE_env_manager.py:132)
[37m[31159 ms][IsaacGymEnvManager] - INFO : Created Isaac Gym Simulation Object (IGE_env_manager.py:136)
[37m[31159 ms][IsaacGymEnvManager] - INFO : Created Isaac Gym Environment (IGE_env_manager.py:43)
[37m[31360 ms][env_manager] - INFO : IGE object instantiated. (env_manager.py:109)
[37m[31360 ms][env_manager] - INFO : Creating warp environment. (env_manager.py:112)
[37m[31360 ms][env_manager] - INFO : Warp environment created. (env_manager.py:114)
[37m[31360 ms][env_manager] - INFO : Creating robot manager. (env_manager.py:118)
[37m[31360 ms][BaseRobot] - INFO : [DONE] Initializing controller (base_robot.py:26)
[37m[31360 ms][BaseRobot] - INFO : Initializing controller lmf2_velocity_control (base_robot.py:29)
[33m[31360 ms][base_multirotor] - WARNING : Creating 16 multirotors. (base_multirotor.py:32)
[37m[31360 ms][env_manager] - INFO : [DONE] Creating robot manager. (env_manager.py:123)
[37m[31360 ms][env_manager] - INFO : [DONE] Creating simulation instance. (env_manager.py:125)
[37m[31360 ms][asset_loader] - INFO : Loading asset: model.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[31361 ms][asset_loader] - INFO : Loading asset: panel.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[31363 ms][asset_loader] - INFO : Loading asset: gate.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[31366 ms][asset_loader] - INFO : Loading asset: small_cube.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[31367 ms][asset_loader] - INFO : Loading asset: 1_x_1_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[31368 ms][asset_loader] - INFO : Loading asset: 0_5_x_0_5_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[31369 ms][asset_loader] - INFO : Loading asset: left_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[31369 ms][asset_loader] - INFO : Loading asset: right_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[31370 ms][asset_loader] - INFO : Loading asset: back_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[31371 ms][asset_loader] - INFO : Loading asset: front_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[31372 ms][asset_loader] - INFO : Loading asset: bottom_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[31373 ms][asset_loader] - INFO : Loading asset: top_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[31374 ms][asset_loader] - INFO : Loading asset: cuboidal_rod.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[31376 ms][env_manager] - INFO : Populating environment 0 (env_manager.py:179)
[33m[31393 ms][robot_manager] - WARNING : 
[33mRobot mass: 1.2400000467896461,
[33mInertia: tensor([[0.0134, 0.0000, 0.0000],
[33m        [0.0000, 0.0144, 0.0000],
[33m        [0.0000, 0.0000, 0.0138]], device='cuda:0'),
[33mRobot COM: tensor([[0., 0., 0., 1.]], device='cuda:0') (robot_manager.py:437)
[33m[31393 ms][robot_manager] - WARNING : Calculated robot mass and inertia for this robot. This code assumes that your robot is the same across environments. (robot_manager.py:440)
[31m[31393 ms][robot_manager] - CRITICAL : If your robot differs across environments you need to perform this computation for each different robot here. (robot_manager.py:443)
[37m[31415 ms][env_manager] - INFO : [DONE] Populating environments. (env_manager.py:75)
[33m[31423 ms][IsaacGymEnvManager] - WARNING : Headless: True (IGE_env_manager.py:424)
[37m[31423 ms][IsaacGymEnvManager] - INFO : Headless mode. Viewer not created. (IGE_env_manager.py:434)
[33m[31454 ms][asset_manager] - WARNING : Number of obstacles to be kept in the environment: 9 (asset_manager.py:32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/control/motor_model.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(self.min_thrust, device=self.device, dtype=torch.float32).expand(
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/control/motor_model.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(self.max_thrust, device=self.device, dtype=torch.float32).expand(
[33m[31647 ms][control_allocation] - WARNING : Control allocation does not account for actuator limits. This leads to suboptimal allocation (control_allocation.py:48)
[37m[31648 ms][WarpSensor] - INFO : Camera sensor initialized (warp_sensor.py:50)
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.
  logger.warn(
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.
  logger.warn(
[isaacgym:gymutil.py] Unknown args:  ['--env=quad_with_obstacles', '--train_for_env_steps=100000000', '--experiment=16env_2nd_test', '--async_rl=True', '--use_env_info_cache=False', '--normalize_input=True']
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:0
GPU Pipeline: enabled
*** Can't create empty tensor
WARNING: allocation matrix is not full rank. Rank: 4
creating render graph
Module warp.utils load on device 'cuda:0' took 1.76 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_camera_kernels load on device 'cuda:0' took 7.98 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_stereo_camera_kernels load on device 'cuda:0' took 11.17 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_lidar_kernels load on device 'cuda:0' took 5.61 ms
finishing capture of render graph
Encoder network initialized.
Defined encoder.
[ImgDecoder] Starting create_model
[ImgDecoder] Done with create_model
Defined decoder.
Loading weights from file:  /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/utils/vae/weights/ICRA_test_set_more_sim_data_kld_beta_3_LD_64_epoch_49.pth
[31m[34290 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[34290 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15],
[31m       device='cuda:0') (navigation_task.py:196)
[31m[34291 ms][navigation_task] - CRITICAL : Time at crash: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0',
[31m       dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 08:53:35,538][11329] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 0. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 08:53:35,538][11329] Avg episode reward: [(0, '-100.000')]
[36m[2025-07-01 08:53:38,853][11329] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 72.4. Samples: 240. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-01 08:53:38,853][11329] Avg episode reward: [(0, '-87.797')]
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/torch/nn/modules/module.py:1194: UserWarning: operator() profile_node %104 : int[] = prim::profile_ivalue(%102)
 does not have profile information (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
[36m[2025-07-01 08:53:43,858][11329] Fps is (10 sec: 246.2, 60 sec: 246.2, 300 sec: 246.2). Total num frames: 2048. Throughput: 0: 250.0. Samples: 2080. Policy #0 lag: (min: 0.0, avg: 0.0, max: 0.0)
[36m[2025-07-01 08:53:43,858][11329] Avg episode reward: [(0, '-89.042')]
[36m[2025-07-01 08:53:49,167][11329] Fps is (10 sec: 397.1, 60 sec: 300.5, 300 sec: 300.5). Total num frames: 4096. Throughput: 0: 220.7. Samples: 3008. Policy #0 lag: (min: 8.0, avg: 8.5, max: 24.0)
[36m[2025-07-01 08:53:49,168][11329] Avg episode reward: [(0, '-87.599')]
[37m[1m[2025-07-01 08:53:49,983][11329] Heartbeat connected on Batcher_0
[37m[1m[2025-07-01 08:53:49,984][11329] Heartbeat connected on LearnerWorker_p0
[37m[1m[2025-07-01 08:53:49,984][11329] Heartbeat connected on InferenceWorker_p0-w0
[37m[1m[2025-07-01 08:53:49,984][11329] Heartbeat connected on RolloutWorker_w0
[31m[51597 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[51597 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[51597 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 08:53:53,853][11329] Fps is (10 sec: 204.9, 60 sec: 223.6, 300 sec: 223.6). Total num frames: 4096. Throughput: 0: 271.7. Samples: 4976. Policy #0 lag: (min: 8.0, avg: 8.5, max: 24.0)
[36m[2025-07-01 08:53:53,853][11329] Avg episode reward: [(0, '-95.158')]
[31m[58247 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[58247 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[58247 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 08:53:58,904][11329] Fps is (10 sec: 210.3, 60 sec: 262.9, 300 sec: 262.9). Total num frames: 6144. Throughput: 0: 294.4. Samples: 6880. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 08:53:58,904][11329] Avg episode reward: [(0, '-93.324')]
[36m[2025-07-01 08:54:03,877][11329] Fps is (10 sec: 408.6, 60 sec: 289.1, 300 sec: 289.1). Total num frames: 8192. Throughput: 0: 311.6. Samples: 8832. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:54:03,877][11329] Avg episode reward: [(0, '-93.499')]
[36m[2025-07-01 08:54:08,882][11329] Fps is (10 sec: 410.5, 60 sec: 307.1, 300 sec: 307.1). Total num frames: 10240. Throughput: 0: 299.4. Samples: 9984. Policy #0 lag: (min: 0.0, avg: 0.5, max: 16.0)
[36m[2025-07-01 08:54:08,883][11329] Avg episode reward: [(0, '-97.592')]
[36m[2025-07-01 08:54:13,953][11329] Fps is (10 sec: 406.5, 60 sec: 319.9, 300 sec: 319.9). Total num frames: 12288. Throughput: 0: 309.5. Samples: 11888. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:54:13,953][11329] Avg episode reward: [(0, '-95.965')]
[36m[2025-07-01 08:54:18,863][11329] Fps is (10 sec: 205.2, 60 sec: 283.6, 300 sec: 283.6). Total num frames: 12288. Throughput: 0: 325.4. Samples: 14096. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:54:18,863][11329] Avg episode reward: [(0, '-93.772')]
[36m[2025-07-01 08:54:23,852][11329] Fps is (10 sec: 206.9, 60 sec: 296.7, 300 sec: 296.7). Total num frames: 14336. Throughput: 0: 331.0. Samples: 15136. Policy #0 lag: (min: 12.0, avg: 12.5, max: 28.0)
[36m[2025-07-01 08:54:23,853][11329] Avg episode reward: [(0, '-93.913')]
[36m[2025-07-01 08:54:28,865][11329] Fps is (10 sec: 409.5, 60 sec: 307.2, 300 sec: 307.2). Total num frames: 16384. Throughput: 0: 334.5. Samples: 17136. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:54:28,865][11329] Avg episode reward: [(0, '-95.356')]
[36m[2025-07-01 08:54:33,868][11329] Fps is (10 sec: 408.9, 60 sec: 316.0, 300 sec: 316.0). Total num frames: 18432. Throughput: 0: 361.5. Samples: 19168. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:54:33,869][11329] Avg episode reward: [(0, '-91.891')]
[36m[2025-07-01 08:54:38,866][11329] Fps is (10 sec: 409.5, 60 sec: 341.3, 300 sec: 323.4). Total num frames: 20480. Throughput: 0: 339.5. Samples: 20256. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 08:54:38,867][11329] Avg episode reward: [(0, '-88.991')]
[31m[98472 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[98472 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[98472 ms][navigation_task] - CRITICAL : Time at crash: tensor([3], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 08:54:43,889][11329] Fps is (10 sec: 408.7, 60 sec: 341.2, 300 sec: 329.6). Total num frames: 22528. Throughput: 0: 345.4. Samples: 22416. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:54:43,890][11329] Avg episode reward: [(0, '-74.368')]
[36m[2025-07-01 08:54:48,893][11329] Fps is (10 sec: 408.5, 60 sec: 342.9, 300 sec: 335.0). Total num frames: 24576. Throughput: 0: 349.7. Samples: 24576. Policy #0 lag: (min: 0.0, avg: 0.5, max: 16.0)
[36m[2025-07-01 08:54:48,894][11329] Avg episode reward: [(0, '-71.359')]
[36m[2025-07-01 08:54:53,870][11329] Fps is (10 sec: 205.2, 60 sec: 341.2, 300 sec: 313.7). Total num frames: 24576. Throughput: 0: 347.8. Samples: 25632. Policy #0 lag: (min: 0.0, avg: 0.5, max: 16.0)
[36m[2025-07-01 08:54:53,870][11329] Avg episode reward: [(0, '-61.065')]
[36m[2025-07-01 08:54:58,889][11329] Fps is (10 sec: 204.9, 60 sec: 341.4, 300 sec: 319.4). Total num frames: 26624. Throughput: 0: 352.9. Samples: 27744. Policy #0 lag: (min: 0.0, avg: 0.5, max: 16.0)
[36m[2025-07-01 08:54:58,889][11329] Avg episode reward: [(0, '-57.987')]
[36m[2025-07-01 08:55:03,879][11329] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 324.6). Total num frames: 28672. Throughput: 0: 351.2. Samples: 29904. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:55:03,879][11329] Avg episode reward: [(0, '-39.399')]
[36m[2025-07-01 08:55:08,852][11329] Fps is (10 sec: 411.1, 60 sec: 341.5, 300 sec: 329.2). Total num frames: 30720. Throughput: 0: 347.7. Samples: 30784. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:55:08,852][11329] Avg episode reward: [(0, '-41.194')]
[36m[2025-07-01 08:55:13,887][11329] Fps is (10 sec: 409.3, 60 sec: 341.7, 300 sec: 333.2). Total num frames: 32768. Throughput: 0: 350.8. Samples: 32928. Policy #0 lag: (min: 8.0, avg: 8.5, max: 24.0)
[36m[2025-07-01 08:55:13,887][11329] Avg episode reward: [(0, '-44.198')]
[36m[2025-07-01 08:55:18,872][11329] Fps is (10 sec: 408.8, 60 sec: 375.4, 300 sec: 336.9). Total num frames: 34816. Throughput: 0: 348.4. Samples: 34848. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:55:18,872][11329] Avg episode reward: [(0, '-32.502')]
[36m[2025-07-01 08:55:23,869][11329] Fps is (10 sec: 205.2, 60 sec: 341.2, 300 sec: 321.4). Total num frames: 34816. Throughput: 0: 347.4. Samples: 35888. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:55:23,873][11329] Avg episode reward: [(0, '-32.965')]
[37m[1m[2025-07-01 08:55:23,924][11329] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_2nd_test/checkpoint_p0/checkpoint_000000272_34816.pth...
[31m[145423 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[145424 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[145424 ms][navigation_task] - CRITICAL : Time at crash: tensor([4], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 08:55:28,858][11329] Fps is (10 sec: 205.1, 60 sec: 341.4, 300 sec: 325.3). Total num frames: 36864. Throughput: 0: 346.9. Samples: 38016. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:55:28,859][11329] Avg episode reward: [(0, '-34.740')]
[36m[2025-07-01 08:55:33,950][11329] Fps is (10 sec: 406.3, 60 sec: 340.9, 300 sec: 328.6). Total num frames: 38912. Throughput: 0: 333.1. Samples: 39584. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:55:33,950][11329] Avg episode reward: [(0, '-16.319')]
[36m[2025-07-01 08:55:38,904][11329] Fps is (10 sec: 203.9, 60 sec: 307.0, 300 sec: 315.4). Total num frames: 38912. Throughput: 0: 318.3. Samples: 39968. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:55:38,905][11329] Avg episode reward: [(0, '-8.780')]
[31m[158800 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[158801 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([13], device='cuda:0') (navigation_task.py:196)
[31m[158802 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 08:55:43,872][11329] Fps is (10 sec: 206.4, 60 sec: 307.3, 300 sec: 319.2). Total num frames: 40960. Throughput: 0: 291.7. Samples: 40864. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:55:43,872][11329] Avg episode reward: [(0, '-15.336')]
[36m[2025-07-01 08:55:48,852][11329] Fps is (10 sec: 205.9, 60 sec: 273.3, 300 sec: 307.2). Total num frames: 40960. Throughput: 0: 272.5. Samples: 42160. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:55:48,852][11329] Avg episode reward: [(0, '-11.627')]
[36m[2025-07-01 08:55:53,881][11329] Fps is (10 sec: 204.6, 60 sec: 307.1, 300 sec: 310.9). Total num frames: 43008. Throughput: 0: 267.6. Samples: 42832. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:55:53,882][11329] Avg episode reward: [(0, '-0.650')]
[36m[2025-07-01 08:55:58,856][11329] Fps is (10 sec: 204.7, 60 sec: 273.2, 300 sec: 300.1). Total num frames: 43008. Throughput: 0: 241.6. Samples: 43792. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:55:58,856][11329] Avg episode reward: [(0, '0.523')]
[36m[2025-07-01 08:56:03,884][11329] Fps is (10 sec: 204.7, 60 sec: 273.0, 300 sec: 303.7). Total num frames: 45056. Throughput: 0: 235.7. Samples: 45456. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:56:03,885][11329] Avg episode reward: [(0, '4.603')]
[36m[2025-07-01 08:56:08,901][11329] Fps is (10 sec: 407.8, 60 sec: 272.8, 300 sec: 307.1). Total num frames: 47104. Throughput: 0: 236.3. Samples: 46528. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:56:08,901][11329] Avg episode reward: [(0, '15.287')]
[36m[2025-07-01 08:56:13,870][11329] Fps is (10 sec: 205.1, 60 sec: 239.0, 300 sec: 297.5). Total num frames: 47104. Throughput: 0: 233.2. Samples: 48512. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:56:13,871][11329] Avg episode reward: [(0, '9.568')]
[36m[2025-07-01 08:56:18,893][11329] Fps is (10 sec: 205.0, 60 sec: 238.8, 300 sec: 300.9). Total num frames: 49152. Throughput: 0: 242.1. Samples: 50464. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:56:18,893][11329] Avg episode reward: [(0, '0.615')]
[36m[2025-07-01 08:56:23,906][11329] Fps is (10 sec: 408.1, 60 sec: 272.9, 300 sec: 304.1). Total num frames: 51200. Throughput: 0: 251.0. Samples: 51264. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 08:56:23,906][11329] Avg episode reward: [(0, '-2.503')]
[36m[2025-07-01 08:56:28,884][11329] Fps is (10 sec: 410.0, 60 sec: 272.9, 300 sec: 307.2). Total num frames: 53248. Throughput: 0: 275.8. Samples: 53280. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 08:56:28,884][11329] Avg episode reward: [(0, '5.164')]
[31m[210834 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[210834 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([7], device='cuda:0') (navigation_task.py:196)
[31m[210834 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 08:56:33,888][11329] Fps is (10 sec: 205.2, 60 sec: 239.2, 300 sec: 298.6). Total num frames: 53248. Throughput: 0: 289.2. Samples: 55184. Policy #0 lag: (min: 11.0, avg: 11.0, max: 11.0)
[36m[2025-07-01 08:56:33,888][11329] Avg episode reward: [(0, '0.265')]
[31m[217588 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[217588 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([10], device='cuda:0') (navigation_task.py:196)
[31m[217588 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 08:56:38,864][11329] Fps is (10 sec: 205.2, 60 sec: 273.2, 300 sec: 301.6). Total num frames: 55296. Throughput: 0: 294.9. Samples: 56096. Policy #0 lag: (min: 4.0, avg: 4.5, max: 20.0)
[36m[2025-07-01 08:56:38,864][11329] Avg episode reward: [(0, '11.224')]
[36m[2025-07-01 08:56:43,853][11329] Fps is (10 sec: 411.0, 60 sec: 273.2, 300 sec: 304.5). Total num frames: 57344. Throughput: 0: 320.0. Samples: 58192. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 08:56:43,853][11329] Avg episode reward: [(0, '27.145')]
[36m[2025-07-01 08:56:48,852][11329] Fps is (10 sec: 410.1, 60 sec: 307.2, 300 sec: 307.2). Total num frames: 59392. Throughput: 0: 332.7. Samples: 60416. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 08:56:48,852][11329] Avg episode reward: [(0, '38.474')]
[31m[229135 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[229135 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([4], device='cuda:0') (navigation_task.py:196)
[31m[229135 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 08:56:53,863][11329] Fps is (10 sec: 409.2, 60 sec: 307.3, 300 sec: 309.8). Total num frames: 61440. Throughput: 0: 332.4. Samples: 61472. Policy #0 lag: (min: 12.0, avg: 12.5, max: 28.0)
[36m[2025-07-01 08:56:53,864][11329] Avg episode reward: [(0, '46.551')]
[36m[2025-07-01 08:56:58,862][11329] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 312.3). Total num frames: 63488. Throughput: 0: 333.9. Samples: 63536. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:56:58,862][11329] Avg episode reward: [(0, '56.781')]
[36m[2025-07-01 08:57:03,890][11329] Fps is (10 sec: 408.5, 60 sec: 341.3, 300 sec: 314.5). Total num frames: 65536. Throughput: 0: 336.0. Samples: 65584. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:57:03,890][11329] Avg episode reward: [(0, '48.032')]
[36m[2025-07-01 08:57:08,890][11329] Fps is (10 sec: 204.2, 60 sec: 307.3, 300 sec: 307.2). Total num frames: 65536. Throughput: 0: 343.6. Samples: 66720. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:57:08,890][11329] Avg episode reward: [(0, '47.869')]
[36m[2025-07-01 08:57:13,854][11329] Fps is (10 sec: 205.5, 60 sec: 341.4, 300 sec: 309.6). Total num frames: 67584. Throughput: 0: 340.9. Samples: 68608. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:57:13,854][11329] Avg episode reward: [(0, '42.477')]
[36m[2025-07-01 08:57:18,901][11329] Fps is (10 sec: 409.2, 60 sec: 341.3, 300 sec: 311.7). Total num frames: 69632. Throughput: 0: 333.1. Samples: 70176. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:57:18,901][11329] Avg episode reward: [(0, '34.586')]
[36m[2025-07-01 08:57:23,874][11329] Fps is (10 sec: 408.8, 60 sec: 341.5, 300 sec: 313.9). Total num frames: 71680. Throughput: 0: 332.4. Samples: 71056. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:57:23,874][11329] Avg episode reward: [(0, '21.593')]
[37m[1m[2025-07-01 08:57:23,920][11329] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_2nd_test/checkpoint_p0/checkpoint_000000560_71680.pth...
[36m[2025-07-01 08:57:28,902][11329] Fps is (10 sec: 204.8, 60 sec: 307.1, 300 sec: 307.2). Total num frames: 71680. Throughput: 0: 322.8. Samples: 72736. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:57:28,902][11329] Avg episode reward: [(0, '25.425')]
[36m[2025-07-01 08:57:33,854][11329] Fps is (10 sec: 205.2, 60 sec: 341.5, 300 sec: 309.4). Total num frames: 73728. Throughput: 0: 306.5. Samples: 74208. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:57:33,854][11329] Avg episode reward: [(0, '24.685')]
[36m[2025-07-01 08:57:38,861][11329] Fps is (10 sec: 205.6, 60 sec: 307.2, 300 sec: 303.0). Total num frames: 73728. Throughput: 0: 301.2. Samples: 75024. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:57:38,861][11329] Avg episode reward: [(0, '18.336')]
[36m[2025-07-01 08:57:43,933][11329] Fps is (10 sec: 203.2, 60 sec: 306.8, 300 sec: 305.1). Total num frames: 75776. Throughput: 0: 282.9. Samples: 76288. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:57:43,934][11329] Avg episode reward: [(0, '19.106')]
[36m[2025-07-01 08:57:48,885][11329] Fps is (10 sec: 204.3, 60 sec: 272.9, 300 sec: 299.1). Total num frames: 75776. Throughput: 0: 270.3. Samples: 77744. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:57:48,886][11329] Avg episode reward: [(0, '16.894')]
[36m[2025-07-01 08:57:53,870][11329] Fps is (10 sec: 206.1, 60 sec: 273.0, 300 sec: 301.3). Total num frames: 77824. Throughput: 0: 258.2. Samples: 78336. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:57:53,871][11329] Avg episode reward: [(0, '22.314')]
[36m[2025-07-01 08:57:59,034][11329] Fps is (10 sec: 403.6, 60 sec: 272.3, 300 sec: 303.1). Total num frames: 79872. Throughput: 0: 245.1. Samples: 79680. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:57:59,034][11329] Avg episode reward: [(0, '22.963')]
[36m[2025-07-01 08:58:03,933][11329] Fps is (10 sec: 203.5, 60 sec: 238.8, 300 sec: 297.6). Total num frames: 79872. Throughput: 0: 239.8. Samples: 80976. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:58:03,933][11329] Avg episode reward: [(0, '29.171')]
[36m[2025-07-01 08:58:08,905][11329] Fps is (10 sec: 207.5, 60 sec: 273.0, 300 sec: 299.7). Total num frames: 81920. Throughput: 0: 234.1. Samples: 81600. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:58:08,905][11329] Avg episode reward: [(0, '35.111')]
[36m[2025-07-01 08:58:13,891][11329] Fps is (10 sec: 205.7, 60 sec: 238.8, 300 sec: 294.3). Total num frames: 81920. Throughput: 0: 226.2. Samples: 82912. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:58:13,891][11329] Avg episode reward: [(0, '22.953')]
[36m[2025-07-01 08:58:18,924][11329] Fps is (10 sec: 204.4, 60 sec: 238.8, 300 sec: 296.3). Total num frames: 83968. Throughput: 0: 221.5. Samples: 84192. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 08:58:18,924][11329] Avg episode reward: [(0, '33.681')]
[36m[2025-07-01 08:58:23,979][11329] Fps is (10 sec: 203.0, 60 sec: 204.4, 300 sec: 291.1). Total num frames: 83968. Throughput: 0: 217.7. Samples: 84848. Policy #0 lag: (min: 7.0, avg: 7.0, max: 7.0)
[36m[2025-07-01 08:58:23,980][11329] Avg episode reward: [(0, '27.230')]
[36m[2025-07-01 08:58:28,862][11329] Fps is (10 sec: 206.1, 60 sec: 239.1, 300 sec: 293.2). Total num frames: 86016. Throughput: 0: 219.0. Samples: 86128. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:58:28,863][11329] Avg episode reward: [(0, '28.845')]
[36m[2025-07-01 08:58:33,907][11329] Fps is (10 sec: 206.3, 60 sec: 204.6, 300 sec: 291.5). Total num frames: 86016. Throughput: 0: 216.1. Samples: 87472. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:58:33,907][11329] Avg episode reward: [(0, '37.731')]
[36m[2025-07-01 08:58:38,858][11329] Fps is (10 sec: 204.9, 60 sec: 238.9, 300 sec: 291.6). Total num frames: 88064. Throughput: 0: 216.6. Samples: 88080. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:58:38,858][11329] Avg episode reward: [(0, '40.223')]
[36m[2025-07-01 08:58:43,899][11329] Fps is (10 sec: 204.9, 60 sec: 204.9, 300 sec: 284.9). Total num frames: 88064. Throughput: 0: 213.6. Samples: 89264. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:58:43,900][11329] Avg episode reward: [(0, '43.813')]
[36m[2025-07-01 08:58:48,943][11329] Fps is (10 sec: 203.1, 60 sec: 238.7, 300 sec: 291.5). Total num frames: 90112. Throughput: 0: 211.9. Samples: 90512. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 08:58:48,943][11329] Avg episode reward: [(0, '40.603')]
[36m[2025-07-01 08:58:53,864][11329] Fps is (10 sec: 205.5, 60 sec: 204.8, 300 sec: 284.7). Total num frames: 90112. Throughput: 0: 215.0. Samples: 91264. Policy #0 lag: (min: 6.0, avg: 6.0, max: 6.0)
[36m[2025-07-01 08:58:53,864][11329] Avg episode reward: [(0, '49.758')]
[36m[2025-07-01 08:58:58,902][11329] Fps is (10 sec: 205.6, 60 sec: 205.3, 300 sec: 284.6). Total num frames: 92160. Throughput: 0: 226.4. Samples: 93104. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:58:58,902][11329] Avg episode reward: [(0, '53.766')]
[36m[2025-07-01 08:59:03,900][11329] Fps is (10 sec: 408.1, 60 sec: 239.1, 300 sec: 284.6). Total num frames: 94208. Throughput: 0: 228.0. Samples: 94448. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:59:03,900][11329] Avg episode reward: [(0, '41.524')]
[31m[364101 ms][navigation_task] - CRITICAL : Crash is happening too soon. (navigation_task.py:195)
[31m[364101 ms][navigation_task] - CRITICAL : Envs crashing too soon: tensor([14], device='cuda:0') (navigation_task.py:196)
[31m[364101 ms][navigation_task] - CRITICAL : Time at crash: tensor([1], device='cuda:0', dtype=torch.int32) (navigation_task.py:197)
[36m[2025-07-01 08:59:08,876][11329] Fps is (10 sec: 205.3, 60 sec: 204.9, 300 sec: 277.8). Total num frames: 94208. Throughput: 0: 230.6. Samples: 95200. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:59:08,876][11329] Avg episode reward: [(0, '24.238')]
[36m[2025-07-01 08:59:13,869][11329] Fps is (10 sec: 205.4, 60 sec: 239.0, 300 sec: 284.6). Total num frames: 96256. Throughput: 0: 241.4. Samples: 96992. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:59:13,869][11329] Avg episode reward: [(0, '34.632')]
[36m[2025-07-01 08:59:18,886][11329] Fps is (10 sec: 409.2, 60 sec: 239.1, 300 sec: 284.6). Total num frames: 98304. Throughput: 0: 244.7. Samples: 98480. Policy #0 lag: (min: 0.0, avg: 0.5, max: 16.0)
[36m[2025-07-01 08:59:18,886][11329] Avg episode reward: [(0, '15.200')]
[36m[2025-07-01 08:59:23,891][11329] Fps is (10 sec: 204.4, 60 sec: 239.3, 300 sec: 277.7). Total num frames: 98304. Throughput: 0: 251.2. Samples: 99392. Policy #0 lag: (min: 0.0, avg: 0.5, max: 16.0)
[36m[2025-07-01 08:59:23,891][11329] Avg episode reward: [(0, '19.750')]
[37m[1m[2025-07-01 08:59:24,037][11329] Saving /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_2nd_test/checkpoint_p0/checkpoint_000000768_98304.pth...
[36m[2025-07-01 08:59:24,122][11329] Removing /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/rl_training/sample_factory/aerialgym_examples/train_dir/16env_2nd_test/checkpoint_p0/checkpoint_000000272_34816.pth
[36m[2025-07-01 08:59:28,867][11329] Fps is (10 sec: 205.2, 60 sec: 238.9, 300 sec: 277.7). Total num frames: 100352. Throughput: 0: 256.9. Samples: 100816. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:59:28,868][11329] Avg episode reward: [(0, '3.111')]
[36m[2025-07-01 08:59:33,893][11329] Fps is (10 sec: 409.5, 60 sec: 273.1, 300 sec: 277.7). Total num frames: 102400. Throughput: 0: 268.4. Samples: 102576. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:59:33,894][11329] Avg episode reward: [(0, '14.058')]
[36m[2025-07-01 08:59:38,910][11329] Fps is (10 sec: 203.9, 60 sec: 238.7, 300 sec: 270.7). Total num frames: 102400. Throughput: 0: 267.5. Samples: 103312. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:59:38,910][11329] Avg episode reward: [(0, '18.684')]
[36m[2025-07-01 08:59:43,898][11329] Fps is (10 sec: 204.7, 60 sec: 273.1, 300 sec: 270.7). Total num frames: 104448. Throughput: 0: 255.7. Samples: 104608. Policy #0 lag: (min: 9.0, avg: 9.0, max: 9.0)
[36m[2025-07-01 08:59:43,898][11329] Avg episode reward: [(0, '7.884')]
[36m[2025-07-01 08:59:49,162][11329] Fps is (10 sec: 399.5, 60 sec: 272.1, 300 sec: 277.4). Total num frames: 106496. Throughput: 0: 265.1. Samples: 106448. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 08:59:49,162][11329] Avg episode reward: [(0, '6.465')]
[36m[2025-07-01 08:59:53,898][11329] Fps is (10 sec: 204.8, 60 sec: 272.9, 300 sec: 270.7). Total num frames: 106496. Throughput: 0: 266.9. Samples: 107216. Policy #0 lag: (min: 3.0, avg: 3.0, max: 3.0)
[36m[2025-07-01 08:59:53,899][11329] Avg episode reward: [(0, '13.748')]
[36m[2025-07-01 08:59:58,871][11329] Fps is (10 sec: 210.9, 60 sec: 273.2, 300 sec: 270.8). Total num frames: 108544. Throughput: 0: 263.8. Samples: 108864. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 08:59:58,872][11329] Avg episode reward: [(0, '22.930')]
[36m[2025-07-01 09:00:03,883][11329] Fps is (10 sec: 410.2, 60 sec: 273.1, 300 sec: 270.7). Total num frames: 110592. Throughput: 0: 274.5. Samples: 110832. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 09:00:03,883][11329] Avg episode reward: [(0, '22.939')]
[36m[2025-07-01 09:00:08,876][11329] Fps is (10 sec: 409.4, 60 sec: 307.2, 300 sec: 270.8). Total num frames: 112640. Throughput: 0: 279.9. Samples: 111984. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 09:00:08,876][11329] Avg episode reward: [(0, '25.056')]
[36m[2025-07-01 09:00:13,879][11329] Fps is (10 sec: 204.9, 60 sec: 273.0, 300 sec: 263.8). Total num frames: 112640. Throughput: 0: 290.8. Samples: 113904. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-01 09:00:13,879][11329] Avg episode reward: [(0, '24.729')]
[36m[2025-07-01 09:00:18,896][11329] Fps is (10 sec: 204.4, 60 sec: 273.0, 300 sec: 270.7). Total num frames: 114688. Throughput: 0: 295.1. Samples: 115856. Policy #0 lag: (min: 1.0, avg: 1.0, max: 1.0)
[36m[2025-07-01 09:00:18,896][11329] Avg episode reward: [(0, '24.803')]
[36m[2025-07-01 09:00:23,864][11329] Fps is (10 sec: 410.2, 60 sec: 307.3, 300 sec: 270.7). Total num frames: 116736. Throughput: 0: 299.3. Samples: 116768. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-01 09:00:23,864][11329] Avg episode reward: [(0, '15.487')]
[37m[1m[2025-07-01 09:00:26,093][11329] Keyboard interrupt detected in the event loop EvtLoop [Runner_EvtLoop, process=main process 11329], exiting...
[37m[1m[2025-07-01 09:00:26,093][11329] Runner profile tree view:
[37m[1mmain_loop: 416.1921
[37m[1m[2025-07-01 09:00:26,093][11329] Collected {0: 116736}, FPS: 280.5