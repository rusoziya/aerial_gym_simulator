Importing module 'gym_38' (/home/ziyar/aerialgym/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_38.so)
Setting GYM_USD_PLUG_INFO_PATH to /home/ziyar/aerialgym/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json
[36m[2025-07-03 17:00:40,762][81784] Queried available GPUs: 0
[37m[1m[2025-07-03 17:00:40,762][81784] Environment var CUDA_VISIBLE_DEVICES is 0
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/torch/utils/cpp_extension.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
  from pkg_resources import packaging  # type: ignore[attr-defined]
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
PyTorch version 1.13.1
Device count 1
/home/ziyar/aerialgym/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/_bindings/src/gymtorch
ninja: no work to do.
Warp 1.0.0-beta.5 initialized:
   CUDA Toolkit: 11.5, Driver: 12.4
   Devices:
     "cpu"    | x86_64
     "cuda:0" | NVIDIA GeForce RTX 4080 Laptop GPU (sm_89)
   Kernel cache: /home/ziyar/.cache/warp/1.0.0-beta.5
[SUBPROCESS] FORCED headless mode for all Sample Factory training: headless=True
[SUBPROCESS] This prevents Isaac Gym viewer conflicts across all processes
[SUBPROCESS] Task action_space_dim: 3
[SUBPROCESS] Target Sample Factory action space: 3D
[SUBPROCESS] Setting num_envs to 16 based on env_agents=16
[SUBPROCESS] Set SF_ENV_AGENTS=16 environment variable
[SUBPROCESS] Config batch_size: 2048
[SUBPROCESS] Using STANDARD CONFIG (16 environments)
Registered quad_with_obstacles_gate and dce_navigation_task_gate in subprocess
[isaacgym:gymutil.py] Unknown args:  ['--env=quad_with_obstacles_gate', '--experiment=base_gate_rewards_classic33', '--train_dir=./train_dir', '--num_workers=1', '--num_envs_per_worker=1', '--env_agents=16', '--obs_key=observations', '--batch_size=2048', '--num_batches_to_accumulate=2', '--num_batches_per_epoch=8', '--num_epochs=4', '--rollout=32', '--learning_rate=0.0003', '--use_rnn=true', '--rnn_size=64', '--rnn_num_layers=1', '--encoder_mlp_layers', '512', '256', '64', '--gamma=0.98', '--reward_scale=0.1', '--max_grad_norm=1.0', '--async_rl=true', '--normalize_input=true', '--use_env_info_cache=false', '--with_wandb=true', '--wandb_project=gate_navigation_dual_camera', '--wandb_user=ziya-ruso-ucl', '--wandb_group=gate_navigation_training', '--wandb_tags', 'aerial_gym', 'gate_navigation', 'dual_camera', 'x500', 'sample_factory', 'memory_optimized', '--save_every_sec=120', '--save_best_every_sec=5', '--train_for_env_steps=100000000', '--save_gifs=true']
Not connected to PVD
+++ Using GPU PhysX
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
Using /home/ziyar/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...
Emitting ninja build file /home/ziyar/.cache/torch_extensions/py38_cu117/gymtorch/build.ninja...
Building extension module gymtorch...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module gymtorch...
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/networkx/classes/graph.py:23: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  from collections import Mapping
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/networkx/classes/reportviews.py:95: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  from collections import Mapping, Set, Iterable
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/networkx/readwrite/graphml.py:346: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.int, "int"), (np.int8, "int"),
/home/ziyar/aerialgym/IsaacGym_Preview_4_Package/isaacgym/python/isaacgym/torch_utils.py:135: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  def get_axis_params(value, axis_idx, x_value=0., dtype=np.float, n_dims=3):
[37m[2014 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task_gate] - INFO : DCE Gate Navigation Task - Using SF_HEADLESS environment variable: False (dce_navigation_task_gate.py:22)
[37m[2014 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task_gate] - INFO : DCE Gate Navigation Task - Final headless mode: False (dce_navigation_task_gate.py:29)
[37m[2014 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task_gate] - INFO : Found SF_ENV_AGENTS environment variable: 16 (dce_navigation_task_gate.py:39)
[37m[2014 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task_gate] - INFO : Detected env_agents=16 from environment - setting environment count. (dce_navigation_task_gate.py:45)
[37m[2015 ms][base_task] - INFO : Setting seed: 2895138453 (base_task.py:38)
[37m[2015 ms][navigation_task_gate] - INFO : Building environment for gate navigation task. (navigation_task_gate.py:48)
[37m[2015 ms][navigation_task_gate] - INFO : Sim Name: base_sim, Env Name: gate_env, Robot Name: lmf2, Controller Name: lmf2_position_control (navigation_task_gate.py:49)
[37m[2015 ms][env_manager] - INFO : Populating environments. (env_manager.py:73)
[37m[2015 ms][env_manager] - INFO : Creating simulation instance. (env_manager.py:87)
[37m[2015 ms][env_manager] - INFO : Instantiating IGE object. (env_manager.py:88)
[37m[2015 ms][IsaacGymEnvManager] - INFO : Creating Isaac Gym Environment (IGE_env_manager.py:41)
[37m[2015 ms][IsaacGymEnvManager] - INFO : Acquiring gym object (IGE_env_manager.py:73)
[37m[2015 ms][IsaacGymEnvManager] - INFO : Acquired gym object (IGE_env_manager.py:75)
[37m[2017 ms][IsaacGymEnvManager] - INFO : Fixing devices (IGE_env_manager.py:89)
[37m[2017 ms][IsaacGymEnvManager] - INFO : Using GPU pipeline for simulation. (IGE_env_manager.py:102)
[37m[2017 ms][IsaacGymEnvManager] - INFO : Sim Device type: cuda, Sim Device ID: 0 (IGE_env_manager.py:105)
[37m[2017 ms][IsaacGymEnvManager] - INFO : Graphics Device ID: 0 (IGE_env_manager.py:119)
[37m[2017 ms][IsaacGymEnvManager] - INFO : Creating Isaac Gym Simulation Object (IGE_env_manager.py:120)
[33m[2017 ms][IsaacGymEnvManager] - WARNING : If you have set the CUDA_VISIBLE_DEVICES environment variable, please ensure that you set it
[33mto a particular one that works for your system to use the viewer or Isaac Gym cameras.
[33mIf you want to run parallel simulations on multiple GPUs with camera sensors,
[33mplease disable Isaac Gym and use warp (by setting use_warp=True), set the viewer to headless. (IGE_env_manager.py:127)
[33m[2017 ms][IsaacGymEnvManager] - WARNING : If you see a segfault in the next lines, it is because of the discrepancy between the CUDA device and the graphics device.
[33mPlease ensure that the CUDA device and the graphics device are the same. (IGE_env_manager.py:132)
Physics Engine: PhysX
Physics Device: cuda:0
GPU Pipeline: enabled
*** Can't create empty tensor
WARNING: allocation matrix is not full rank. Rank: 4
[37m[3076 ms][IsaacGymEnvManager] - INFO : Created Isaac Gym Simulation Object (IGE_env_manager.py:136)
[37m[3076 ms][IsaacGymEnvManager] - INFO : Created Isaac Gym Environment (IGE_env_manager.py:43)
[37m[3280 ms][env_manager] - INFO : IGE object instantiated. (env_manager.py:109)
[37m[3280 ms][env_manager] - INFO : Creating warp environment. (env_manager.py:112)
[37m[3280 ms][env_manager] - INFO : Warp environment created. (env_manager.py:114)
[37m[3280 ms][env_manager] - INFO : Creating robot manager. (env_manager.py:118)
[37m[3280 ms][BaseRobot] - INFO : [DONE] Initializing controller (base_robot.py:26)
[37m[3280 ms][BaseRobot] - INFO : Initializing controller lmf2_position_control (base_robot.py:29)
[33m[3280 ms][base_multirotor] - WARNING : Creating 16 multirotors. (base_multirotor.py:32)
[37m[3280 ms][env_manager] - INFO : [DONE] Creating robot manager. (env_manager.py:123)
[37m[3280 ms][env_manager] - INFO : [DONE] Creating simulation instance. (env_manager.py:125)
[37m[3281 ms][asset_loader] - INFO : Loading asset: model.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[3281 ms][asset_loader] - INFO : Loading asset: gate.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[3284 ms][asset_loader] - INFO : Loading asset: 1_x_1_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[3285 ms][asset_loader] - INFO : Loading asset: cuboidal_rod.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[3286 ms][asset_loader] - INFO : Loading asset: left_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[3287 ms][asset_loader] - INFO : Loading asset: right_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[3288 ms][asset_loader] - INFO : Loading asset: front_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[3289 ms][asset_loader] - INFO : Loading asset: back_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[3290 ms][asset_loader] - INFO : Loading asset: bottom_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[3291 ms][asset_loader] - INFO : Loading asset: top_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[3292 ms][asset_loader] - INFO : Loading asset: 0_5_x_0_5_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[3293 ms][asset_loader] - INFO : Loading asset: small_cube.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[3295 ms][env_manager] - INFO : Populating environment 0 (env_manager.py:179)
[33m[3665 ms][robot_manager] - WARNING : 
[33mRobot mass: 1.2400000467896461,
[33mInertia: tensor([[0.0134, 0.0000, 0.0000],
[33m        [0.0000, 0.0144, 0.0000],
[33m        [0.0000, 0.0000, 0.0138]], device='cuda:0'),
[33mRobot COM: tensor([[0., 0., 0., 1.]], device='cuda:0') (robot_manager.py:437)
[33m[3665 ms][robot_manager] - WARNING : Calculated robot mass and inertia for this robot. This code assumes that your robot is the same across environments. (robot_manager.py:440)
[31m[3665 ms][robot_manager] - CRITICAL : If your robot differs across environments you need to perform this computation for each different robot here. (robot_manager.py:443)
[37m[3685 ms][env_manager] - INFO : [DONE] Populating environments. (env_manager.py:75)
[33m[3693 ms][IsaacGymEnvManager] - WARNING : Headless: False (IGE_env_manager.py:424)
[37m[3693 ms][IsaacGymEnvManager] - INFO : Creating viewer (IGE_env_manager.py:426)
[33m[3769 ms][IGE_viewer_control] - WARNING : Instructions for using the viewer with the keyboard:
[33mESC: Quit
[33mV: Toggle Viewer Sync
[33mS: Sync Frame Time
[33mF: Toggle Camera Follow
[33mP: Toggle Camera Follow Type
[33mR: Reset All Environments
[33mUP: Switch Target Environment Up
[33mDOWN: Switch Target Environment Down
[33mSPACE: Pause Simulation
[33m (IGE_viewer_control.py:153)
[37m[3769 ms][IsaacGymEnvManager] - INFO : Created viewer (IGE_env_manager.py:432)
[33m[3809 ms][asset_manager] - WARNING : Number of obstacles to be kept in the environment: 10 (asset_manager.py:32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/control/motor_model.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(self.min_thrust, device=self.device, dtype=torch.float32).expand(
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/control/motor_model.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(self.max_thrust, device=self.device, dtype=torch.float32).expand(
[33m[4037 ms][control_allocation] - WARNING : Control allocation does not account for actuator limits. This leads to suboptimal allocation (control_allocation.py:48)
[37m[4038 ms][WarpSensor] - INFO : Camera sensor initialized (warp_sensor.py:50)
creating render graph
Module warp.utils load on device 'cuda:0' took 1.32 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_camera_kernels load on device 'cuda:0' took 7.70 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_stereo_camera_kernels load on device 'cuda:0' took 11.48 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_lidar_kernels load on device 'cuda:0' took 5.55 ms
finishing capture of render graph
Encoder network initialized.
Defined encoder.
[ImgDecoder] Starting create_model
[ImgDecoder] Done with create_model
Defined decoder.
Loading weights from file:  /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/utils/vae/weights/ICRA_test_set_more_sim_data_kld_beta_3_LD_64_epoch_49.pth
[AerialGymVecEnv] GIF saving ENABLED for dual cameras (drone + static)
[AerialGymVecEnv] Forced action space shape: (3,)
[AerialGymVecEnv] is_multiagent: True, num_agents: 16
[AerialGymVecEnv] Detected observation space: 145D
[AerialGymVecEnv] Using GATE NAVIGATION configuration (145D = 17D basic + 64D drone VAE + 64D static camera VAE)
[make_aerialgym_env] Final action space shape: (3,)
[make_aerialgym_env] Action space: Box(-1.0, 1.0, (3,), float32)
[isaacgym:gymutil.py] Unknown args:  ['--env=quad_with_obstacles_gate', '--experiment=base_gate_rewards_classic33', '--train_dir=./train_dir', '--num_workers=1', '--num_envs_per_worker=1', '--env_agents=16', '--obs_key=observations', '--batch_size=2048', '--num_batches_to_accumulate=2', '--num_batches_per_epoch=8', '--num_epochs=4', '--rollout=32', '--learning_rate=0.0003', '--use_rnn=true', '--rnn_size=64', '--rnn_num_layers=1', '--encoder_mlp_layers', '512', '256', '64', '--gamma=0.98', '--reward_scale=0.1', '--max_grad_norm=1.0', '--async_rl=true', '--normalize_input=true', '--use_env_info_cache=false', '--with_wandb=true', '--wandb_project=gate_navigation_dual_camera', '--wandb_user=ziya-ruso-ucl', '--wandb_group=gate_navigation_training', '--wandb_tags', 'aerial_gym', 'gate_navigation', 'dual_camera', 'x500', 'sample_factory', 'memory_optimized', '--save_every_sec=120', '--save_best_every_sec=5', '--train_for_env_steps=100000000', '--save_gifs=true']
[37m[4652 ms][navigation_task_gate] - INFO : Setting up static camera for gate navigation... (navigation_task_gate.py:473)
[37m[4652 ms][navigation_task_gate] - INFO : Static camera properties: 480x270, FOV: 87.0° (navigation_task_gate.py:492)
[37m[4714 ms][navigation_task_gate] - INFO : ✓ Static camera setup complete (navigation_task_gate.py:509)
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.
  logger.warn(
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.
  logger.warn(
[36m[2025-07-03 17:00:45,670][81936] Env info: EnvInfo(obs_space=Dict('obs': Box(-inf, inf, (145,), float32)), action_space=Box(-1.0, 1.0, (3,), float32), num_agents=16, gpu_actions=True, gpu_observations=True, action_splits=None, all_discrete=None, frameskip=1, reward_shaping_scheme=None, env_info_protocol_version=1)
[33m[2025-07-03 17:00:46,423][81784] In serial mode all components run on the same process. Only use async_rl and serial mode together for debugging.
[36m[2025-07-03 17:00:46,424][81784] Starting experiment with the following configuration:
[36mhelp=False
[36malgo=APPO
[36menv=quad_with_obstacles_gate
[36mexperiment=base_gate_rewards_classic33
[36mtrain_dir=./train_dir
[36mrestart_behavior=resume
[36mdevice=gpu
[36mseed=None
[36mnum_policies=1
[36masync_rl=True
[36mserial_mode=True
[36mbatched_sampling=True
[36mnum_batches_to_accumulate=2
[36mworker_num_splits=1
[36mpolicy_workers_per_policy=1
[36mmax_policy_lag=1000
[36mnum_workers=1
[36mnum_envs_per_worker=1
[36mbatch_size=2048
[36mnum_batches_per_epoch=8
[36mnum_epochs=4
[36mrollout=32
[36mrecurrence=32
[36mshuffle_minibatches=False
[36mgamma=0.98
[36mreward_scale=0.1
[36mreward_clip=1000.0
[36mvalue_bootstrap=True
[36mnormalize_returns=True
[36mexploration_loss_coeff=0.001
[36mvalue_loss_coeff=2.0
[36mkl_loss_coeff=0.1
[36mexploration_loss=entropy
[36mgae_lambda=0.95
[36mppo_clip_ratio=0.2
[36mppo_clip_value=1.0
[36mwith_vtrace=False
[36mvtrace_rho=1.0
[36mvtrace_c=1.0
[36moptimizer=adam
[36madam_eps=1e-06
[36madam_beta1=0.9
[36madam_beta2=0.999
[36mmax_grad_norm=1.0
[36mlearning_rate=0.0003
[36mlr_schedule=kl_adaptive_epoch
[36mlr_schedule_kl_threshold=0.016
[36mlr_adaptive_min=1e-06
[36mlr_adaptive_max=0.01
[36mobs_subtract_mean=0.0
[36mobs_scale=1.0
[36mnormalize_input=True
[36mnormalize_input_keys=None
[36mdecorrelate_experience_max_seconds=0
[36mdecorrelate_envs_on_one_worker=True
[36mactor_worker_gpus=[0]
[36mset_workers_cpu_affinity=True
[36mforce_envs_single_thread=False
[36mdefault_niceness=0
[36mlog_to_file=True
[36mexperiment_summaries_interval=10
[36mflush_summaries_interval=30
[36mstats_avg=100
[36msummaries_use_frameskip=True
[36mheartbeat_interval=20
[36mheartbeat_reporting_interval=180
[36mtrain_for_env_steps=100000000
[36mtrain_for_seconds=10000000000
[36msave_every_sec=120
[36mkeep_checkpoints=5
[36mload_checkpoint_kind=latest
[36msave_milestones_sec=-1
[36msave_best_every_sec=5
[36msave_best_metric=reward
[36msave_best_after=100000
[36mbenchmark=False
[36mencoder_mlp_layers=[512, 256, 64]
[36mencoder_conv_architecture=convnet_simple
[36mencoder_conv_mlp_layers=[]
[36muse_rnn=True
[36mrnn_size=64
[36mrnn_type=gru
[36mrnn_num_layers=1
[36mdecoder_mlp_layers=[]
[36mnonlinearity=elu
[36mpolicy_initialization=torch_default
[36mpolicy_init_gain=1.0
[36mactor_critic_share_weights=True
[36madaptive_stddev=True
[36mcontinuous_tanh_scale=0.0
[36minitial_stddev=1.0
[36muse_env_info_cache=False
[36menv_gpu_actions=True
[36menv_gpu_observations=True
[36menv_frameskip=1
[36menv_framestack=1
[36mpixel_format=CHW
[36muse_record_episode_statistics=False
[36mwith_wandb=True
[36mwandb_user=ziya-ruso-ucl
[36mwandb_project=gate_navigation_dual_camera
[36mwandb_group=gate_navigation_training
[36mwandb_job_type=SF
[36mwandb_tags=['aerial_gym', 'gate_navigation', 'dual_camera', 'x500', 'sample_factory', 'memory_optimized']
[36mwith_pbt=False
[36mpbt_mix_policies_in_one_env=True
[36mpbt_period_env_steps=5000000
[36mpbt_start_mutation=20000000
[36mpbt_replace_fraction=0.3
[36mpbt_mutation_rate=0.15
[36mpbt_replace_reward_gap=0.1
[36mpbt_replace_reward_gap_absolute=1e-06
[36mpbt_optimize_gamma=False
[36mpbt_target_objective=true_objective
[36mpbt_perturb_min=1.1
[36mpbt_perturb_max=1.5
[36menv_agents=16
[36mheadless=False
[36msave_gifs=True
[36mobs_key=observations
[36msubtask=None
[36mige_api_version=preview4
[36meval_stats=False
[36maction_space_dim=3
[36mcommand_line=--env=quad_with_obstacles_gate --experiment=base_gate_rewards_classic33 --train_dir=./train_dir --num_workers=1 --num_envs_per_worker=1 --env_agents=16 --obs_key=observations --batch_size=2048 --num_batches_to_accumulate=2 --num_batches_per_epoch=8 --num_epochs=4 --rollout=32 --learning_rate=0.0003 --use_rnn=true --rnn_size=64 --rnn_num_layers=1 --encoder_mlp_layers 512 256 64 --gamma=0.98 --reward_scale=0.1 --max_grad_norm=1.0 --async_rl=true --normalize_input=true --use_env_info_cache=false --with_wandb=true --wandb_project=gate_navigation_dual_camera --wandb_user=ziya-ruso-ucl --wandb_group=gate_navigation_training --wandb_tags aerial_gym gate_navigation dual_camera x500 sample_factory memory_optimized --save_every_sec=120 --save_best_every_sec=5 --train_for_env_steps=100000000 --headless=false --save_gifs=true
[36mcli_args={'env': 'quad_with_obstacles_gate', 'experiment': 'base_gate_rewards_classic33', 'train_dir': './train_dir', 'async_rl': True, 'num_batches_to_accumulate': 2, 'num_workers': 1, 'num_envs_per_worker': 1, 'batch_size': 2048, 'num_batches_per_epoch': 8, 'num_epochs': 4, 'rollout': 32, 'gamma': 0.98, 'reward_scale': 0.1, 'max_grad_norm': 1.0, 'learning_rate': 0.0003, 'normalize_input': True, 'train_for_env_steps': 100000000, 'save_every_sec': 120, 'save_best_every_sec': 5, 'encoder_mlp_layers': [512, 256, 64], 'use_rnn': True, 'rnn_size': 64, 'rnn_num_layers': 1, 'use_env_info_cache': False, 'with_wandb': True, 'wandb_user': 'ziya-ruso-ucl', 'wandb_project': 'gate_navigation_dual_camera', 'wandb_group': 'gate_navigation_training', 'wandb_tags': ['aerial_gym', 'gate_navigation', 'dual_camera', 'x500', 'sample_factory', 'memory_optimized'], 'env_agents': 16, 'headless': False, 'save_gifs': True, 'obs_key': 'observations'}
[36mgit_hash=a54f99d681da80aa6215176cae93d2948a30ac42
[36mgit_repo_name=git@github.com:rusoziya/aerial_gym_simulator.git
[36mwandb_unique_id=base_gate_rewards_classic33_20250703_170036_977401
[36m[2025-07-03 17:00:46,424][81784] Saving configuration to ./train_dir/base_gate_rewards_classic33/config.json...
[36m[2025-07-03 17:00:46,462][81784] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[36m[2025-07-03 17:00:46,463][81784] Rollout worker 0 uses device cuda:0
[36m[2025-07-03 17:00:46,475][81784] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[37m[1m[2025-07-03 17:00:46,475][81784] InferenceWorker_p0-w0: min num requests: 1
[36m[2025-07-03 17:00:46,476][81784] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[37m[1m[2025-07-03 17:00:46,477][81784] Starting seed is not provided
[36m[2025-07-03 17:00:46,477][81784] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[36m[2025-07-03 17:00:46,477][81784] Initializing actor-critic model on device cuda:0
[36m[2025-07-03 17:00:46,477][81784] RunningMeanStd input shape: (145,)
[36m[2025-07-03 17:00:46,479][81784] RunningMeanStd input shape: (1,)
[36m[2025-07-03 17:00:46,504][81784] Created Actor Critic model with architecture:
[36m[2025-07-03 17:00:46,505][81784] ActorCriticSharedWeights(
[36m  (obs_normalizer): ObservationNormalizer(
[36m    (running_mean_std): RunningMeanStdDictInPlace(
[36m      (running_mean_std): ModuleDict(
[36m        (obs): RunningMeanStdInPlace()
[36m      )
[36m    )
[36m  )
[36m  (returns_normalizer): RecursiveScriptModule(original_name=RunningMeanStdInPlace)
[36m  (encoder): MultiInputEncoder(
[36m    (encoders): ModuleDict(
[36m      (obs): MlpEncoder(
[36m        (mlp_head): RecursiveScriptModule(
[36m          original_name=Sequential
[36m          (0): RecursiveScriptModule(original_name=Linear)
[36m          (1): RecursiveScriptModule(original_name=ELU)
[36m          (2): RecursiveScriptModule(original_name=Linear)
[36m          (3): RecursiveScriptModule(original_name=ELU)
[36m          (4): RecursiveScriptModule(original_name=Linear)
[36m          (5): RecursiveScriptModule(original_name=ELU)
[36m        )
[36m      )
[36m    )
[36m  )
[36m  (core): ModelCoreRNN(
[36m    (core): GRU(64, 64)
[36m  )
[36m  (decoder): MlpDecoder(
[36m    (mlp): Identity()
[36m  )
[36m  (critic_linear): Linear(in_features=64, out_features=1, bias=True)
[36m  (action_parameterization): ActionParameterizationDefault(
[36m    (distribution_linear): Linear(in_features=64, out_features=6, bias=True)
[36m  )
[36m)
[36m[2025-07-03 17:00:46,943][81784] Using optimizer <class 'torch.optim.adam.Adam'>
[33m[2025-07-03 17:00:46,943][81784] No checkpoints found
[36m[2025-07-03 17:00:46,943][81784] Did not load from checkpoint, starting from scratch!
[36m[2025-07-03 17:00:46,943][81784] Initialized policy 0 weights for model version 0
[36m[2025-07-03 17:00:46,944][81784] LearnerWorker_p0 finished initialization!
[36m[2025-07-03 17:00:46,944][81784] Using GPUs [0] for process 0 (actually maps to GPUs [0])
[37m[1m[2025-07-03 17:00:46,951][81784] Inference worker 0-0 is ready!
[37m[1m[2025-07-03 17:00:46,951][81784] All inference workers are ready! Signal rollout workers to start!
[36m[2025-07-03 17:00:46,951][81784] EnvRunner 0-0 uses policy 0
[37m[12147 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task_gate] - INFO : DCE Gate Navigation Task - Using SF_HEADLESS environment variable: False (dce_navigation_task_gate.py:22)
[37m[12148 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task_gate] - INFO : DCE Gate Navigation Task - Final headless mode: False (dce_navigation_task_gate.py:29)
[37m[12148 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task_gate] - INFO : Found SF_ENV_AGENTS environment variable: 16 (dce_navigation_task_gate.py:39)
[37m[12148 ms][aerial_gym.examples.dce_rl_navigation.dce_navigation_task_gate] - INFO : Detected env_agents=16 from environment - setting environment count. (dce_navigation_task_gate.py:45)
[37m[12148 ms][base_task] - INFO : Setting seed: 2582173677 (base_task.py:38)
[37m[12148 ms][navigation_task_gate] - INFO : Building environment for gate navigation task. (navigation_task_gate.py:48)
[37m[12148 ms][navigation_task_gate] - INFO : Sim Name: base_sim, Env Name: gate_env, Robot Name: lmf2, Controller Name: lmf2_position_control (navigation_task_gate.py:49)
[37m[12148 ms][env_manager] - INFO : Populating environments. (env_manager.py:73)
[37m[12148 ms][env_manager] - INFO : Creating simulation instance. (env_manager.py:87)
[37m[12148 ms][env_manager] - INFO : Instantiating IGE object. (env_manager.py:88)
[37m[12148 ms][IsaacGymEnvManager] - INFO : Creating Isaac Gym Environment (IGE_env_manager.py:41)
[37m[12148 ms][IsaacGymEnvManager] - INFO : Acquiring gym object (IGE_env_manager.py:73)
[37m[12148 ms][IsaacGymEnvManager] - INFO : Acquired gym object (IGE_env_manager.py:75)
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:0
GPU Pipeline: enabled
*** Can't create empty tensor
WARNING: allocation matrix is not full rank. Rank: 4
creating render graph
Module warp.utils load on device 'cuda:0' took 1.71 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_camera_kernels load on device 'cuda:0' took 8.79 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_stereo_camera_kernels load on device 'cuda:0' took 12.53 ms
Module aerial_gym.sensors.warp.warp_kernels.warp_lidar_kernels load on device 'cuda:0' took 6.73 ms
finishing capture of render graph
Encoder network initialized.
Defined encoder.
[ImgDecoder] Starting create_model
[ImgDecoder] Done with create_model
Defined decoder.
Loading weights from file:  /home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/utils/vae/weights/ICRA_test_set_more_sim_data_kld_beta_3_LD_64_epoch_49.pth
[AerialGymVecEnv] GIF saving ENABLED for dual cameras (drone + static)
[AerialGymVecEnv] Forced action space shape: (3,)
[AerialGymVecEnv] is_multiagent: True, num_agents: 16
[AerialGymVecEnv] Detected observation space: 145D
[AerialGymVecEnv] Using GATE NAVIGATION configuration (145D = 17D basic + 64D drone VAE + 64D static camera VAE)
[make_aerialgym_env] Final action space shape: (3,)
[make_aerialgym_env] Action space: Box(-1.0, 1.0, (3,), float32)
[37m[12150 ms][IsaacGymEnvManager] - INFO : Fixing devices (IGE_env_manager.py:89)
[37m[12150 ms][IsaacGymEnvManager] - INFO : Using GPU pipeline for simulation. (IGE_env_manager.py:102)
[37m[12150 ms][IsaacGymEnvManager] - INFO : Sim Device type: cuda, Sim Device ID: 0 (IGE_env_manager.py:105)
[37m[12150 ms][IsaacGymEnvManager] - INFO : Graphics Device ID: 0 (IGE_env_manager.py:119)
[37m[12150 ms][IsaacGymEnvManager] - INFO : Creating Isaac Gym Simulation Object (IGE_env_manager.py:120)
[33m[12150 ms][IsaacGymEnvManager] - WARNING : If you have set the CUDA_VISIBLE_DEVICES environment variable, please ensure that you set it
[33mto a particular one that works for your system to use the viewer or Isaac Gym cameras.
[33mIf you want to run parallel simulations on multiple GPUs with camera sensors,
[33mplease disable Isaac Gym and use warp (by setting use_warp=True), set the viewer to headless. (IGE_env_manager.py:127)
[33m[12150 ms][IsaacGymEnvManager] - WARNING : If you see a segfault in the next lines, it is because of the discrepancy between the CUDA device and the graphics device.
[33mPlease ensure that the CUDA device and the graphics device are the same. (IGE_env_manager.py:132)
[37m[13137 ms][IsaacGymEnvManager] - INFO : Created Isaac Gym Simulation Object (IGE_env_manager.py:136)
[37m[13137 ms][IsaacGymEnvManager] - INFO : Created Isaac Gym Environment (IGE_env_manager.py:43)
[37m[13339 ms][env_manager] - INFO : IGE object instantiated. (env_manager.py:109)
[37m[13339 ms][env_manager] - INFO : Creating warp environment. (env_manager.py:112)
[37m[13339 ms][env_manager] - INFO : Warp environment created. (env_manager.py:114)
[37m[13339 ms][env_manager] - INFO : Creating robot manager. (env_manager.py:118)
[37m[13339 ms][BaseRobot] - INFO : [DONE] Initializing controller (base_robot.py:26)
[37m[13339 ms][BaseRobot] - INFO : Initializing controller lmf2_position_control (base_robot.py:29)
[33m[13339 ms][base_multirotor] - WARNING : Creating 16 multirotors. (base_multirotor.py:32)
[37m[13339 ms][env_manager] - INFO : [DONE] Creating robot manager. (env_manager.py:123)
[37m[13339 ms][env_manager] - INFO : [DONE] Creating simulation instance. (env_manager.py:125)
[37m[13339 ms][asset_loader] - INFO : Loading asset: model.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[13340 ms][asset_loader] - INFO : Loading asset: gate.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[13344 ms][asset_loader] - INFO : Loading asset: 1_x_1_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[13345 ms][asset_loader] - INFO : Loading asset: 0_5_x_0_5_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[13346 ms][asset_loader] - INFO : Loading asset: left_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[13347 ms][asset_loader] - INFO : Loading asset: right_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[13348 ms][asset_loader] - INFO : Loading asset: front_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[13349 ms][asset_loader] - INFO : Loading asset: back_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[13350 ms][asset_loader] - INFO : Loading asset: bottom_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[13351 ms][asset_loader] - INFO : Loading asset: top_wall.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[13352 ms][asset_loader] - INFO : Loading asset: cuboidal_rod.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[13353 ms][asset_loader] - INFO : Loading asset: small_cube.urdf for the first time. Next use of this asset will be via the asset buffer. (asset_loader.py:71)
[37m[13354 ms][env_manager] - INFO : Populating environment 0 (env_manager.py:179)
[33m[13371 ms][robot_manager] - WARNING : 
[33mRobot mass: 1.2400000467896461,
[33mInertia: tensor([[0.0134, 0.0000, 0.0000],
[33m        [0.0000, 0.0144, 0.0000],
[33m        [0.0000, 0.0000, 0.0138]], device='cuda:0'),
[33mRobot COM: tensor([[0., 0., 0., 1.]], device='cuda:0') (robot_manager.py:437)
[33m[13371 ms][robot_manager] - WARNING : Calculated robot mass and inertia for this robot. This code assumes that your robot is the same across environments. (robot_manager.py:440)
[31m[13371 ms][robot_manager] - CRITICAL : If your robot differs across environments you need to perform this computation for each different robot here. (robot_manager.py:443)
[37m[13391 ms][env_manager] - INFO : [DONE] Populating environments. (env_manager.py:75)
[33m[13400 ms][IsaacGymEnvManager] - WARNING : Headless: False (IGE_env_manager.py:424)
[37m[13400 ms][IsaacGymEnvManager] - INFO : Creating viewer (IGE_env_manager.py:426)
[33m[13478 ms][IGE_viewer_control] - WARNING : Instructions for using the viewer with the keyboard:
[33mESC: Quit
[33mV: Toggle Viewer Sync
[33mS: Sync Frame Time
[33mF: Toggle Camera Follow
[33mP: Toggle Camera Follow Type
[33mR: Reset All Environments
[33mUP: Switch Target Environment Up
[33mDOWN: Switch Target Environment Down
[33mSPACE: Pause Simulation
[33m (IGE_viewer_control.py:153)
[37m[13478 ms][IsaacGymEnvManager] - INFO : Created viewer (IGE_env_manager.py:432)
[33m[13509 ms][asset_manager] - WARNING : Number of obstacles to be kept in the environment: 10 (asset_manager.py:32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/control/motor_model.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(self.min_thrust, device=self.device, dtype=torch.float32).expand(
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/control/motor_model.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  torch.tensor(self.max_thrust, device=self.device, dtype=torch.float32).expand(
[33m[13703 ms][control_allocation] - WARNING : Control allocation does not account for actuator limits. This leads to suboptimal allocation (control_allocation.py:48)
[37m[13704 ms][WarpSensor] - INFO : Camera sensor initialized (warp_sensor.py:50)
[37m[14321 ms][navigation_task_gate] - INFO : Setting up static camera for gate navigation... (navigation_task_gate.py:473)
[37m[14321 ms][navigation_task_gate] - INFO : Static camera properties: 480x270, FOV: 87.0° (navigation_task_gate.py:492)
[37m[14384 ms][navigation_task_gate] - INFO : ✓ Static camera setup complete (navigation_task_gate.py:509)
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.
  logger.warn(
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: [33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.
  logger.warn(
[GIF] Episode 0 terminated - saving GIFs (every 100 episodes)
[GIF] Saved drone depth: ./gif_episodes/episode_0000_drone_depth.gif
[GIF] Saved drone segmentation: ./gif_episodes/episode_0000_drone_seg.gif
[GIF] Saved static depth: ./gif_episodes/episode_0000_static_depth.gif
[GIF] Saved static segmentation: ./gif_episodes/episode_0000_static_seg.gif
[GIF] Saved merged dual camera: ./gif_episodes/episode_0000_merged_dual_camera.gif
[36m[2025-07-03 17:00:52,205][81784] Fps is (10 sec: nan, 60 sec: nan, 300 sec: nan). Total num frames: 0. Throughput: 0: nan. Samples: 0. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 17:00:52,205][81784] Avg episode reward: [(0, '-100.000')]
[33m[18612 ms][IGE_viewer_control] - WARNING : Camera follow: True (IGE_viewer_control.py:217)
[36m[2025-07-03 17:00:55,012][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 17.1. Samples: 48. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 17:00:55,012][81784] Avg episode reward: [(0, '-111.578')]
[36m[2025-07-03 17:00:59,988][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 86.3. Samples: 672. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 17:00:59,988][81784] Avg episode reward: [(0, '-94.739')]
[36m[2025-07-03 17:01:04,999][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 95.0. Samples: 1216. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 17:01:04,999][81784] Avg episode reward: [(0, '-86.906')]
[37m[1m[2025-07-03 17:01:06,596][81784] Heartbeat connected on Batcher_0
[37m[1m[2025-07-03 17:01:06,596][81784] Heartbeat connected on LearnerWorker_p0
[37m[1m[2025-07-03 17:01:06,596][81784] Heartbeat connected on InferenceWorker_p0-w0
[37m[1m[2025-07-03 17:01:06,596][81784] Heartbeat connected on RolloutWorker_w0
[36m[2025-07-03 17:01:09,989][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 143.9. Samples: 2560. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 17:01:09,990][81784] Avg episode reward: [(0, '-85.962')]
[36m[2025-07-03 17:01:15,036][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 168.2. Samples: 3840. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 17:01:15,037][81784] Avg episode reward: [(0, '-86.353')]
[36m[2025-07-03 17:01:19,997][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 161.8. Samples: 4496. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 17:01:19,997][81784] Avg episode reward: [(0, '-86.296')]
[36m[2025-07-03 17:01:25,012][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 179.0. Samples: 5872. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 17:01:25,012][81784] Avg episode reward: [(0, '-85.244')]
[36m[2025-07-03 17:01:30,005][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 192.6. Samples: 7280. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 17:01:30,006][81784] Avg episode reward: [(0, '-85.926')]
[36m[2025-07-03 17:01:35,010][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 201.1. Samples: 8608. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 17:01:35,011][81784] Avg episode reward: [(0, '-87.284')]
[33m[64185 ms][navigation_task_gate] - WARNING : Gate Navigation Curriculum Level: 3, Progress: 0.0 (navigation_task_gate.py:420)
[33m[64185 ms][navigation_task_gate] - WARNING : 
[33mSuccess Rate: 0.0
[33mCrash Rate: 1.0
[33mTimeout Rate: 0.0 (navigation_task_gate.py:423)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/task/navigation_task_gate/navigation_task_gate.py:430: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.infos["curriculum/success_rate"] = torch.tensor(success_rate, dtype=torch.float32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/task/navigation_task_gate/navigation_task_gate.py:431: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.infos["curriculum/crash_rate"] = torch.tensor(crash_rate, dtype=torch.float32)
/home/ziyar/aerialgym/aerialgym_ws/src/aerial_gym_simulator/aerial_gym/task/navigation_task_gate/navigation_task_gate.py:432: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.infos["curriculum/timeout_rate"] = torch.tensor(timeout_rate, dtype=torch.float32)
[36m[2025-07-03 17:01:40,016][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 206.9. Samples: 9360. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 17:01:40,016][81784] Avg episode reward: [(0, '-86.010')]
[36m[2025-07-03 17:01:45,028][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 221.7. Samples: 10656. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 17:01:45,028][81784] Avg episode reward: [(0, '-84.856')]
[36m[2025-07-03 17:01:50,034][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 239.5. Samples: 12000. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 17:01:50,035][81784] Avg episode reward: [(0, '-86.298')]
[36m[2025-07-03 17:01:55,014][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 223.5. Samples: 12624. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 17:01:55,015][81784] Avg episode reward: [(0, '-85.120')]
[36m[2025-07-03 17:02:00,025][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 224.8. Samples: 13952. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 17:02:00,025][81784] Avg episode reward: [(0, '-85.753')]
[GIF] Episode 100 terminated - saving GIFs (every 100 episodes)
[GIF] Saved drone depth: ./gif_episodes/episode_0001_drone_depth.gif
[GIF] Saved drone segmentation: ./gif_episodes/episode_0001_drone_seg.gif
[GIF] Saved static depth: ./gif_episodes/episode_0001_static_depth.gif
[GIF] Saved static segmentation: ./gif_episodes/episode_0001_static_seg.gif
[GIF] Saved merged dual camera: ./gif_episodes/episode_0001_merged_dual_camera.gif
[36m[2025-07-03 17:02:05,020][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 0.0). Total num frames: 0. Throughput: 0: 240.9. Samples: 15344. Policy #0 lag: (min: -1.0, avg: -1.0, max: -1.0)
[36m[2025-07-03 17:02:05,020][81784] Avg episode reward: [(0, '-87.089')]
/home/ziyar/miniforge3/envs/aerialgym/lib/python3.8/site-packages/torch/nn/modules/module.py:1194: UserWarning: operator() profile_node %104 : int[] = prim::profile_ivalue(%102)
 does not have profile information (Triggered internally at /opt/conda/conda-bld/pytorch_1670525541702/work/torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)
  return forward_call(*input, **kwargs)
[36m[2025-07-03 17:02:10,551][81784] Fps is (10 sec: 1556.5, 60 sec: 270.5, 300 sec: 209.1). Total num frames: 16384. Throughput: 0: 222.4. Samples: 16000. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 17:02:10,552][81784] Avg episode reward: [(0, '-87.295')]
[36m[2025-07-03 17:02:15,034][81784] Fps is (10 sec: 1636.0, 60 sec: 273.1, 300 sec: 197.8). Total num frames: 16384. Throughput: 0: 219.2. Samples: 17152. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 17:02:15,035][81784] Avg episode reward: [(0, '-85.759')]
[36m[2025-07-03 17:02:20,010][81784] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 186.6). Total num frames: 16384. Throughput: 0: 203.7. Samples: 17776. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 17:02:20,010][81784] Avg episode reward: [(0, '-86.002')]
[33m[108998 ms][navigation_task_gate] - WARNING : Gate Navigation Curriculum Level: 3, Progress: 0.0 (navigation_task_gate.py:420)
[33m[108998 ms][navigation_task_gate] - WARNING : 
[33mSuccess Rate: 0.0
[33mCrash Rate: 1.0
[33mTimeout Rate: 0.0 (navigation_task_gate.py:423)
[36m[2025-07-03 17:02:25,041][81784] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 176.5). Total num frames: 16384. Throughput: 0: 213.6. Samples: 18976. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 17:02:25,041][81784] Avg episode reward: [(0, '-86.163')]
[36m[2025-07-03 17:02:29,976][81784] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 167.6). Total num frames: 16384. Throughput: 0: 213.9. Samples: 20272. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 17:02:29,976][81784] Avg episode reward: [(0, '-86.055')]
[36m[2025-07-03 17:02:35,038][81784] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 159.3). Total num frames: 16384. Throughput: 0: 212.6. Samples: 21568. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 17:02:35,039][81784] Avg episode reward: [(0, '-85.325')]
[36m[2025-07-03 17:02:40,022][81784] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 152.0). Total num frames: 16384. Throughput: 0: 214.0. Samples: 22256. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 17:02:40,022][81784] Avg episode reward: [(0, '-85.959')]
[37m[1m[2025-07-03 17:02:40,089][81784] Saving ./train_dir/base_gate_rewards_classic33/checkpoint_p0/checkpoint_000000032_16384.pth...
[36m[2025-07-03 17:02:44,997][81784] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 145.3). Total num frames: 16384. Throughput: 0: 213.5. Samples: 23552. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 17:02:44,997][81784] Avg episode reward: [(0, '-86.573')]
[36m[2025-07-03 17:02:50,031][81784] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 139.1). Total num frames: 16384. Throughput: 0: 196.2. Samples: 24176. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 17:02:50,031][81784] Avg episode reward: [(0, '-85.500')]
[36m[2025-07-03 17:02:55,037][81784] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 133.4). Total num frames: 16384. Throughput: 0: 214.0. Samples: 25520. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 17:02:55,038][81784] Avg episode reward: [(0, '-85.662')]
[36m[2025-07-03 17:03:00,075][81784] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 128.1). Total num frames: 16384. Throughput: 0: 216.7. Samples: 26912. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 17:03:00,076][81784] Avg episode reward: [(0, '-84.569')]
[36m[2025-07-03 17:03:05,100][81784] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 123.3). Total num frames: 16384. Throughput: 0: 222.5. Samples: 27808. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 17:03:05,100][81784] Avg episode reward: [(0, '-86.624')]
[36m[2025-07-03 17:03:10,023][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 118.9). Total num frames: 16384. Throughput: 0: 206.3. Samples: 28256. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 17:03:10,023][81784] Avg episode reward: [(0, '-86.561')]
[33m[157545 ms][navigation_task_gate] - WARNING : Gate Navigation Curriculum Level: 3, Progress: 0.0 (navigation_task_gate.py:420)
[33m[157546 ms][navigation_task_gate] - WARNING : 
[33mSuccess Rate: 0.0
[33mCrash Rate: 1.0
[33mTimeout Rate: 0.0 (navigation_task_gate.py:423)
[36m[2025-07-03 17:03:15,057][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 114.7). Total num frames: 16384. Throughput: 0: 195.9. Samples: 29104. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 17:03:15,057][81784] Avg episode reward: [(0, '-86.891')]
[GIF] Episode 200 terminated - saving GIFs (every 100 episodes)
[GIF] Saved drone depth: ./gif_episodes/episode_0002_drone_depth.gif
[GIF] Saved drone segmentation: ./gif_episodes/episode_0002_drone_seg.gif
[GIF] Saved static depth: ./gif_episodes/episode_0002_static_depth.gif
[GIF] Saved static segmentation: ./gif_episodes/episode_0002_static_seg.gif
[GIF] Saved merged dual camera: ./gif_episodes/episode_0002_merged_dual_camera.gif
[36m[2025-07-03 17:03:19,978][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 110.9). Total num frames: 16384. Throughput: 0: 177.3. Samples: 29536. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 17:03:19,979][81784] Avg episode reward: [(0, '-85.940')]
[36m[2025-07-03 17:03:25,042][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 107.2). Total num frames: 16384. Throughput: 0: 179.1. Samples: 30320. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 17:03:25,043][81784] Avg episode reward: [(0, '-86.068')]
[36m[2025-07-03 17:03:30,070][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 103.8). Total num frames: 16384. Throughput: 0: 167.9. Samples: 31120. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 17:03:30,070][81784] Avg episode reward: [(0, '-84.509')]
[36m[2025-07-03 17:03:35,079][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 100.6). Total num frames: 16384. Throughput: 0: 172.6. Samples: 31952. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 17:03:35,080][81784] Avg episode reward: [(0, '-85.391')]
[36m[2025-07-03 17:03:40,028][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 97.6). Total num frames: 16384. Throughput: 0: 152.2. Samples: 32368. Policy #0 lag: (min: 15.0, avg: 15.0, max: 15.0)
[36m[2025-07-03 17:03:40,029][81784] Avg episode reward: [(0, '-85.365')]
[36m[2025-07-03 17:03:45,029][81784] Fps is (10 sec: 1646.7, 60 sec: 272.9, 300 sec: 189.6). Total num frames: 32768. Throughput: 0: 137.7. Samples: 33104. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-03 17:03:45,029][81784] Avg episode reward: [(0, '-85.906')]
[36m[2025-07-03 17:03:49,983][81784] Fps is (10 sec: 1645.8, 60 sec: 273.3, 300 sec: 184.3). Total num frames: 32768. Throughput: 0: 131.2. Samples: 33696. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-03 17:03:49,983][81784] Avg episode reward: [(0, '-85.955')]
[36m[2025-07-03 17:03:54,982][81784] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 179.3). Total num frames: 32768. Throughput: 0: 148.0. Samples: 34912. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-03 17:03:54,982][81784] Avg episode reward: [(0, '-85.773')]
[36m[2025-07-03 17:03:59,976][81784] Fps is (10 sec: 0.0, 60 sec: 273.5, 300 sec: 174.5). Total num frames: 32768. Throughput: 0: 155.3. Samples: 36080. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-03 17:03:59,977][81784] Avg episode reward: [(0, '-85.255')]
[36m[2025-07-03 17:04:04,991][81784] Fps is (10 sec: 0.0, 60 sec: 273.6, 300 sec: 170.0). Total num frames: 32768. Throughput: 0: 158.9. Samples: 36688. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-03 17:04:04,991][81784] Avg episode reward: [(0, '-85.354')]
[36m[2025-07-03 17:04:10,027][81784] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 165.6). Total num frames: 32768. Throughput: 0: 168.2. Samples: 37888. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-03 17:04:10,028][81784] Avg episode reward: [(0, '-85.470')]
[33m[216777 ms][navigation_task_gate] - WARNING : Gate Navigation Curriculum Level: 3, Progress: 0.0 (navigation_task_gate.py:420)
[33m[216777 ms][navigation_task_gate] - WARNING : 
[33mSuccess Rate: 0.0
[33mCrash Rate: 1.0
[33mTimeout Rate: 0.0 (navigation_task_gate.py:423)
[36m[2025-07-03 17:04:14,980][81784] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 161.6). Total num frames: 32768. Throughput: 0: 179.6. Samples: 39184. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-03 17:04:14,980][81784] Avg episode reward: [(0, '-84.318')]
[36m[2025-07-03 17:04:20,024][81784] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 157.7). Total num frames: 32768. Throughput: 0: 175.5. Samples: 39840. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-03 17:04:20,024][81784] Avg episode reward: [(0, '-85.184')]
[36m[2025-07-03 17:04:25,046][81784] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 154.0). Total num frames: 32768. Throughput: 0: 189.4. Samples: 40896. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-03 17:04:25,047][81784] Avg episode reward: [(0, '-85.277')]
[36m[2025-07-03 17:04:30,017][81784] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 150.4). Total num frames: 32768. Throughput: 0: 191.7. Samples: 41728. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-03 17:04:30,017][81784] Avg episode reward: [(0, '-84.835')]
[36m[2025-07-03 17:04:35,011][81784] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 147.1). Total num frames: 32768. Throughput: 0: 188.3. Samples: 42176. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-03 17:04:35,012][81784] Avg episode reward: [(0, '-86.323')]
[36m[2025-07-03 17:04:40,011][81784] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 143.8). Total num frames: 32768. Throughput: 0: 179.4. Samples: 42992. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-03 17:04:40,011][81784] Avg episode reward: [(0, '-85.453')]
[37m[1m[2025-07-03 17:04:40,136][81784] Saving ./train_dir/base_gate_rewards_classic33/checkpoint_p0/checkpoint_000000064_32768.pth...
[36m[2025-07-03 17:04:45,053][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 140.7). Total num frames: 32768. Throughput: 0: 172.9. Samples: 43872. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-03 17:04:45,054][81784] Avg episode reward: [(0, '-86.086')]
[36m[2025-07-03 17:04:50,052][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 137.8). Total num frames: 32768. Throughput: 0: 178.6. Samples: 44736. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-03 17:04:50,052][81784] Avg episode reward: [(0, '-85.822')]
[GIF] Episode 300 terminated - saving GIFs (every 100 episodes)
[GIF] Saved drone depth: ./gif_episodes/episode_0003_drone_depth.gif
[GIF] Saved drone segmentation: ./gif_episodes/episode_0003_drone_seg.gif
[GIF] Saved static depth: ./gif_episodes/episode_0003_static_depth.gif
[GIF] Saved static segmentation: ./gif_episodes/episode_0003_static_seg.gif
[GIF] Saved merged dual camera: ./gif_episodes/episode_0003_merged_dual_camera.gif
[36m[2025-07-03 17:04:55,024][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 134.9). Total num frames: 32768. Throughput: 0: 162.1. Samples: 45184. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-03 17:04:55,025][81784] Avg episode reward: [(0, '-85.393')]
[36m[2025-07-03 17:05:00,049][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 132.2). Total num frames: 32768. Throughput: 0: 152.3. Samples: 46048. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-03 17:05:00,050][81784] Avg episode reward: [(0, '-85.288')]
[36m[2025-07-03 17:05:04,991][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 129.6). Total num frames: 32768. Throughput: 0: 148.4. Samples: 46512. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-03 17:05:04,991][81784] Avg episode reward: [(0, '-84.771')]
[36m[2025-07-03 17:05:10,073][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 127.1). Total num frames: 32768. Throughput: 0: 152.4. Samples: 47760. Policy #0 lag: (min: 16.0, avg: 16.2, max: 48.0)
[36m[2025-07-03 17:05:10,074][81784] Avg episode reward: [(0, '-85.693')]
[33m[276533 ms][navigation_task_gate] - WARNING : Gate Navigation Curriculum Level: 3, Progress: 0.0 (navigation_task_gate.py:420)
[33m[276534 ms][navigation_task_gate] - WARNING : 
[33mSuccess Rate: 0.0
[33mCrash Rate: 1.0
[33mTimeout Rate: 0.0 (navigation_task_gate.py:423)
[36m[2025-07-03 17:05:15,187][81784] Fps is (10 sec: 1606.9, 60 sec: 272.1, 300 sec: 186.9). Total num frames: 49152. Throughput: 0: 161.5. Samples: 49024. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:05:15,187][81784] Avg episode reward: [(0, '-85.486')]
[36m[2025-07-03 17:05:20,046][81784] Fps is (10 sec: 1642.9, 60 sec: 273.0, 300 sec: 183.5). Total num frames: 49152. Throughput: 0: 162.0. Samples: 49472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:05:20,047][81784] Avg episode reward: [(0, '-85.905')]
[36m[2025-07-03 17:05:24,988][81784] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 180.2). Total num frames: 49152. Throughput: 0: 162.9. Samples: 50320. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:05:24,988][81784] Avg episode reward: [(0, '-86.011')]
[36m[2025-07-03 17:05:30,084][81784] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 176.9). Total num frames: 49152. Throughput: 0: 162.7. Samples: 51200. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:05:30,085][81784] Avg episode reward: [(0, '-85.238')]
[36m[2025-07-03 17:05:35,085][81784] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 173.8). Total num frames: 49152. Throughput: 0: 153.1. Samples: 51632. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:05:35,086][81784] Avg episode reward: [(0, '-85.722')]
[36m[2025-07-03 17:05:40,021][81784] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 170.8). Total num frames: 49152. Throughput: 0: 161.8. Samples: 52464. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:05:40,022][81784] Avg episode reward: [(0, '-84.971')]
[36m[2025-07-03 17:05:45,080][81784] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 167.8). Total num frames: 49152. Throughput: 0: 161.3. Samples: 53312. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:05:45,081][81784] Avg episode reward: [(0, '-85.212')]
[36m[2025-07-03 17:05:50,011][81784] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 166.6). Total num frames: 49152. Throughput: 0: 160.3. Samples: 53728. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:05:50,012][81784] Avg episode reward: [(0, '-85.762')]
[36m[2025-07-03 17:05:55,022][81784] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 49152. Throughput: 0: 152.4. Samples: 54608. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:05:55,023][81784] Avg episode reward: [(0, '-85.821')]
[36m[2025-07-03 17:05:59,997][81784] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 166.6). Total num frames: 49152. Throughput: 0: 143.9. Samples: 55472. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:05:59,997][81784] Avg episode reward: [(0, '-85.633')]
[36m[2025-07-03 17:06:05,050][81784] Fps is (10 sec: 0.0, 60 sec: 272.8, 300 sec: 166.6). Total num frames: 49152. Throughput: 0: 142.6. Samples: 55888. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:06:05,051][81784] Avg episode reward: [(0, '-85.644')]
[36m[2025-07-03 17:06:10,057][81784] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 49152. Throughput: 0: 142.4. Samples: 56736. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:06:10,057][81784] Avg episode reward: [(0, '-85.757')]
[36m[2025-07-03 17:06:15,057][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 49152. Throughput: 0: 141.2. Samples: 57552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:06:15,057][81784] Avg episode reward: [(0, '-83.912')]
[33m[344219 ms][navigation_task_gate] - WARNING : Gate Navigation Curriculum Level: 3, Progress: 0.0 (navigation_task_gate.py:420)
[33m[344219 ms][navigation_task_gate] - WARNING : 
[33mSuccess Rate: 0.0
[33mCrash Rate: 1.0
[33mTimeout Rate: 0.0 (navigation_task_gate.py:423)
[36m[2025-07-03 17:06:20,077][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 49152. Throughput: 0: 156.8. Samples: 58688. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:06:20,077][81784] Avg episode reward: [(0, '-85.706')]
[36m[2025-07-03 17:06:25,014][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 49152. Throughput: 0: 150.4. Samples: 59232. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:06:25,014][81784] Avg episode reward: [(0, '-87.010')]
[36m[2025-07-03 17:06:29,981][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 49152. Throughput: 0: 158.2. Samples: 60416. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:06:29,981][81784] Avg episode reward: [(0, '-85.909')]
[GIF] Episode 400 terminated - saving GIFs (every 100 episodes)
[GIF] Saved drone depth: ./gif_episodes/episode_0004_drone_depth.gif
[GIF] Saved drone segmentation: ./gif_episodes/episode_0004_drone_seg.gif
[GIF] Saved static depth: ./gif_episodes/episode_0004_static_depth.gif
[GIF] Saved static segmentation: ./gif_episodes/episode_0004_static_seg.gif
[GIF] Saved merged dual camera: ./gif_episodes/episode_0004_merged_dual_camera.gif
[36m[2025-07-03 17:06:34,998][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 49152. Throughput: 0: 161.5. Samples: 60992. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:06:34,999][81784] Avg episode reward: [(0, '-84.124')]
[36m[2025-07-03 17:06:40,001][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 49152. Throughput: 0: 170.0. Samples: 62256. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:06:40,001][81784] Avg episode reward: [(0, '-85.325')]
[37m[1m[2025-07-03 17:06:40,084][81784] Saving ./train_dir/base_gate_rewards_classic33/checkpoint_p0/checkpoint_000000096_49152.pth...
[36m[2025-07-03 17:06:45,014][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 49152. Throughput: 0: 180.9. Samples: 63616. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:06:45,014][81784] Avg episode reward: [(0, '-86.035')]
[36m[2025-07-03 17:06:50,034][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 49152. Throughput: 0: 200.2. Samples: 64896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:06:50,035][81784] Avg episode reward: [(0, '-85.885')]
[36m[2025-07-03 17:06:54,993][81784] Fps is (10 sec: 1641.8, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 65536. Throughput: 0: 196.5. Samples: 65568. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-03 17:06:54,993][81784] Avg episode reward: [(0, '-85.904')]
[36m[2025-07-03 17:07:00,009][81784] Fps is (10 sec: 1642.6, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 65536. Throughput: 0: 207.9. Samples: 66896. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-03 17:07:00,009][81784] Avg episode reward: [(0, '-85.648')]
[36m[2025-07-03 17:07:05,031][81784] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 166.9). Total num frames: 65536. Throughput: 0: 212.1. Samples: 68224. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-03 17:07:05,031][81784] Avg episode reward: [(0, '-85.480')]
[33m[391782 ms][navigation_task_gate] - WARNING : Gate Navigation Curriculum Level: 3, Progress: 0.0 (navigation_task_gate.py:420)
[33m[391782 ms][navigation_task_gate] - WARNING : 
[33mSuccess Rate: 0.0
[33mCrash Rate: 1.0
[33mTimeout Rate: 0.0 (navigation_task_gate.py:423)
[36m[2025-07-03 17:07:09,993][81784] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 166.6). Total num frames: 65536. Throughput: 0: 215.2. Samples: 68912. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-03 17:07:09,993][81784] Avg episode reward: [(0, '-86.404')]
[36m[2025-07-03 17:07:15,019][81784] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 166.6). Total num frames: 65536. Throughput: 0: 219.2. Samples: 70288. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-03 17:07:15,019][81784] Avg episode reward: [(0, '-85.848')]
[36m[2025-07-03 17:07:19,996][81784] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 166.6). Total num frames: 65536. Throughput: 0: 220.8. Samples: 70928. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-03 17:07:19,996][81784] Avg episode reward: [(0, '-87.156')]
[36m[2025-07-03 17:07:25,017][81784] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 65536. Throughput: 0: 221.4. Samples: 72224. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-03 17:07:25,018][81784] Avg episode reward: [(0, '-84.746')]
[36m[2025-07-03 17:07:30,009][81784] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 166.6). Total num frames: 65536. Throughput: 0: 217.6. Samples: 73408. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-03 17:07:30,009][81784] Avg episode reward: [(0, '-85.283')]
[36m[2025-07-03 17:07:35,035][81784] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 166.6). Total num frames: 65536. Throughput: 0: 204.4. Samples: 74096. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-03 17:07:35,036][81784] Avg episode reward: [(0, '-86.488')]
[36m[2025-07-03 17:07:40,023][81784] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 65536. Throughput: 0: 218.2. Samples: 75392. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-03 17:07:40,023][81784] Avg episode reward: [(0, '-84.933')]
[GIF] Episode 500 terminated - saving GIFs (every 100 episodes)
[GIF] Saved drone depth: ./gif_episodes/episode_0005_drone_depth.gif
[GIF] Saved drone segmentation: ./gif_episodes/episode_0005_drone_seg.gif
[GIF] Saved static depth: ./gif_episodes/episode_0005_static_depth.gif
[GIF] Saved static segmentation: ./gif_episodes/episode_0005_static_seg.gif
[GIF] Saved merged dual camera: ./gif_episodes/episode_0005_merged_dual_camera.gif
[36m[2025-07-03 17:07:45,029][81784] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 65536. Throughput: 0: 215.4. Samples: 76592. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-03 17:07:45,029][81784] Avg episode reward: [(0, '-85.867')]
[36m[2025-07-03 17:07:50,052][81784] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 65536. Throughput: 0: 199.7. Samples: 77216. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-03 17:07:50,053][81784] Avg episode reward: [(0, '-85.597')]
[33m[440109 ms][navigation_task_gate] - WARNING : Gate Navigation Curriculum Level: 3, Progress: 0.0 (navigation_task_gate.py:420)
[33m[440109 ms][navigation_task_gate] - WARNING : 
[33mSuccess Rate: 0.0
[33mCrash Rate: 1.0
[33mTimeout Rate: 0.0 (navigation_task_gate.py:423)
[36m[2025-07-03 17:07:55,015][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.7). Total num frames: 65536. Throughput: 0: 210.7. Samples: 78400. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-03 17:07:55,015][81784] Avg episode reward: [(0, '-85.514')]
[36m[2025-07-03 17:08:00,026][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.7). Total num frames: 65536. Throughput: 0: 208.7. Samples: 79680. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-03 17:08:00,026][81784] Avg episode reward: [(0, '-85.908')]
[36m[2025-07-03 17:08:05,020][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 65536. Throughput: 0: 221.0. Samples: 80880. Policy #0 lag: (min: 28.0, avg: 28.0, max: 28.0)
[36m[2025-07-03 17:08:05,021][81784] Avg episode reward: [(0, '-84.801')]
[36m[2025-07-03 17:08:10,005][81784] Fps is (10 sec: 1641.7, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 81920. Throughput: 0: 207.0. Samples: 81536. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-03 17:08:10,006][81784] Avg episode reward: [(0, '-86.473')]
[36m[2025-07-03 17:08:15,018][81784] Fps is (10 sec: 1638.8, 60 sec: 273.1, 300 sec: 222.1). Total num frames: 81920. Throughput: 0: 209.4. Samples: 82832. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-03 17:08:15,018][81784] Avg episode reward: [(0, '-86.436')]
[36m[2025-07-03 17:08:20,033][81784] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.2). Total num frames: 81920. Throughput: 0: 207.3. Samples: 83424. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-03 17:08:20,033][81784] Avg episode reward: [(0, '-86.081')]
[36m[2025-07-03 17:08:25,043][81784] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 81920. Throughput: 0: 198.0. Samples: 84304. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-03 17:08:25,043][81784] Avg episode reward: [(0, '-84.232')]
[36m[2025-07-03 17:08:30,096][81784] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 222.1). Total num frames: 81920. Throughput: 0: 189.6. Samples: 85136. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-03 17:08:30,096][81784] Avg episode reward: [(0, '-86.022')]
[36m[2025-07-03 17:08:35,007][81784] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 81920. Throughput: 0: 185.8. Samples: 85568. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-03 17:08:35,007][81784] Avg episode reward: [(0, '-85.471')]
[36m[2025-07-03 17:08:40,007][81784] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 81920. Throughput: 0: 183.5. Samples: 86656. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-03 17:08:40,008][81784] Avg episode reward: [(0, '-84.849')]
[37m[1m[2025-07-03 17:08:40,072][81784] Saving ./train_dir/base_gate_rewards_classic33/checkpoint_p0/checkpoint_000000160_81920.pth...
[36m[2025-07-03 17:08:45,034][81784] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 81920. Throughput: 0: 179.9. Samples: 87776. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-03 17:08:45,034][81784] Avg episode reward: [(0, '-84.950')]
[36m[2025-07-03 17:08:49,992][81784] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 166.6). Total num frames: 81920. Throughput: 0: 164.0. Samples: 88256. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-03 17:08:49,992][81784] Avg episode reward: [(0, '-84.992')]
[33m[496753 ms][navigation_task_gate] - WARNING : Gate Navigation Curriculum Level: 3, Progress: 0.0 (navigation_task_gate.py:420)
[33m[496754 ms][navigation_task_gate] - WARNING : 
[33mSuccess Rate: 0.0
[33mCrash Rate: 1.0
[33mTimeout Rate: 0.0 (navigation_task_gate.py:423)
[36m[2025-07-03 17:08:55,019][81784] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 81920. Throughput: 0: 175.2. Samples: 89424. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-03 17:08:55,019][81784] Avg episode reward: [(0, '-85.683')]
[36m[2025-07-03 17:08:59,981][81784] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 166.6). Total num frames: 81920. Throughput: 0: 168.3. Samples: 90400. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-03 17:08:59,982][81784] Avg episode reward: [(0, '-85.465')]
[36m[2025-07-03 17:09:05,038][81784] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 81920. Throughput: 0: 167.1. Samples: 90944. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-03 17:09:05,039][81784] Avg episode reward: [(0, '-84.766')]
[36m[2025-07-03 17:09:10,017][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 81920. Throughput: 0: 175.0. Samples: 92176. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-03 17:09:10,017][81784] Avg episode reward: [(0, '-85.755')]
[GIF] Episode 600 terminated - saving GIFs (every 100 episodes)
[GIF] Saved drone depth: ./gif_episodes/episode_0006_drone_depth.gif
[GIF] Saved drone segmentation: ./gif_episodes/episode_0006_drone_seg.gif
[GIF] Saved static depth: ./gif_episodes/episode_0006_static_depth.gif
[GIF] Saved static segmentation: ./gif_episodes/episode_0006_static_seg.gif
[GIF] Saved merged dual camera: ./gif_episodes/episode_0006_merged_dual_camera.gif
[36m[2025-07-03 17:09:15,039][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 81920. Throughput: 0: 176.6. Samples: 93072. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-03 17:09:15,039][81784] Avg episode reward: [(0, '-86.151')]
[36m[2025-07-03 17:09:19,987][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.7). Total num frames: 81920. Throughput: 0: 176.8. Samples: 93520. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-03 17:09:19,988][81784] Avg episode reward: [(0, '-84.981')]
[36m[2025-07-03 17:09:25,008][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 81920. Throughput: 0: 176.0. Samples: 94576. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-03 17:09:25,008][81784] Avg episode reward: [(0, '-84.998')]
[36m[2025-07-03 17:09:29,996][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 81920. Throughput: 0: 182.6. Samples: 95984. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-03 17:09:29,996][81784] Avg episode reward: [(0, '-83.801')]
[36m[2025-07-03 17:09:35,025][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 81920. Throughput: 0: 184.4. Samples: 96560. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-03 17:09:35,026][81784] Avg episode reward: [(0, '-84.384')]
[36m[2025-07-03 17:09:39,981][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.7). Total num frames: 81920. Throughput: 0: 180.8. Samples: 97552. Policy #0 lag: (min: 30.0, avg: 30.0, max: 30.0)
[36m[2025-07-03 17:09:39,981][81784] Avg episode reward: [(0, '-84.545')]
[36m[2025-07-03 17:09:45,002][81784] Fps is (10 sec: 1642.2, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 98304. Throughput: 0: 187.6. Samples: 98848. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-03 17:09:45,003][81784] Avg episode reward: [(0, '-86.160')]
[33m[553039 ms][navigation_task_gate] - WARNING : Gate Navigation Curriculum Level: 3, Progress: 0.0 (navigation_task_gate.py:420)
[33m[553040 ms][navigation_task_gate] - WARNING : 
[33mSuccess Rate: 0.0
[33mCrash Rate: 1.0
[33mTimeout Rate: 0.0 (navigation_task_gate.py:423)
[36m[2025-07-03 17:09:50,033][81784] Fps is (10 sec: 1629.8, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 98304. Throughput: 0: 204.5. Samples: 100144. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-03 17:09:50,034][81784] Avg episode reward: [(0, '-84.912')]
[36m[2025-07-03 17:09:55,022][81784] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 98304. Throughput: 0: 191.3. Samples: 100784. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-03 17:09:55,022][81784] Avg episode reward: [(0, '-85.584')]
[36m[2025-07-03 17:10:00,023][81784] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 98304. Throughput: 0: 201.0. Samples: 102112. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-03 17:10:00,024][81784] Avg episode reward: [(0, '-85.507')]
[36m[2025-07-03 17:10:04,988][81784] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 222.2). Total num frames: 98304. Throughput: 0: 206.2. Samples: 102800. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-03 17:10:04,988][81784] Avg episode reward: [(0, '-85.193')]
[36m[2025-07-03 17:10:10,054][81784] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 166.7). Total num frames: 98304. Throughput: 0: 210.6. Samples: 104064. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-03 17:10:10,054][81784] Avg episode reward: [(0, '-85.553')]
[36m[2025-07-03 17:10:14,981][81784] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 166.7). Total num frames: 98304. Throughput: 0: 203.1. Samples: 105120. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-03 17:10:14,981][81784] Avg episode reward: [(0, '-85.913')]
[36m[2025-07-03 17:10:20,012][81784] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 98304. Throughput: 0: 215.9. Samples: 106272. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-03 17:10:20,013][81784] Avg episode reward: [(0, '-85.592')]
[36m[2025-07-03 17:10:24,987][81784] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 166.7). Total num frames: 98304. Throughput: 0: 210.5. Samples: 107024. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-03 17:10:24,987][81784] Avg episode reward: [(0, '-86.086')]
[36m[2025-07-03 17:10:30,027][81784] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 166.6). Total num frames: 98304. Throughput: 0: 200.8. Samples: 107888. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-03 17:10:30,028][81784] Avg episode reward: [(0, '-85.146')]
[36m[2025-07-03 17:10:35,034][81784] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 98304. Throughput: 0: 182.4. Samples: 108352. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-03 17:10:35,035][81784] Avg episode reward: [(0, '-83.107')]
[GIF] Episode 700 terminated - saving GIFs (every 100 episodes)
[GIF] Saved drone depth: ./gif_episodes/episode_0007_drone_depth.gif
[GIF] Saved drone segmentation: ./gif_episodes/episode_0007_drone_seg.gif
[GIF] Saved static depth: ./gif_episodes/episode_0007_static_depth.gif
[GIF] Saved static segmentation: ./gif_episodes/episode_0007_static_seg.gif
[GIF] Saved merged dual camera: ./gif_episodes/episode_0007_merged_dual_camera.gif
[36m[2025-07-03 17:10:40,056][81784] Fps is (10 sec: 0.0, 60 sec: 272.7, 300 sec: 166.6). Total num frames: 98304. Throughput: 0: 186.9. Samples: 109200. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-03 17:10:40,057][81784] Avg episode reward: [(0, '-84.726')]
[37m[1m[2025-07-03 17:10:40,171][81784] Saving ./train_dir/base_gate_rewards_classic33/checkpoint_p0/checkpoint_000000192_98304.pth...
[36m[2025-07-03 17:10:45,019][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 98304. Throughput: 0: 178.2. Samples: 110128. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-03 17:10:45,019][81784] Avg episode reward: [(0, '-86.122')]
[33m[610385 ms][navigation_task_gate] - WARNING : Gate Navigation Curriculum Level: 3, Progress: 0.0 (navigation_task_gate.py:420)
[33m[610385 ms][navigation_task_gate] - WARNING : 
[33mSuccess Rate: 0.0
[33mCrash Rate: 1.0
[33mTimeout Rate: 0.0 (navigation_task_gate.py:423)
[36m[2025-07-03 17:10:50,034][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 98304. Throughput: 0: 192.2. Samples: 111456. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-03 17:10:50,034][81784] Avg episode reward: [(0, '-85.828')]
[36m[2025-07-03 17:10:55,086][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 98304. Throughput: 0: 180.8. Samples: 112208. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-03 17:10:55,087][81784] Avg episode reward: [(0, '-84.594')]
[36m[2025-07-03 17:11:00,015][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 98304. Throughput: 0: 181.2. Samples: 113280. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-03 17:11:00,015][81784] Avg episode reward: [(0, '-85.622')]
[36m[2025-07-03 17:11:05,012][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 98304. Throughput: 0: 170.7. Samples: 113952. Policy #0 lag: (min: 2.0, avg: 2.0, max: 2.0)
[36m[2025-07-03 17:11:05,013][81784] Avg episode reward: [(0, '-84.952')]
[36m[2025-07-03 17:11:10,023][81784] Fps is (10 sec: 1637.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 114688. Throughput: 0: 174.8. Samples: 114896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:11:10,023][81784] Avg episode reward: [(0, '-85.550')]
[37m[1m[2025-07-03 17:11:10,094][81784] Saving new best policy, reward=-85.550!
[36m[2025-07-03 17:11:15,049][81784] Fps is (10 sec: 1632.3, 60 sec: 272.8, 300 sec: 222.2). Total num frames: 114688. Throughput: 0: 182.3. Samples: 116096. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:11:15,050][81784] Avg episode reward: [(0, '-83.428')]
[37m[1m[2025-07-03 17:11:15,130][81784] Saving new best policy, reward=-83.428!
[36m[2025-07-03 17:11:19,977][81784] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 114688. Throughput: 0: 184.4. Samples: 116640. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:11:19,977][81784] Avg episode reward: [(0, '-84.464')]
[36m[2025-07-03 17:11:25,001][81784] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 114688. Throughput: 0: 195.4. Samples: 117984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:11:25,001][81784] Avg episode reward: [(0, '-84.514')]
[36m[2025-07-03 17:11:30,002][81784] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 114688. Throughput: 0: 194.9. Samples: 118896. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:11:30,002][81784] Avg episode reward: [(0, '-83.978')]
[36m[2025-07-03 17:11:34,994][81784] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 114688. Throughput: 0: 179.4. Samples: 119520. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:11:34,995][81784] Avg episode reward: [(0, '-86.416')]
[36m[2025-07-03 17:11:40,044][81784] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.1). Total num frames: 114688. Throughput: 0: 191.8. Samples: 120832. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:11:40,044][81784] Avg episode reward: [(0, '-82.964')]
[37m[1m[2025-07-03 17:11:40,110][81784] Saving new best policy, reward=-82.964!
[33m[665785 ms][navigation_task_gate] - WARNING : Gate Navigation Curriculum Level: 3, Progress: 0.0 (navigation_task_gate.py:420)
[33m[665786 ms][navigation_task_gate] - WARNING : 
[33mSuccess Rate: 0.0
[33mCrash Rate: 1.0
[33mTimeout Rate: 0.0 (navigation_task_gate.py:423)
[36m[2025-07-03 17:11:45,043][81784] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.1). Total num frames: 114688. Throughput: 0: 195.4. Samples: 122080. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:11:45,043][81784] Avg episode reward: [(0, '-85.097')]
[36m[2025-07-03 17:11:50,026][81784] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 114688. Throughput: 0: 208.6. Samples: 123344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:11:50,026][81784] Avg episode reward: [(0, '-85.117')]
[36m[2025-07-03 17:11:55,004][81784] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 166.6). Total num frames: 114688. Throughput: 0: 202.0. Samples: 123984. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:11:55,005][81784] Avg episode reward: [(0, '-84.875')]
[36m[2025-07-03 17:11:59,994][81784] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 166.6). Total num frames: 114688. Throughput: 0: 206.1. Samples: 125360. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:11:59,994][81784] Avg episode reward: [(0, '-85.264')]
[GIF] Episode 800 terminated - saving GIFs (every 100 episodes)
[GIF] Saved drone depth: ./gif_episodes/episode_0008_drone_depth.gif
[GIF] Saved drone segmentation: ./gif_episodes/episode_0008_drone_seg.gif
[GIF] Saved static depth: ./gif_episodes/episode_0008_static_depth.gif
[GIF] Saved static segmentation: ./gif_episodes/episode_0008_static_seg.gif
[GIF] Saved merged dual camera: ./gif_episodes/episode_0008_merged_dual_camera.gif
[36m[2025-07-03 17:12:04,995][81784] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 114688. Throughput: 0: 209.0. Samples: 126048. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:12:04,995][81784] Avg episode reward: [(0, '-84.837')]
[36m[2025-07-03 17:12:10,015][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 114688. Throughput: 0: 207.9. Samples: 127344. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:12:10,015][81784] Avg episode reward: [(0, '-83.855')]
[36m[2025-07-03 17:12:15,105][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 114688. Throughput: 0: 212.5. Samples: 128480. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:12:15,105][81784] Avg episode reward: [(0, '-85.320')]
[36m[2025-07-03 17:12:20,063][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 114688. Throughput: 0: 216.6. Samples: 129280. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:12:20,063][81784] Avg episode reward: [(0, '-85.778')]
[36m[2025-07-03 17:12:25,013][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 114688. Throughput: 0: 196.8. Samples: 129680. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:12:25,013][81784] Avg episode reward: [(0, '-85.203')]
[36m[2025-07-03 17:12:30,041][81784] Fps is (10 sec: 1642.0, 60 sec: 272.9, 300 sec: 222.2). Total num frames: 131072. Throughput: 0: 195.6. Samples: 130880. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 17:12:30,041][81784] Avg episode reward: [(0, '-85.368')]
[36m[2025-07-03 17:12:35,066][81784] Fps is (10 sec: 1629.7, 60 sec: 272.7, 300 sec: 222.1). Total num frames: 131072. Throughput: 0: 193.2. Samples: 132048. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 17:12:35,067][81784] Avg episode reward: [(0, '-85.343')]
[33m[723618 ms][navigation_task_gate] - WARNING : Gate Navigation Curriculum Level: 3, Progress: 0.0 (navigation_task_gate.py:420)
[33m[723618 ms][navigation_task_gate] - WARNING : 
[33mSuccess Rate: 0.0
[33mCrash Rate: 1.0
[33mTimeout Rate: 0.0 (navigation_task_gate.py:423)
[36m[2025-07-03 17:12:39,977][81784] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 222.2). Total num frames: 131072. Throughput: 0: 188.9. Samples: 132480. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 17:12:39,977][81784] Avg episode reward: [(0, '-84.181')]
[37m[1m[2025-07-03 17:12:40,048][81784] Saving ./train_dir/base_gate_rewards_classic33/checkpoint_p0/checkpoint_000000256_131072.pth...
[36m[2025-07-03 17:12:40,052][81784] Removing ./train_dir/base_gate_rewards_classic33/checkpoint_p0/checkpoint_000000032_16384.pth
[36m[2025-07-03 17:12:45,053][81784] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 131072. Throughput: 0: 184.6. Samples: 133680. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 17:12:45,053][81784] Avg episode reward: [(0, '-83.297')]
[36m[2025-07-03 17:12:50,008][81784] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 131072. Throughput: 0: 183.4. Samples: 134304. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 17:12:50,008][81784] Avg episode reward: [(0, '-84.443')]
[36m[2025-07-03 17:12:54,978][81784] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 131072. Throughput: 0: 181.8. Samples: 135520. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 17:12:54,979][81784] Avg episode reward: [(0, '-83.551')]
[36m[2025-07-03 17:13:00,002][81784] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 131072. Throughput: 0: 183.9. Samples: 136736. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 17:13:00,002][81784] Avg episode reward: [(0, '-84.283')]
[36m[2025-07-03 17:13:05,015][81784] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 166.6). Total num frames: 131072. Throughput: 0: 197.5. Samples: 138160. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 17:13:05,015][81784] Avg episode reward: [(0, '-82.832')]
[37m[1m[2025-07-03 17:13:05,077][81784] Saving new best policy, reward=-82.832!
[36m[2025-07-03 17:13:10,015][81784] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 166.6). Total num frames: 131072. Throughput: 0: 203.7. Samples: 138848. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 17:13:10,015][81784] Avg episode reward: [(0, '-83.778')]
[36m[2025-07-03 17:13:15,033][81784] Fps is (10 sec: 0.0, 60 sec: 273.4, 300 sec: 166.6). Total num frames: 131072. Throughput: 0: 205.5. Samples: 140128. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 17:13:15,034][81784] Avg episode reward: [(0, '-84.539')]
[36m[2025-07-03 17:13:20,016][81784] Fps is (10 sec: 0.0, 60 sec: 273.3, 300 sec: 166.6). Total num frames: 131072. Throughput: 0: 210.0. Samples: 141488. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 17:13:20,017][81784] Avg episode reward: [(0, '-84.722')]
[36m[2025-07-03 17:13:24,982][81784] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 166.7). Total num frames: 131072. Throughput: 0: 215.4. Samples: 142176. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 17:13:24,983][81784] Avg episode reward: [(0, '-85.394')]
[GIF] Episode 900 terminated - saving GIFs (every 100 episodes)
[GIF] Saved drone depth: ./gif_episodes/episode_0009_drone_depth.gif
[GIF] Saved drone segmentation: ./gif_episodes/episode_0009_drone_seg.gif
[GIF] Saved static depth: ./gif_episodes/episode_0009_static_depth.gif
[GIF] Saved static segmentation: ./gif_episodes/episode_0009_static_seg.gif
[GIF] Saved merged dual camera: ./gif_episodes/episode_0009_merged_dual_camera.gif
[36m[2025-07-03 17:13:29,998][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 131072. Throughput: 0: 218.9. Samples: 143520. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 17:13:29,998][81784] Avg episode reward: [(0, '-84.382')]
[33m[777285 ms][navigation_task_gate] - WARNING : Gate Navigation Curriculum Level: 3, Progress: 0.0 (navigation_task_gate.py:420)
[33m[777285 ms][navigation_task_gate] - WARNING : 
[33mSuccess Rate: 0.0
[33mCrash Rate: 1.0
[33mTimeout Rate: 0.0 (navigation_task_gate.py:423)
[36m[2025-07-03 17:13:35,046][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 131072. Throughput: 0: 233.4. Samples: 144816. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 17:13:35,046][81784] Avg episode reward: [(0, '-83.397')]
[36m[2025-07-03 17:13:39,995][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 131072. Throughput: 0: 221.8. Samples: 145504. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 17:13:39,995][81784] Avg episode reward: [(0, '-83.956')]
[36m[2025-07-03 17:13:44,979][81784] Fps is (10 sec: 0.0, 60 sec: 0.0, 300 sec: 166.6). Total num frames: 131072. Throughput: 0: 225.5. Samples: 146880. Policy #0 lag: (min: 26.0, avg: 26.0, max: 26.0)
[36m[2025-07-03 17:13:44,979][81784] Avg episode reward: [(0, '-84.126')]
[36m[2025-07-03 17:13:49,976][81784] Fps is (10 sec: 1641.5, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 147456. Throughput: 0: 208.9. Samples: 147552. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:13:49,976][81784] Avg episode reward: [(0, '-84.363')]
[36m[2025-07-03 17:13:55,014][81784] Fps is (10 sec: 1632.6, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 147456. Throughput: 0: 224.0. Samples: 148928. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:13:55,014][81784] Avg episode reward: [(0, '-84.570')]
[36m[2025-07-03 17:14:00,013][81784] Fps is (10 sec: 0.0, 60 sec: 273.0, 300 sec: 222.2). Total num frames: 147456. Throughput: 0: 224.5. Samples: 150224. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:14:00,013][81784] Avg episode reward: [(0, '-83.903')]
[36m[2025-07-03 17:14:05,006][81784] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 147456. Throughput: 0: 209.5. Samples: 150912. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:14:05,006][81784] Avg episode reward: [(0, '-84.900')]
[36m[2025-07-03 17:14:10,015][81784] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.2). Total num frames: 147456. Throughput: 0: 222.8. Samples: 152208. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:14:10,015][81784] Avg episode reward: [(0, '-83.254')]
[36m[2025-07-03 17:14:15,026][81784] Fps is (10 sec: 0.0, 60 sec: 273.1, 300 sec: 222.1). Total num frames: 147456. Throughput: 0: 223.9. Samples: 153600. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:14:15,026][81784] Avg episode reward: [(0, '-83.832')]
[36m[2025-07-03 17:14:19,982][81784] Fps is (10 sec: 0.0, 60 sec: 273.2, 300 sec: 222.2). Total num frames: 147456. Throughput: 0: 210.8. Samples: 154288. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:14:19,983][81784] Avg episode reward: [(0, '-80.904')]
[37m[1m[2025-07-03 17:14:20,067][81784] Saving new best policy, reward=-80.904!
[36m[2025-07-03 17:14:25,011][81784] Fps is (10 sec: 0.0, 60 sec: 272.9, 300 sec: 222.1). Total num frames: 147456. Throughput: 0: 223.9. Samples: 155584. Policy #0 lag: (min: 31.0, avg: 31.0, max: 31.0)
[36m[2025-07-03 17:14:25,011][81784] Avg episode reward: [(0, '-84.075')]
[33m[830392 ms][navigation_task_gate] - WARNING : Gate Navigation Curriculum Level: 3, Progress: 0.0 (navigation_task_gate.py:420)
[33m[830392 ms][navigation_task_gate] - WARNING : 
[33mSuccess Rate: 0.0
[33mCrash Rate: 1.0
